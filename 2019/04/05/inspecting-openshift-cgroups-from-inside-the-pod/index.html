<!doctype html><html lang=en-us><head><meta http-equiv=x-clacks-overhead content="GNU Terry Pratchett"><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" href=https://major.io/favicon.ico><title>Inspecting OpenShift cgroups from inside the pod | Major Hayden's Blog ðŸ¤ </title><meta name=title content="Inspecting OpenShift cgroups from inside the pod"><meta name=description content="My team at Red Hat builds a lot of kernels in OpenShift pods as part of our work with the Continuous Kernel Integration (CKI) project. We have lots of different pod sizes depending on the type of work we are doing and our GitLab runners spawn these pods based on the tags in our GitLab CI pipeline.
Compiling with make When you compile a large software project, such as the Linux kernel, you can use multiple CPU cores to speed up the build."><meta name=keywords content="openshift,ansible,security,linux,"><meta property="og:title" content="Inspecting OpenShift cgroups from inside the pod"><meta property="og:description" content="My team at Red Hat builds a lot of kernels in OpenShift pods as part of our work with the Continuous Kernel Integration (CKI) project. We have lots of different pod sizes depending on the type of work we are doing and our GitLab runners spawn these pods based on the tags in our GitLab CI pipeline.
Compiling with make When you compile a large software project, such as the Linux kernel, you can use multiple CPU cores to speed up the build."><meta property="og:type" content="article"><meta property="og:url" content="https://major.io/2019/04/05/inspecting-openshift-cgroups-from-inside-the-pod/"><meta property="og:image" content="https://major.io/images/2019-04-05-inspecting-cgroups.jpg"><meta property="article:section" content="posts"><meta property="article:published_time" content="2019-04-05T00:00:00+00:00"><meta property="article:modified_time" content="2019-04-05T00:00:00+00:00"><meta property="og:site_name" content="Major Hayden's Blog ðŸ¤ "><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://major.io/images/2019-04-05-inspecting-cgroups.jpg"><meta name=twitter:title content="Inspecting OpenShift cgroups from inside the pod"><meta name=twitter:description content="My team at Red Hat builds a lot of kernels in OpenShift pods as part of our work with the Continuous Kernel Integration (CKI) project. We have lots of different pod sizes depending on the type of work we are doing and our GitLab runners spawn these pods based on the tags in our GitLab CI pipeline.
Compiling with make When you compile a large software project, such as the Linux kernel, you can use multiple CPU cores to speed up the build."><meta name=twitter:site content="@mhayden"><meta itemprop=name content="Inspecting OpenShift cgroups from inside the pod"><meta itemprop=description content="My team at Red Hat builds a lot of kernels in OpenShift pods as part of our work with the Continuous Kernel Integration (CKI) project. We have lots of different pod sizes depending on the type of work we are doing and our GitLab runners spawn these pods based on the tags in our GitLab CI pipeline.
Compiling with make When you compile a large software project, such as the Linux kernel, you can use multiple CPU cores to speed up the build."><meta itemprop=datePublished content="2019-04-05T00:00:00+00:00"><meta itemprop=dateModified content="2019-04-05T00:00:00+00:00"><meta itemprop=wordCount content="1051"><meta itemprop=image content="https://major.io/images/2019-04-05-inspecting-cgroups.jpg"><meta itemprop=keywords content="openshift,ansible,security,linux,"><meta name=referrer content="no-referrer-when-downgrade"><style>body{font-family:Verdana,sans-serif;margin:auto;padding:20px;max-width:820px;text-align:left;background-color:#fff;word-wrap:break-word;overflow-wrap:break-word;line-height:1.5;color:#444}h1,h2,h3,h4,h5,h6,strong,b{color:#222}a{color:#3273dc}.title{text-decoration:none;border:0}.title span{font-weight:400}nav a{margin-right:10px}textarea{width:100%;font-size:16px}input{font-size:16px}content{line-height:1.6}table{width:100%}img{max-width:100%}code{padding:2px 5px;background-color:#f2f2f2}pre code{color:#222;display:block;padding:20px;white-space:pre-wrap;font-size:14px}div.highlight code{background-color:unset;color:initial}blockquote{border-left:1px solid #999;color:#222;padding-left:20px;font-style:italic}footer{padding:25px;text-align:center}.helptext{color:#777;font-size:small}.errorlist{color:#eba613;font-size:small}ul.blog-posts{list-style-type:none;padding:unset}ul.blog-posts li{display:flex}ul.blog-posts li span{flex:0 0 130px}ul.blog-posts li a:visited{color:#8b6fcb}@media(prefers-color-scheme:dark){body{background-color:#333;color:#ddd}h1,h2,h3,h4,h5,h6,strong,b{color:#eee}a{color:#8cc2dd}code{background-color:#777}pre code{color:#ddd}blockquote{color:#ccc}textarea,input{background-color:#252525;color:#ddd}.helptext{color:#aaa}}</style></head><body><header><a href=../../../../ class=title><h2>Major Hayden's Blog ðŸ¤ </h2></a><nav><a href=../../../../>Home</a>
<a href=../../../../ham-radio-faq>Ham Radio</a>
<a href=../../../../icanhazip-com-faq>icanhazip FAQ</a>
<a href=../../../../posts>Posts</a></nav></header><main><h1>Inspecting OpenShift cgroups from inside the pod</h1><i><time datetime=2019-04-05 pubdate>2019-04-05</time></i>
<content><p><img src=../../../../images/2019-04-05-inspecting-cgroups.jpg alt=walking_through_rock_valley></p><p>My team at Red Hat builds a lot of kernels in OpenShift pods as part of our
work with the <a href=https://cki-project.org/>Continuous Kernel Integration (CKI)</a> project. We have lots of
different pod sizes depending on the type of work we are doing and our GitLab
runners spawn these pods based on the tags in our GitLab CI pipeline.</p><h2 id=compiling-with-make>Compiling with <code>make</code></h2><p>When you compile a large software project, such as the Linux kernel, you can
use multiple CPU cores to speed up the build. GNU&rsquo;s <code>make</code> does this with the
<code>-j</code> argument. Running <code>make</code> with <code>-j10</code> means that you want to run 10 jobs
while compiling. This would keep 10 CPU cores busy.</p><p>Setting the number too high causes more contention from the CPU and can
reduce performance. Setting the number too low means that you are spending
more time compiling than you would if you used all of your CPU cores.</p><p>Every once in a while, we adjusted our runners to use a different amount of
CPUs or memory and then we had to adjust our pipeline to reflect the new CPU
count. This was time consuming and error prone.</p><p>Many people just use <code>nproc</code> to determine the CPU core count. It works well
with make:</p><pre><code>make -j$(nproc)
</code></pre><h2 id=problems-with-containers>Problems with containers</h2><p>The handy <code>nproc</code> doesn&rsquo;t work well for OpenShift. If you start a pod on
OpenShift and limit it to a single CPU core, <code>nproc</code> tells you something very
wrong:</p><pre><code>$ nproc
32
</code></pre><p>We applied the single CPU limit with OpenShift, so what&rsquo;s the problem? The
issue is how <code>nproc</code> looks for CPUs. Here&rsquo;s a snippet of <code>strace</code> output:</p><pre><code>sched_getaffinity(0, 128, [0, 1, 2, 3, 4, 5]) = 8
fstat(1, {st_mode=S_IFCHR|0620, st_rdev=makedev(0x88, 0x6), ...}) = 0
write(1, &quot;6\n&quot;, 26
)                      = 2
</code></pre><p>The <a href=https://linux.die.net/man/2/sched_getaffinity>sched_getaffinity</a> syscall looks to see which CPUs are allowed to run
the process and returns a count of those. OpenShift doesn&rsquo;t prevent us from
seeing the CPUs of the underlying system (the VM or bare metal host
underneath our containers), but it uses cgroups to limit how much CPU time we
can use.</p><h2 id=reading-cgroups>Reading cgroups</h2><p>Getting cgroup data is easy! Just change into the <code>/sys/fs/cgroup/</code> directory
and look around:</p><pre><code>$ cd /sys/fs/cgroup/
$ ls -al cpu/
ls: cannot open directory 'cpu/': Permission denied
</code></pre><p><strong>Ouch.</strong> OpenShift makes this a little more challenging. We&rsquo;re not allowed to
wander around in the land of cgroups without a map to exactly what we want.</p><p>My Fedora workstation shows a bunch of CPU cgroup settings:</p><pre><code>$ ls -al /sys/fs/cgroup/cpu/
total 0
dr-xr-xr-x.  2 root root   0 Apr  5 01:40 .
drwxr-xr-x. 14 root root 360 Apr  5 01:40 ..
-rw-r--r--.  1 root root   0 Apr  5 13:08 cgroup.clone_children
-rw-r--r--.  1 root root   0 Apr  5 01:40 cgroup.procs
-r--r--r--.  1 root root   0 Apr  5 13:08 cgroup.sane_behavior
-r--r--r--.  1 root root   0 Apr  5 13:08 cpuacct.stat
-rw-r--r--.  1 root root   0 Apr  5 13:08 cpuacct.usage
-r--r--r--.  1 root root   0 Apr  5 13:08 cpuacct.usage_all
-r--r--r--.  1 root root   0 Apr  5 13:08 cpuacct.usage_percpu
-r--r--r--.  1 root root   0 Apr  5 13:08 cpuacct.usage_percpu_sys
-r--r--r--.  1 root root   0 Apr  5 13:08 cpuacct.usage_percpu_user
-r--r--r--.  1 root root   0 Apr  5 13:08 cpuacct.usage_sys
-r--r--r--.  1 root root   0 Apr  5 13:08 cpuacct.usage_user
-rw-r--r--.  1 root root   0 Apr  5 09:10 cpu.cfs_period_us
-rw-r--r--.  1 root root   0 Apr  5 13:08 cpu.cfs_quota_us
-rw-r--r--.  1 root root   0 Apr  5 09:10 cpu.shares
-r--r--r--.  1 root root   0 Apr  5 13:08 cpu.stat
-rw-r--r--.  1 root root   0 Apr  5 13:08 notify_on_release
-rw-r--r--.  1 root root   0 Apr  5 13:08 release_agent
-rw-r--r--.  1 root root   0 Apr  5 13:08 tasks
</code></pre><p>OpenShift uses the <a href=https://en.wikipedia.org/wiki/Completely_Fair_Scheduler>Completely Fair Scheduler (CFS)</a> to limit CPU time. Here&rsquo;s a quick excerpt from the <a href=https://www.kernel.org/doc/Documentation/scheduler/sched-bwc.txt>kernel documentation</a>:</p><blockquote><p>Quota and period are managed within the cpu subsystem via cgroupfs.</p><p>cpu.cfs_quota_us: the total available run-time within a period (in microseconds)
cpu.cfs_period_us: the length of a period (in microseconds)
cpu.stat: exports throttling statistics [explained further below]</p><p>The default values are:
cpu.cfs_period_us=100ms
cpu.cfs_quota=-1</p><p>A value of -1 for cpu.cfs_quota_us indicates that the group does not have any
bandwidth restriction in place, such a group is described as an unconstrained
bandwidth group. This represents the traditional work-conserving behavior for
CFS.</p><p>Writing any (valid) positive value(s) will enact the specified bandwidth limit.
The minimum quota allowed for the quota or period is 1ms. There is also an
upper bound on the period length of 1s. Additional restrictions exist when
bandwidth limits are used in a hierarchical fashion, these are explained in
more detail below.</p><p>Writing any negative value to cpu.cfs_quota_us will remove the bandwidth limit
and return the group to an unconstrained state once more.</p><p>Any updates to a group&rsquo;s bandwidth specification will result in it becoming
unthrottled if it is in a constrained state.</p></blockquote><p>Let&rsquo;s see if inspecting <code>cpu.cfs_quota_us</code> can help us:</p><pre><code>$ cat /sys/fs/cgroup/cpu/cpu.cfs_quota_us
10000
</code></pre><p>Now we&rsquo;re getting somewhere. But what does <em>10000</em> mean here? OpenShift
operates on the concept of <em>millicores</em> of CPU time, or 1/1000 of a CPU. 500
millicores is half a CPU and 1000 millicores is a whole CPU.</p><p>The pod in this example is assigned 100 millicores. Now we know that we can
take the output of <code>/sys/fs/cgroup/cpu/cpu.cfs_quota_us</code>, divide by 100, and
get our millicores.</p><p>We can make a script like this:</p><div class=highlight><pre style=color:#586e75;background-color:#eee8d5;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#268bd2>CFS_QUOTA</span>=<span style=color:#859900>$(</span>cat /sys/fs/cgroup/cpu/cpu.cfs_quota_us<span style=color:#859900>)</span>
<span style=color:#859900>if</span> [ <span style=color:#268bd2>$CFS_QUOTA</span> -lt <span style=color:#2aa198;font-weight:700>100000</span> ]; <span style=color:#859900>then</span>
  <span style=color:#268bd2>CPUS_AVAILABLE</span>=<span style=color:#2aa198;font-weight:700>1</span>
<span style=color:#859900>else</span>
  <span style=color:#268bd2>CPUS_AVAILABLE</span>=<span style=color:#859900>$(</span>expr <span style=color:#2aa198>${</span><span style=color:#268bd2>CFS_QUOTA</span><span style=color:#2aa198>}</span> / <span style=color:#2aa198;font-weight:700>100</span> / 1000<span style=color:#859900>)</span>
<span style=color:#859900>fi</span>
<span style=color:#cb4b16>echo</span> <span style=color:#2aa198>&#34;Found </span><span style=color:#2aa198>${</span><span style=color:#268bd2>CPUS_AVAILABLE</span><span style=color:#2aa198>}</span><span style=color:#2aa198> CPUS&#34;</span>
make -j<span style=color:#2aa198>${</span><span style=color:#268bd2>CPUS_AVAILABLE</span><span style=color:#2aa198>}</span> ...
</code></pre></div><p>The script checks for the value of the quota and divides by 100,000 to get
the number of cores. If the share is set to something less than 100,000, then
a core count of 1 is assigned. <em>(Pro tip: <code>make</code> does not like being told to
compile with zero jobs.)</em></p><h2 id=reading-memory-limits>Reading memory limits</h2><p>There are other limits you can read and inspect in a pod, including the
available RAM. As we found with <code>nproc</code>, <code>free</code> is not very helpful:</p><div class=highlight><pre style=color:#586e75;background-color:#eee8d5;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#93a1a1;font-style:italic># An OpenShift pod with 200MB RAM</span>
$ free -m
              total        used        free      shared  buff/cache   available
Mem:          <span style=color:#2aa198;font-weight:700>32008</span>       <span style=color:#2aa198;font-weight:700>12322</span>         <span style=color:#2aa198;font-weight:700>880</span>          <span style=color:#2aa198;font-weight:700>31</span>       <span style=color:#2aa198;font-weight:700>18805</span>       <span style=color:#2aa198;font-weight:700>19246</span>
Swap:             <span style=color:#2aa198;font-weight:700>0</span>           <span style=color:#2aa198;font-weight:700>0</span>           <span style=color:#2aa198;font-weight:700>0</span>
</code></pre></div><p>But the cgroups tell the truth:</p><pre><code>$ cat /sys/fs/cgroup/memory/memory.limit_in_bytes
209715200
</code></pre><p>If you run Java applications in a container, like Jenkins (or Jenkins
slaves), be sure to use the <code>-XX:+UseCGroupMemoryLimitForHeap</code> option. That
will cause Java to look at the cgroups to determine its heap size.</p><p><em>Photo credit: <a href=https://commons.wikimedia.org/wiki/File:Roca_de_la_Ley,_Parque_Nacional_de_%C3%9Eingvellir,_Su%C3%B0urland,_Islandia,_2014-08-16,_DD_022.JPG>Wikipedia</a></em></p></content><p><a href=https://major.io/tags/openshift/>#openshift</a>
<a href=https://major.io/tags/ansible/>#ansible</a>
<a href=https://major.io/tags/security/>#security</a>
<a href=https://major.io/tags/linux/>#linux</a></p></main><footer><script async src=https://media.ethicalads.io/media/client/ethicalads.min.js></script><div data-ea-publisher=major-io data-ea-type=text></div><hr>CC-BY-SA 4.0 &#187;
Made with <a href=https://gohugo.io/>Hugo</a> &#187;
Theme based on <a href=https://github.com/janraasch/hugo-bearblog/>Hugo Ê•â€¢á´¥â€¢Ê” Bear</a><br></footer></body></html>