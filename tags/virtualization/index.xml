<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>virtualization on Major Hayden's Blog ðŸ¤ </title><link>https://major.io/tags/virtualization/</link><description>Recent content in virtualization on Major Hayden's Blog ðŸ¤ </description><generator>Hugo -- gohugo.io</generator><lastBuildDate>Fri, 24 Feb 2017 16:06:24 +0000</lastBuildDate><atom:link href="https://major.io/tags/virtualization/index.xml" rel="self" type="application/rss+xml"/><item><title>OpenStack isnâ€™t dead. Itâ€™s boring. Thatâ€™s a good thing.</title><link>https://major.io/2017/02/24/openstack-isnt-dead-its-boring-thats-a-good-thing/</link><pubDate>Fri, 24 Feb 2017 16:06:24 +0000</pubDate><guid>https://major.io/2017/02/24/openstack-isnt-dead-its-boring-thats-a-good-thing/</guid><description>NOTE: The opinions shared in this post are mine alone and are not related to my employer in any way.
The first OpenStack Project Teams Gathering (PTG) event was held this week in Atlanta. The week was broken into two parts: cross-project work on Monday and Tuesday, and individual projects Wednesday through Friday. I was there for the first two days and heard a few discussions that started the same way.</description></item><item><title>OpenStack instances come online with multiple network ports attached</title><link>https://major.io/2016/08/03/openstack-instances-come-online-with-multiple-network-ports-attached/</link><pubDate>Wed, 03 Aug 2016 14:40:16 +0000</pubDate><guid>https://major.io/2016/08/03/openstack-instances-come-online-with-multiple-network-ports-attached/</guid><description>I ran into an interesting problem recently in my production OpenStack deployment that runs the Mitaka release. On various occasions, instances were coming online with multiple network ports attached, even though I only asked for one network port.
The problem If I issued a build request for ten instances, I&amp;rsquo;d usually end up with this:
6 instances with one network port attached 2-3 instances with two network ports attached (not what I want) 1-2 instances with three or four network ports attached (definitely not what I want) When I examined the instances with multiple network ports attached, I found that one of the network ports would be marked as up while the others would be marked as down.</description></item><item><title>Talk Recap: Automated security hardening with OpenStack-Ansible</title><link>https://major.io/2016/04/26/talk-recap-automated-security-hardening-openstack-ansible/</link><pubDate>Tue, 26 Apr 2016 21:19:02 +0000</pubDate><guid>https://major.io/2016/04/26/talk-recap-automated-security-hardening-openstack-ansible/</guid><description>Today is the second day of the OpenStack Summit in Austin and I offered up a talk on host security hardening in OpenStack clouds. You can download the slides or watch the video here:
Here&amp;rsquo;s a quick recap of the talk and the conversations afterward:
Security tug-of-war Information security is a challenging task, mainly because it is more than just a technical problem. Technology is a big part of it, but communication, culture, and compromise are also critical.</description></item><item><title>systemd-networkd and macvlan interfaces</title><link>https://major.io/2015/10/26/systemd-networkd-and-macvlan-interfaces/</link><pubDate>Mon, 26 Oct 2015 13:50:36 +0000</pubDate><guid>https://major.io/2015/10/26/systemd-networkd-and-macvlan-interfaces/</guid><description>I spent some time working with macvlan interfaces on KVM hypervisors last weekend. They&amp;rsquo;re interesting because they&amp;rsquo;re not really a bridge. It allows you to assign multiple MAC addresses to a single interface and then allow the kernel to filter traffic into tap interfaces based on the MAC address in the packet. If you&amp;rsquo;re looking for a highly detailed explanation, head on over to waldner&amp;rsquo;s blog for a deep dive into the technology and the changes that come along with it.</description></item><item><title>Build a high performance KVM hypervisor on Rackspaceâ€™s OnMetal servers</title><link>https://major.io/2015/08/28/build-a-high-performance-kvm-hypervisor-on-rackspaces-onmetal-servers/</link><pubDate>Fri, 28 Aug 2015 14:00:16 +0000</pubDate><guid>https://major.io/2015/08/28/build-a-high-performance-kvm-hypervisor-on-rackspaces-onmetal-servers/</guid><description>I received some good feedback about my post on systemd-networkd and bonded interfaces on Rackspace&amp;rsquo;s OnMetal servers, and I decided to write about another use case. Recent product updates allow you to attach a Cloud Block Storage volume, and this opens up quite a few new possibilities for deployments.
So why not create a high-performance KVM hypervisor on an OnMetal server? Let&amp;rsquo;s do this.
Disclaimer WHOA THERE. These are amazing servers and because of that, they&amp;rsquo;re priced much differently than Cloud Servers are.</description></item><item><title>Live migration failures with KVM and libvirt</title><link>https://major.io/2015/08/03/live-migration-failures-with-kvm-and-libvirt/</link><pubDate>Mon, 03 Aug 2015 13:13:30 +0000</pubDate><guid>https://major.io/2015/08/03/live-migration-failures-with-kvm-and-libvirt/</guid><description>I decided to change some of my infrastructure back to KVM again, and the overall experience has been quite good in Fedora 22. Using libvirt with KVM is a breeze and the virt-manager tools make it even easier. However, I ran into some problems while trying to migrate virtual machines from one server to another.
The error # virsh migrate --live --copy-storage-all bastion qemu+ssh://root@192.168.250.33/system error: internal error: unable to execute QEMU command 'drive-mirror': Failed to connect socket: Connection timed out That error message wasn&amp;rsquo;t terribly helpful.</description></item><item><title>Try out LXC with an Ansible playbook</title><link>https://major.io/2014/12/17/try-lxc-ansible-playbook/</link><pubDate>Wed, 17 Dec 2014 13:50:26 +0000</pubDate><guid>https://major.io/2014/12/17/try-lxc-ansible-playbook/</guid><description>The world of containers is constantly evolving lately. The latest turn of events involves the CoreOS developers when they announced Rocket as an alternative to Docker. However, LXC still lingers as a very simple path to begin using containers.
When I talk to people about LXC, I often hear people talk about how difficult it is to get started with LXC. After all, Docker provides an easy-to-use image downloading function that allows you to spin up multiple different operating systems in Docker containers within a few minutes.</description></item><item><title>Configure static IP addresses for Project Atomicâ€™s KVM image</title><link>https://major.io/2014/04/23/configure-static-ip-addresses-for-project-atomics-kvm-image/</link><pubDate>Wed, 23 Apr 2014 15:14:39 +0000</pubDate><guid>https://major.io/2014/04/23/configure-static-ip-addresses-for-project-atomics-kvm-image/</guid><description>Amid all of the Docker buzz at the Red Hat Summit, Project Atomic was launched. It&amp;rsquo;s a minimalistic Fedora 20 image with a few tweaks, including rpm-ostree and geard.
There are great instructions on the site for firing up a test instance under KVM but my test server doesn&amp;rsquo;t have a DHCP server on its network. You can use Project Atomic with static IP addresses fairly easily:
Create a one-line /etc/sysconfig/network:</description></item><item><title>Launch secure LXC containers on Fedora 20 using SELinux and sVirt</title><link>https://major.io/2014/04/21/launch-secure-lxc-containers-on-fedora-20-using-selinux-and-svirt/</link><pubDate>Tue, 22 Apr 2014 04:11:00 +0000</pubDate><guid>https://major.io/2014/04/21/launch-secure-lxc-containers-on-fedora-20-using-selinux-and-svirt/</guid><description>Getting started with LXC is a bit awkward and I&amp;rsquo;ve assembled this guide for anyone who wants to begin experimenting with LXC containers in Fedora 20. As an added benefit, you can follow almost every step shown here when creating LXC containers on Red Hat Enterprise Linux 7 Beta (which is based on Fedora 19).
You&amp;rsquo;ll need a physical machine or a VM running Fedora 20 to get started. (You could put a container in a container, but things get a little dicey with that setup.</description></item><item><title>Xen hackathon coming up in London</title><link>https://major.io/2014/03/27/xen-hackathon-coming-up-in-london/</link><pubDate>Thu, 27 Mar 2014 13:20:34 +0000</pubDate><guid>https://major.io/2014/03/27/xen-hackathon-coming-up-in-london/</guid><description>If you enjoy using Xen, join members of the Xen Project community and Rackspace at the Xen Hackathon in London. The two day event starts on May 29th.
Use these links to get more information:
Hackathon announcement and travel/venue/registration information Discussion topics You don&amp;rsquo;t need to be a developer to join the event. It&amp;rsquo;s a great networking opportunity and you can take time to learn more about virtualization and how Xen works under the hood.</description></item><item><title>Docker, trusted builds, and Fedora 20</title><link>https://major.io/2014/03/26/docker-trusted-builds-and-fedora-20/</link><pubDate>Wed, 26 Mar 2014 05:17:58 +0000</pubDate><guid>https://major.io/2014/03/26/docker-trusted-builds-and-fedora-20/</guid><description>Docker is a hot topic in the Linux world at the moment and I decided to try out the new trusted build process. Long story short, you put your Dockerfile along with any additional content into your GitHub repository, link your GitHub account with Docker, and then fire off a build. The Docker index labels it as &amp;ldquo;trusted&amp;rdquo; since it was build from source files in your repository.
I set off to build a Dockerfile to provision a container that would run all of the icanhazip services.</description></item><item><title>virt-manager: â€˜NoneTypeâ€™ object has no attribute â€˜cpusâ€™</title><link>https://major.io/2014/03/06/virt-manager-nonetype-object-has-no-attribute-cpus/</link><pubDate>Thu, 06 Mar 2014 18:44:58 +0000</pubDate><guid>https://major.io/2014/03/06/virt-manager-nonetype-object-has-no-attribute-cpus/</guid><description>After upgrading my Fedora 20 Xen hypervisor to virt-manager 1.0.0, I noticed that I couldn&amp;rsquo;t open the console or VM details for any of my guests. Running virt-manager --debug gave me the following traceback:
Traceback (most recent call last): File &amp;quot;/usr/share/virt-manager/virtManager/engine.py&amp;quot;, line 803, in _show_vm_helper details = self._get_details_dialog(uri, uuid) File &amp;quot;/usr/share/virt-manager/virtManager/engine.py&amp;quot;, line 760, in _get_details_dialog obj = vmmDetails(con.get_vm(uuid)) File &amp;quot;/usr/share/virt-manager/virtManager/details.py&amp;quot;, line 530, in __init__ self.init_details() File &amp;quot;/usr/share/virt-manager/virtManager/details.py&amp;quot;, line 990, in init_details for name in [c.</description></item><item><title>Installing Xen on Fedora 20</title><link>https://major.io/2014/02/27/installing-xen-on-fedora-20/</link><pubDate>Fri, 28 Feb 2014 03:43:27 +0000</pubDate><guid>https://major.io/2014/02/27/installing-xen-on-fedora-20/</guid><description>I&amp;rsquo;ve written about installing Xen on Fedora 19 and earlier versions on this blog before. Let&amp;rsquo;s tackle it on Fedora 20.
Start with the Xen hypervisor and the basic toolset first:
yum -y install xen xen-hypervisor xen-libs xen-runtime systemctl enable xend.service systemctl enable xendomains.service Get GRUB2 in order:
# grep ^menuentry /boot/grub2/grub.cfg | cut -d &amp;quot;'&amp;quot; -f2 Fedora, with Linux 3.13.4-200.fc20.x86_64 Fedora, with Linux 0-rescue-c9dcecb251df472fbc8b4e620a749f6d Fedora, with Xen hypervisor # grub2-set-default 'Fedora, with Xen hypervisor' # grub2-editenv list saved_entry=Fedora, with Xen hypervisor # grub2-mkconfig -o /boot/grub2/grub.</description></item><item><title>PXE boot Fedora 19 using a Mikrotik firewall</title><link>https://major.io/2013/07/23/pxe-boot-fedora-19-using-a-mikrotik-firewall/</link><pubDate>Tue, 23 Jul 2013 21:47:33 +0000</pubDate><guid>https://major.io/2013/07/23/pxe-boot-fedora-19-using-a-mikrotik-firewall/</guid><description>Outside of the RHCA exams, I haven&amp;rsquo;t configured a PXE system for my personal needs. A colleague demoed his PXE setup for me and I was hooked. Once I realized how much time I could save when I&amp;rsquo;m building and tearing down virtual machines, it made complete sense. This post will show you how to configure PXE and tftpd in Mikrotik&amp;rsquo;s RouterOS to boot and install Fedora 19 (as well as provide rescue environments).</description></item><item><title>Boot VMâ€™s with virt-manager and libvirt with ISOâ€™s stored remotely via samba/cifs</title><link>https://major.io/2013/07/06/boot-vms-with-virt-manager-and-libvirt-with-isos-stored-remotely-via-sambacifs/</link><pubDate>Sun, 07 Jul 2013 01:51:10 +0000</pubDate><guid>https://major.io/2013/07/06/boot-vms-with-virt-manager-and-libvirt-with-isos-stored-remotely-via-sambacifs/</guid><description>Pairing virt-manager with KVM makes booting new VM&amp;rsquo;s pretty darned easy. I have a QNAP NAS at home with a bunch of ISO&amp;rsquo;s stored in share available to guests and I wanted to use that with libvirt to boot new VM&amp;rsquo;s. (By the way, if you&amp;rsquo;re looking for an off-the-shelf NAS that is built with solid hardware and pretty reliable software, try one of the QNAP devices. You still get access to many of the usual commands that you would normally find on a Linux box for emergencies.</description></item><item><title>Red Hat Summit 2013 Recap</title><link>https://major.io/2013/06/14/red-hat-summit-2013-recap/</link><pubDate>Sat, 15 Jun 2013 00:25:50 +0000</pubDate><guid>https://major.io/2013/06/14/red-hat-summit-2013-recap/</guid><description>The 2013 Red Hat Summit was my second one and I enjoyed it more than last year. Quite a few people asked for a recap and some takeaways from the Summit and that&amp;rsquo;s what I hope to do in this post.
Keynotes
It&amp;rsquo;s quite apparent that Red Hat is taking a more assertive - and sometimes aggressive - stance against closed source, overpriced solutions that prevent consumers from getting things done.</description></item><item><title>Installing the Xen hypervisor on Fedora 19</title><link>https://major.io/2013/06/02/installing-the-xen-hypervisor-on-fedora-19/</link><pubDate>Mon, 03 Jun 2013 04:27:43 +0000</pubDate><guid>https://major.io/2013/06/02/installing-the-xen-hypervisor-on-fedora-19/</guid><description>It&amp;rsquo;s been a little while since I last posted about installing Xen on Fedora, so I figured that Fedora 19&amp;rsquo;s beta release was as good a time as any to write a new post. To get started, you&amp;rsquo;ll need to get Fedora 19 installed on your favorite hardware (or virtual machine).
Install the Xen hypervisor and tools. Also, ensure that both of the necessary daemons are running on each boot:</description></item><item><title>Migrate KVM virtual machines from CentOS 6 to Fedora 18 without the luxury of shared storage</title><link>https://major.io/2013/05/22/migrate-kvm-virtual-machines-from-centos-6-to-fedora-18-without-the-luxury-of-shared-storage/</link><pubDate>Wed, 22 May 2013 15:15:36 +0000</pubDate><guid>https://major.io/2013/05/22/migrate-kvm-virtual-machines-from-centos-6-to-fedora-18-without-the-luxury-of-shared-storage/</guid><description>I&amp;rsquo;ve converted one of my KVM hypervisors from CentOS 6 to Fedora 18 and now comes the task of migrating my virtual machines off of my single remaining CentOS 6 hypervisor. This is definitely on a budget, so there&amp;rsquo;s no shared storage to make this process easier.
Here&amp;rsquo;s how I did it:
Migrate the logical volume
My first VM to migrate is my Fedora development VM where I build and test new packages.</description></item><item><title>Changing your ssh serverâ€™s port from the default: Is it worth it?</title><link>https://major.io/2013/05/14/changing-your-ssh-servers-port-from-the-default-is-it-worth-it/</link><pubDate>Wed, 15 May 2013 04:43:41 +0000</pubDate><guid>https://major.io/2013/05/14/changing-your-ssh-servers-port-from-the-default-is-it-worth-it/</guid><description>Changing my ssh port from the default port (22) has been one of my standard processes for quite some time when I build new servers or virtual machines. However, I see arguments crop up regularly about it (like this reddit thread or this other one).
Before I go any further, let&amp;rsquo;s settle the &amp;ldquo;security through obscurity&amp;rdquo; argument. (This could probably turn into its own post but I&amp;rsquo;ll be brief for now.</description></item><item><title>virt-manager wonâ€™t release the mouse when using ssh forwarding from OS X</title><link>https://major.io/2013/03/20/virt-manager-wont-release-the-mouse-when-using-ssh-forwarding-from-os-x/</link><pubDate>Wed, 20 Mar 2013 05:26:56 +0000</pubDate><guid>https://major.io/2013/03/20/virt-manager-wont-release-the-mouse-when-using-ssh-forwarding-from-os-x/</guid><description>The latest versions of virt-manager don&amp;rsquo;t release the mouse pointer when you&amp;rsquo;re doing X forwarding to a machine running OS X. This can lead to a rather frustrating user experience since your mouse pointer is totally stuck in the window. Although this didn&amp;rsquo;t affect me with CentOS 6 hosts, Fedora 18 hosts were a problem.
There&amp;rsquo;s a relatively elegant fix from btm.geek that solved it for me. On your Mac, exit X11/Xquartz and create an ~/.</description></item><item><title>Late night virtualization frustration with kvm</title><link>https://major.io/2013/03/20/late-night-virtualization-frustration-with-kvm/</link><pubDate>Wed, 20 Mar 2013 05:07:21 +0000</pubDate><guid>https://major.io/2013/03/20/late-night-virtualization-frustration-with-kvm/</guid><description>I dragged out an old Aopen MP57-D tonight that was just sitting in the closet and decided to load up kvm on Fedora 18. I soon found myself staring at a very brief error message upon bootup:
kvm: disabled by bios After a reboot, the BIOS screen was up and I saw that Virtualization and VT-d were both enabled. Trusted execution (TXT) was disabled, so I enabled it for kicks and rebooted.</description></item><item><title>SELinux, Xen, and block devices in Fedora 17</title><link>https://major.io/2012/07/10/selinux-xen-and-block-devices-in-fedora-17/</link><pubDate>Tue, 10 Jul 2012 05:05:33 +0000</pubDate><guid>https://major.io/2012/07/10/selinux-xen-and-block-devices-in-fedora-17/</guid><description>If you try to run Xen without libvirt on Fedora 17 with SELinux in enforcing mode, you&amp;rsquo;ll be butting heads with SELinux in no time. You&amp;rsquo;ll probably be staring at something like this:
# xm create -c fedora17 Using config file &amp;quot;/etc/xen/fedora17&amp;quot;. Error: Disk isn't accessible If you have setroubleshoot and setroubleshoot-server installed, you should have a friendly message in /var/log/messages telling you the source of the problem:
setroubleshoot: SELinux is preventing /usr/bin/python2.</description></item><item><title>Red Hat Summit 2012: Thursday</title><link>https://major.io/2012/06/28/red-hat-summit-2012-thursday/</link><pubDate>Fri, 29 Jun 2012 04:54:41 +0000</pubDate><guid>https://major.io/2012/06/28/red-hat-summit-2012-thursday/</guid><description>Thursday has felt like the busiest, most jam-packed day of the week. The morning started off with three keynotes from HP, Intel, and Red Hat&amp;rsquo;s CTO, Brian Stevens.
HP&amp;rsquo;s message centered around converged cloud and that customers don&amp;rsquo;t need an all or nothing solution. They can pull the best pieces from every type of hosting to do what&amp;rsquo;s best for their business. The presentation from Intel was extremely heavy on the marketing side and didn&amp;rsquo;t have much to do with Red Hat.</description></item><item><title>Installing XenServer 6.0.2 on an AOpen MP57</title><link>https://major.io/2012/03/12/installing-xenserver-6-0-2-on-an-aopen-mp57/</link><pubDate>Mon, 12 Mar 2012 17:00:56 +0000</pubDate><guid>https://major.io/2012/03/12/installing-xenserver-6-0-2-on-an-aopen-mp57/</guid><description>Getting XenServer installed on some unusual platforms takes a bit of work and the AOpen MP57 is a challenging platform for a XenServer 6.0.2 installation.
My MP57 box came with the i57QMx-vP motherboard. If yours came with something else, this post may or may not work for you.
You&amp;rsquo;ll need the XenServer 6 installation ISO burned to a CD to get started. Boot the CD in your MP57 and wait for the initial boot screen to appear.</description></item><item><title>Preparing for Red Hat Exams</title><link>https://major.io/2012/02/28/preparing-for-red-hat-exams/</link><pubDate>Tue, 28 Feb 2012 21:35:28 +0000</pubDate><guid>https://major.io/2012/02/28/preparing-for-red-hat-exams/</guid><description>I originally wrote this post for the Rackspace Blogbut I&amp;rsquo;ve posted it here just in case anyone following my blog&amp;rsquo;s feed finds it useful. Feel free to share your feedback!Getting yourself ready for any type of examination is usually a stressful experience that involves procrastination and some late nights leading up to the test. Every time I take one, I always say to myself, â€œIâ€™m really going to get ahead of this next time and study early.</description></item></channel></rss>