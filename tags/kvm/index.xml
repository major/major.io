<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>kvm on Major Hayden's Blog ðŸ¤ </title><link>https://major.io/tags/kvm/</link><description>Recent content in kvm on Major Hayden's Blog ðŸ¤ </description><generator>Hugo -- gohugo.io</generator><lastBuildDate>Thu, 18 May 2017 16:58:56 +0000</lastBuildDate><atom:link href="https://major.io/tags/kvm/index.xml" rel="self" type="application/rss+xml"/><item><title>Fixing OpenStack noVNC consoles that ignore keyboard input</title><link>https://major.io/2017/05/18/fixing-openstack-novnc-consoles-that-ignore-keyboard-input/</link><pubDate>Thu, 18 May 2017 16:58:56 +0000</pubDate><guid>https://major.io/2017/05/18/fixing-openstack-novnc-consoles-that-ignore-keyboard-input/</guid><description>I opened up a noVNC console to a virtual machine today in my OpenStack cloud but found that the console wouldn&amp;rsquo;t take keyboard input. The Send Ctrl-Alt-Del button in the top right of the window worked just fine, but I couldn&amp;rsquo;t type anywhere in the console. This happened on an Ocata OpenStack cloud deployed with OpenStack-Ansible on CentOS 7.
Test the network path The network path to the console is a little deep for this deployment, but here&amp;rsquo;s a quick explanation:</description></item><item><title>OpenStack instances come online with multiple network ports attached</title><link>https://major.io/2016/08/03/openstack-instances-come-online-with-multiple-network-ports-attached/</link><pubDate>Wed, 03 Aug 2016 14:40:16 +0000</pubDate><guid>https://major.io/2016/08/03/openstack-instances-come-online-with-multiple-network-ports-attached/</guid><description>I ran into an interesting problem recently in my production OpenStack deployment that runs the Mitaka release. On various occasions, instances were coming online with multiple network ports attached, even though I only asked for one network port.
The problem If I issued a build request for ten instances, I&amp;rsquo;d usually end up with this:
6 instances with one network port attached 2-3 instances with two network ports attached (not what I want) 1-2 instances with three or four network ports attached (definitely not what I want) When I examined the instances with multiple network ports attached, I found that one of the network ports would be marked as up while the others would be marked as down.</description></item><item><title>Whatâ€™s Happening in OpenStack-Ansible (WHOA) â€“ July 2016</title><link>https://major.io/2016/07/22/whats-happening-in-openstack-ansible-whoa-july-2016/</link><pubDate>Fri, 22 Jul 2016 15:48:18 +0000</pubDate><guid>https://major.io/2016/07/22/whats-happening-in-openstack-ansible-whoa-july-2016/</guid><description>This post is the second installment in the series of What&amp;rsquo;s Happening in OpenStack-Ansible (WHOA) posts that I&amp;rsquo;m assembling each month. My goal is to inform more people about what we&amp;rsquo;re doing in the OpenStack-Ansible community and bring on more contributors to the project.
July brought lots of changes for the OpenStack-Ansible project and the remaining work for the Newton release is coming together well. Many of the changes made in the Newton branch have made deployments faster, more reliable and more repeatable.</description></item><item><title>Whatâ€™s Happening in OpenStack-Ansible (WHOA) â€“ June 2016</title><link>https://major.io/2016/06/15/whats-happening-openstack-ansible-whoa-june-2016/</link><pubDate>Wed, 15 Jun 2016 19:58:52 +0000</pubDate><guid>https://major.io/2016/06/15/whats-happening-openstack-ansible-whoa-june-2016/</guid><description>The world of OpenStack moves quickly. Each day brings new features, new bug fixes, and new ways of thinking. The OpenStack-Ansible community strives to understand these changes and make them easier for operators to implement.
The OpenStack-Ansible project is a collection of playbooks and roles written by operators for operators. These playbooks make it easier to deploy, maintain, and upgrade an OpenStack cloud.
Keeping up with the changes in the OpenStack-Ansible project is challenging.</description></item><item><title>systemd-networkd and macvlan interfaces</title><link>https://major.io/2015/10/26/systemd-networkd-and-macvlan-interfaces/</link><pubDate>Mon, 26 Oct 2015 13:50:36 +0000</pubDate><guid>https://major.io/2015/10/26/systemd-networkd-and-macvlan-interfaces/</guid><description>I spent some time working with macvlan interfaces on KVM hypervisors last weekend. They&amp;rsquo;re interesting because they&amp;rsquo;re not really a bridge. It allows you to assign multiple MAC addresses to a single interface and then allow the kernel to filter traffic into tap interfaces based on the MAC address in the packet. If you&amp;rsquo;re looking for a highly detailed explanation, head on over to waldner&amp;rsquo;s blog for a deep dive into the technology and the changes that come along with it.</description></item><item><title>Build a high performance KVM hypervisor on Rackspaceâ€™s OnMetal servers</title><link>https://major.io/2015/08/28/build-a-high-performance-kvm-hypervisor-on-rackspaces-onmetal-servers/</link><pubDate>Fri, 28 Aug 2015 14:00:16 +0000</pubDate><guid>https://major.io/2015/08/28/build-a-high-performance-kvm-hypervisor-on-rackspaces-onmetal-servers/</guid><description>I received some good feedback about my post on systemd-networkd and bonded interfaces on Rackspace&amp;rsquo;s OnMetal servers, and I decided to write about another use case. Recent product updates allow you to attach a Cloud Block Storage volume, and this opens up quite a few new possibilities for deployments.
So why not create a high-performance KVM hypervisor on an OnMetal server? Let&amp;rsquo;s do this.
Disclaimer WHOA THERE. These are amazing servers and because of that, they&amp;rsquo;re priced much differently than Cloud Servers are.</description></item><item><title>Live migration failures with KVM and libvirt</title><link>https://major.io/2015/08/03/live-migration-failures-with-kvm-and-libvirt/</link><pubDate>Mon, 03 Aug 2015 13:13:30 +0000</pubDate><guid>https://major.io/2015/08/03/live-migration-failures-with-kvm-and-libvirt/</guid><description>I decided to change some of my infrastructure back to KVM again, and the overall experience has been quite good in Fedora 22. Using libvirt with KVM is a breeze and the virt-manager tools make it even easier. However, I ran into some problems while trying to migrate virtual machines from one server to another.
The error # virsh migrate --live --copy-storage-all bastion qemu+ssh://root@192.168.250.33/system error: internal error: unable to execute QEMU command 'drive-mirror': Failed to connect socket: Connection timed out That error message wasn&amp;rsquo;t terribly helpful.</description></item><item><title>Performance benchmarks: KVM vs. Xen</title><link>https://major.io/2014/06/22/performance-benchmarks-kvm-vs-xen/</link><pubDate>Sun, 22 Jun 2014 17:00:14 +0000</pubDate><guid>https://major.io/2014/06/22/performance-benchmarks-kvm-vs-xen/</guid><description>After having some interesting discussions last week around KVM and Xen performance improvements over the past years, I decided to do a little research on my own. The last complete set of benchmarks I could find were from the Phoronix Haswell tests in 2013. There were some other benchmarks from 2011 but those were hotly debated due to the Xen patches headed into kernel 3.0.
The 2011 tests had a good list of benchmarks and I&amp;rsquo;ve done my best to replicate that list here three years later.</description></item><item><title>Migrate KVM virtual machines from CentOS 6 to Fedora 18 without the luxury of shared storage</title><link>https://major.io/2013/05/22/migrate-kvm-virtual-machines-from-centos-6-to-fedora-18-without-the-luxury-of-shared-storage/</link><pubDate>Wed, 22 May 2013 15:15:36 +0000</pubDate><guid>https://major.io/2013/05/22/migrate-kvm-virtual-machines-from-centos-6-to-fedora-18-without-the-luxury-of-shared-storage/</guid><description>I&amp;rsquo;ve converted one of my KVM hypervisors from CentOS 6 to Fedora 18 and now comes the task of migrating my virtual machines off of my single remaining CentOS 6 hypervisor. This is definitely on a budget, so there&amp;rsquo;s no shared storage to make this process easier.
Here&amp;rsquo;s how I did it:
Migrate the logical volume
My first VM to migrate is my Fedora development VM where I build and test new packages.</description></item><item><title>Late night virtualization frustration with kvm</title><link>https://major.io/2013/03/20/late-night-virtualization-frustration-with-kvm/</link><pubDate>Wed, 20 Mar 2013 05:07:21 +0000</pubDate><guid>https://major.io/2013/03/20/late-night-virtualization-frustration-with-kvm/</guid><description>I dragged out an old Aopen MP57-D tonight that was just sitting in the closet and decided to load up kvm on Fedora 18. I soon found myself staring at a very brief error message upon bootup:
kvm: disabled by bios After a reboot, the BIOS screen was up and I saw that Virtualization and VT-d were both enabled. Trusted execution (TXT) was disabled, so I enabled it for kicks and rebooted.</description></item><item><title>Red Hat Summit 2012: Thursday</title><link>https://major.io/2012/06/28/red-hat-summit-2012-thursday/</link><pubDate>Fri, 29 Jun 2012 04:54:41 +0000</pubDate><guid>https://major.io/2012/06/28/red-hat-summit-2012-thursday/</guid><description>Thursday has felt like the busiest, most jam-packed day of the week. The morning started off with three keynotes from HP, Intel, and Red Hat&amp;rsquo;s CTO, Brian Stevens.
HP&amp;rsquo;s message centered around converged cloud and that customers don&amp;rsquo;t need an all or nothing solution. They can pull the best pieces from every type of hosting to do what&amp;rsquo;s best for their business. The presentation from Intel was extremely heavy on the marketing side and didn&amp;rsquo;t have much to do with Red Hat.</description></item></channel></rss>