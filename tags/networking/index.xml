<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>networking on Major Hayden ðŸ¤ </title><link>https://major.io/tags/networking/</link><description>Recent content in networking on Major Hayden ðŸ¤ </description><generator>Hugo -- gohugo.io</generator><copyright>&lt;a href="https://creativecommons.org/licenses/by-sa/2.0/" target="_blank" rel="noopener">CC BY-SA 2.0&lt;/a></copyright><lastBuildDate>Tue, 19 Mar 2019 00:00:00 +0000</lastBuildDate><atom:link href="https://major.io/tags/networking/index.xml" rel="self" type="application/rss+xml"/><item><title>Get a /56 from Spectrum using wide-dhcpv6</title><link>https://major.io/2019/03/19/get-a-slash-56-from-spectrum-using-wide-dhcpv6/</link><pubDate>Tue, 19 Mar 2019 00:00:00 +0000</pubDate><guid>https://major.io/2019/03/19/get-a-slash-56-from-spectrum-using-wide-dhcpv6/</guid><description>After writing my last post on my IPv6 woes with my Pixel 3, some readers asked how I&amp;rsquo;m handling IPv6 on my router lately. I wrote about this previously when Spectrum was Time Warner Cable and I was using Mikrotik network devices.
There is a good post from 2015 on the blog and it still works today:
Time Warner Road Runner, Linux, and large IPv6 subnets I am still using that same setup today, but some readers found it difficult to find the post since Time Warner Cable has renamed to Spectrum.</description></item><item><title>Pixel 3 Wi-Fi drops constantly</title><link>https://major.io/2019/03/17/pixel-3-wifi-drops-constantly/</link><pubDate>Sun, 17 Mar 2019 00:00:00 +0000</pubDate><guid>https://major.io/2019/03/17/pixel-3-wifi-drops-constantly/</guid><description>We have two Google Pixel phones in our house: a Pixel 2 and a Pixel 3. Both of them drop off our home wireless network regularly. It causes lots of problems with various applications on the phones, especially casting video via Chromecast.
At the time when I first noticed the drops, I was using a pair of wireless access points (APs) from Engenius:
EAP600 EAP1200H Also, here&amp;rsquo;s what I knew at the time:</description></item><item><title>Allow a port range with firewalld</title><link>https://major.io/2019/01/04/allow-port-range-with-firewalld/</link><pubDate>Fri, 04 Jan 2019 00:00:00 +0000</pubDate><guid>https://major.io/2019/01/04/allow-port-range-with-firewalld/</guid><description>Managing iptables gets a lot easier with firewalld. You can manage rules for the IPv4 and IPv6 stacks using the same commands and it provides fine-grained controls for various &amp;ldquo;zones&amp;rdquo; of network sources and destinations.
Quick example Here&amp;rsquo;s an example of allowing an arbitrary port (for netdata) through the firewall with iptables and firewalld on Fedora:
## iptables iptables -A INPUT -j ACCEPT -p tcp --dport 19999 ip6tables -A INPUT -j ACCEPT -p tcp --dport 19999 service iptables save service ip6tables save ## firewalld firewall-cmd --add-port=19999/tcp --permanent In this example, firewall-cmd allows us to allow a TCP port through the firewall with a much simpler interface and the change is made permanent with the --permanent argument.</description></item><item><title>Ensuring keepalived starts after the network is ready</title><link>https://major.io/2017/12/15/ensuring-keepalived-starts-network-ready/</link><pubDate>Fri, 15 Dec 2017 21:18:37 +0000</pubDate><guid>https://major.io/2017/12/15/ensuring-keepalived-starts-network-ready/</guid><description>After a recent OpenStack-Ansible (OSA) deployment on CentOS, I found that keepalived was not starting properly at boot time:
Keepalived_vrrp[801]: Cant find interface br-mgmt for vrrp_instance internal !!! Keepalived_vrrp[801]: Truncating auth_pass to 8 characters Keepalived_vrrp[801]: VRRP is trying to assign ip address 172.29.236.11/32 to unknown br-mgmt interface !!! go out and fix your conf !!! Keepalived_vrrp[801]: Cant find interface br-mgmt for vrrp_instance external !!! Keepalived_vrrp[801]: Truncating auth_pass to 8 characters Keepalived_vrrp[801]: VRRP is trying to assign ip address 192.</description></item><item><title>OpenStack-Ansible networking on CentOS 7 with systemd-networkd</title><link>https://major.io/2017/04/13/openstack-ansible-on-centos-7-with-systemd-networkd/</link><pubDate>Thu, 13 Apr 2017 13:18:09 +0000</pubDate><guid>https://major.io/2017/04/13/openstack-ansible-on-centos-7-with-systemd-networkd/</guid><description>Although OpenStack-Ansible doesn&amp;rsquo;t fully support CentOS 7 yet, the support is almost ready. I have a four node Ocata cloud deployed on CentOS 7, but I decided to change things around a bit and use systemd-networkd instead of NetworkManager or the old rc scripts.
This post will explain how to configure the network for an OpenStack-Ansible cloud on CentOS 7 with systemd-networkd.
Each one of my OpenStack hosts has four network interfaces and each one has a specific task:</description></item><item><title>systemd-networkd on Ubuntu 16.04 LTS (Xenial)</title><link>https://major.io/2017/01/15/systemd-networkd-on-ubuntu-16-04-lts-xenial/</link><pubDate>Sun, 15 Jan 2017 15:24:40 +0000</pubDate><guid>https://major.io/2017/01/15/systemd-networkd-on-ubuntu-16-04-lts-xenial/</guid><description>My OpenStack cloud depends on Ubuntu, and the latest release of OpenStack-Ansible (what I use to deploy OpenStack) requires Ubuntu 16.04 at a minimum. I tried upgrading the servers in place from Ubuntu 14.04 to 16.04, but that didn&amp;rsquo;t work so well. Those servers wouldn&amp;rsquo;t boot and the only recourse was a re-install.
Once I finished re-installing them (and wrestling with several installer bugs in Ubuntu 16.04), it was time to set up networking.</description></item><item><title>Talk Recap: Holistic Security for OpenStack Clouds</title><link>https://major.io/2016/10/31/talk-recap-holistic-security-for-openstack-clouds/</link><pubDate>Mon, 31 Oct 2016 15:52:47 +0000</pubDate><guid>https://major.io/2016/10/31/talk-recap-holistic-security-for-openstack-clouds/</guid><description>Thanks to everyone who attended my talk at the OpenStack Summit in Barcelona! I really enjoyed sharing some tips with the audience and it was great to meet some attendees in person afterwards.
If you weren&amp;rsquo;t able to make it, don&amp;rsquo;t fret! This post will cover some of the main points of the talk and link to the video and slides.
Purpose OpenStack clouds are inherently complex. Operating a cloud involves a lot of moving pieces in software, hardware, and networking.</description></item><item><title>HTTP/2 for the blog and icanhazip.com</title><link>https://major.io/2016/09/13/http2-for-the-blog-and-icanhazip-com/</link><pubDate>Tue, 13 Sep 2016 13:47:05 +0000</pubDate><guid>https://major.io/2016/09/13/http2-for-the-blog-and-icanhazip-com/</guid><description>I&amp;rsquo;ve recently updated this blog and icanhazip.com to enable HTTP/2! This probably won&amp;rsquo;t have much of an effect on users who query icanhazip.com with automated tools, but it should deliver the content on this blog a little faster. If you&amp;rsquo;re using an older, non-HTTP/2 client - don&amp;rsquo;t worry. All of the sites will continue working for you as they always have.
Head on over to Wikipedia to learn more about HTTP/2 and its benefits.</description></item><item><title>OpenStack instances come online with multiple network ports attached</title><link>https://major.io/2016/08/03/openstack-instances-come-online-with-multiple-network-ports-attached/</link><pubDate>Wed, 03 Aug 2016 14:40:16 +0000</pubDate><guid>https://major.io/2016/08/03/openstack-instances-come-online-with-multiple-network-ports-attached/</guid><description>I ran into an interesting problem recently in my production OpenStack deployment that runs the Mitaka release. On various occasions, instances were coming online with multiple network ports attached, even though I only asked for one network port.
The problem If I issued a build request for ten instances, I&amp;rsquo;d usually end up with this:
6 instances with one network port attached 2-3 instances with two network ports attached (not what I want) 1-2 instances with three or four network ports attached (definitely not what I want) When I examined the instances with multiple network ports attached, I found that one of the network ports would be marked as up while the others would be marked as down.</description></item><item><title>Setting up a telnet handler for OpenStack Zuul CI jobs in GNOME 3</title><link>https://major.io/2016/07/22/setting-up-a-telnet-handler-in-gnome-3/</link><pubDate>Fri, 22 Jul 2016 19:44:07 +0000</pubDate><guid>https://major.io/2016/07/22/setting-up-a-telnet-handler-in-gnome-3/</guid><description>The OpenStack Zuul system has gone through some big changes recently, and one of those changes is around how you monitor a running CI job. I work on OpenStack-Ansible quite often, and the gate jobs can take almost an hour to complete at times. It can be helpful to watch the output of a Zuul job to catch a problem or follow a breakpoint.
New Zuul In the previous version of Zuul, you could access the Jenkins server that was running the CI job and monitor its progress right in your browser.</description></item><item><title>Whatâ€™s Happening in OpenStack-Ansible (WHOA) â€“ July 2016</title><link>https://major.io/2016/07/22/whats-happening-in-openstack-ansible-whoa-july-2016/</link><pubDate>Fri, 22 Jul 2016 15:48:18 +0000</pubDate><guid>https://major.io/2016/07/22/whats-happening-in-openstack-ansible-whoa-july-2016/</guid><description>This post is the second installment in the series of What&amp;rsquo;s Happening in OpenStack-Ansible (WHOA) posts that I&amp;rsquo;m assembling each month. My goal is to inform more people about what we&amp;rsquo;re doing in the OpenStack-Ansible community and bring on more contributors to the project.
July brought lots of changes for the OpenStack-Ansible project and the remaining work for the Newton release is coming together well. Many of the changes made in the Newton branch have made deployments faster, more reliable and more repeatable.</description></item><item><title>Join me on Thursday to talk about OpenStack LBaaS and security hardening</title><link>https://major.io/2016/07/19/join-me-on-thursday-to-talk-about-openstack-lbaas-and-security/</link><pubDate>Tue, 19 Jul 2016 14:09:40 +0000</pubDate><guid>https://major.io/2016/07/19/join-me-on-thursday-to-talk-about-openstack-lbaas-and-security/</guid><description>If you want to learn more about load balancers and security hardening in OpenStack clouds, join me on Thursday for the Rackspace Office Hours podcast! Walter Bentley, Kenneth Hui and I will be talking about some of the new features available in the 12.2 release of Rackspace Private Cloud powered by OpenStack.
The release has a tech preview of OpenStack&amp;rsquo;s Load Balancer as a Service project. The new LBaaSv2 API is stable and makes it easy to create load balancers, add pools, and add members.</description></item><item><title>Whatâ€™s Happening in OpenStack-Ansible (WHOA) â€“ June 2016</title><link>https://major.io/2016/06/15/whats-happening-openstack-ansible-whoa-june-2016/</link><pubDate>Wed, 15 Jun 2016 19:58:52 +0000</pubDate><guid>https://major.io/2016/06/15/whats-happening-openstack-ansible-whoa-june-2016/</guid><description>The world of OpenStack moves quickly. Each day brings new features, new bug fixes, and new ways of thinking. The OpenStack-Ansible community strives to understand these changes and make them easier for operators to implement.
The OpenStack-Ansible project is a collection of playbooks and roles written by operators for operators. These playbooks make it easier to deploy, maintain, and upgrade an OpenStack cloud.
Keeping up with the changes in the OpenStack-Ansible project is challenging.</description></item><item><title>Troubleshooting OpenStack network connectivity</title><link>https://major.io/2016/05/16/troubleshooting-openstack-network-connectivity/</link><pubDate>Tue, 17 May 2016 02:43:41 +0000</pubDate><guid>https://major.io/2016/05/16/troubleshooting-openstack-network-connectivity/</guid><description>NOTE: This post is a work in progress. If you find something that I missed, feel free to leave a comment. I&amp;rsquo;ve made plenty of silly mistakes, but I&amp;rsquo;m sure I&amp;rsquo;ll make a few more. :)
Completing a deployment of an OpenStack cloud is an amazing feeling. There is so much automation and power at your fingertips as soon as you&amp;rsquo;re finished. However, the mood quickly turns sour when you create that first instance and it never responds to pings.</description></item><item><title>802.1x with NetworkManager using nmcli</title><link>https://major.io/2016/05/03/802-1x-networkmanager-using-nmcli/</link><pubDate>Tue, 03 May 2016 19:23:24 +0000</pubDate><guid>https://major.io/2016/05/03/802-1x-networkmanager-using-nmcli/</guid><description>Authenticating to a wired or wireless network using 802.1x is simple using NetworkManager&amp;rsquo;s GUI client. However, this gets challenging on headless servers without a graphical interface. The nmcli command isn&amp;rsquo;t able to store credentials in a keyring and this causes problems when you try to configure an interfaces with 802.1x authentication.
If you aren&amp;rsquo;t familiar with 802.1x, there is some light reading and heavier reading available on the topic.
Start by setting some basic configurations on the interface using the nmcli editor shell:</description></item><item><title>Lessons learned: Five years of colocation</title><link>https://major.io/2016/04/22/lessons-learned-four-years-colocation-hosting/</link><pubDate>Fri, 22 Apr 2016 13:30:52 +0000</pubDate><guid>https://major.io/2016/04/22/lessons-learned-four-years-colocation-hosting/</guid><description>Back in 2011, I decided to try out a new method for hosting my websites and other applications: colocation. Before that, I used shared hosting, VPS providers (&amp;ldquo;cloud&amp;rdquo; wasn&amp;rsquo;t a popular thing back then), and dedicated servers. Each had their drawbacks in different areas. Some didn&amp;rsquo;t perform well, some couldn&amp;rsquo;t recover from failure well, and some were terribly time consuming to maintain.
This post will explain why I decided to try colocation and will hopefully help you avoid some of my mistakes.</description></item><item><title>Enable IPv6 privacy in NetworkManager</title><link>https://major.io/2016/04/17/enable-ipv6-privacy-networkmanager/</link><pubDate>Sun, 17 Apr 2016 16:35:57 +0000</pubDate><guid>https://major.io/2016/04/17/enable-ipv6-privacy-networkmanager/</guid><description>On most IPv6-enabled networks, network addresses are distributed via stateless address autoconfiguration (SLAAC). That is a fancy way to say that hosts on an IPv6 network will configure their own IP addresses.
The process usually works like this:
The host sends out a router solicitation request: Hey, who is the router around here? The router replies with a prefix: I am the router and your IPv6 address should start with this prefix.</description></item><item><title>Automated Letâ€™s Encrypt DNS challenges with Rackspace Cloud DNS</title><link>https://major.io/2016/03/31/automated-lets-encrypt-dns-challenges-with-rackspace-cloud-dns/</link><pubDate>Thu, 31 Mar 2016 19:39:50 +0000</pubDate><guid>https://major.io/2016/03/31/automated-lets-encrypt-dns-challenges-with-rackspace-cloud-dns/</guid><description>Let&amp;rsquo;s Encrypt has taken the world by storm by providing free SSL certificates that can be renewed via automated methods. They have issued over 1.4 million certificates since launch in the fall of 2015.
If you are not familiar with how Let&amp;rsquo;s Encrypt operates, here is an extremely simple explanation:
Create a private key Make a request for a new certificate Complete the challenge process You have a certificate! That is highly simplified, but there is plenty of detail available on how the whole system works.</description></item><item><title>Tinkering with systemdâ€™s predictable network names</title><link>https://major.io/2016/01/20/tinkering-with-systemds-predictable-network-names/</link><pubDate>Wed, 20 Jan 2016 19:46:52 +0000</pubDate><guid>https://major.io/2016/01/20/tinkering-with-systemds-predictable-network-names/</guid><description>I&amp;rsquo;ve talked about predictable network names (and seemingly unpredictable ones) on the blog before, but some readers asked me how they could alter the network naming to fit a particular situation. Oddly enough, my Supermicro 5028D-T4NT has a problem with predictable names and it&amp;rsquo;s a great example to use here.
The problem There&amp;rsquo;s plenty of detail in my post about the Supermicro 5028D-T4NT, but the basic gist is that something within the firmware is causing the all of the network cards in the server to show up as onboard.</description></item><item><title>systemd-networkd and macvlan interfaces</title><link>https://major.io/2015/10/26/systemd-networkd-and-macvlan-interfaces/</link><pubDate>Mon, 26 Oct 2015 13:50:36 +0000</pubDate><guid>https://major.io/2015/10/26/systemd-networkd-and-macvlan-interfaces/</guid><description>I spent some time working with macvlan interfaces on KVM hypervisors last weekend. They&amp;rsquo;re interesting because they&amp;rsquo;re not really a bridge. It allows you to assign multiple MAC addresses to a single interface and then allow the kernel to filter traffic into tap interfaces based on the MAC address in the packet. If you&amp;rsquo;re looking for a highly detailed explanation, head on over to waldner&amp;rsquo;s blog for a deep dive into the technology and the changes that come along with it.</description></item><item><title>GRE tunnels with systemd-networkd</title><link>https://major.io/2015/10/16/gre-tunnels-with-systemd-networkd/</link><pubDate>Fri, 16 Oct 2015 23:54:52 +0000</pubDate><guid>https://major.io/2015/10/16/gre-tunnels-with-systemd-networkd/</guid><description>Switching to systemd-networkd for managing your networking interfaces makes things quite a bit simpler over standard networking scripts or NetworkManager. Aside from being easier to configure, it uses fewer resources on your system, which can be handy for smaller virtual machines or containers.
Managing tunnels between interfaces is also easier with systemd-networkd. This post will show you how to set up a GRE tunnel between two hosts running systemd-networkd.
Getting started You&amp;rsquo;ll need two hosts running a recent version of systemd-networkd.</description></item><item><title>First thoughts: Linux on the Supermicro 5028D-TN4T</title><link>https://major.io/2015/09/28/first-thoughts-linux-on-the-supermicro-5028d-t4nt/</link><pubDate>Mon, 28 Sep 2015 12:55:51 +0000</pubDate><guid>https://major.io/2015/09/28/first-thoughts-linux-on-the-supermicro-5028d-t4nt/</guid><description>I&amp;rsquo;ve recently moved over to Rackspace&amp;rsquo;s OpenStack Private Cloud team and the role is full of some great challenges. One of those challenges was figuring out a home lab for testing.
The search My first idea was to pick up some lower-power machines that would give me some infrastructure at a low price with a low power bill as well. I found some Dell Optiplex 3020&amp;rsquo;s on Newegg with Haswell i3&amp;rsquo;s that came in at a good price point.</description></item><item><title>Build a high performance KVM hypervisor on Rackspaceâ€™s OnMetal servers</title><link>https://major.io/2015/08/28/build-a-high-performance-kvm-hypervisor-on-rackspaces-onmetal-servers/</link><pubDate>Fri, 28 Aug 2015 14:00:16 +0000</pubDate><guid>https://major.io/2015/08/28/build-a-high-performance-kvm-hypervisor-on-rackspaces-onmetal-servers/</guid><description>I received some good feedback about my post on systemd-networkd and bonded interfaces on Rackspace&amp;rsquo;s OnMetal servers, and I decided to write about another use case. Recent product updates allow you to attach a Cloud Block Storage volume, and this opens up quite a few new possibilities for deployments.
So why not create a high-performance KVM hypervisor on an OnMetal server? Let&amp;rsquo;s do this.
Disclaimer WHOA THERE. These are amazing servers and because of that, they&amp;rsquo;re priced much differently than Cloud Servers are.</description></item><item><title>Fedora 23 Alpha in boot.rackspace.com</title><link>https://major.io/2015/08/27/fedora-23-alpha-in-boot-rackspace-com/</link><pubDate>Thu, 27 Aug 2015 13:03:57 +0000</pubDate><guid>https://major.io/2015/08/27/fedora-23-alpha-in-boot-rackspace-com/</guid><description>Fedora 23&amp;rsquo;s Alpha release was announced earlier this month and work is underway for the beta release. The full list of dates for the Fedora 23 release is in the Fedora wiki.
If you&amp;rsquo;d like to try Fedora 23 Alpha a little sooner, check out boot.rackspace.com. I added support for Fedora 23 in the menus last night.
Quick start If you want to get underway quickly, simply download the boot.rackspace.com ISO and attach it to a virtual machine:</description></item><item><title>Build a network router and firewall with Fedora 22 and systemd-networkd</title><link>https://major.io/2015/08/27/build-a-network-router-and-firewall-with-fedora-22-and-systemd-networkd/</link><pubDate>Thu, 27 Aug 2015 12:38:43 +0000</pubDate><guid>https://major.io/2015/08/27/build-a-network-router-and-firewall-with-fedora-22-and-systemd-networkd/</guid><description>This post originally appeared on the Fedora Magazine blog.
One of my favorite features of Fedora 22 is systemd-networkd and all of the new features that came with it in recent systemd versions. The configuration files are easy to read, bridging is simple, and tunnels are resilient.
I&amp;rsquo;ve recently started using a small Linux server at home again as a network router and firewall. However, I used systemd-networkd this time and had some great results.</description></item></channel></rss>