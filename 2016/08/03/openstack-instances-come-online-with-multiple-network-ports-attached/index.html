<!doctype html><html lang=en><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="ie=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=author content="Major Hayden"><meta name=description content="I ran into an interesting problem recently in my production OpenStack deployment that runs the Mitaka release. On various occasions, instances were coming online with multiple network ports attached, even though I only asked for one network port.
The problem If I issued a build request for ten instances, I&amp;rsquo;d usually end up with this:
 6 instances with one network port attached 2-3 instances with two network ports attached (not what I want) 1-2 instances with three or four network ports attached (definitely not what I want)  When I examined the instances with multiple network ports attached, I found that one of the network ports would be marked as up while the others would be marked as down."><meta name=keywords content=",kvm,network,networking,openstack,python,virtualization"><meta name=robots content="noodp"><meta name=theme-color content><link rel=canonical href=https://major.io/2016/08/03/openstack-instances-come-online-with-multiple-network-ports-attached/><title>OpenStack instances come online with multiple network ports attached :: Major Hayden ðŸ¤  â€” Words of wisdom from a social nerd</title><link href=https://cdnjs.cloudflare.com/ajax/libs/flag-icon-css/3.2.1/css/flag-icon.min.css rel=stylesheet type=text/css><link rel=stylesheet href=../../../../main.min.5dcefbf8102eb536dd3e2de53ffebfa58599ab2435c241a0db81728a5e015f2e.css><link rel=apple-touch-icon sizes=180x180 href=../../../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../../../favicon-16x16.png><link rel=manifest href=../../../../site.webmanifest><link rel=mask-icon href=../../../../safari-pinned-tab.svg color=#252627><link rel="shortcut icon" href=../../../../favicon.ico><meta name=msapplication-TileColor content="#252627"><meta name=theme-color content="#252627"><meta itemprop=name content="OpenStack instances come online with multiple network ports attached"><meta itemprop=description content="I ran into an interesting problem recently in my production OpenStack deployment that runs the Mitaka release. On various occasions, instances were coming online with multiple network ports attached, even though I only asked for one network port.
The problem If I issued a build request for ten instances, I&rsquo;d usually end up with this:
 6 instances with one network port attached 2-3 instances with two network ports attached (not what I want) 1-2 instances with three or four network ports attached (definitely not what I want)  When I examined the instances with multiple network ports attached, I found that one of the network ports would be marked as up while the others would be marked as down."><meta itemprop=datePublished content="2016-08-03T14:40:16+00:00"><meta itemprop=dateModified content="2016-08-03T14:40:16+00:00"><meta itemprop=wordCount content="1128"><meta itemprop=image content="https://major.io"><meta itemprop=keywords content="kvm,network,networking,openstack,python,virtualization,"><meta property="article:section" content="Blog Posts"><meta property="article:published_time" content="2016-08-03 14:40:16 +0000 UTC"><meta name=twitter:card content="summary_large_image"><meta name=twitter:title content="OpenStack instances come online with multiple network ports attached :: Major Hayden ðŸ¤ "><meta name=twitter:description content><meta name=twitter:site content="https://major.io"><meta name=twitter:creator content="Major Hayden"><meta name=twitter:image content><style type=text/css>html{letter-spacing:unset}</style></head><body class=dark-theme><div class=container><header class=header><span class=header__inner><a href=../../../../ style=text-decoration:none><div class=logo><span class=logo__mark>></span>
<span class=logo__text>$ echo major.io</span>
<span class=logo__cursor style=visibility:hidden></span></div></a><span class=header__right><nav class=menu><ul class=menu__inner><li><a href=https://major.io/ham-radio-faq>hamradio</a></li><li><a href=https://major.io/icanhazip-com-faq>icanhazip</a></li><li><a href=https://major.io/posts/>posts</a></li></ul></nav><span class=menu-trigger><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M0 0h24v24H0z" fill="none"/><path d="M3 18h18v-2H3v2zm0-5h18v-2H3v2zm0-7v2h18V6H3z"/></svg></span><span class="theme-toggle unselectable"><svg class="theme-toggler" width="24" height="24" viewBox="0 0 48 48" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M22 41C32.4934 41 41 32.4934 41 22 41 11.5066 32.4934 3 22 3 11.5066 3 3 11.5066 3 22s8.5066 19 19 19zM7 22C7 13.7157 13.7157 7 22 7V37C13.7157 37 7 30.2843 7 22z"/></svg></span></span></span></header><div class=content><main class=post><div class=post-info></p></div><article><h2 class=post-title><a href=https://major.io/2016/08/03/openstack-instances-come-online-with-multiple-network-ports-attached/>OpenStack instances come online with multiple network ports attached</a></h2><div class=post-content><p><img src=../../../../wp-content/uploads/2016/08/308357541_222d1b2e2a_b-e1470234736818.jpg alt=1></p><p>I ran into an interesting problem recently in my production OpenStack deployment that runs the Mitaka release. On various occasions, instances were coming online with multiple network ports attached, even though I only asked for one network port.</p><h2 id=the-problem>The problem</h2><p>If I issued a build request for ten instances, I&rsquo;d usually end up with this:</p><ul><li>6 instances with one network port attached</li><li>2-3 instances with two network ports attached <em>(not what I want)</em></li><li>1-2 instances with three or four network ports attached <em>(<strong>definitely</strong> not what I want)</em></li></ul><p>When I examined the instances with multiple network ports attached, I found that one of the network ports would be marked as <em>up</em> while the others would be marked as <em>down</em>. However, the IP addresses associated with those extra ports would still be associated with the instance in horizon and via the nova API. All of the network ports seemed to be fully configured on the neutron side.</p><h2 id=digging-into-neutron>Digging into neutron</h2><p>The neutron API logs are fairly chatty, especially while instances are building, but I found two interesting log lines for one of my instances:</p><pre><code>172.29.236.41,172.29.236.21 - - [02/Aug/2016 14:03:11] &quot;GET /v2.0/ports.json?tenant_id=a7b0519330ed481884431102a72dd04c&amp;device_id=05eef1bb-5356-43d9-86c9-4d9854d4d46b HTTP/1.1&quot; 200 2137 0.025282
172.29.236.11,172.29.236.21 - - [02/Aug/2016 14:03:15] &quot;GET /v2.0/ports.json?tenant_id=a7b0519330ed481884431102a72dd04c&amp;device_id=05eef1bb-5356-43d9-86c9-4d9854d4d46b HTTP/1.1&quot; 200 3098 0.027803
</code></pre><p>There are two requests to create network ports for this instance and neutron is allocating ports to both requests. This would normally be just fine, but I only asked for one network port on this instance.</p><p>The IP addresses making the requests are unusual, though. <code>172.29.236.11</code> and <code>172.29.236.41</code> are two of the hypervisors within my cloud. Why are both of them asking neutron for network ports? Only one of those hypervisors should be building my instance, not both. After checking both hypervisors, I verified that the instance was only provisioned on one of the hosts and not both.</p><h2 id=looking-at-nova-compute>Looking at nova-compute</h2><p>The instance ended up on the <code>172.29.236.11</code> hypervisor once it finished building and the logs on that hypervisor looked fine:</p><pre><code>nova.virt.libvirt.driver [-] [instance: 05eef1bb-5356-43d9-86c9-4d9854d4d46b] Instance spawned successfully.
</code></pre><p>I logged into the <code>172.29.236.41</code> hypervisor since it was the one that asked neutron for a port but it never built the instance. The logs there had a much different story:</p><pre><code>[instance: 05eef1bb-5356-43d9-86c9-4d9854d4d46b] Instance failed to spawn
Traceback (most recent call last):
  File &quot;/openstack/venvs/nova-13.3.0/lib/python2.7/site-packages/nova/compute/manager.py&quot;, line 2218, in _build_resources
    yield resources
  File &quot;/openstack/venvs/nova-13.3.0/lib/python2.7/site-packages/nova/compute/manager.py&quot;, line 2064, in _build_and_run_instance
    block_device_info=block_device_info)
  File &quot;/openstack/venvs/nova-13.3.0/lib/python2.7/site-packages/nova/virt/libvirt/driver.py&quot;, line 2773, in spawn
    admin_pass=admin_password)
  File &quot;/openstack/venvs/nova-13.3.0/lib/python2.7/site-packages/nova/virt/libvirt/driver.py&quot;, line 3191, in _create_image
    instance, size, fallback_from_host)
  File &quot;/openstack/venvs/nova-13.3.0/lib/python2.7/site-packages/nova/virt/libvirt/driver.py&quot;, line 6765, in _try_fetch_image_cache
    size=size)
  File &quot;/openstack/venvs/nova-13.3.0/lib/python2.7/site-packages/nova/virt/libvirt/imagebackend.py&quot;, line 251, in cache
    *args, **kwargs)
  File &quot;/openstack/venvs/nova-13.3.0/lib/python2.7/site-packages/nova/virt/libvirt/imagebackend.py&quot;, line 591, in create_image
    prepare_template(target=base, max_size=size, *args, **kwargs)
  File &quot;/openstack/venvs/nova-13.3.0/lib/python2.7/site-packages/oslo_concurrency/lockutils.py&quot;, line 271, in inner
    return f(*args, **kwargs)
  File &quot;/openstack/venvs/nova-13.3.0/lib/python2.7/site-packages/nova/virt/libvirt/imagebackend.py&quot;, line 241, in fetch_func_sync
    fetch_func(target=target, *args, **kwargs)
  File &quot;/openstack/venvs/nova-13.3.0/lib/python2.7/site-packages/nova/virt/libvirt/utils.py&quot;, line 429, in fetch_image
    max_size=max_size)
  File &quot;/openstack/venvs/nova-13.3.0/lib/python2.7/site-packages/nova/virt/images.py&quot;, line 120, in fetch_to_raw
    max_size=max_size)
  File &quot;/openstack/venvs/nova-13.3.0/lib/python2.7/site-packages/nova/virt/images.py&quot;, line 110, in fetch
    IMAGE_API.download(context, image_href, dest_path=path)
  File &quot;/openstack/venvs/nova-13.3.0/lib/python2.7/site-packages/nova/image/api.py&quot;, line 182, in download
    dst_path=dest_path)
  File &quot;/openstack/venvs/nova-13.3.0/lib/python2.7/site-packages/nova/image/glance.py&quot;, line 383, in download
    _reraise_translated_image_exception(image_id)
  File &quot;/openstack/venvs/nova-13.3.0/lib/python2.7/site-packages/nova/image/glance.py&quot;, line 682, in _reraise_translated_image_exception
    six.reraise(new_exc, None, exc_trace)
  File &quot;/openstack/venvs/nova-13.3.0/lib/python2.7/site-packages/nova/image/glance.py&quot;, line 381, in download
    image_chunks = self._client.call(context, 1, 'data', image_id)
  File &quot;/openstack/venvs/nova-13.3.0/lib/python2.7/site-packages/nova/image/glance.py&quot;, line 250, in call
    result = getattr(client.images, method)(*args, **kwargs)
  File &quot;/openstack/venvs/nova-13.3.0/lib/python2.7/site-packages/glanceclient/v1/images.py&quot;, line 148, in data
    % urlparse.quote(str(image_id)))
  File &quot;/openstack/venvs/nova-13.3.0/lib/python2.7/site-packages/glanceclient/common/http.py&quot;, line 275, in get
    return self._request('GET', url, **kwargs)
  File &quot;/openstack/venvs/nova-13.3.0/lib/python2.7/site-packages/glanceclient/common/http.py&quot;, line 267, in _request
    resp, body_iter = self._handle_response(resp)
  File &quot;/openstack/venvs/nova-13.3.0/lib/python2.7/site-packages/glanceclient/common/http.py&quot;, line 83, in _handle_response
    raise exc.from_response(resp, resp.content)
ImageNotFound: Image 8feacda9-91fd-48ce-b983-54f7b6de6650 could not be found.
</code></pre><p>This is one of those occasions where I was glad to find an exception in the log. The image that couldn&rsquo;t be found is an image I&rsquo;ve used regularly in the environment before, and I know it exists.</p><h2 id=gandering-at-glance>Gandering at glance</h2><p>First off, I asked glance what it knew about the image:</p><pre><code>$ openstack image show 8feacda9-91fd-48ce-b983-54f7b6de6650
+------------------+------------------------------------------------------+
| Field            | Value                                                |
+------------------+------------------------------------------------------+
| checksum         | 8de08e3fe24ee788e50a6a508235aa64                     |
| container_format | bare                                                 |
| created_at       | 2016-08-03T01:25:34Z                                 |
| disk_format      | qcow2                                                |
| file             | /v2/images/8feacda9-91fd-48ce-b983-54f7b6de6650/file |
| id               | 8feacda9-91fd-48ce-b983-54f7b6de6650                 |
| min_disk         | 0                                                    |
| min_ram          | 0                                                    |
| name             | Fedora 24                                            |
| owner            | a7b0519330ed481884431102a72dd04c                     |
| properties       | description=''                                       |
| protected        | False                                                |
| schema           | /v2/schemas/image                                    |
| size             | 204590080                                            |
| status           | active                                               |
| tags             |                                                      |
| updated_at       | 2016-08-03T01:25:39Z                                 |
| virtual_size     | None                                                 |
| visibility       | public                                               |
+------------------+------------------------------------------------------+
</code></pre><p>If glance knows about the image, why can&rsquo;t that hypervisor build an instance with that image? While I was scratching my head, <a href=https://twitter.com/cloudnull>Kevin Carter</a> walked by my desk and joined in the debugging.</p><p>He asked about how I had deployed glance and what storage backend I was using. I was using the regular file storage backend since I don&rsquo;t have swift deployed in the environment. He asked me how many glance nodes I had (I had two) and if I was doing anything to sync the images between the glance nodes.</p><p>Then it hit me.</p><p><a href=../../../../wp-content/uploads/2016/08/stitch_frustrated.gif></a></p><p>Although both glance nodes knew about the image (since that data is in the database), <strong>only one of the glance nodes had the actual image content (the actual qcow2 file) stored</strong>. That means that if a hypervisor requests the image from a glance node that knows about the image but doesn&rsquo;t have it stored, the hypervisor won&rsquo;t be able to retrieve the image.</p><p>Unfortunately, the checks go in this order on the nova-compute side:</p><ol><li>Ask glance if this image exists and if this tenant can use it</li><li>Configure the network</li><li>Retrieve the image</li></ol><p>If a hypervisor rolls through steps one and two without issues, but then fails on step 3, the network port will be provisioned but won&rsquo;t come up on the instance. There&rsquo;s nothing that cleans up that port in the Mitaka release, so it requires manual intervention.</p><h2 id=the-fix>The fix</h2><p>As a temporary workaround, I took one of the glance nodes offline so that only one glance node is being used. After hundreds of builds, all of the instances came up with only one network port attached!</p><p>There are a few options for long-term fixes.</p><p>I could deploy swift and put glance images into swift. That would allow me to use multiple glance nodes with the same swift backend. Another option would be to use an existing swift deployment, such as Rackspace&rsquo;s Cloud Files product.</p><p>Since I&rsquo;m not eager to deploy swift in my environment for now, I decided to remove the second glance node and reconfigure nova to use only one glance node. That means I&rsquo;m running with only one glance node and a failure there could be highly annoying. However, that trade-off is fine with me until I can get around to deploying swift.</p><p><strong>UPDATE:</strong> I&rsquo;ve opened <a href=https://bugs.launchpad.net/nova/+bug/1609526>a bug</a> for nova so that the network ports are cleaned up if the instance fails to build.</p><p><em>Photo credit: <a href=https://flic.kr/p/tfpXk>Flickr: pascalcharest</a></em></p></div></article><hr><div class=post-info><p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-tag meta-icon"><path d="M20.59 13.41l-7.17 7.17a2 2 0 01-2.83.0L2 12V2h10l8.59 8.59a2 2 0 010 2.82z"/><line x1="7" y1="7" x2="7" y2="7"/></svg><span class=tag><a href=https://major.io/tags/kvm>kvm</a></span><span class=tag><a href=https://major.io/tags/network>network</a></span><span class=tag><a href=https://major.io/tags/networking>networking</a></span><span class=tag><a href=https://major.io/tags/openstack>openstack</a></span><span class=tag><a href=https://major.io/tags/python>python</a></span><span class=tag><a href=https://major.io/tags/virtualization>virtualization</a></span></p></div></main></div><footer class=footer><div class=footer__inner><div class=footer__content><span>&copy; 2021</span>
<span><a href=https://major.io>Major Hayden</a></span>
<span><a href=https://creativecommons.org/licenses/by-sa/2.0/ target=_blank rel=noopener>CC BY-SA 2.0</a></span>
<span><a href=https://major.io/posts/index.xml target=_blank title=rss><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 20 20" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-rss"><path d="M4 11a9 9 0 019 9"/><path d="M4 4a16 16 0 0116 16"/><circle cx="5" cy="19" r="1"/></svg></a></span></div></div><div class=footer__inner><div class=footer__content><span>Powered by <a href=http://gohugo.io>Hugo</a></span>
<span>Made with &#10084; by <a href=https://github.com/rhazdon>rhazdon</a></span></div></div></footer></div><script type=text/javascript src=../../../../bundle.min.2d5469329143160ae2456a69c3c76dc2d0a3b212b46afe291a51bd68650ed6f8697e001dab54f1c272c77ce08092a8c55e5bb4314e0ee334aab4b927ec896638.js integrity="sha512-LVRpMpFDFgriRWppw8dtwtCjshK0av4pGlG9aGUO1vhpfgAdq1TxwnLHfOCAkqjFXlu0MU4O4zSqtLkn7IlmOA=="></script></body></html>