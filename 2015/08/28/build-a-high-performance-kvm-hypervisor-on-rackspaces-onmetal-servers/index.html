<!doctype html><html lang=en><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="ie=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=author content="Major Hayden"><meta name=description content="I received some good feedback about my post on systemd-networkd and bonded interfaces on Rackspace&amp;rsquo;s OnMetal servers, and I decided to write about another use case. Recent product updates allow you to attach a Cloud Block Storage volume, and this opens up quite a few new possibilities for deployments.
So why not create a high-performance KVM hypervisor on an OnMetal server? Let&amp;rsquo;s do this.
Disclaimer WHOA THERE. These are amazing servers and because of that, they&amp;rsquo;re priced much differently than Cloud Servers are."><meta name=keywords content=",fedora,kvm,networking,rackspace,raid,virtualization"><meta name=robots content="noodp"><meta name=theme-color content><link rel=canonical href=https://major.io/2015/08/28/build-a-high-performance-kvm-hypervisor-on-rackspaces-onmetal-servers/><title>Build a high performance KVM hypervisor on Rackspaceâ€™s OnMetal servers :: Major Hayden ðŸ¤  â€” Words of wisdom from a social nerd</title><link href=https://cdnjs.cloudflare.com/ajax/libs/flag-icon-css/3.5.0/css/flag-icon.min.css rel=stylesheet type=text/css><link rel=stylesheet href=../../../../main.89fa80e2143f71bd5c96a7c94e531dec3276a367e22f87e74a76026ab64680bf.css><link rel=apple-touch-icon sizes=180x180 href=../../../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../../../favicon-16x16.png><link rel=manifest href=../../../../site.webmanifest><link rel=mask-icon href=../../../../safari-pinned-tab.svg color=#252627><link rel="shortcut icon" href=../../../../favicon.ico><meta name=msapplication-TileColor content="#252627"><meta name=theme-color content="#252627"><meta itemprop=name content="Build a high performance KVM hypervisor on Rackspaceâ€™s OnMetal servers"><meta itemprop=description content="I received some good feedback about my post on systemd-networkd and bonded interfaces on Rackspace&rsquo;s OnMetal servers, and I decided to write about another use case. Recent product updates allow you to attach a Cloud Block Storage volume, and this opens up quite a few new possibilities for deployments.
So why not create a high-performance KVM hypervisor on an OnMetal server? Let&rsquo;s do this.
Disclaimer WHOA THERE. These are amazing servers and because of that, they&rsquo;re priced much differently than Cloud Servers are."><meta itemprop=datePublished content="2015-08-28T14:00:16+00:00"><meta itemprop=dateModified content="2015-08-28T14:00:16+00:00"><meta itemprop=wordCount content="1026"><meta itemprop=image content="https://major.io"><meta itemprop=keywords content="fedora,kvm,networking,rackspace,raid,virtualization,"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://major.io"><meta name=twitter:title content="Build a high performance KVM hypervisor on Rackspaceâ€™s OnMetal servers"><meta name=twitter:description content="I received some good feedback about my post on systemd-networkd and bonded interfaces on Rackspace&rsquo;s OnMetal servers, and I decided to write about another use case. Recent product updates allow you to attach a Cloud Block Storage volume, and this opens up quite a few new possibilities for deployments.
So why not create a high-performance KVM hypervisor on an OnMetal server? Let&rsquo;s do this.
Disclaimer WHOA THERE. These are amazing servers and because of that, they&rsquo;re priced much differently than Cloud Servers are."><meta name=twitter:site content="@mhayden"><meta property="article:section" content="Blog Posts"><meta property="article:published_time" content="2015-08-28 14:00:16 +0000 UTC"><style type=text/css>html{letter-spacing:unset}</style></head><body><div class=container><header class=header><span class=header__inner><a href=../../../../ style=text-decoration:none><div class=logo><span class=logo__mark>></span>
<span class=logo__text>$ echo major.io</span>
<span class=logo__cursor style=visibility:hidden></span></div></a><span class=header__right><nav class=menu><ul class=menu__inner><li><a href=../../../../ham-radio-faq>hamradio</a></li><li><a href=../../../../icanhazip-com-faq>icanhazip</a></li><li><a href=../../../../posts/>posts</a></li></ul></nav><span class=menu-trigger><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M0 0h24v24H0z" fill="none"/><path d="M3 18h18v-2H3v2zm0-5h18v-2H3v2zm0-7v2h18V6H3z"/></svg></span><span class="theme-toggle unselectable"><svg class="theme-toggler" width="24" height="24" viewBox="0 0 48 48" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M22 41c10.4934.0 19-8.5066 19-19C41 11.5066 32.4934 3 22 3 11.5066 3 3 11.5066 3 22s8.5066 19 19 19zM7 22C7 13.7157 13.7157 7 22 7V37C13.7157 37 7 30.2843 7 22z"/></svg></span></span></span></header><div class=content><main class=post><div class=post-info></p></div><article><h2 class=post-title><a href=https://major.io/2015/08/28/build-a-high-performance-kvm-hypervisor-on-rackspaces-onmetal-servers/>Build a high performance KVM hypervisor on Rackspaceâ€™s OnMetal servers</a></h2><div class=post-content><p>I received some good feedback about <a href=../../../../2015/08/21/using-systemd-networkd-with-bonding-on-rackspaces-onmetal-servers/>my post on systemd-networkd and bonded interfaces</a> on <a href=http://www.rackspace.com/en-us/cloud/servers/onmetal>Rackspace&rsquo;s OnMetal servers</a>, and I decided to write about another use case. Recent product updates allow you to <a href=http://www.rackspace.com/knowledge_center/article/attach-a-cloud-block-storage-volume-to-an-onmetal-server>attach a Cloud Block Storage volume</a>, and this opens up quite a few new possibilities for deployments.</p><p>So why not create a high-performance KVM hypervisor on an OnMetal server? Let&rsquo;s do this.</p><h2 id=disclaimer>Disclaimer</h2><p><strong><em>WHOA THERE.</em></strong> These are amazing servers and because of that, they&rsquo;re priced much differently than Cloud Servers are. Be sure to review the pricing for <a href=http://www.rackspace.com/en-us/cloud/servers/onmetal>OnMetal</a> and <a href=http://www.rackspace.com/cloud/block-storage>Cloud Block Storage</a> before going through this guide. <strong>Don&rsquo;t end up with an unexpected bill by building one of these servers and forgetting to destroy it.</strong></p><h2 id=building-the-server>Building the server</h2><p>We can build our server using command line tools. One of my tools, <a href=https://github.com/major/supernova>supernova</a>, makes this quite easy. My IAD environment is called <code>prodiad</code> and I can boot an OnMetal server like this:</p><pre><code>supernova prodiad boot \
  --flavor onmetal-memory1 \
  --image 4c361a4a-51b4-4e29-8a35-3b0e25e49ee1 \
  --key_name personal_servers \
  --poll \
  kvm-onmetal
</code></pre><p>In the command above, I&rsquo;ve built an OnMetal Memory server. I&rsquo;ll end up with some hardware like this:</p><ul><li>Dual Intel Xeon E5-2630 v2 2.6Ghz</li><li>12 cores total</li><li>512GB RAM</li><li>10Gbps connectivity</li><li>32GB disk</li></ul><p>Everything looks amazing except for the storage â€” but we&rsquo;ll fix that soon. I&rsquo;ve also built the server with Fedora 22 and provided my public ssh key.</p><p>Wait a few minutes after running the supernova command and you should be back to a prompt. Verify that your new OnMetal server is pinging, but keep in mind it may still be in the process of booting up or configuring itself.</p><h2 id=adding-storage>Adding storage</h2><p>Getting additional storage for an OnMetal server is done in two steps: provisioning the LUN and attaching it to the host. This is a bit easier in Cloud Servers since the actual attachment is done behind the scenes. You end up with a disk that attaches itself to the virtual machine at the hypervisor layer. OnMetal is a little different, but the process is still very straightforward.</p><p>Let&rsquo;s start by making four 100GB SSD volumes. We will eventually put these into a RAID 10 volume.</p><div class=highlight><pre style=color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=color:#719e07>for</span> i in <span style=color:#586e75>`</span>seq <span style=color:#2aa198>1</span> 4<span style=color:#586e75>`</span>; <span style=color:#719e07>do</span>
    supernova prodiad volume-create <span style=color:#cb4b16>\
</span><span style=color:#cb4b16></span>      --display-name onmetal-kvm-<span style=color:#2aa198>${</span><span style=color:#268bd2>i</span><span style=color:#2aa198>}</span> <span style=color:#cb4b16>\
</span><span style=color:#cb4b16></span>      --volume-type SSD <span style=color:#cb4b16>\
</span><span style=color:#cb4b16></span>      <span style=color:#2aa198>100</span>
<span style=color:#719e07>done</span>
</code></pre></div><p>We can list our new volumes:</p><pre><code>$ supernova prodiad volume-list
+--------------------------------------+-----------+---------------+------+-------------+-------------+
| ID                                   | Status    | Display Name  | Size | Volume Type | Attached to |
+--------------------------------------+-----------+---------------+------+-------------+-------------+
| 0beb1f81-eb04-4aca-9b14-c952f9eb81e2 | available | onmetal-kvm-4 | 100  | SSD         |             |
| 83b9d6d9-e7eb-4b53-9342-fa2fd3670bb4 | available | onmetal-kvm-3 | 100  | SSD         |             |
| a593cbbe-089f-4ede-81f4-003717b2309f | available | onmetal-kvm-2 | 100  | SSD         |             |
| 2c51e09f-d984-4de5-8852-c0f9c6176e00 | available | onmetal-kvm-1 | 100  | SSD         |             |
+--------------------------------------+-----------+---------------+------+-------------+-------------+
</code></pre><p>It&rsquo;s now time to attach our volumes to our OnMetal server. Let&rsquo;s get our OnMetal server&rsquo;s UUID:</p><pre><code>$ supernova prodiad list --name kvm-onmetal --minimal
[SUPERNOVA] Running nova against prodiad...
+--------------------------------------+-------------+
| ID                                   | Name        |
+--------------------------------------+-------------+
| 6a80d0b9-ce3e-4693-bedb-d843fea7cb0b | kvm-onmetal |
+--------------------------------------+-------------+
</code></pre><p>Now we&rsquo;re ready to attach the volumes:</p><pre><code>ONMETAL_UUID=6a80d0b9-ce3e-4693-bedb-d843fea7cb0b
supernova prodiad volume-attach $ONMETAL_UUID 2c51e09f-d984-4de5-8852-c0f9c6176e00
supernova prodiad volume-attach $ONMETAL_UUID a593cbbe-089f-4ede-81f4-003717b2309f
supernova prodiad volume-attach $ONMETAL_UUID 83b9d6d9-e7eb-4b53-9342-fa2fd3670bb4
supernova prodiad volume-attach $ONMETAL_UUID 0beb1f81-eb04-4aca-9b14-c952f9eb81e2
</code></pre><p>Let&rsquo;s log into the OnMetal server and get it ready. Install the <code>iscsi-initator-utils</code> package and set up the services:</p><pre><code>dnf -y install iscsi-initiator-utils
systemctl enable iscsid
systemctl start iscsid
</code></pre><p>Our iSCSI IQN data is in our OnMetal server&rsquo;s metadata. Grab your metadata JSON with this command:</p><pre><code>supernova prodiad show 6a80d0b9-ce3e-4693-bedb-d843fea7cb0b | grep metadata
</code></pre><p>If you copy/paste the JSON data into a file, you can use Python to make the JSON easier to read:</p><pre><code>cat iscsi_metadata.json | python -m json.tool
</code></pre><p>Start by putting your server&rsquo;s initiator name into a file. It should be called <code>initiator_name</code> in the JSON data.</p><pre><code>echo InitiatorName=iqn.2008-10.org.openstack:735f1804-bf47-4b28-b9fc-cbff3995635e &gt; /etc/iscsi/initiatorname.iscsi
</code></pre><p>Do the iSCSI logins for each `target_iqn` and `target_portal` in your JSON output. It should look something like this each time:</p><pre><code># iscsiadm -m discovery --type sendtargets --portal $TARGET_PORTAL
# iscsiadm -m node --targetname=$TARGET_IQN --portal $TARGET_PORTAL --login
</code></pre><p>When you&rsquo;re all done, you should have four new disks:</p><pre><code># ls /dev/disk/by-path/
ip-10.190.141.11:3260-iscsi-iqn.2010-11.com.rackspace:a593cbbe-089f-4ede-81f4-003717b2309f-lun-0
ip-10.190.141.44:3260-iscsi-iqn.2010-11.com.rackspace:0beb1f81-eb04-4aca-9b14-c952f9eb81e2-lun-0
ip-10.190.142.17:3260-iscsi-iqn.2010-11.com.rackspace:2c51e09f-d984-4de5-8852-c0f9c6176e00-lun-0
ip-10.190.143.103:3260-iscsi-iqn.2010-11.com.rackspace:83b9d6d9-e7eb-4b53-9342-fa2fd3670bb4-lun-0
</code></pre><h2 id=building-the-raid-volume>Building the RAID volume</h2><p>We can build the raid volume using the paths from above to prevent against device name changes later. Let&rsquo;s make a RAID 10 volume:</p><pre><code>dnf -y install mdadm
mdadm --create /dev/md0 --level=10 --raid-devices=4 /dev/disk/by-path/*
</code></pre><p>Check the status of the new RAID volume:</p><pre><code># cat /proc/mdstat
Personalities : [raid10]
md0 : active raid10 sdd[3] sdc[2] sdb[1] sde[0]
      209584128 blocks super 1.2 512K chunks 2 near-copies [4/4] [UUUU]
      [&gt;....................]  resync =  0.7% (1534400/209584128) finish=15.8min speed=219200K/sec
</code></pre><p>Come on, our storage volumes are faster than that. Let&rsquo;s speed it up a bit:</p><pre><code># sysctl -w dev.raid.speed_limit_max=99999999
# cat /proc/mdstat
Personalities : [raid10]
md0 : active raid10 sdd[3] sdc[2] sdb[1] sde[0]
      209584128 blocks super 1.2 512K chunks 2 near-copies [4/4] [UUUU]
      [====&gt;................]  resync = 21.1% (44229312/209584128) finish=2.9min speed=925564K/sec
</code></pre><p>That&rsquo;s more like it. Let&rsquo;s put a XFS filesystem on the volume and get it mounted:</p><pre><code>dnf -y install xfsprogs
mkfs.xfs /dev/md0
mkdir /mnt/raid
echo &quot;/dev/md0 /mnt/raid xfs defaults,noatime 0 1&quot; &gt;&gt; /etc/fstab
mount -a
</code></pre><h2 id=getting-kvm-going>Getting KVM going</h2><p>It&rsquo;s time to get packages updated and installed:</p><pre><code>dnf -y upgrade
dnf -y install libvirt libvirt-daemon* virt-install virt-manager xorg-x11-xauth gnome-icon-theme gnome-themes-standard dejavu*
systemctl start libvirtd
systemctl enable libvirtd
</code></pre><p>We can create a qcow volume and begin installing Fedora into a virtual machine:</p><pre><code>qemu-img create -f qcow2 /mnt/raid/fedora-kvm.qcow2 20G
virt-install --name=fedora-kvm --ram=16384 \
    --vcpus=4 --os-variant=fedora21 --accelerate \
    --hvm --network network=default \
    --disk /mnt/raid/fedora-kvm.qcow2 \
    --location http://iad.mirror.rackspace.com/fedora/releases/22/Server/x86_64/os/ \
    --noautoconsole --graphics vnc --autostart
</code></pre><p>Logout and then ssh to the server again, this time with <code>-Y</code> for X forwarding. Run <code>virt-manager</code> and verify that the VM is running.</p><p><img src=../../../../wp-content/uploads/2015/08/virt-manager-listing.png alt=6></p><p>Double-click on the virtual machine listed there and the anaconda installer should be on the screen.</p><p><img src=../../../../wp-content/uploads/2015/08/onmetal-kvm-vm.png alt=installer_screenshot></p><p>Let the installation complete and you&rsquo;ll have a KVM virtual machine ready to roll!</p><h2 id=additional-thoughts>Additional thoughts</h2><p>Obviously, this is a very manual process. It could be automated with scripts, or an orchestration framework, like Ansible. In addition, deployment of virtual machines could be automated with OpenStack. However, my goal here was to demonstrate a new use case for OnMetal servers. I&rsquo;ll add the automation to my long list of to-do&rsquo;s.</p></div></article><hr><div class=post-info><p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-tag meta-icon"><path d="M20.59 13.41l-7.17 7.17a2 2 0 01-2.83.0L2 12V2h10l8.59 8.59a2 2 0 010 2.82z"/><line x1="7" y1="7" x2="7" y2="7"/></svg><span class=tag><a href=https://major.io/tags/fedora/>fedora</a></span>
<span class=tag><a href=https://major.io/tags/kvm/>kvm</a></span>
<span class=tag><a href=https://major.io/tags/networking/>networking</a></span>
<span class=tag><a href=https://major.io/tags/rackspace/>rackspace</a></span>
<span class=tag><a href=https://major.io/tags/raid/>raid</a></span>
<span class=tag><a href=https://major.io/tags/virtualization/>virtualization</a></span></p><p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-folder meta-icon"><path d="M22 19a2 2 0 01-2 2H4a2 2 0 01-2-2V5a2 2 0 012-2h5l2 3h9a2 2 0 012 2z"/></svg><span class=tag><a href=https://major.io/categories/blog-posts/>Blog Posts</a></span></p></div></main></div><footer class=footer><div class=footer__inner><div class=footer__content><span>&copy; 2021</span>
<span><a href=https://major.io>Major Hayden</a></span>
<span><a href=https://creativecommons.org/licenses/by-sa/2.0/ target=_blank rel=noopener>CC BY-SA 2.0</a></span><span><a href=https://major.io/posts/index.xml target=_blank title=rss><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 20 20" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-rss"><path d="M4 11a9 9 0 019 9"/><path d="M4 4a16 16 0 0116 16"/><circle cx="5" cy="19" r="1"/></svg></a></span></div></div><div class=footer__inner><div class=footer__content><span>Powered by <a href=http://gohugo.io>Hugo</a></span>
<span>Made with &#10084; by <a href=https://github.com/rhazdon>Djordje Atlialp</a></span></div></div></footer></div><script type=text/javascript src=../../../../bundle.min.e9f93b80e78a22e6f04cbb5f73e0f9c4ba60ff73a2a0ef85965c688f93dd1f2722a282e30e485603e4c65b1b346720e35f213435ec5556e196a97a68d097c80f.js integrity="sha512-6fk7gOeKIubwTLtfc+D5xLpg/3OioO+Fllxoj5PdHyciooLjDkhWA+TGWxs0ZyDjXyE0NexVVuGWqXpo0JfIDw=="></script></body></html>