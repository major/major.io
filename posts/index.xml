<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Posts on Major Hayden 🤠</title><link>https://major.io/posts/</link><description>Recent content in Posts on Major Hayden 🤠</description><generator>Hugo -- gohugo.io</generator><copyright>&lt;a href="https://creativecommons.org/licenses/by-sa/2.0/" target="_blank" rel="noopener">CC BY-SA 2.0&lt;/a></copyright><lastBuildDate>Sun, 24 Jan 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://major.io/posts/index.xml" rel="self" type="application/rss+xml"/><item><title>Which stock broker should you use?</title><link>https://major.io/2021/01/24/which-stock-broker-should-you-use/</link><pubDate>Sun, 24 Jan 2021 00:00:00 +0000</pubDate><guid>https://major.io/2021/01/24/which-stock-broker-should-you-use/</guid><description>🤔 This is another post in a set of posts on options trading. If you are new to options trading, you may want to start with some of my earlier posts.
One of my sports coaches in high school used to say: &amp;ldquo;It&amp;rsquo;s not the tool, it&amp;rsquo;s the fool.&amp;rdquo; This was his reminder that when something goes wrong in the game, it&amp;rsquo;s usually the fault of the person and not the equipment.</description><content type="html"><![CDATA[<p>🤔 <em>This is another post in a set of posts on <a href="/tags/options/">options trading</a>. If you are
new to options trading, you may want to start with some of my earlier posts.</em></p>
<figure>
    <img src="/images/2021-01-24-street-buildings.jpg"
         alt="Street with buildings"/> 
</figure>

<p>One of my sports coaches in high school used to say: &ldquo;It&rsquo;s not the tool, it&rsquo;s
the fool.&rdquo; This was his reminder that when something goes wrong in the game,
it&rsquo;s usually the fault of the person and not the equipment.</p>
<p>However, when it comes to investing, your choice of brokerage can be critical.
There are many factors to consider. My focus in this post will be brokerages
for US-based traders.</p>
<p>In the end, every broker gives you access to trade various things in the
market, including options, equity, futures contracts, and more. They are often
different in many areas.</p>
<h2 id="data-and-research">Data and research</h2>
<p>Start by looking at the data that a brokerage provides for your trades. I
usually look for the following:</p>
<ul>
<li>Stock calendars that include earnings dates, product events, and other stock
events.</li>
<li>Information about company financials and SEC filings.</li>
<li>Real-time quotes for equities and options. <em>(Extremely important.)</em></li>
<li>Charting with technical analysis, such as moving averages.</li>
<li>Watchlists for your favorite stocks.</li>
</ul>
<p>Try to look for brokerages that put as much of this data in a place that is
easy to access. For example, a brokerage that puts earnings event data in the
watchlist or close to where you make a trade can help you avoid making
mistakes around binary events.</p>
<p>Keep in mind that some brokerages charge extra for detailed research data or
they may charge for real-time quotes (especially real-time options quotes).
(Costs are coming up later in the post.)</p>
<h2 id="trading-inferfaces">Trading inferfaces</h2>
<p>Be sure that the brokerage allows you to trade where you want to trade. If you
can&rsquo;t trade at work on a computer, be sure your brokerage has a good Android
or iPhone application. If you can only use a web browser to trade, be sure
that the broker has a first-class browser-based experience.</p>
<p>Some brokers have full desktop applications that provide the best experience.
Be sure to try out their applications on your computer. Almost every broker
with a desktop client works on Windows, most work on Mac, and a few work on
Linux. Ensure you have a strong enough CPU with enough memory to run the
application well.</p>
<p>For trading, ensure that the broker allows a variety of order types, such as
day, good til canceled (GTC), trailing stop, and order cancels other (OCO).
Day and GTC orders are what I use about 99% of the time, but trailing stops
can be handy for swing trades. If you can do a simultaneous cancel/replace
(similar to being able to edit an active order), that&rsquo;s also a plus.</p>
<p>I enjoy brokers that provide a full confirmation step of the order that allows
me to read my potential order in plain English along with potential risks and
profits. Everyone has made mistakes on an order from time to time, but you
certainly don&rsquo;t want to get caught selling a call when you meant to sell a
put. 😬</p>
<h2 id="contact-options">Contact options</h2>
<p>Most traders won&rsquo;t need to use the customer service team at most brokerages
since these platforms are built for traders to do as many things as possible
on their own. After all, it is expensive to staff a large customer service
team to do things that traders should be able to do by themselves.</p>
<p>Sometimes things do go wrong and that&rsquo;s when things get complicated. Early
assignments on sold options have caught me by surprise in the past and I&rsquo;ve
used customer service agents to ensure I handled the exercise process
carefully.</p>
<p>When you get into a difficult situation and lots of your money is on the line,
do you want someone you can call immediately? Is an email ticketing system
enough for you? How about live chat as an in between option? Decide what is
important to you and choose a broker with the right customer service options.</p>
<p>Live chat is acceptable for me but I prefer a toll-free number to call.</p>
<h2 id="moving-money">Moving money</h2>
<p>Look for brokers that make it easy to connect a bank account for deposits and
withdrawals. Some brokers will give you a portion of your deposit to use while
the remainder clears. Some may give you all of your deposit to trade
immediately, but you can only buy stock with it until it clears.</p>
<p>Read the fine print for your broker to ensure you know how long it takes for
deposits and withdrawals to clear. Be sure to read about any potential fees
for moving money. If you have more than one account with the broker, find out
what is involved with moving money (and stocks) between your accounts.</p>
<h2 id="trade-execution">Trade execution</h2>
<p>Find out if the broker sends orders directly to the exchange or through a
third party. Third parties can reduce cost, but they can sometimes add delays
to trades or cause them to be executed at prices which are not the best.</p>
<p>When you are buying and selling stocks with limit orders, the difference here
could be a few pennies or less. However, if you&rsquo;re buying stocks in groups of
100 or more, those pennies add up quickly. For options, a $0.05 price
difference is $5 worth of profit (or loss)!</p>
<p>Poor trade execution can cost a whole lot more than the fees from a brokerage
with better execution. 💸</p>
<h2 id="cost">Cost</h2>
<p>I saved the discussion of cost for last because there are a lot of factors
involved here. Many low cost (or no cost) brokers are a great deal for certain
trades. Others can be terrible. Lower cost doesn&rsquo;t always make sense.</p>
<p>When I analyze a brokerage&rsquo;s costs, I ask questions like these:</p>
<ul>
<li>Can I get detailed company research for free?</li>
<li>How much does it cost to get real-time quotes for stocks and options?</li>
<li>What is the cost per equity or option trade?</li>
<li>If I get assigned stock on a sold option, what fees are involved?</li>
<li>What fees to I pay to exercise an option?</li>
<li>Will my costs go down as I make more trades?</li>
</ul>
<p>Some brokerages have special fee structures where you can avoid certain fees
based on <em>how</em> you trade. As example, Fidelity does not charge me an options
fee if I buy back a sold option for $0.65 or less. If I sell an option for
$1.50, I enter a buy order for $0.75 most of the time (to collect 50% profit).
However, I can save about $0.70 per trade if I bump that buy order down to
$0.65. <em>Read all of the fine print!</em> 🤓</p>
<h2 id="my-experience-with-brokerages">My experience with brokerages</h2>
<p>I&rsquo;ve tried quite a few brokerages as I&rsquo;ve learned to trade options and here
are my thoughts for each with my most preferred brokerages at the top of the
list. I currently use TD for almost all of my trades and I use Fidelity to
trade options in my HSA.</p>
<h3 id="td-ameritrade--thinkorswim">TD Ameritrade / ThinkOrSwim</h3>
<p>TD checks a lot of boxes for me. The full ThinkOrSwim desktop client works
solidly on Linux, their Android applications are easy to use, and their web
interfaces are straightforward. Options trades are $0.65 each, but the direct
trade execution is totally worth it. There&rsquo;s no fee for being assigned.</p>
<p>The desktop application is <strong>full</strong> of data and it is <strong>completely and utterly
overwhelming</strong> at first. It takes time to learn the system and where you can
find all of the things you need. Over time, it becomes much easier to find
information and make trades. The charting is incredibly detailed and quick to
render.</p>
<p>The trade confirmation process at TD is really good and I&rsquo;ve caught some
mistakes in the confirmation process. Real-time quotes are included for free
and the updates are <em>fast</em>. You can configure the update rate to make it less
dramatic.</p>
<p>TD and ThinkOrSwim give you a one-stop shop. You can do all of your trading,
screening, and research all in one place. Their customer service is a
toll-free phone call away or you can use the live chat that&rsquo;s built into the
ThinkOrSwim application.</p>
<h3 id="tastyworks">Tastyworks</h3>
<p>If your goal is to purely trade options, Tastyworks is a great platform. You
can buy stocks, too, but that&rsquo;s not their top priority. Their order entry
process for options (buying and selling) is superb and I&rsquo;ve never found one
that I liked better.</p>
<p>Once you place your trade, you can put in a 50% profit order with a couple of
clicks. Tracking your trade&rsquo;s process is done with a handy progress bar.</p>
<p>Trade execution is really fast and the pricing structure is interesting. You
pay $1 per option trade to open with a $10 maximum fee. Closing the trade is
free. If you get assigned, there&rsquo;s a $5 fee.</p>
<p>The real downside about Tastyworks is the mobile experience. The Android
application is really difficult to use and everything oddly abbreviated. One
could argue that you shouldn&rsquo;t do much of your trading from a mobile device,
but I do like to adjust trades on the go from my phone.</p>
<p>Another complaint I have is that transferring money or equities between
multiple accounts must be done with signed paper documents. It&rsquo;s not possible
to do the process online. With other brokers, such as TD or Fidelity, you can
do this instantly via self-service processes on their websites.</p>
<p>You will need to do your research somewhere else. Tastyworks has charting and
tracking of earnings dates, but that&rsquo;s about it.</p>
<h3 id="fidelity">Fidelity</h3>
<p>Fidelity has full desktop client, but it does not run on Linux, so I haven&rsquo;t
used it yet. Their website works well, although it is pretty basic.</p>
<p>Their trade execution is quite fast and I often find that they improve my
limit order price by $1-3 on each option contract. The fees per option trade
are around $0.69 and that&rsquo;s fairly close to the industry standard $0.65.</p>
<p>There are some research and charting tools on Fidelity&rsquo;s website, but I prefer
to do my research elsewhere.</p>
<h3 id="robinhood">Robinhood</h3>
<p>Say what you will about Robinhood, but it&rsquo;s an all around decent brokerage.
You can get extra research data, real-time quotes, and margin for just $5 per
month. Stock trades and options trades are free (but there are some small
$0.01-$0.02 clearing fees for some trades). Even OTC trades, such as NTDOY,
are free.</p>
<p>My main complaint about Robinhood is around trade execution. I usually set my
limit orders on options trades near the mark price (halfway between bid and
ask). I often have a difficult time getting an option trade to fill unless I
adjust the price close to the bid. This isn&rsquo;t a problem on TD or Fidelity.
Spending $0.65 for TD&rsquo;s better execution makes sense when I&rsquo;m losing $1-2 per
trade on Robinhood due to bad fills.</p>
<p>Robinhood&rsquo;s customer service has always been superb, but it&rsquo;s only available
via email. I was sweating through an assignment on a put credit spread one
Saturday morning and I was able to get a reply in a few hours with the right
advice.</p>
<p>Their Android and website interfaces are superb and fast. When trading gets
really busy, the website often lags and it can be difficult to enter trades.
I&rsquo;ve found it difficult to get trades done during the first 15-30 minutes of
the day. In some situations, my trade has already executed but the website
doesn&rsquo;t show it.</p>
<h3 id="webull">Webull</h3>
<p>Webull is another low/no-cost brokerage. Their Android and website interfaces
provide some great ways to get research donw fast. They present a ton of
information in a small amount of space that would normally require a lot of
legwork at other brokerages. I often use my Android Webull app for quick
research and charting.</p>
<p>As with Robinhood, trade execution is a challenge. Also, I got into a few
situations where I couldn&rsquo;t enter a GTC order for options trades.</p>
<p>You can&rsquo;t queue orders outside of certain hours either. Sometimes I would want
to adjust an order right before going to sleep but I wasn&rsquo;t allowed to change
my queued order until the next morning just before the market&rsquo;s open.</p>
<p><em>Photo credit: <a href="https://unsplash.com/@edanco">Edan Cohen on Unsplash</a></em></p>
]]></content></item><item><title>Choosing options to sell</title><link>https://major.io/2021/01/23/choosing-options-to-sell/</link><pubDate>Sat, 23 Jan 2021 00:00:00 +0000</pubDate><guid>https://major.io/2021/01/23/choosing-options-to-sell/</guid><description>🤔 This is another post in a set of posts on options trading. If you are new to options trading, you may want to start with some of my earlier posts.
You know your terminology, you know your max loss, and you are ready to start. Now you&amp;rsquo;re faced with the difficult question all new options traders face:
How do I choose which option to sell?
At first, you learn to look for the trades that are right for you.</description><content type="html"><![CDATA[<p>🤔 <em>This is another post in a set of posts on <a href="/tags/options/">options trading</a>. If you are
new to options trading, you may want to start with some of my earlier posts.</em></p>
<figure>
    <img src="/images/2020-01-23-paragliding-mountains.jpg"
         alt="Paragliding in the mountains"/> 
</figure>

<p>You know your terminology, you know your max loss, and you are ready to start.
Now you&rsquo;re faced with the difficult question all new options traders face:</p>
<blockquote>
<p>How do I choose which option to sell?</p>
</blockquote>
<p>At first, you learn to look for the trades that are right for you. This post
explains some of the things I look for when I make a trade, and how I compare
different trades to see which one is right for me.</p>
<h2 id="determine-how-much-you-want-to-risk">Determine how much you want to risk</h2>
<p>If you&rsquo;re starting the wheel strategy, then you are selling a put contract on
a stock at a particular strike price. Options contracts involve 100 shares of
the underlying stock, so you can multiply the strike price by 100 to know how
much money you are risking on the trade.</p>
<p>For example, if you sell a $90 put option on AMD, then you are risking $9,000
on the trade. The chances of AMD rocketing to $0 is extremely unlikely, but
anything is possible.</p>
<p>Compare the size of your trade to the size of your account. Risking $9,000 in
a $10,000 account may not be the best idea for new traders since you are
putting 90% of your capital at risk. The goal is to avoid &ldquo;blowing up&rdquo; &ndash; when
you lose so much that you don&rsquo;t have enough capital left for good trades. 💣</p>
<p>If my conviction on a trade is very strong (I know the company, I know the
stock&rsquo;s personality, and I&rsquo;ve done my homework), then risking 15-20% of my
account on the trade seems reasonable. For companies I know less about, I
won&rsquo;t go past about 5-10% of my account on the trade.</p>
<h2 id="watch-your-stocks-calendar">Watch your stock&rsquo;s calendar</h2>
<p>There a certain events in the lifetime of a stock that can cause wild swings
in price. Selling options around the time of these events creates increased
risk because you really don&rsquo;t know what happens during the event. Some of
these events include:</p>
<ul>
<li>
<p><strong>Dividends.</strong> These are announced far in advance and stocks will often
creep up just before the dividend date and then fall a bit after. If your
option buyer wants the dividend badly, you may get assigned early.</p>
</li>
<li>
<p><strong>Splits.</strong> Stock splits are usually safe for options selling since your
contract is automatically adjusted without you doing anything. However, if a
stock does a significant split, like a 5:1, then a new influx of buyers with
a new buying style may get into the stock. The stock&rsquo;s &ldquo;personality&rdquo; can
change quickly.</p>
</li>
<li>
<p><strong>Product events and announcements.</strong> Launch events, such as Apple or
Google&rsquo;s new phone launches, or Tesla&rsquo;s battery days, can have a huge impact
on the stock price. The stock can fall even if the launch looks spectacular.</p>
</li>
<li>
<p><strong>Earnings.</strong> Definitely watch out for these. Stock prices do some idiotic
things around earnings time. <em>I avoid these in almost all conditions unless I
have a lot of conviction about the stock.</em></p>
</li>
</ul>
<p>I&rsquo;ve written about earnings in previous <a href="/tags/options/">options trading</a> post, but it bears
repeating: earnings are unpredictable. Even if the earnings report is stellar,
you may see the price fall off drastically. Why? Perhaps investors want to
take profit. Perhaps investors were expecting more on earnings and the
expectations were already &ldquo;priced in&rdquo;.</p>
<p>Don&rsquo;t sell options around earnings unless you know what you&rsquo;re doing. Even if
you think you know what you&rsquo;re doing, you probably don&rsquo;t. 😉</p>
<h2 id="choosing-a-strike-price">Choosing a strike price</h2>
<p>Once you know the stocks that fit your trading style, it&rsquo;s now time to choose
your strike price. IThere&rsquo;s a critical options calculation to know here:
delta.</p>
<p>Delta runs from 0 to 1 and some software represents it as a percentage. It
describes how much the option price moves as the underlying price moves. Here
are some examples:</p>
<ul>
<li><strong>1.00 delta (or 100%):</strong> The option price moves at the same rate as the
underlying price. If the stock price goes up $10, the option price goes up
$10.</li>
<li><strong>0.50 delta (or 50%):</strong> The option price moves at half the rate of the
underlying stock. If the stock goes up $10, the option price goes up $5.</li>
<li><strong>0.25 delta (or 25%):</strong> If the stock goes up $10, the option price goes up
$2.50.</li>
</ul>
<p>Most of my puts are sold near the 0.25 delta mark. This means that there is
roughly a 75% chance that my put will finish out of the money (I keep the
premium and there is no loss). There is a 25% chance that my put finishes in
the money and I will be assigned stock (possibly at a loss).</p>
<p>We could spend all afternoon talking about delta and theta, but a good start
is to sell your puts somewhere near 0.25 delta. I will sometimes move towards
0.30-0.35 delta if I feel very bullish about a stock or I will move towards
0.20 delta if my conviction is less strong.</p>
<h2 id="calculate-your-return">Calculate your return</h2>
<p>We wouldn&rsquo;t sell options if we didn&rsquo;t expect a return! It&rsquo;s a good idea to
know your potential return for any trade you make. Let&rsquo;s take an example trade
and calculate our maximum loss and potential return.</p>
<ul>
<li>AMD is trading at $92.79.</li>
<li>You choose to sell a put at the $85 strike (.26 delta) that gives $2.42
premium and expires 2021-02-19.</li>
<li>Maximum loss is <code>strike price - premium received</code>. That&rsquo;s <code>8500 - 242 = $8,258</code> in the absolute worst case <em><strong>the world is ending</strong></em> scenario.</li>
<li>Your breakeven point is $82.58. As long as AMD stays above that price, you
make money.</li>
</ul>
<p>You can calculate your return with: <code>bid / (strike - bid) * 100</code>. For our
trade, that&rsquo;s: <code>2.42 / (85 - 2.42) * 100 = 2.9%</code>. If AMD stays over $85
through the life of the contract, you get a 2.9% return.</p>
<p>There&rsquo;s another angle we can use to analyze the trade: an annualized return.
Annualized returns consider how long you had to tie up your capital on a trade
while you wait for your return. Would you rather make a 2.9% return in one
week or one year? I&rsquo;d much rather make it in a week.</p>
<p>The annualized return calculation extends the calculation we&rsquo;ve already done
above: <code>(potential return / days held) * 365</code>. We know we have a 2.9%
potential return and 28 days to expiration, so we can calculate the annualized
return: <code>(2.9 / 28) * 365 = 37.8%</code></p>
<p>If you did this trade successfully over and over again all year long, you
could possibly get a 37.8% return at the end of the year on these trades.
Don&rsquo;t take this as a given, though. I usually use this to compare different
trades against each other to see which one is a better use of my money.</p>
<h2 id="comparing-trades">Comparing trades</h2>
<p>Let&rsquo;s say you like the AMD trade from the previous section, but you&rsquo;re also
looking at another stock that you really like. How do you choose which one to
sell? I usually consider the annualized return. If I can make more money with
the same amount of capital, I&rsquo;ll go for that trade.</p>
<p>FUBO is another stock I follow and I have strong conviction on it as well.
Here&rsquo;s a trade there:</p>
<ul>
<li>FUBO is trading at $37.72.</li>
<li>You choose to sell three puts at the $30 strike (.22 delta) that gives $2.13
premium each and expires 2021-02-19.</li>
<li>Maximum loss is <code>strike price - premium received</code>. That&rsquo;s <code>3000 - 213 = $2,787</code> Since we are selling three contracts, max loss is $8,361.</li>
<li>Your breakeven point is $27.87. As long as FUBO stays above that price, you
make money.</li>
<li>Return: <code>2.13 / (30 - 2.13) * 100 = 7.6%</code></li>
<li>Annualized return: <code>(7.6 / 28) * 365 = 99%</code></li>
</ul>
<p>The FUBO put has a much higher annualized return, which gets my attention.
However, FUBO is riskier for me since the stock has a much shorter history and
it is much more volatile. Sure, I can make more money with this trade, but the
risk is substantially higher.</p>
<p>After checking the calendars, I found that AMD has earnings in the next week!
That breaks my trading rules since I avoid earnings under almost all
conditions!</p>
<h2 id="making-the-trade">Making the trade</h2>
<p>Every stock and options trade has a bid/ask spread. Buyers say &ldquo;I will pay
$1.00 for this trade&rdquo; and that&rsquo;s the bid. Sellers say &ldquo;I will only sell if
someone pays me $2.00&rdquo; and that&rsquo;s the ask.</p>
<p>Under most situations, I sell at the midpoint of the bid/ask spread. This
means that on a spread of 1.00-1.10, I will sell at 1.05. The trade executes
within 30 seconds unless the stock price is moving quickly.</p>
<p>If the stock is moving fast or if you really want to be sure your trade gets
in, sell at the bid price. That almost always guarantees that a buyer is
waiting to buy your contract as soon as you write it.</p>
<p>You can sell at the ask or even higher if you really want to, but your
execution could be delayed or it may never execute.</p>
<p>Congratulations! You sold your first option. What do you do now? <em>(Keep
reading.)</em></p>
<h2 id="after-the-trade">After the trade</h2>
<p>Your trade may turn red immediately after it executes: don&rsquo;t panic! Most
brokers show the midpoint price (or &ldquo;mark&rdquo;) and if you sold at the bid price,
your trade will be red at first. Also, if the stock price dipped a little,
your trade will be red for a short while. Take a deep breath.</p>
<p>One of the first things I do (after <a href="https://thetagang.com/mhayden">logging my trades on thetagang.com</a>), is
to put in a buy order at a 50% gain. Consider our AMD trade earlier ($85 put
with $2.42 premium). I would enter a buy order for $1.21 with &ldquo;good til
canceled&rdquo; (GTC). There&rsquo;s no more work to do! As soon as I can get a 50% gain,
the trade executes and I keep my $121 profit!</p>
<p>Why stop at 50%? I have several reasons:</p>
<ul>
<li>It reduces my chance of loss because my money is at risk for less time.</li>
<li>Emotion is removed from the trade. My trade is closed the moment 50% gains
are reached.</li>
<li>I have work and kids. I don&rsquo;t have time to hover over my broker interface
all day long.</li>
<li>It almost always leads to a better annualized return.</li>
</ul>
<p>A better return with a smaller profit? How can that be? 🤔</p>
<p>Let&rsquo;s <a href="https://thetagang.com/mhayden/4378b667-d325-4b82-92ca-138f340acacb">take an example from a PLTR trade I did last month</a>. I sold two $25
puts for $2.04 premium each that had a 128% annualized return. However, the
50% trigger closed the trade in two days when PLTR rocketed upward
unexpectedly.</p>
<p>If I held the trade from January 20 to February 12 and PLTR finished above
$25, I would have made $408 (128% annualized return). Instead, I made $204 in
two days (Jan 20-22). What&rsquo;s our annualized return here?</p>
<ul>
<li>Potential return: <code>2.04 / (25 - 2.04) * 100 = 8.9%</code></li>
<li>Annualized return: <code>8.9 / 2 * 365 = 1,624%</code></li>
</ul>
<p>Wow! 🤯</p>
<p>That&rsquo;s 1,624% versus our original 128% annualized return and my money was only
at risk for two days. What if PLTR suddenly drops next week? I avoid that risk
by collecting profit early and freeing up capital for another trade.</p>
<p>One big part of risk management in the market is to keep your capital at risk
for the shortest amount of time possible to make the returns you want.</p>
<p>Good luck! 🍀</p>
<p><em>Photo credit: <a href="https://unsplash.com/@tomas_nz">Tomas Sobek on Unsplash</a></em></p>
]]></content></item><item><title>Lessons learned from selling puts</title><link>https://major.io/2021/01/04/lessons-learned-from-selling-puts/</link><pubDate>Mon, 04 Jan 2021 00:00:00 +0000</pubDate><guid>https://major.io/2021/01/04/lessons-learned-from-selling-puts/</guid><description>🤔 This is another post in a set of posts on options trading on the blog. If you are confused on terminology, go back and start with the first post and the series and work your way to this one.
My options trading journey started back in September 2020 and I learned a lot in a short amount of time. This post covers some of the lessons I&amp;rsquo;ve learned along the way.</description><content type="html"><![CDATA[<p>🤔 <em>This is another post in a set of posts on <a href="/tags/options/">options trading</a> on the blog.
If you are confused on terminology, go back and start with the first post and
the series and work your way to this one.</em></p>
<figure>
    <img src="/images/2021-01-04-dark-road-mountains.jpg"
         alt="Dark Road with Mountains"/> 
</figure>

<p>My options trading journey started back in September 2020 and I learned a lot
in a short amount of time. This post covers some of the lessons I&rsquo;ve learned
along the way.</p>
<p>Edwin Lefèvre says it best as Larry Livingston in <a href="https://en.wikipedia.org/wiki/Reminiscences_of_a_Stock_Operator">Reminiscences of a Stock
Operator</a>:</p>
<blockquote>
<p>Whenever I have lost money in the stock market I have always considered that
I have learned something; that if I have lost money I have gained
experience, so that the money really went for a tuition fee.</p>
</blockquote>
<h2 id="make-a-set-of-rules-and-stick-to-them">Make a set of rules and stick to them</h2>
<p>This was one I learned early on from <a href="https://twitter.com/realthetagang">Joonie</a>, the leader of the Theta Gang.
<a href="https://rss.com/podcasts/thetagang/">His podcasts</a> covered this topic frequently and the importance of this step
cannot be understated. Everyone needs a strong set of rules when making trades
so that emotion can be removed from trades.</p>
<p>Emotion easily sneaks in and clouds your judgement whether your investments
are green or red. When emotion takes over full control, you become &ldquo;tilted&rdquo;
(as Joonie says), and you make poor choices. You can quickly over-leverage
yourself and ruin a winning position. You can also throw good money after bad
and make your losing positions worse.</p>
<p>Start by building a set of rules that you can follow, or better yet, add to a
screener (more on that next). Here is my rule list as of today for selling
puts:</p>
<ul>
<li>
<p>The underlying stock must be a stock I would enjoy holding for an extended
period (potentially weeks or months).</p>
</li>
<li>
<p>The underlying stock must be priced higher than $10.</p>
</li>
<li>
<p>Choose a trade between -.20 to -.30 delta on a monthly expiration date (no
weeklies). This is roughly 70-80% chance of profit.</p>
</li>
<li>
<p>Make trades on an underlying that is moving sideways or has a long upward
trend above the 50 day exponential moving average (EMA), but avoid any
underlying stock with spikes or gaps up that aren&rsquo;t explained by solid news.
<em>(This avoid pump and dumps or other manipulative patterns.)</em></p>
</li>
<li>
<p>The monthly expiration should be between 21-60 days away from the current
date.</p>
</li>
<li>
<p>A trade should have an annualized return over 20%.</p>
</li>
<li>
<p>No earnings reports or other news should be scheduled before expiration.
<em>(Earnings are dangerous and unpredictable, even if you somehow get an
advanced copy of the filing.)</em></p>
</li>
</ul>
<p>By following your rules closely, you avoid making trades that you regret.
However, market conditions could lead you to bend one of these rules. For
example, if the market is fairly steady and the underlying has been moving
sideways for a while, I may move closer to -.30 to -.40 delta (about 60-70%
chance of profit).</p>
<h2 id="know-exactly-when-you-will-exit-the-trade">Know exactly when you will exit the trade</h2>
<p>This one is separate from the rules list above because it is important all by
itself. When you enter any trade in the market, have an <em>exact</em> exit strategy
in mind.</p>
<p>For my trades, I always exit when I have reached a 50% profit. If I sell a $90
put on AMD and get $2.50 in premium, I immediately enter an order to buy it
back at $1.25. This takes all emotion out of the trade and it ensures that I
won&rsquo;t miss out on profits if I am away from the computer. It&rsquo;s a great feeling
to suddenly get a notification from your broker that you made money when you
least expect it. 🤗</p>
<p>Always remember: profit is profit. I may make $1.25 on that trade while
someone else makes $2.50, but my capital is freed up earlier for other trades
and my profit is secured. I&rsquo;d take a 50% gain over a loss of any size.</p>
<h2 id="wait-on-trade-ideas-to-come-to-you">Wait on trade ideas to come to you</h2>
<p>Sometimes the best trade is not to trade at all. My trading rules are fairly
easy to pack into a scanner and that&rsquo;s usually how I research my trades. I
scan for options on <a href="https://barchart.com/">Barchart</a> and I will occasionally use <a href="https://finviz.com/">finviz</a> to find
new stocks that should be on my radar.</p>
<p>I get an email from Barchart about an hour after the market opens with my list
of options that meet my criteria. From there, I decide on which ones to trade
and which ones to skip. If there&rsquo;s something good in the list, I&rsquo;ll make a
trade. Otherwise I&rsquo;ll wait for the conditions to line up with my investment
goals and rules.</p>
<h2 id="trade-outside-the-crazy-market-hours">Trade outside the crazy market hours</h2>
<p>I avoid trading during the first hour the market is open (9:30-10:30AM
Eastern) and the last hour (3:00-4:00PM Eastern). The trading volume is really
high around these times and it can be difficult to figure out what a stock is
doing during those times. Day traders and swing traders are extremely active
during this time.</p>
<h2 id="be-patient-with-limit-orders">Be patient with limit orders</h2>
<p>Always use a limit order when selling options and be patient with them. For
example, if the bid/ask spread is $1.00 to $1.10 and you decide to set your
limit order to $1.05, be patient. Many people will rush to lower the limit
when the trade does not execute immediately, but you should stick with your
order.</p>
<p>Selling puts on volatile stocks allows you to collect premium, but volatile
stocks have volatile options, too. I usually set my limit orders right in the
middle of the bid/ask spread and wait. About 90% of the time, the order
executes within a few minutes because the stock price is volatile.</p>
<h2 id="trade-where-other-people-are-trading">Trade where other people are trading</h2>
<p>Be sure to find out where the <a href="https://www.barchart.com/options/most-active/stocks?viewName=main">most active options</a> are being traded in the
market. Options volume helps you enter trades quickly at good prices. It also
helps you exit when it&rsquo;s time to take a profit. Stocks with low volume options
trading can provide good premiums, but it can be challenging to exit the trade
when you&rsquo;re ready to capture profit or limit your loss.</p>
<p>Use care when you follow <a href="https://www.barchart.com/options/unusual-activity/stocks">unusual options activity</a> reports. It can be
exciting to jump in on trades when you see lots of money pouring into puts and
calls.</p>
<p>However, many of these big options trades are hedges from larger firms who
want to avoid losses or capture extra gains for their clients. There&rsquo;s also
some market manipulations strategies here where people buy tons of calls on a
stock in the hopes that market makers will buy lots of shares.</p>
<h2 id="scrutinize-skyrocketing-stocks">Scrutinize skyrocketing stocks</h2>
<p>Sometimes you&rsquo;ll see a stock that has traded sideways or posted small gains
day after day and then it suddenly shoots straight up (often called
&ldquo;mooning&rdquo;). These look like great targets for selling puts at first, but you
should be cautious.</p>
<p>Stocks sometimes do this when big news comes out about a company. For example,
if a small semiconductor company makes a deal with Apple to put chips in new
laptops, there&rsquo;s a good chance that the small semiconductor stock will moon
wildly. The market does this because the company&rsquo;s valuation is now in flux.
Is this company&rsquo;s valuation now 50% more? Double? Quadruple? Be careful until
the market decides on the new valuation.</p>
<p>You may also see situations where stocks go through the roof and there is no
news, no big insider trading, and no significant industry news. This is where
you should be <strong>extremely cautious</strong>. Prices often do this when the stock is
being manipulated or when activist investors are at work.</p>
<p>The worst case is that the stock is headed into a &ldquo;pump and dump&rdquo; scheme where
shares are rapidly being bought in the hopes that other investors will buy it
up thinking that some news is about to come out (the &ldquo;pump&rdquo;). Once a lot of
new investors pile on, the group buying the shares stops and sells all their
shares (the &ldquo;dump&rdquo;).</p>
<p>The dump side often involves investors called &ldquo;shorts&rdquo; who short the stock and
cause the price to go down further. This is a dangerous move for shorts if the
buyers keep buying as this could lead the price to skyrocket again and force
shorts out of their shares.</p>
<h2 id="avoid-trading-around-earnings">Avoid trading around earnings</h2>
<p>Earnings are some of the wildest times in the market and they&rsquo;re incredibly
hard to predict. I&rsquo;ve seen companies post excellent earnings reports with
glowing numbers, great sales, and excellent predictions. As soon as the
earnings come out, the stock falls 20%. 🤯</p>
<p>Keep in mind that valuation is a tricky thing and that many investors won&rsquo;t
agree on a valuation for a particular stock. A great example is that one of
the analysts following Tesla raised the price target from $90 to $105. Tesla
is trading at over $700 today. Again, valuation is in the eye of the beholder.</p>
<p>I&rsquo;ve also seen companies post terrible earnings reports and their stock
remains flat or goes up. There&rsquo;s a chance that investors already predicted the
bad results and they&rsquo;re priced in already.</p>
<p>Something that may look good at the moment may turn out awful later. For
example, if a company releases earnings after the market close, they may only
release a PDF after the market closes that includes their SEC filing data.
That data may look fantastic and the stock moons immediately. Later, when the
company has their conference call, they announce they&rsquo;re acquiring another
company and they are revising future estimates down by 20%. The stock prices
falls through the floor.</p>
<p>Even if you had an advanced copy of a company&rsquo;s earnings filing, it would
still be incredibly difficult for you to make a trade that will make a profit
once the market is closed.</p>
<h2 id="track-your-trades">Track your trades</h2>
<p>Keep yourself honest by tracking your trades. I track mine on <a href="https://thetagang.com/mhayden">thetagang.com</a>
and it&rsquo;s a free way to analyze your trading strategy. You can find other
people trading the same stocks and ask them questions. There is also a list of
trending stocks that is updated frequently and this can help you build your
own watchlist.</p>
<p>You can learn a lot from reading notes from other traders about their trades.
I encourage you to leave good notes as well since this tests your conviction
on the trade. If you&rsquo;re not confident enough to explain to other people why
you made the trade, then why make it in the first place?</p>
<p>There are a litany of spreadsheets out there for tracking trades as well, but
the best one I found is the <a href="https://www.twoinvesting.com/2016/10/options-tracker-spreadsheet/">Options Tracker Spreadsheet from 2 Investing</a>. It
pulls stock quotes directly from Google Finance and it calculates helpful
metrics, including annual return metrics.</p>
<h2 id="like-water-off-a-ducks-back">Like water off a duck&rsquo;s back</h2>
<p>When you win, take time to understand what worked in your favor so you can
repeat it.</p>
<p>When you lose, take time to understand what went wrong and what concrete
things you plan to change.</p>
<p>I was up almost $3,300 at the end of 2020 and I chased a skyrocketing stock,
FUBO, much longer than I should have. It mooned without much news and I kept
chasing it. That led to a loss of over $5,000 and my end of year finish was
negative.</p>
<p>My mistake was that I had so many winning trades back to back that I got
&ldquo;tilted&rdquo; and violated my rules. Initially, everything looked good, but once it
turned after hours, there was nothing I could do. I continued to hold and
hoped that the conditions would change, but nothing changed. The loss stopped
the bleeding and I would have easily lost $2,000 more if I had not exited when
I did.</p>
<p>After this failure, I went back to my list of rules and made them more strict.
I&rsquo;m also working on a script that allows me to maintain a watchlist and let
quality trades filter through that match all of my rules. I plan to put the
script on GitHub soon once it works. I also shared my failure with other
people and told them what I thought I did wrong.</p>
<p>The loss still hurts, but I&rsquo;m trading again to make up the loss. My goal is
still to donate a percentage of my gains to charity, so I keep that goal in
mind. The key is to stay in the game.</p>
<p><em>Photo credit: <a href="https://unsplash.com/@artem_kniaz">Artem Kniaz on Unsplash</a></em></p>
]]></content></item><item><title>Know your max loss</title><link>https://major.io/2020/12/09/know-your-max-loss/</link><pubDate>Wed, 09 Dec 2020 00:00:00 +0000</pubDate><guid>https://major.io/2020/12/09/know-your-max-loss/</guid><description>This is the third post in a series of posts on options trading. You can read this one out of order since it applies to almost everything you do in the stock market, including buying shares of stock.
What is &amp;ldquo;max loss&amp;rdquo; The maximum loss on a trade is the worst outcome possible. Knowing this number helps you protect your account from huge losses and avoids terrible situations such as margin calls.</description><content type="html"><![CDATA[<figure>
    <img src="/images/2020-12-09-subway-tunnel.jpg"
         alt="Subway Tunnel"/> 
</figure>

<p>This is the third post in a <a href="/tags/options/">series of posts on options trading</a>. You can read
this one out of order since it applies to almost everything you do in the
stock market, including buying shares of stock.</p>
<h2 id="what-is-max-loss">What is &ldquo;max loss&rdquo;</h2>
<p>The maximum loss on a trade is the worst outcome possible. Knowing this number
helps you protect your account from huge losses and avoids terrible situations
such as margin calls. <em>(This is when your broker is forced to arbitrarily sell
off things in your account or demand money from you to get your account above
a zero balance.)</em></p>
<p>When I first started learning about options trading, many people told me it
was too dangerous. They told me &ldquo;just buy shares since it&rsquo;s safer.&rdquo; If you
begin analyzing maximum loss, you quickly realize that buying shares can be
more risky than certain types of options.</p>
<h2 id="maximum-loss-when-buying-shares">Maximum loss when buying shares</h2>
<p>As an example, let&rsquo;s say you own 100 shares of Wal-Mart (WMT). It&rsquo;s trading
around $148 today, so that means you have $14,800 in total equity. Your
maximum loss would occur if the stock fell to zero and your total loss would
be $14,800.</p>
<p>A major retailer losing 100% of its value in a very short period of time is
highly unlikely, but it&rsquo;s possible. There&rsquo;s nothing that prevents it from
happening (other than the stock being halted in the market), and there is a
chance that you could lose your entire investment.</p>
<p>It took Enron about a year to drop to nearly zero, but some other drops have
been much more abrupt. According to <a href="https://www.investopedia.com/investing/biggest-singleday-market-cap-drops-us-stocks/">Investopedia</a>, some stocks have had some
rough days:</p>
<ul>
<li>Facebook lost $119B (2018-07-26)</li>
<li>Intel lost $90B (2000-09-22)</li>
<li>MSFT lost $80B (2000-04-03)</li>
<li>AAPL lost $60B (2013-01-24)</li>
</ul>
<p>Buying shares is a great way to start in investing for your future, but it&rsquo;s
important to know your maximum amount of loss and ensure you&rsquo;re prepared to
hold the stock through the down times.</p>
<h2 id="maximum-loss-when-buying-options">Maximum loss when buying options</h2>
<p>When you buy options, your maximum loss is the amount of premium you paid for
the option. If you pay $200 for a call on a stock, your max loss is $200. The
same goes for puts.</p>
<p>The maximum loss scenario for bought options is when the option expires <em>out
of the money</em>. For a call, this means the stock price was under your strike
price at the expiration time. For a put, the stock price would need to be
above your strike price.</p>
<h2 id="maximum-loss-when-selling-a-put">Maximum loss when selling a put</h2>
<p>There are two types of puts:</p>
<ul>
<li><em>cash secured puts</em> - You sell a put when you have cash to to cover the
strike price.</li>
<li><em>naked puts</em> - You sell a put without having cash to cover the strike price.</li>
</ul>
<p>Some of this was covered in the <a href="/2020/12/07/the-dark-side-selling-options/">The Dark Side: Selling Options</a> post, but
it&rsquo;s worth talking about it here again. When you sell a put, you are obligated
to purchase the stock at the strike price.</p>
<p>The max loss scenario here happens when the stock price falls to zero. Sure,
that is unlikely, but it is possible. You will keep your premium from selling
the put, however.</p>
<p>An important thing to remember is that your options are <em>capped</em>. If you sell
a put at a $100 strike price and collect $200 in premium, your losses are
capped at $9,800. The stock can&rsquo;t go below zero (thankfully).</p>
<p>With a cash secured put, you have cash on hand to deal with the loss and
purchase the stock. However, with a naked put, you don&rsquo;t have cash to cover
the loss. Your broker will issue a margin call and begin selling off other
equities in your account to cover the loss. If that fails, they will demand
that you deposit money to cover the loss.</p>
<p>💣 <strong>Never sell naked puts. Ever. Seriously.</strong></p>
<h2 id="maximum-loss-when-selling-a-call">Maximum loss when selling a call</h2>
<p>Here&rsquo;s where things can become horrifying. When you sell a call, there are two
main types:</p>
<ul>
<li><em>covered call</em> - You own 100 shares of the stock and use them as collateral
to sell a call option.</li>
<li><em>naked call</em> - You sell a call for a stock but you don&rsquo;t own any shares.</li>
</ul>
<p>Let&rsquo;s work with the covered call example first. Let&rsquo;s say you own 100 shares
of XYZ and that stock is trading at $100 per share. You think the stock might
go up a little but not a lot, so you sell a call at the $105 strike price.</p>
<p>The best outcome would be for the stock to stay between $100-$105 so you don&rsquo;t
lose money on the shares you own and your call is not exercised. There&rsquo;s no
losses there.</p>
<p>What if the stock flew to $125? You get to keep the premium for selling your
call option, but you have to sell those shares for $105 (since that was your
strike price). Sure, you did lose $20 per share of profit ($2,000 total), but
you still made money. You received the premium and your stock gained $5 per
share ($500). You just missed out on some of the gains.</p>
<p>What if the stock falls to $50? You still keep your premium, but your $10,000
of equity (100 shares x $100) is now $5,000 of equity. You took a $5,000 loss.
It&rsquo;s the same loss you would have had if you bought shares and didn&rsquo;t sell any
options. However, you are getting the small benefit of the premium you
received when you sold the call option.</p>
<p>In all of these scenarios, your losses are <em>capped</em>. You know what you can
possibly lose, but you know the limit of the loss.</p>
<h2 id="never-sell-naked-calls">Never sell naked calls</h2>
<p><strong>This loss is potentially so bad that it needs its own section.</strong> Covered
calls are nice because your collateral appreciates in value to cap your upside
losses. What if you didn&rsquo;t own the stock?</p>
<p>Let&rsquo;s say you sold a call on XYZ for $105 again and it was trading at $100 per
share. However, you don&rsquo;t own any shares this time.</p>
<p>If the stock climbs to $104.99, your option expires out of the money and you
keep your premium. Nice!</p>
<p>If the stock falls to $50, your option expires out of the money and you keep
your premium. Nice!</p>
<p>If the stock climbs to $125, you have a problem. You agreed to sell 100 shares
to the buyer for $105 per share. You now have to buy 100 shares at $125 each
(since that&rsquo;s the current stock price) and you have to sell them for $105
each. That&rsquo;s a $2,000 loss right there.</p>
<p>However, let&rsquo;s say that XYZ is a biotechnology firm and they report that they
have a treatment for almost any cancer in the human body with a 14 day home
treatment with almost no side effects. This is a life-altering change for
people suffering from cancer. The stock climbs to $450 and investors say it
will go higher. By the time the option expiration hits, the stock is worth
$600.</p>
<p>What now? You sold a call option for $105. That means you have to buy 100
shares of the stock at its current price ($600 each) and sell that stock for
$105 each. The math start to hurt:</p>
<ul>
<li>You purchase 100 shares at $600 each = $60,000.</li>
<li>You sell 100 shares at $105 each = $10,500.</li>
<li>You lose $49,500. 🤯</li>
</ul>
<p>💣 <strong>Never sell naked calls. Ever. Seriously.</strong></p>
<h2 id="know-your-max-loss">Know your max loss</h2>
<p>Before you enter a trade, know what the worst case scenario will bring and
ensure your account is prepared for it.</p>
<p>I entered a trade earlier this year where I had a 85% percent chance of profit
and the trade looked fantastic in my account. I had money to cover the loss
but I doubted I would need it.</p>
<p>The company suddenly dropped news within 48 hours of entering the trade. The
news said that one of their acquisitions was a little slower and more
expensive than they thought and their Q3 number would be revised down. The
numbers were revised down a lot. <strong>My nice gain turned into a $660 loss
that I was forced to look at every day for a month until the expiration.</strong> It
was a daily reminder of two things:</p>
<ol>
<li>Anything can happen in the market.</li>
<li>I was fully prepared for maximum loss.</li>
</ol>
<p>💸</p>
<p><em>Photo credit: <a href="https://unsplash.com/@trapnation">Andre Benz on Unsplash</a></em></p>
]]></content></item><item><title>The Dark Side: Selling Options</title><link>https://major.io/2020/12/07/the-dark-side-selling-options/</link><pubDate>Mon, 07 Dec 2020 00:00:00 +0000</pubDate><guid>https://major.io/2020/12/07/the-dark-side-selling-options/</guid><description>I&amp;rsquo;ve started a series of posts on options trading and if you haven&amp;rsquo;t read yesterdays post, Options trading introduction, you should start there first.
Recap: Buying calls and puts Just to recap the introductory post:
Buying a call gives you the right to buy a stock at a certain strike price Buying a put gives you the right to sell a stock at a certain strike price Buying calls is a great way to capture gains from a stock that is trending upwards an buying puts provides an insurance policy in case your stock drops.</description><content type="html"><![CDATA[<figure>
    <img src="/images/2020-12-07-dark-hills.jpg"
         alt="Dark Hills"/> 
</figure>

<p>I&rsquo;ve started a <a href="/tags/options/">series of posts on options trading</a> and if you haven&rsquo;t read
yesterdays post, <a href="/2020/12/06/options-trading-introduction/">Options trading introduction</a>, you should start there first.</p>
<h2 id="recap-buying-calls-and-puts">Recap: Buying calls and puts</h2>
<p>Just to recap the introductory post:</p>
<ul>
<li>Buying a call gives you the <em>right</em> to buy a stock at a certain strike price</li>
<li>Buying a put gives you the <em>right</em> to sell a stock at a certain strike price</li>
</ul>
<p>Buying calls is a great way to capture gains from a stock that is trending
upwards an buying puts provides an insurance policy in case your stock drops.</p>
<p>You can make money in multiple ways:</p>
<ul>
<li>Exercising a call option to buy stock at a reduced price (without owning the
stock first)</li>
<li>Exercising a put option to sell your stock at a higher price (and reduce
losses on your owned stock)</li>
<li>The stock moves towards (and hopefully, past) your strike price and you can
sell the option to someone else for a profit</li>
</ul>
<p>But there&rsquo;s an entirely other world out there and it involves <strong>selling options</strong>.</p>
<h2 id="rights-versus-obligations">Rights versus Obligations</h2>
<p>So far, we&rsquo;ve only talked about rights. When you buy an options contract, you
have the right to do nothing, exercise the option, or sell it away to someone
else.</p>
<p>Selling options means you <strong>must fulfill an obligation.</strong> We aren&rsquo;t talking
about selling an option you purchased &ndash; that&rsquo;s simply called <em>selling to
close</em>. What we&rsquo;re talking about here is a different concept: <em>selling to
open</em>. This is sometimes called <em>writing a put</em> or <em>writing a call</em>.</p>
<p>When you sell a contract, you are paid a sum of money called a <em>premium</em>. The
buyer is the one paying the premium to you. Let&rsquo;s dig into what&rsquo;s happening
here:</p>
<ul>
<li>You are going out into the market and saying &ldquo;I&rsquo;d like to get $200 to sell
this put that expires on December 31&rdquo;</li>
<li>A buyer can choose to take you up on your offer, give you $200, and the
contract is written</li>
<li><strong>The contract is written in stone until the expiration date</strong></li>
</ul>
<p>It&rsquo;s key to remember here that there are only two ways out of an options
contract that you sell:</p>
<ol>
<li>You buy it back (hopefully at a profit). This is called <em>buying to close</em>.</li>
<li>The option expires.</li>
</ol>
<p>There are three warnings we must cover before we go any further:</p>
<p>💣 <strong>Never sell an options contract that you do not intend to fulfill.</strong> Sure,
it may sound amazing to receive $4,000 in an instant to sell an option, but
eventually that option expires and that can sometimes be a disaster.</p>
<p>💣 <strong>Never assume that you will be able to buy to close your sold contract
before expiration.</strong> The market does weird things and you may be stuck with
holding your option through expiration or taking a loss before expiration.</p>
<p>💣 <strong>Never sell options contracts if you don&rsquo;t have stock or cash to cover
them.</strong> Getting stuck in a margin call (or worse) with your broker is not a
good place to be.</p>
<p>With that said, about 95% of my current trades involve selling options. More
on that later.</p>
<h2 id="selling-puts">Selling puts</h2>
<p>We should talk about selling puts first because that concept is a little
easier. When you sell a put, you specify a few things:</p>
<ul>
<li>Underlying stock ticket (such as AMD, WMT, or TM)</li>
<li>Expiration date</li>
<li>How much premium you want (this is the <em>ask</em> portion of the bid/ask spread)</li>
<li>The strike price</li>
</ul>
<p>Selling puts can make money in a few different ways. The stock can go up, go
sideways (where it hovers around a certain price), or go down a little.</p>
<p>Let&rsquo;s go through an example. You think AMD is likely going to keep its same
price or go up over the next few weeks. If AMD is trading at $100 now, you
decide to sell a put for $90 and collect $200 in premium.</p>
<p>🛑 <strong>Before entering this trade, ensure you have <em>at least</em> $9,000 of cash in
your account.</strong> 100 shares of AMD could potentially cost you $9,000 if you are
assigned &ndash; more on that below.</p>
<p>Now what?</p>
<ul>
<li>AMD skyrockets up to $125
<ul>
<li>Your put loses value rapidly and you can buy it back for only $25 after a
few days. You just made $175 in profit since you originally received $200
for selling the contract and paid $25 to close it.</li>
<li>You can let your put expire and keep the $200 premium. (Risky!)</li>
</ul>
</li>
<li>AMD gets stuck at $100
<ul>
<li>Over time, your put loses value and you can buy it back at a
cheaper rate before it expires (optional).</li>
<li>You can let your put expire and keep the $200 premium. (Risky!)</li>
</ul>
</li>
<li>AMD falls to $85
<ul>
<li>You&rsquo;re now in the money and that&rsquo;s not great for a sold option.</li>
<li>The buyer will likely exercise the option they bought from you and you
will be <em>assigned</em> at a loss. (Keep reading)</li>
</ul>
</li>
</ul>
<h2 id="assignment-on-puts">Assignment on puts</h2>
<p>Some traders hear about <em>assignment</em> and they run off to hide. Assignment
means that the buyer of your contract wants to use their right to exercise.</p>
<p>Let&rsquo;s go back to the very last example above where we sold a put for $90 and
AMD fell to $85. This situation feels scary until you break it down:</p>
<ul>
<li>You keep your $200 no matter what. You sold the contract and that&rsquo;s yours.</li>
<li>You must buy 100 shares of AMD at $90 each.</li>
</ul>
<p>But wait, you have to buy AMD at $90 even though AMD fell to $85! That&rsquo;s your
obligation for selling the contract. That means you will lose $500 on the
stock purchase. (You bought $8,500 of AMD stock but had to pay $9,000 for it.)</p>
<p>All together, you lost $300. You paid $500 extra for the AMD shares but you
kept your $200 premium for selling the contract. But, now you have 100 AMD
shares worth $8,500 in your account!</p>
<p>What do you do with those?</p>
<h2 id="selling-calls">Selling calls</h2>
<p>Now you have 100 shares of AMD in your account worth $8,500 and you&rsquo;re still
down $300. It&rsquo;s time to talk about selling calls. Selling calls and puts
involve the same concepts of specifying a strike price, how much premium you
want, and an expiration date.</p>
<p>You take a look at the options chain and you see that you can sell an AMD call
for the $90 strike price for $250 and it expires in a few weeks. This is
called a <em>covered call</em> since you own 100 shares as collateral for the
contract.</p>
<p>You sell the call and now there are some potential outcomes:</p>
<ul>
<li>AMD falls to $80
<ul>
<li>Your call loses value rapidly and you can buy it back for a tiny amount.
If you buy your call back at $25, then you made $225. ($250 premium
originally received minus $25 you paid to close it)</li>
<li>Go back and sell another call, collect more premium, and increase your
profit.</li>
<li>Allow the call to expire worthless and keep your full $250 premium. (Risky!)</li>
</ul>
</li>
<li>AMD gets stuck at $85
<ul>
<li>Your call gradually loses value and you can buy it back for a tiny amount.</li>
<li>Go back and sell another call, collect more premium, and increase your
profit.</li>
</ul>
</li>
<li>AMD climbs to $95
<ul>
<li>You&rsquo;re now in the money here.</li>
<li>When the option expires, the buyer of your contract will likely want to
exercise and they will buy your shares at $90 each (your strike price).</li>
<li>Your shares are gone. (But keep reading.)</li>
</ul>
</li>
</ul>
<h2 id="assignment-on-calls">Assignment on calls</h2>
<p>Getting assigned on puts means you have to buy the stock at your stike price.
Getting assigned on calls is a little different. You <em>sell</em> your stock at the
strike price to the person who bought your call option contract.</p>
<p>Let&rsquo;s go back to the previous example where you sold a $90 call option for
$250 and AMD went to $95 per share:</p>
<ul>
<li>You keep your $250 premium since you sold the contract.</li>
<li>You sell your 100 AMD shares at $90 each (your strike price) even though AMD
is trading at $95.</li>
</ul>
<p>Technically, you lost out on $500 of profit that you could have received if
you had not sold the call. However, you keep the $250 of premium and you can
begin selling puts again.</p>
<h2 id="why-not-just-buy-stock-and-hold-it">Why not just buy stock and hold it?</h2>
<p>I have accounts where I buy stock and hold it for a long time. However,
selling options allows you to collect premium and get profits even in a market
that isn&rsquo;t moving much.</p>
<p>If a stock gets stuck in a sideways pattern where it doesn&rsquo;t move much for
weeks, buying and holding the stock might give you a very small gain. During
that time, you may have been able to get a better return selling options on
that stock and collecting the premium.</p>
<h2 id="whats-the-worst-that-can-happen">What&rsquo;s the worst that can happen?</h2>
<p>It&rsquo;s always important to know what your max loss is for any trade. Losses
begin when the stock price breaches your breakeven price:</p>
<ul>
<li>Put breakeven = strike price - premium received</li>
<li>Call breakeven = strike price + premium received</li>
</ul>
<p>If you sell a $90 put for $200 and the stock falls to $89, you are still up
$100. If you sell a $90 call for $200 and the stock goes up to $91.50, you are
still up $50.</p>
<p>However, if you are selling puts or calls, there is a chance that the stock
could completely crater and go to zero. The chances of that happening to a
well-known company that has been on the market for years is <strong>very low</strong>, but
it&rsquo;s important to know max loss.</p>
<p>Let&rsquo;s say you sold a put on company XYZ at the $100 strike and received $500
in premium. The stock somehow falls to $1 per share. Your max loss here is
<code>((100 x $100) - (100 * x $1)) - $500 = $9,400</code>. The buyer of your options
contract will <strong>surely</strong> want to exercise so they can sell their XYZ stock at
$100 instead of $1.</p>
<p>On the call side, let&rsquo;s say you sold a $110 call on XYZ for $500 and the stock
fell to $1. The 100 shares of XYZ you own would now be worth $1 per share, but
you do keep your $500 premium. If you had owned the shares without selling
that call at that time, you would have suffered the same fate (without getting
$500 in premium).</p>
<h2 id="the-wheel">The Wheel</h2>
<p>The strategy shown here is called the wheel strategy. It involves these steps:</p>
<ol>
<li>Sell puts and collect premium</li>
<li>If unassigned, go back to step 1</li>
<li>If assigned, sell calls and collect premium</li>
<li>If unassigned, go back to step 3</li>
<li>If assigned, go back to step 1</li>
</ol>
<p>This strategy will be covered in the next post!</p>
<p><em>Photo credit: <a href="https://unsplash.com/@ripato">Ricardo Gomez Angel on Unsplash</a></em></p>
]]></content></item><item><title>Options trading introduction</title><link>https://major.io/2020/12/06/options-trading-introduction/</link><pubDate>Sun, 06 Dec 2020 00:00:00 +0000</pubDate><guid>https://major.io/2020/12/06/options-trading-introduction/</guid><description>I&amp;rsquo;m always on the hunt for new things to learn and one of my newer interests is around trading options on the stock market. Much like technology, it becomes much easier to understand when you peel back the layers of archaic terminology, snake oil salespeople, and debunked data.
Stay tuned for additional posts that go into greater detail. This post covers the basics about what an option is (and is not) and how you can compare it to other investment vehicles, like stocks.</description><content type="html"><![CDATA[<figure>
    <img src="/images/2020-12-06-snow-mountain.jpg"
         alt="Snowy Mountain"/> 
</figure>

<p>I&rsquo;m always on the hunt for new things to learn and one of my newer interests
is around trading options on the stock market. Much like technology, it
becomes much easier to understand when you peel back the layers of archaic
terminology, snake oil salespeople, and debunked data.</p>
<p>Stay tuned for additional posts that go into greater detail. This post covers
the basics about what an option is (and is not) and how you can compare it to
other investment vehicles, like stocks.</p>
<h2 id="investing-in-stocks">Investing in stocks</h2>
<p>Most of us are likely familiar with how investing in stocks works to some
extent. The process goes something like this:</p>
<ul>
<li>Deposit money into a brokerage account</li>
<li>Find a company that has good qualities for value and growth</li>
<li>Buy shares</li>
<li>Wait</li>
</ul>
<p>If the company performs well <strong>and</strong> other investors also see value in the
company, your investment should gain value. For example, If you buy 10 shares
of a company for $100 each ($1,000 total investment), and the stock value goes
up by 10%, you will have 10 shares at $110 each ($1,100 total investment).</p>
<p>Stocks go up very often, but they go down, too. If the stock lost 10% of its
value, you would own 10 shares of a $90 stock and your total investment would
fall to $900.</p>
<p>Sometimes the stock market does downright silly things and companies with
great fundamentals go down, while others with horrible numbers keep going up.</p>
<p>A. Gary Shilling once said:</p>
<blockquote>
<p>Markets can remain irrational a lot longer than you and I can remain
solvent.</p>
</blockquote>
<p>Nothing could be more true.</p>
<h2 id="how-are-stocks-traded">How are stocks traded?</h2>
<p>This could be an entire post in itself, but I&rsquo;ll keep things simple. When you
buy a stock, you&rsquo;re buying it from someone else. That could be another
investor, a hedge fund,a mutual fund maintainer, or a massive company.</p>
<p>The other entity will set the price they are willing to sell some stock. This
is called the <em>ask</em>. Buyers in the market set the price at which they want to
buy, called the <em>bid</em>. When you look at the highest bid and the lowest ask for
a stock at one time, that&rsquo;s called the <em>bid/ask spread</em> or often just <em>the
spread</em>.</p>
<p>If nobody sets a bid at or equal to anyone&rsquo;s ask price, no stock changes
hands. Either the buyer needs to raise the price that they are willing to pay
or the sellers need to come down on their asking price.</p>
<p>Look for stocks with a narrow bid/ask spread under $0.05-$0.10. This means the
stock is liquid (easily bought and sold) and you&rsquo;ll avoid a weird situation
called <em>slippage</em>, where the bid/ask spread is really wide and your trade may
not execute as a good price. (Limit orders help here. More on that later.)</p>
<h2 id="what-is-similar-about-options">What is similar about options?</h2>
<p>Options have bid/ask spreads, just like stock. Options are called derivatives
because they are derived from a certain stock. That stock is called the
<em>underlying</em>, since the options depend heavily on the stock&rsquo;s performance.
(Not every stock on the market has options, though.)</p>
<h2 id="what-is-different-about-options">What is different about options?</h2>
<p>Options are contracts. Every options contract has rights, obligations, and an
expiration date.</p>
<p>Buyers of options contracts have rights (things they can choose to do) and
sellers have obligations (things they must do). Everything starts with a buyer
or seller who wants to open a contract. Once it&rsquo;s open, it can be closed via
different methods.</p>
<p>It revolves around two main types of contracts: calls and puts.</p>
<h2 id="buying-calls">Buying calls</h2>
<p>If you buy a call option contract, you have the right to buy shares at the
price you specify, which is called the <em>strike price</em>. <em>(You can sell calls,
too, but let&rsquo;s keep this simple for now.)</em></p>
<p>AMD is a popular stock for options traders and we can use it as an example.
Let&rsquo;s say that AMD is trading at $100 per share and you think their next chip
launch will be really successful. A call could help you catch some of the
<em>potential</em> upswing in the stock.</p>
<p>Let&rsquo;s say you buy a call at a strike price of $105 for $200 and the contract
expires December 13. What have you done here and what are your rights?</p>
<ul>
<li>You made a <em>buy to open</em> order for a call contract that cost you $200.</li>
<li>You cannot trade this option after the market close on December 13.</li>
</ul>
<p>What are the potential outcomes before expiration? Let&rsquo;s analyze what AMD
could do during that time:</p>
<ul>
<li>Best outcome: AMD rises to $110
<ul>
<li>You are <em>in the money</em></li>
<li>Your options contract will increase in value and you can sell it before
expiration.</li>
<li>You can choose to <em>exercise</em> the option, which means you are using your
right to buy 100 shares of AMD at $105 each, even though the stock is
trading at $110! Your profit would be <code>(100 * $110) - (100 * $105) = $500</code></li>
</ul>
</li>
<li>Good outcome: AMD rises to $104.99
<ul>
<li>You are <em>at the money</em></li>
<li>During this time, your call option goes up in value since the stock price
has moved really close to your strike price of $105.</li>
<li>You can sell your option contract for a profit before expiration.</li>
</ul>
</li>
<li>Worst outcome: AMD falls below $100
<ul>
<li>You are <em>out of the money</em></li>
<li>Your options contract will be worth a lot less since the stock price has
moved away from your strike price.</li>
<li>If your options contract still has some value left, you could sell it to
get back some of your losses.</li>
<li>If AMD falls <em>really</em> far below $100, you may find that your option has no
value. This is called <em>max loss</em> and it hurts.</li>
</ul>
</li>
</ul>
<p>💣 Just in case you missed it: <strong>you can easily lose all of your investment
with options</strong>.</p>
<p>With stocks, a company can take a heavy loss, but you still have some value
left. With options, that same scenario means your original $200 investment is
completely gone.</p>
<h2 id="buying-puts">Buying puts</h2>
<p>Buying a put, called a long put, is a different thing entirely. You are
choosing a price where you are willing to sell 100 shares of stock you own.
It&rsquo;s a great insurance policy for stock that you own.</p>
<p>Going back to the AMD example, let&rsquo;s say you own 100 shares of AMD at $100
each. If you buy a put at $90, you have the right to sell your stock at $90
even if AMD is trading lower than $90.</p>
<p>Let&rsquo;s go back to our $100 per share AMD example and you buy a $90 put for AMD
that costs $200 and expires December 13. There are some possible outcomes:</p>
<ul>
<li>AMD rises to $110
<ul>
<li>Your put option will lose lots of value, but it was your insurance policy
anyway.</li>
<li>You can sell your put as the expiration gets closer, or you can let it
expire on December 13th. It&rsquo;s worthless at that point, so nothing happens.</li>
</ul>
</li>
<li>AMD drops to $90.01
<ul>
<li>Your put will increase in value since the stock price has moved close to
your strike price. You can choose to sell it before it expires to get a
profit. (Keep in mind that the stock you own has lost value, too.)</li>
</ul>
</li>
<li>AMD drops to $80
<ul>
<li>You can exercise your put contract and sell your 100 shares of AMD for $90
each, even though the current price is $80 per share! This will net you
$10 per share, or $1,000 total. Your losses from the stock&rsquo;s fall are
limited.</li>
</ul>
</li>
</ul>
<p>You might be asking: What happens if I think a stock will go down, I buy a
put, but I don&rsquo;t own the underlying stock?</p>
<p>If the stock drops, you can sell the put for a profit as the value goes up.
However, if you choose to exercise it, you will have a <em>short position</em> in
your brokerage account. This means you&rsquo;ve borrowed stock from your broker and
they will likely charge fees and/or interest for this.</p>
<p>If AMD&rsquo;s price quickly rises afterwards, you will owe your brokerage the
difference. <em><strong>This is painful!</strong></em> Just imagine borrowing 100 shares at $80
each from your broker only to see the stock fly up to $120 per share. You will
owe your broker that $4,000 difference. 🥵</p>
<h2 id="exercising">Exercising</h2>
<p>In your personal life, I usually recommend exercising regularly. When it comes
to options, exercising may not be your best bet. There are plenty of
situations where exercising an options contract is the worst idea.</p>
<p>Let&rsquo;s go back to our original example about buying a call on AMD for $105 when
the stock is at $100. As an example, the stock price climbs to $110 before
expiration and stays there on Friday afternoon. You are excited about your
profits and you tell your broker to exercise your contract.</p>
<p>Your broker submits the exercise request to CBOE, the options clearinghouse.
The CBOE has <a href="https://markets.cboe.com/exchange_traded_stock/equity_options_spec/">some detailed rules about exercising</a> that I highly recommend
reading.</p>
<p>Let&rsquo;s say that something happens after hours at AMD that turns out to be
really bad. Maybe there&rsquo;s a delay on new chips, an accounting irregularity, or
something else. The stock crashes after hours after you have asked for the
exercise. The stock could be down to $80 by that point.</p>
<p>Since you asked for the exercise, the stock you expected to get (100 shares at
$105 each) will actually be <strong>100 shares at $80</strong> each by the time your
exercise is finished. This is an extreme example, but always consider what can
happen during the exercise process.</p>
<p>For a detailed explanation of potential outcomes here, and how to avoid them,
I highly recommend the <a href="https://rss.com/podcasts/thetagang/78866/">Options Expire Saturday</a> episode of the Theta Gang
Podcast. (The podcast is excellent overall.)</p>
<h2 id="whats-next">What&rsquo;s next?</h2>
<p>👶🏻 <em><strong>This introduction barely scratches the surface of options contracts,
so don&rsquo;t start trading yet.</strong></em></p>
<p>I&rsquo;m planning future posts about selling options contracts, how to choose the
options to sell and buy, as well as many other ways you can gain (and lose)
money with options contracts.</p>
<p><em>Photo credit: <a href="https://unsplash.com/@johnwestrock">Dave Hoefler on Unsplash</a></em></p>
]]></content></item><item><title>Build AWS images with Image Builder</title><link>https://major.io/2020/06/19/build-aws-images-with-imagebuilder/</link><pubDate>Fri, 19 Jun 2020 00:00:00 +0000</pubDate><guid>https://major.io/2020/06/19/build-aws-images-with-imagebuilder/</guid><description>The AMIs provided by most Linux distributions in AWS work well for most use cases. However, there are those times when you need a customized image to support a certain configuration or to speed up CI processes.
You can get a customized image via a few methods:
Build from an existing AMI, customize it, and snapshot it. Use an automated tool, such as Packer, to automate #1. Build your own image locally in KVM, VMware, or Virtualbox and upload the image into S3, import it into an EC2, and create an AMI from the snapshot.</description><content type="html"><![CDATA[<figure>
    <img src="/images/2020-06-19-aluminum-factory.jpg"
         alt="Aluminum factory"/> 
</figure>

<p>The AMIs provided by most Linux distributions in AWS work well for most use
cases. However, there are those times when you need a customized image to
support a certain configuration or to speed up CI processes.</p>
<p>You can get a customized image via a few methods:</p>
<ol>
<li>Build from an existing AMI, customize it, and snapshot it.</li>
<li>Use an automated tool, such as Packer, to automate #1.</li>
<li>Build your own image locally in KVM, VMware, or Virtualbox and upload the
image into S3, import it into an EC2, and create an AMI from the snapshot.</li>
</ol>
<p>My preferred option is the last method since the installation happens locally
and the image is first booted in AWS. This ensures that log files and
configurations are clean on first boot. Although this method produces the best
result, it has plenty of steps that can go wrong.</p>
<h2 id="importing-an-image-into-aws-the-hard-way">Importing an image into AWS (the hard way)</h2>
<p>AWS has <a href="https://docs.aws.amazon.com/vm-import/latest/userguide/vmimport-image-import.html">documentation for importing an image</a> and the basic steps include:</p>
<ol>
<li>Install into a VM locally and customize it.</li>
<li>Snapshot the image and upload it into an S3 bucket.</li>
<li>Create an IAM role for <code>vmimport</code> so that EC2 can pull the image from S3
and import it.</li>
<li>Run <code>aws ec2 import-snapshot</code> to tell EC2 to import the image.</li>
<li>Monitor the output of <code>aws ec2 describe-import-snapshot-tasks</code> until the
snapshot fully imports into EC2. It might fail to import, so you need to be
prepared for that. (If that happens, go back to step 4.)</li>
<li>Get the snapshot ID from the import.</li>
<li>Run <code>aws ec2 register-image</code> to create the AMI from the snapshot ID.</li>
</ol>
<p>This is a lot of manual work. 😩</p>
<h2 id="using-image-builder-to-make-images">Using Image Builder to make images</h2>
<p>Image Builder has two main components:</p>
<ul>
<li><a href="https://github.com/osbuild/osbuild-composer">osbuild-composer</a> takes an image configuration and generates instructions
for the image build stages (and optionally uploads an image to a cloud)</li>
<li><a href="https://github.com/osbuild/osbuild">osbuild</a> takes those instructions and builds an image</li>
</ul>
<p>The support for uploading to clouds first arrived in Fedora 32 and this post
will use that release for generating images.</p>
<p>To get started, install <code>osbuild-composer</code> along with <code>composer-cli</code>, a
command line interface to create images. Start the socket for
<code>osbuild-composer</code> as well:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text"># dnf -y install composer-cli osbuild-composer
# systemctl enable --now osbuild-composer.socket
</code></pre></div><p>Verify that everything is working:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text"># composer-cli sources list
fedora
updates
fedora-modular
updates-modular
</code></pre></div><p>We now need an image blueprint. A blueprint is a TOML file that provides some
basic specifications for the image, such as which packages to install, which
services should start at boot time, and the system&rsquo;s time zone. Refer to the
<a href="https://weldr.io/lorax/lorax-composer.html#blueprints">Lorax composer documentation</a> for a full list of options.</p>
<p>In this example, we will build a small image with nginx to serve a website.
Here&rsquo;s the TOML file:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-toml" data-lang="toml">name = <span style="color:#2aa198">&#34;aws-nginx&#34;</span>
description = <span style="color:#2aa198">&#34;AWS nginx image&#34;</span>
version = <span style="color:#2aa198">&#34;0.0.1&#34;</span>

[[packages]]
name = <span style="color:#2aa198">&#34;chrony&#34;</span>

[[packages]]
name = <span style="color:#2aa198">&#34;cloud-utils-growpart&#34;</span>

[[packages]]
name = <span style="color:#2aa198">&#34;nginx&#34;</span>

[customizations.kernel]
append = <span style="color:#2aa198">&#34;no_timer_check console=hvc0 LANG=en_US.UTF-8&#34;</span>

[customizations.services]
enabled = [<span style="color:#2aa198">&#34;chronyd&#34;</span>, <span style="color:#2aa198">&#34;nginx&#34;</span>]

[customizations.timezone]
timezome = <span style="color:#2aa198">&#34;UTC&#34;</span>
</code></pre></div><p>Our specification says:</p>
<ul>
<li>Build an image with <code>nginx</code> and ensure it starts at boot time</li>
<li>Install <code>chrony</code> for time synchronization, set the time zone to UTC, and
start it at boot time.</li>
<li>Install <code>cloud-utils-growpart</code> so that cloud-init can automatically grow the
root filesystem on the first boot</li>
<li>Add some kernel boot parameters to ensure the serial console works in AWS</li>
</ul>
<p>Push the blueprint into <code>osbuild-composer</code> and ensure the packages are
available. (The <code>depsolve</code> check is optional, but I recommend it so you can
find any typos in your package names.)</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text"># composer-cli blueprints push aws-image.toml
# composer-cli blueprints depsolve aws-nginx
blueprint: aws-nginx v0.0.1
    acl-2.2.53-5.fc32.x86_64
    alternatives-1.11-6.fc32.x86_64
    audit-libs-3.0-0.19.20191104git1c2f876.fc32.x86_64
    ...
</code></pre></div><p>We can now build the image:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text"># composer-cli --json compose start aws-nginx ami
{
    &#34;build_id&#34;: &#34;285c1ee8-6b9e-4725-9c4c-346eafae86de&#34;,
    &#34;status&#34;: true
}
# composer-cli --json compose status 285c1ee8-6b9e-4725-9c4c-346eafae86de
[
    {
        &#34;id&#34;: &#34;285c1ee8-6b9e-4725-9c4c-346eafae86de&#34;,
        &#34;blueprint&#34;: &#34;aws-nginx&#34;,
        &#34;version&#34;: &#34;0.0.1&#34;,
        &#34;compose_type&#34;: &#34;ami&#34;,
        &#34;image_size&#34;: 0,
        &#34;status&#34;: &#34;RUNNING&#34;,
        &#34;created&#34;: 1592578852.962228,
        &#34;started&#34;: 1592578852.987541,
        &#34;finished&#34;: null
    }
]
</code></pre></div><p>Our image is building! After a few minutes, the image is ready:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text"># composer-cli --json compose status 285c1ee8-6b9e-4725-9c4c-346eafae86de
[
    {
        &#34;id&#34;: &#34;285c1ee8-6b9e-4725-9c4c-346eafae86de&#34;,
        &#34;blueprint&#34;: &#34;aws-nginx&#34;,
        &#34;version&#34;: &#34;0.0.1&#34;,
        &#34;compose_type&#34;: &#34;ami&#34;,
        &#34;image_size&#34;: 6442450944,
        &#34;status&#34;: &#34;FINISHED&#34;,
        &#34;created&#34;: 1592578852.962228,
        &#34;started&#34;: 1592578852.987541,
        &#34;finished&#34;: 1592579061.3364012
    }
]
# composer-cli compose image 285c1ee8-6b9e-4725-9c4c-346eafae86de
285c1ee8-6b9e-4725-9c4c-346eafae86de-image.vhdx: 1304.00 MB
# ls -alh 285c1ee8-6b9e-4725-9c4c-346eafae86de-image.vhdx
-rw-r--r--. 1 root root 1.3G Jun 19 15:12 285c1ee8-6b9e-4725-9c4c-346eafae86de-image.vhdx
</code></pre></div><p>We can take this image, upload it to S3 and import it into AWS using the
process mentioned earlier in this post. Or, we can have osbuild-composer do
this for us.</p>
<h2 id="preparing-for-automatic-aws-upload">Preparing for automatic AWS upload</h2>
<p>Start by making a bucket in S3 in your preferred region. Mine is called
<code>mhayden-image-uploads</code>:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text"># aws --region us-east-2 s3 mb s3://mhayden-image-uploads
make_bucket: mhayden-image-uploads
</code></pre></div><p>Now we need a role that allows EC2 to import images for us. Save this file as
<code>vmimport.json</code>:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json">{
   <span style="color:#268bd2">&#34;Version&#34;</span>: <span style="color:#2aa198">&#34;2012-10-17&#34;</span>,
   <span style="color:#268bd2">&#34;Statement&#34;</span>: [
      {
         <span style="color:#268bd2">&#34;Effect&#34;</span>: <span style="color:#2aa198">&#34;Allow&#34;</span>,
         <span style="color:#268bd2">&#34;Principal&#34;</span>: { <span style="color:#268bd2">&#34;Service&#34;</span>: <span style="color:#2aa198">&#34;vmie.amazonaws.com&#34;</span> },
         <span style="color:#268bd2">&#34;Action&#34;</span>: <span style="color:#2aa198">&#34;sts:AssumeRole&#34;</span>,
         <span style="color:#268bd2">&#34;Condition&#34;</span>: {
            <span style="color:#268bd2">&#34;StringEquals&#34;</span>:{
               <span style="color:#268bd2">&#34;sts:Externalid&#34;</span>: <span style="color:#2aa198">&#34;vmimport&#34;</span>
            }
         }
      }
   ]
}
</code></pre></div><p>We now need a policy to apply to the <code>vmimport</code> role that allows EC2 to use
the role to download the image, import it, and register an AMI <strong>(replace the
bucket name with your S3 bucket)</strong>. Save this as <code>vmimport-policy.json</code>:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json">{
   <span style="color:#268bd2">&#34;Version&#34;</span>:<span style="color:#2aa198">&#34;2012-10-17&#34;</span>,
   <span style="color:#268bd2">&#34;Statement&#34;</span>:[
      {
         <span style="color:#268bd2">&#34;Effect&#34;</span>: <span style="color:#2aa198">&#34;Allow&#34;</span>,
         <span style="color:#268bd2">&#34;Action&#34;</span>: [
            <span style="color:#2aa198">&#34;s3:GetBucketLocation&#34;</span>,
            <span style="color:#2aa198">&#34;s3:GetObject&#34;</span>,
            <span style="color:#2aa198">&#34;s3:ListBucket&#34;</span>
         ],
         <span style="color:#268bd2">&#34;Resource&#34;</span>: [
            <span style="color:#2aa198">&#34;arn:aws:s3:::mhayden-image-uploads&#34;</span>,
            <span style="color:#2aa198">&#34;arn:aws:s3:::mhayden-image-uploads/*&#34;</span>
         ]
      },
      {
         <span style="color:#268bd2">&#34;Effect&#34;</span>: <span style="color:#2aa198">&#34;Allow&#34;</span>,
         <span style="color:#268bd2">&#34;Action&#34;</span>: [
            <span style="color:#2aa198">&#34;ec2:ModifySnapshotAttribute&#34;</span>,
            <span style="color:#2aa198">&#34;ec2:CopySnapshot&#34;</span>,
            <span style="color:#2aa198">&#34;ec2:RegisterImage&#34;</span>,
            <span style="color:#2aa198">&#34;ec2:Describe*&#34;</span>
         ],
         <span style="color:#268bd2">&#34;Resource&#34;</span>: <span style="color:#2aa198">&#34;*&#34;</span>
      }
   ]
}
</code></pre></div><p>Add the role and the policy to IAM:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text"># aws iam create-role --role-name vmimport \
    --assume-role-policy-document &#34;file://vmimport.json&#34;
# aws iam put-role-policy --role-name vmimport --policy-name vmimport \
    --policy-document &#34;file://vmimport-policy.json&#34;
</code></pre></div><h2 id="building-an-image-with-automatic-upload">Building an image with automatic upload</h2>
<p>We can use our same TOML blueprint we created earlier and provide one
additional TOML file that provides AWS configuration and credentials. Create
an <code>aws-config.toml</code> file with the following content:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-toml" data-lang="toml">provider = <span style="color:#2aa198">&#34;aws&#34;</span>

[settings]
accessKeyID = <span style="color:#2aa198">&#34;***&#34;</span>
secretAccessKey = <span style="color:#2aa198">&#34;***&#34;</span>
bucket = <span style="color:#2aa198">&#34;mhayden-image-uploads&#34;</span>
region = <span style="color:#2aa198">&#34;us-east-2&#34;</span>
key = <span style="color:#2aa198">&#34;fedora-32-image-from-my-blog-post&#34;</span>
</code></pre></div><p>Add your AWS credentials here along with your S3 bucket, preferred AWS region,
and an image key. The image key is the name applied to the snapshot and the
resulting AMI.</p>
<p>Now we can build our AMI and have it automatically uploaded:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text"># composer-cli --json compose start aws-nginx ami fedora-32-image-from-my-blog-post aws-config.toml
{
    &#34;build_id&#34;: &#34;f343b20d-70f9-467a-9157-f9b4fc90ee87&#34;,
    &#34;status&#34;: true
}
# composer-cli --json compose info f343b20d-70f9-467a-9157-f9b4fc90ee87
{
    &#34;id&#34;: &#34;f343b20d-70f9-467a-9157-f9b4fc90ee87&#34;,
    &#34;config&#34;: &#34;&#34;,
    &#34;blueprint&#34;: {
        &#34;name&#34;: &#34;aws-nginx&#34;,
        &#34;description&#34;: &#34;AWS nginx image&#34;,
        &#34;version&#34;: &#34;0.0.1&#34;,
        &#34;packages&#34;: [
            {
                &#34;name&#34;: &#34;chrony&#34;
            },
            {
                &#34;name&#34;: &#34;cloud-utils-growpart&#34;
            },
            {
                &#34;name&#34;: &#34;nginx&#34;
            }
        ],
        &#34;modules&#34;: [],
        &#34;groups&#34;: [],
        &#34;customizations&#34;: {
            &#34;kernel&#34;: {
                &#34;append&#34;: &#34;no_timer_check console=hvc0 LANG=en_US.UTF-8&#34;
            },
            &#34;timezone&#34;: {},
            &#34;services&#34;: {
                &#34;enabled&#34;: [
                    &#34;chronyd&#34;,
                    &#34;nginx&#34;
                ]
            }
        }
    },
    &#34;commit&#34;: &#34;&#34;,
    &#34;deps&#34;: {
        &#34;packages&#34;: []
    },
    &#34;compose_type&#34;: &#34;ami&#34;,
    &#34;queue_status&#34;: &#34;RUNNING&#34;,
    &#34;image_size&#34;: 6442450944,
    &#34;uploads&#34;: [
        {
            &#34;uuid&#34;: &#34;e747be78-87e2-48b9-b0d2-cc1bb393a9e4&#34;,
            &#34;status&#34;: &#34;RUNNING&#34;,
            &#34;provider_name&#34;: &#34;aws&#34;,
            &#34;image_name&#34;: &#34;fedora-32-image-from-my-blog-post&#34;,
            &#34;creation_time&#34;: 1592580775.438667,
            &#34;settings&#34;: {
                &#34;region&#34;: &#34;us-east-2&#34;,
                &#34;accessKeyID&#34;: &#34;***&#34;,
                &#34;secretAccessKey&#34;: &#34;***&#34;,
                &#34;bucket&#34;: &#34;mhayden-image-uploads&#34;,
                &#34;key&#34;: &#34;fedora-32-image-from-my-blog-post&#34;
            }
        }
    ]
}
</code></pre></div><p>The output now shows an <code>uploads</code> section with the AWS upload details
included. This process may take some time, especially if your upload speed is
low. You can follow along with <code>composer-cli --json compose info</code> or you can
monitor the system journal:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text"># journalctl -af -o cat -u osbuild-worker@1.service
Running job f343b20d-70f9-467a-9157-f9b4fc90ee87
2020/06/19 15:57:37 [AWS] 🚀 Uploading image to S3: mhayden-image-uploads/fedora-32-image-from-my-blog-post
2020/06/19 15:58:03 [AWS] 📥 Importing snapshot from image: mhayden-image-uploads/fedora-32-image-from-my-blog-post
2020/06/19 15:58:03 [AWS] ⏱ Waiting for snapshot to finish importing: import-snap-0f4baff3e1eb945a8
2020/06/19 16:04:50 [AWS] 🧹 Deleting image from S3: mhayden-image-uploads/fedora-32-image-from-my-blog-post
2020/06/19 16:04:51 [AWS] 📋 Registering AMI from imported snapshot: snap-0cf822f1441f9e407
2020/06/19 16:04:51 [AWS] 🎉 AMI registered: ami-0d0873cc888ab12a2
</code></pre></div><p>I ran this job on a small instance at Vultr and the whole process took about
10 minutes. The AWS image import process can vary a bit, but it&rsquo;s usually in
the range of 5-15 minutes.</p>
<p>At this point, I can take my new AMI (in my case, it&rsquo;s
<code>ami-0d0873cc888ab12a2</code>) and build instances at EC2! 🎉</p>
<h2 id="wrapping-up">Wrapping up</h2>
<p>Although there is some work involved in laying the groundwork for importing
images into EC2, this work only needs to be done one time. You can re-use your
existing AWS credentials TOML file over and over for new images that are made
from different blueprints.</p>
<p>You can also do almost all of this work via the <a href="https://cockpit-project.org/">cockpit web interface</a> using
the <code>cockpit-composer</code> package if you prefer. The only downside to that method
is that some image customizations cannot be made through cockpit and some TOML
blueprint editing with <code>composer-cli</code> is needed. Look for that in a future
blog post.</p>
<p><em>Photo credit: <a href="https://commons.wikimedia.org/wiki/File:Shelekhov-aluminium-factory-4.jpg">Wikimedia Commons</a></em></p>
]]></content></item><item><title>My experience with keto so far</title><link>https://major.io/2020/06/11/my-experience-with-keto-so-far/</link><pubDate>Thu, 11 Jun 2020 00:00:00 +0000</pubDate><guid>https://major.io/2020/06/11/my-experience-with-keto-so-far/</guid><description>I&amp;rsquo;ve talked about some of my experiences with altering my diet on Twitter and many people have asked about my experiences with keto so far. Note that I don&amp;rsquo;t call it &amp;ldquo;the keto diet&amp;rdquo; because it&amp;rsquo;s much more than a diet: it&amp;rsquo;s a lifestyle change. Sure, you alter what you eat, but you begin to think differently about how you fuel your body.
Before we start, I&amp;rsquo;d like to note that:</description><content type="html"><![CDATA[<figure>
    <img src="/images/2020-06-11-plates-and-bowls.jpg"
         alt="Brno skyline"/> 
</figure>

<p>I&rsquo;ve talked about some of my experiences with altering my diet on Twitter and
many people have asked about my experiences with keto so far. Note that I
don&rsquo;t call it &ldquo;the keto diet&rdquo; because it&rsquo;s much more than a diet: it&rsquo;s a
<em>lifestyle</em> change. Sure, you alter what you eat, but you begin to think
differently about how you fuel your body.</p>
<p>Before we start, I&rsquo;d like to note that:</p>
<ul>
<li>
<p><strong>I am definitely not a medical professional or a dietician.</strong> If you plan
to make any big changes to what you eat, talk to an expert first. Certain
people with certain conditions could be harmed by a drastic change in diet
or lifestyle.</p>
</li>
<li>
<p><strong>Everyone reacts differently to lifestyle changes.</strong> Your experiences are
not guaranteed to match mine. Yours could easily be better (or worse).</p>
</li>
<li>
<p><strong>I have nothing to sell you.</strong> The keto lifestyle is effectively open
source since there&rsquo;s nothing to buy and no memberships to maintain. There
are tons of different ways to start and maintain it, and you can choose the
right method for you.</p>
</li>
</ul>
<h2 id="what-is-keto">What is keto?</h2>
<p>When you adopt a keto lifestyle, you change what fuel is used primarily in
your body. It&rsquo;s a high fat, moderate protein, low carbohydrate approach.</p>
<p>Low carbohydrate diets are not new. They were studied heavily in the early
1900&rsquo;s as a <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6123870/">potential remedy for
epilepsy</a>. Doctors
noticed that patients who fasted had fewer problems, but the patients couldn&rsquo;t
maintain fasting for extended periods. Once they put the patients on a high
fat diet and the body went into ketosis (where it burns fat for fuel), the
patients had fewer problems.</p>
<p>These diets are still used today for children and adults with seizure
disorders and other brain-related conditions. To learn more about these
patients and how fat works in your body, read Dr. Mark Hyman&rsquo;s <a href="https://www.goodreads.com/book/show/34203748-eat-fat-get-thin">Eat Fat Get
Thin</a>. He
provides tons of easy to understand reasons why a high fat diet is better and
he provides links to hundreds of studies and publications about it.</p>
<h2 id="time-for-bacon-all-day">Time for bacon all day!</h2>
<p>Hold on for a moment.</p>
<p>As a friend explained to me, doing keto the right way is like getting the
right octane gasoline from a reputable gas station. It burns clean and make
your car run well. You can choose a different octane or choose a shady gas
station and your car will still run, but it might make noise, run slower, or
send you to the mechanic more often.</p>
<p>People often talk about &ldquo;dirty&rdquo; and &ldquo;clean&rdquo; keto. If you focus purely on your
carbohydrate target and use any foods you want to fill up your calories from
fat, you will end up in ketosis. However, some of these foods will slow your
weight loss progress (if that&rsquo;s your goal) and they may rob some of the
additional keto health benefits from you.</p>
<h2 id="i-thought-it-was-just-weight-loss">I thought it was just weight loss?</h2>
<p>There are two big benefits here: weight loss and health benefits.</p>
<p>First, let&rsquo;s talk about weight loss. You can check Reddit&rsquo;s
<a href="https://reddit.com/r/keto">r/keto</a> for some amazing weight loss stories.
People have loss hundreds of pounds and kept it off!</p>
<p>I started around 195 lbs (88.5 kg) and I am 6'1&quot; (1.85 m) tall. After starting
keto, my weight <em>increased</em> (as did my blood pressure). I was frustrated as
the food change was difficult and I felt like I was doing a lot of work for
nothing. Online forums reassured me that this was part of the process and that
I should keep pushing.</p>
<p>So I kept pushing and stayed with it. Around the 3-4 week mark, the weight
slowly began to come down. Within two months, I was down to 180 (81.6 kg).
Within three months, I was at 174 (78.9 kg) and realized I didn&rsquo;t want to be
that thin! After increasing my fat intake a bit more, I&rsquo;m able to hover around
180 lbs and that&rsquo;s very comfortable for me.</p>
<p>Even at 180 lbs, I looked different. Some of the troublesome fat that I dealt
with (love handles!) shrank some. I moved from a 36 in most pants down to a 34
(and a 32 in some).</p>
<p>The best part about all of this is that my body stays between 178-182 lbs
reliably without needing to take any drastic action. It is easy to maintain.</p>
<h2 id="what-are-the-health-benefits">What are the health benefits?</h2>
<p>I&rsquo;ve had mild high blood pressure for a few years and I was taking medication
(lisinopril) to reduce it. The medicine worked fairly well and and kept me in
a better range. However, I had spells where I would stand from sitting and I
felt like I was going to pass out. I felt tired more often and felt
lightheaded after working out.</p>
<p>I&rsquo;ve also had asthma for years and I&rsquo;ve taken various medications
(montelukast, albeuterol, olopatadine, flonase) to deal with that and my
seasonal allergies. My asthma has put me in the ER more than once.</p>
<p>After about four months of keto, my lightheaded spells got worse and my doctor
told me to stop taking blood pressure medication. I watched my blood pressure
closely and found that I no longer needed medication for it! <em>(That was one
medication I was really glad to throw away.)</em></p>
<p>I&rsquo;m about 8 months in now and I was able to stop taking all allergy and asthma
medications about a month ago. This really makes me think that something was
in my diet that was causing me harm.</p>
<h2 id="what-do-i-stop-eating">What do I stop eating?</h2>
<p>Looking at keto as a &ldquo;What do I stop eating?&rdquo; question will make it much more
difficult to maintain. Here are some of the things I eat regularly:</p>
<ul>
<li>Avocados</li>
<li>Berries (raspberries, blackberries, strawberries)</li>
<li>Butter</li>
<li>Cheese (mostly the harder ones)</li>
<li>Coffee</li>
<li>Eggs (add chorizo for more flavor)</li>
<li>Fish (salmon, trout, limited tuna)</li>
<li>Ghee (butter with dairy removed; goes great in coffee)</li>
<li>Meats (beef, chicken, pork, and yes, bacon)</li>
<li>MCT oil (medium chain triglycerides from coconuts)</li>
<li>Mushrooms</li>
<li>Nuts (walnuts, pecans, almonds, limited peanuts)</li>
<li>Olive oil</li>
<li>Vegetables (lettuce, zucchini, eggplant, celery, many more)</li>
</ul>
<p>This is just a small subset of what I like to eat. If you want a comprehensive
keto grocery shopping list, the experts at <a href="https://www.dietdoctor.com/low-carb/keto/foods/list">Diet
Doctor</a> have you covered.</p>
<p>But yes, there are some things that I avoid as much as possible:</p>
<ul>
<li>Anything that says &ldquo;diet&rdquo; on the package</li>
<li>Beer 😭</li>
<li>Chips and crackers</li>
<li>Gluten <em>(difficult to totally avoid)</em></li>
<li>High-fructose corn syrup (HFCS)</li>
<li>Malitol (used as a sugar substitute but it spikes blood sugar 🤦🏻‍♂️)</li>
<li>Potatoes</li>
<li>Sugar</li>
<li>White wine</li>
</ul>
<p>To succeed at keto, you need to keep your body in sustained ketosis. Anything
that spikes your blood sugar (sometimes called <em>high glycemic load</em>) must go.</p>
<p>Don&rsquo;t fear: some alcohol is okay! Avoid beer, sugary mixed drinks, and white
whines. Red wines from Europe and South America are great, and you can have
low sugar liquor like gin, bourbon, whiskey, scotch, tequila, and vodka. Too
much alcohol will stunt your progress towards weight loss and other health
benefits.</p>
<p>💣 <em><strong>PLEASE NOTE: Hangovers are significantly worse and longer lasting on
keto.</strong></em> Studies are underway to figure out why this is the case, but Dr.
Hyman speculates in his book that it&rsquo;s related to how the liver operates in a
lower-carb setting.</p>
<h2 id="okay-it-cant-all-be-perfect">Okay, it can&rsquo;t all be perfect.</h2>
<p>And you&rsquo;re right. Getting into ketosis is not fun at all.</p>
<p>Many people talk about the &ldquo;keto flu&rdquo;, which is a period that lasts anywhere
from a few days to a week where you feel pretty terrible. Your energy level
will drop, exercising will be difficult, and you will be irritable. This is
the period where your body has exhausted its supply of easily accessible
glucose and it&rsquo;s changing gears to burn fat.</p>
<p>I&rsquo;ve gone through it three times (more on that later), and it&rsquo;s not fun.
Here&rsquo;s a list of what I went through:</p>
<ul>
<li>exhaustion</li>
<li>increased trips to the bathroom</li>
<li>irritability</li>
<li>lack of focus</li>
<li>muscle cramps</li>
<li>temporary weight gain</li>
</ul>
<p>To make this as easy as possible (and short in duration), do these things:</p>
<ul>
<li>eat consistently (do not fast)</li>
<li>exercise consistently, but make it lighter than usual</li>
<li>get electrolytes daily (low carb, of course)</li>
<li>keep fat intake high (you need to encourage your body to burn it)</li>
<li>sleep at least 8 hours</li>
</ul>
<p>The fog will slowly begin to lift and you will feel great when you reach the
other side.</p>
<h2 id="what-happens-when-i-eat-bad-things">What happens when I eat bad things?</h2>
<p>This happens to all of us. I was two months in on keto when Thanksgiving
arrived. That&rsquo;s a big holiday in the US where we generally eat a lot, and a
lot of what we eat is carbs. Some potatoes, pumpkin pie, and beer tempted me
and I woke up the next morning feeling awful. My stomach cramped and I felt
nauseated.</p>
<p>Many people suggested getting right back into keto with high fat foods and
exercise. I persevered with that and felt better by the end of the day.</p>
<p>If you do make a bad choice and spike your blood sugar, your body will crave
more sugar and put you into a keto-busting spiral. Your digestive system biome
that changed to work with a high-fat diet will get confused with a sudden
influx of carbs and it will cramp. However, if you get back on track quickly,
those problems will subside.</p>
<h2 id="people-make-me-feel-guilty-for-not-eating-things">People make me feel guilty for not eating things.</h2>
<p>I&rsquo;ve heard things like these a hundred times:</p>
<ul>
<li>&ldquo;It&rsquo;s not like one piece of cake is going to screw up your whole diet.&rdquo;</li>
<li>&ldquo;She spent a lot of time cooking that and now you&rsquo;re not going to eat it?&rdquo;</li>
<li>&ldquo;You&rsquo;re thin. Why do you need to do keto?&rdquo;</li>
<li>&ldquo;I&rsquo;ve eaten bread all my life and I&rsquo;m just fine.&rdquo;</li>
</ul>
<p>Remember that this is a choice <strong>you make</strong> for <strong>your body.</strong> Don&rsquo;t ever say
&ldquo;I can&rsquo;t eat that&rdquo; because your brain will see your lifestyle change as
limitation. I usually say &ldquo;Thank you, but I don&rsquo;t eat that&rdquo; or &ldquo;I love what
you made, but that&rsquo;s not something I eat.&rdquo;</p>
<h2 id="what-do-i-do-when-i-get-invited-out-for-dinner">What do I do when I get invited out for dinner?</h2>
<p>This is probably the toughest part. What do you do if you get asked to go to
dinner or if you&rsquo;re on the road and you need to get food? You don&rsquo;t really
notice how much of the American diet is made from carbohydrates until you
start avoiding them. <strong>They are everywhere.</strong></p>
<p>Vegetarians and vegans have a similar problem here. Try to look at the menu
ahead of time and figure out the things that you can eat.</p>
<p>There are two approaches here:</p>
<ul>
<li>Choose something you want to eat and ask the server if you can substitute
anything you don&rsquo;t want to eat.</li>
<li>Order something and eat what you like. Leave the rest.</li>
</ul>
<p>I&rsquo;ve had lots of luck by asking for different vegetables to replace potatoes
as a side dish. There are some of those times where I can&rsquo;t substitute and I
end up leaving food behind. I feel bad about wasting food, but I&rsquo;ll choose
that option if it&rsquo;s the only one I have.</p>
<p>When everyone starts to order dessert, get yourself a low carb treat!
Sometimes I&rsquo;ll ask for a cup of coffee or some scotch as a dessert. Everyone
does dessert differently</p>
<h2 id="i-have-more-questions">I have more questions!</h2>
<p>Great! There is only so much content I can put into this post. Send me a
message <a href="https://twitter.com/majorhayden">on Twitter</a> or email me at major at
mhtx dot net.</p>
<p>Here are some great resources that I use:</p>
<ul>
<li><a href="https://www.bulletproof.com/">Bulletproof</a> produces a diet that is similar
to keto but it may help people with digestive disorders. It aligns with the
<a href="https://www.health.harvard.edu/diet-and-weight-loss/a-new-diet-to-manage-irritable-bowel-syndrome">FODMAP</a>
diet which may help you if you suffer from conditions like IBS and you want
to lose weight. Their products are also top notch.</li>
<li><a href="https://www.dietdoctor.com/">Diet Doctor</a> has tons of useful information on
various diets that benefit your health and their guidance is easy to
understand.</li>
<li><a href="https://drhyman.com/">Dr. Hyman</a> is an expert on low-carb diets and their
health benefits. He has a great podcast and lots of free resources to guide
you.</li>
<li><a href="https://www.reddit.com/r/keto">Reddit&rsquo;s r/keto</a> has inspiring stories and
plenty of guidance for you if you get stuck.</li>
</ul>
<p><em>Photo credit: <a href="https://www.pexels.com/photo/ceramic-plates-3847482/">Pexels</a></em></p>
]]></content></item><item><title>Make diacritics easy in Linux</title><link>https://major.io/2020/02/13/make-diacritics-easy-in-linux/</link><pubDate>Thu, 13 Feb 2020 00:00:00 +0000</pubDate><guid>https://major.io/2020/02/13/make-diacritics-easy-in-linux/</guid><description>Diacritics are all of the small things that are added on, above, or below certain characters in various languages. Some examples include tildes (ñ), accents (á), or other marks (š). These marks are little hints that help you know how to pronounce a word properly (and they sometimes change the definition of a word entirely).
They are often skipped by non-native language speakers, and sometimes even by native speakers, but I have done my best to make a habit of including them when I can.</description><content type="html"><![CDATA[<figure>
    <img src="/images/2020-02-13-diacritics-on-license-plate.jpg"
         alt="Brno skyline"/> 
</figure>

<p><a href="https://en.wikipedia.org/wiki/Diacritic">Diacritics</a> are all of the small things that are added on, above, or below
certain characters in various languages. Some examples include tildes (ñ),
accents (á), or other marks (š). These marks are little hints that help you
know how to pronounce a word properly (and they sometimes change the
definition of a word entirely).</p>
<p>They are often skipped by non-native language speakers, and sometimes even by
native speakers, but I have done my best to make a habit of including them
when I can.</p>
<p>Pronounciation can change drastically with a certain mark. For example, a
common Czech name is Tomaš. The š on the end makes a <em>sh</em> sound insead of a
the normal <em>sss</em> sound.</p>
<p>In Spanish, the word for Spain is España. The ñ has a <em>n</em> sound followed by a
<em>yah</em> sound. If you leave off the ñ, you end up with a sound like <em>ana</em> on the
end instead of <em>an-yah</em>.</p>
<p>Leaving out diacritics can also lead to terrible results, such as a famous
Spanish mistake:</p>
<ul>
<li>Mi papá tiene cincuenta años <em>(My Dad is fifty years old)</em></li>
<li>Mi papa tiene cincuenta anos <em>(My potato has fifty anuses)</em></li>
</ul>
<p>This could <strong>obviously</strong> lead to some confusion. 🤭</p>
<h2 id="first-attempts-failures">First attempts (failures)</h2>
<p>At first, I found myself going to <a href="https://www.online-toolz.com/tools/character-map.php">online character maps</a> and I would
copy/paste the character I wanted. Typing in Spanish quickly became painful
with the constant back and forth to copy certain characters.</p>
<p>I knew there had to be a better way.</p>
<h2 id="altgr">AltGr</h2>
<p>After some research, I found that there are some keyboards with a special Alt
key on the right side of the space bar called the <a href="https://en.wikipedia.org/wiki/AltGr_key">AltGr</a> key. It&rsquo;s a special
modifier key that lets you type characters that are not easy with your
keyboard layout.</p>
<p>Luckily, you can tell your computer to pretend like you have an AltGr key to
the right of the keyboard and you get access to all of the international
characters via key combinations.</p>
<p>For Linux, you can run this in any terminal:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">setxkbmap us -variant altgr-intl
</code></pre></div><p>I add this command in my <code>~/.config/i3/config</code> for i3:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">exec_always --no-startup-id &#34;setxkbmap us -variant altgr-intl&#34;
</code></pre></div><p>Most window managers give you the option to change the keyboard layout for
your session in the window manager settings.</p>
<p>In GNOME, open <em>Settings</em>, click <em>Region &amp; Language</em>, and click the plus (+)
below the list of layouts. Choose <em>English</em> and then choose <em>English (US, alt.
intl.)</em> from the list. You can switch from layout to layout in GNOME, but
AltGr works well for me as a default.</p>
<h2 id="trying-it-out">Trying it out</h2>
<p>Once you have AltGr enabled, here are some quick things to try:</p>
<ul>
<li>AltGr + Shift + ~, release keys, press n: <code>ñ</code></li>
<li>AltGr + &lsquo;, release keys, press a: <code>á</code></li>
<li>AltGr + Shift + ., release keys, press s: <code>š</code></li>
<li>AltGr + s: <code>ß</code></li>
</ul>
<p>Take a look at <a href="https://en.wikipedia.org/wiki/AltGr_key">AltGr</a> on Wikipedia for lots more combinations.</p>
<p><em>Photo credit: naleag_deco on <a href="https://www.flickr.com/photos/53088812@N00/1304824528/">Flickr</a></em></p>
]]></content></item><item><title>My Travel Guide to Brno</title><link>https://major.io/2020/01/30/my-travel-guide-to-brno/</link><pubDate>Thu, 30 Jan 2020 00:00:00 +0000</pubDate><guid>https://major.io/2020/01/30/my-travel-guide-to-brno/</guid><description>Come to the Czech Republic and discover the beautiful city of Brno. I just wrapped up my third visit to the city and I can&amp;rsquo;t wait to come back! The city is full of history, culture, and delicious food.
Here&amp;rsquo;s my travel guide to Brno!
Getting to Brno Brno has an airport, but the flights are limited and sometimes expensive. Some coworkers have found good deals on these flights (especially via Ryanair from London&amp;rsquo;s Stansted Airport), but I prefer the train.</description><content type="html"><![CDATA[<figure>
    <img src="/images/2020-01-30-brno-skyline.jpg"
         alt="Brno skyline"/> 
</figure>

<p>Come to the Czech Republic and discover the beautiful city of <a href="https://en.wikipedia.org/wiki/Brno">Brno</a>. I just
wrapped up my third visit to the city and I can&rsquo;t wait to come back! The city
is full of history, culture, and delicious food.</p>
<p>Here&rsquo;s my travel guide to Brno!</p>
<h2 id="getting-to-brno">Getting to Brno</h2>
<p>Brno has <a href="https://en.wikipedia.org/wiki/Brno%E2%80%93Tu%C5%99any_Airport">an airport</a>, but the flights are limited and sometimes expensive.
Some coworkers have found good deals on these flights (especially via Ryanair
from London&rsquo;s Stansted Airport), but I prefer the train.</p>
<p>I prefer to fly into Vienna, Austria and catch the <a href="https://www.regiojet.com/">Regiojet</a> train from
Vienna&rsquo;s main train station (Hauptbanhof, or Hbf.) to Brno&rsquo;s main train
station (hlavní nádraží). The train is usually €10 or less and it takes about
90 minutes. You can get snacks or a full meal on the train. They also have
beer, wine, and coffee (the coffee is free!). They take credit cards on the
train for anything you buy.</p>
<p>The only downside of the Regiojet train is that you will need to take a train
from Vienna&rsquo;s airport train station (Flughafen) to Vienna&rsquo;s main train station
(Wien Hauptbanhof). It&rsquo;s a quick 15 minute trip that costs €5 or less.</p>
<p>There is also an <a href="https://www.oebb.at/en/reiseplanung-services/im-zug/unsere-zuege/railjet.html">ÖBB Railjet</a> train that leaves directly from Vienna&rsquo;s
airport train station (Flughafen Hbf.). It&rsquo;s convenient since you don&rsquo;t need
to take a train from the airport to the main train station, but it usually
costs €20 or more.</p>
<h2 id="getting-around-brno">Getting around Brno</h2>
<p>Brno has an <em>extensive</em> tram, bus, and train system that runs 24 hours a day.
You have plenty of options for getting tickets for trams and buses:</p>
<ul>
<li>At most stops, you can get a 24 hour ticket with coins (more on money later)</li>
<li>The main train station has passes for multiple days (5 days, 14 days,
and longer)</li>
<li>Purchase tickets on <a href="https://www.brnoid.cz/en/eoc">BrnoID</a> and attach the ticket to a contactless credit
or debit card</li>
<li>Some buses and trams have a contactless card terminal on them (but don&rsquo;t
count on it being there)</li>
<li>Send SMS to buy tickets (see signs at stops, requires Czech phone number)</li>
</ul>
<p><strong>Don&rsquo;t get on without a valid ticket!</strong> When they check for tickets (and they
do!), you could get hit with a fine of €15-€40. That&rsquo;s downright silly when a
two week ticket is usually around €11.</p>
<p>Buying tickets in Brno&rsquo;s main train station is generally easy, but it costs
more than using BrnoID.</p>
<p>As for tram and bus etiquette, I&rsquo;ve learned a few things:</p>
<ul>
<li>Try move away from the doors when you board (sometimes this is difficult
when it&rsquo;s crowded)</li>
<li>Make room for people in wheelchairs and with baby carriages in the doorway
areas</li>
<li>Offer up your seat to the elderly or to people who really need
to sit down</li>
<li>Make your way towards the door early and press the green button on the poles
near the door when you want to get off</li>
<li>If you ride in the late evening or night time, you may hear <em>na znamení</em> &ndash;
if you do, then it means your stop will be skipped if you don&rsquo;t push the
button to get off (pay attention!)</li>
<li>When you want to board the train, be sure to push the buttons next to the
doors on the outside of the tram so they open (no need to do this at the
big stops like the main train station or Česka since all of the doors open
anyway)</li>
<li>The cars in the rear of the trams seem to be the least crowded</li>
</ul>
<p>If you only remember one thing, remember this: <em><strong>trams always have the right
of way</strong></em>. As my coworker in Brno says &ldquo;cars stop, trams do not&rdquo;. Do not
assume that the trams will stop in front of you, even at stations. Stay out of
the way until the tram has fully passed you or fully stopped.</p>
<p>The trams stop just before 11PM, so be sure to check the schedules or Google
Maps prior to heading out at night time. The night buses run all night long
but there are some long gaps between stops late at night. The night buses are
quite lively, so please don&rsquo;t get distracted and forget about my <em>na znamení</em>
note above. 😀</p>
<h2 id="speaking-czech">Speaking Czech</h2>
<p>I&rsquo;m an American who speaks English and limited Spanish, and Czech is a
difficult language for me. There are sounds in Czech that are totally new to
you and they will take a lot of practice before you get them right. However,
Czech people are really pleased when you make an attempt to speak <em>some</em>
Czech, so it&rsquo;s worth knowing a few things:</p>
<ul>
<li>dobrý den: good day</li>
<li>dobré ráno: good morning</li>
<li>ahoj: hello (and goodbye) for a friend, someone you know
(sounds like &ldquo;ahoy!&quot;)</li>
<li>děkuji: thank you</li>
<li>prosím: please, can I help you, here you go, casual version of
&ldquo;you&rsquo;re welcome&rdquo;</li>
<li>nemáš zač: you&rsquo;re welcome (more formal)</li>
<li>pivo: beer (follow with <em>prosím</em>)</li>
<li>vino: wine (just like Spanish!)</li>
</ul>
<p>If these look difficult, do your best to work on <em>dobrý den</em> and <em>děkuji</em>. In
my experience, most people I met would put on a big smile when they heard me
try some basic Czech. One of the workers at my hotel saw me every morning and
after a few days of me saying &ldquo;dobrý den&rdquo; to her before breakfast, she finally
said &ldquo;getting better, good job!&rdquo; 🤗</p>
<p>The vowels in Czech are almost exactly the same as Spanish (including accented
ones). If you see a consonant with a hat, like č, add an <em>h</em> after it. A č
sounds like <em>ch</em> in chair. A š sounds like <em>sh</em> in shut.</p>
<p>If you see a hat on a vowel, like ě, try to mix in a <em>y</em> sound like in yarn.
These are difficult.</p>
<p>I&rsquo;m told the most difficult letter is the ř. It&rsquo;s a sound we don&rsquo;t have in
Latin languages. I seem to get better at it after a few beers.</p>
<p>A friend in Brno told me that you speak Czech like you care a lot about the
first syllable and you don&rsquo;t care about the others. Put most of your emphasis
on the early part of the word and avoid doing that in the middle and end.</p>
<h2 id="food-and-drinks">Food and drinks</h2>
<p>You had better come hungry (and thirsty) because the food here is delicious.
Brno has plenty of delicious meats, vegetables, pastries, beer, and wine.
Being vegetarian or vegan in Brno isn&rsquo;t exactly easy, but it&rsquo;s entirely
possible.</p>
<p>My absolute favorite items are bramboraky (thick potato pancakes), trdelník
(cake made on a spit), and kolače (round pastry with plum in the middle).
Czech people also love cheesecake because you can find it everywhere. Meat
dishes like pork knee, rump steak, and beef goulash are top notch.</p>
<p>As for drinks, pilsner beer is really popular. You can find beers like
Starobrno and Pilsner Urquell everywhere, but I&rsquo;d encourage you to look for
some other beers like Chotěboř (it&rsquo;s a tough one to say). There are lots of
microbrweries all over town.</p>
<p>Wine in Brno is delicious. Moravia (southern Czech Republic) is full of
wineries and they extend into Sloviakia and Croatia. If you enjoy red wine,
you are in for a treat. Try a local Cabernet or Frankovka. Even the Merlot
wine here tastes amazing. As for white wines, my favorite is the Pálava</p>
<p>If you tire of Czech food, Brno has excellent food from around the world,
especially Indian, Italian, and Thai food.</p>
<p>Breakfasts include lots of familiar foods, including eggs, bacon, beans,
fruit, pastries, coffee, and tea. Filtered coffee (American-style) is hard to
come by, but you can order an americano at most coffee shops even if it&rsquo;s not
on the menu. Try a good espresso at least once. You can finish it fast and you
get a good jolt in a few minutes.</p>
<h2 id="safety">Safety</h2>
<p>I&rsquo;ve felt safer in Brno than in some cities in Texas. As with any big city,
travel with groups when you can and be sure you know where you&rsquo;re going before
you go. Czech people try to keep to themselves on streets and public
transportation, so if you&rsquo;re minding your own business, you are most likely
never going to be bothered.</p>
<h2 id="money">Money</h2>
<p>The Czechs use Koruna (&ldquo;crowns&rdquo; in English). Amounts are usually shown in full
units (no cents like with US dollars) and there are coins for 50 CZK and
lower. The bills start at 100 CZK.</p>
<p>I recommend using an ATM when you arrive since you&rsquo;ll get the best rate.
However, stay away from the ATMs that are very close to the train station.
I&rsquo;ve seen fees as high as €10 at ATMs near the station! The city center is a
few blocks away with plenty of ATMs with little or no fees.</p>
<p>As with most of Europe, cards are widely accepted. Chip cards are required and
contactless cards are really helpful. Most payment terminals will allow you to
pay with a quick tap of your contactless card and it&rsquo;s quite handy. American
cards still require a signature and you may find that Czech people are stunned
when their payment terminal demands that they collect a signature from you.</p>
<p>Before you travel, be sure to let your credit and debit card companies know
about your travel so that you won&rsquo;t trigger fraud alerts.</p>
<h2 id="enjoy">Enjoy</h2>
<p>I&rsquo;ve probably missed a lot of things in this post, but these are the things
that come to mind right now. Thanks for reading the post and I hope you get to
enjoy a trip to Brno soon!</p>
]]></content></item><item><title>Disable Nvidia GPU on the Thinkpad T490</title><link>https://major.io/2020/01/24/disable-nvidia-gpu-thinkpad-t490/</link><pubDate>Fri, 24 Jan 2020 00:00:00 +0000</pubDate><guid>https://major.io/2020/01/24/disable-nvidia-gpu-thinkpad-t490/</guid><description>I wrote about installing Linux on the Lenovo ThinkPad T490 last month and one of the biggest challenges was getting graphics working properly. The T490 comes with an option where you can get a discrete Nvidia MX250 GPU and it packs plenty of power in a small footprint.
It also brings along a few issues.
Awful battery life There are many times where it would be helpful to fully disable the Nvidia card to extend battery life when graphics processing is not needed.</description><content type="html"><![CDATA[<figure>
    <img src="/images/20191212-t490.png"
         alt="Lenovo ThinkPad T490"/> 
</figure>

<p>I wrote about <a href="/2019/12/12/thinkpad-t490-fedora-install-tips/">installing Linux on the Lenovo ThinkPad T490 last month</a> and
one of the biggest challenges was getting graphics working properly. The T490
comes with an option where you can get a discrete Nvidia MX250 GPU and it
packs plenty of power in a small footprint.</p>
<p>It also brings along a few issues.</p>
<h2 id="awful-battery-life">Awful battery life</h2>
<p>There are many times where it would be helpful to fully disable the Nvidia
card to extend battery life when graphics processing is not needed. The MX250
is a Pascal family GPU and those GPUs require signed drivers, so nouveau will
not work.</p>
<p>There is a handy kernel feature called <a href="https://01.org/linuxgraphics/gfx-docs/drm/gpu/vga-switcheroo.html">VGA Switcheroo</a> <em>(yes, that is the
name)</em>. It gives you a quick method for turning the GPU on and off.
Unfortunately, that does require the nouveau module to work with the card.</p>
<p>The Nvidia drivers attempt to take the card into a low power mode called P8,
but it&rsquo;s not low enough. Removing the <code>nvidia</code> module causes the card to run
with full power and that makes things even worse.</p>
<p>Darn. It&rsquo;s time to fix some other problems. 😟</p>
<h2 id="suspend-and-resume">Suspend and resume</h2>
<p>There are issues with suspend and resume with the Nvidia drivers after Linux
4.8. If you close the lid on the laptop, the laptop suspends properly and you
can see the pulsating LED light on the lid.</p>
<p>Open the lid after a few seconds and you will see a black screen (possibly
with a kernel trace) that looks like this:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">[   51.435212] ACPI: Waking up from system sleep state S3
[   51.517986] ACPI: EC: interrupt unblocked
[   51.567244] nvidia 0000:2d:00.0: Refused to change power state, currently in D3
</code></pre></div><p>The laptop will lock up and the fans will spin up shortly after. The only
remedy is a hard power off.</p>
<p>This is related to a Nvidia driver bug that surfaced after Linux 4.8 added
<a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/7.4_release_notes/chap-red_hat_enterprise_linux-7.4_release_notes-kernel_parameters_changes">per-port PCIe power management</a>. That feature allows the kernels to handle
PCIe power management for each port individually. It helps certain PCIe devies
(or portions of those devices) to go into various power saving modes
independently.</p>
<p>You can work around this issue by adding <code>pcie_port_pm=off</code> to your kernel
command line. I added it and my suspend/resume worked well after a reboot.</p>
<p>This leads to another problem:</p>
<h2 id="even-worse-battery-life">Even worse battery life</h2>
<p>Getting suspend and resume back was a nice improvement, but I noticed that my
battery life dropped significantly. I went from 6 hours (which was not great)
down to 3-4 hours. That&rsquo;s terrible.</p>
<p>I booted my laptop into i3wm and ran <code>powertop</code> in a terminal. The idle power
usage bounced between 10-12 watts with a single terminal open and i3status
updating my status line.</p>
<p>So I was left with a choice:</p>
<ul>
<li>
<p>Leave the Nvidia card enabled with <code>pcie_port_pm=off</code> set, enjoy my
suspend/resume, and suffer through terrible battery life 😫</p>
</li>
<li>
<p>Remove <code>pcie_port_pm=off</code>, save battery life, and deal with hard lockups if
I attempt to suspend 😭</p>
</li>
</ul>
<p>Both options were terrible.</p>
<p>I knew there was only one good choice: <strong>find a way to disable the Nvidia card
by default and only enable it when I need it</strong>.</p>
<h2 id="digging-deep">Digging deep</h2>
<p>If you can&rsquo;t control your hardware well enough in the OS, and you can control
it in the BIOS, the only option remaining is to examine your ACPI tables. This
requires dumping the DSDT and SSDT tables from the laptop. These tables
provide a map of instructions for taking all kinds of actions with the
hardware on the laptop, including turning devices on and off.</p>
<p>🔥 <strong>DISCLAIMER: Tinkering with DSDT and SSDT files can damage your machine
if you are not familiar with the process. All changes in these files must be
made with extreme care and you should try the <em>smallest</em> possible change first
to reduce the risks.</strong></p>
<p>We need some tools to dump the ACPI tables and decompile them into a DSL that
we can read as humans:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">dnf install acpica-tools
</code></pre></div><p>Make a directory to hold the files and dump the ACPI tables:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">mkdir ~/dsdt
cd ~/dsdt
sudo acpidump -b
</code></pre></div><p>You should have plenty of files ending in <code>.dat</code> in the directory. These are
the compiled ACPI tables and they are difficult to read unless you love hex.
You can decompile them with <code>iasl</code> and move the compiled files out of the way:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">iasl -d *.dat
mkdir raw
mv *.dat raw/
</code></pre></div><p>You can find the decompiled files in my <a href="https://gitlab.com/majorhayden/t490-dsdt">T490 DSDT repository on GitLab</a>.</p>
<p>We need to find some details on the discrete GPU. Running a <code>grep</code> on the
<code>.dsl</code> files in the directory shows some mentions in the <code>ssdt10.dsl</code>:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">  {
      Local0 [One] = 0x03
      TGPU = \_SB.PCI0.LPCB.EC.HKEY.GPTL /* External reference */
      Local0 [0x08] = TGPU /* \_SB_.PCI0.RP09.PEGP.TGPU */
      Return (Local0)
  }
</code></pre></div><p>So the GPU is represented in the ACPI tables as <code>SB_.PCI0.RP09.PEGP</code>. Let&rsquo;s
grep for that:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">$ grep -l SB_.PCI0.RP09.PEGP *.dsl
dsdt.dsl
ssdt10.dsl
ssdt11.dsl
ssdt14.dsl
</code></pre></div><p>So the card appears in <code>ssdt11.dsl</code>. Examine that file and you will
find:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">  Method (_ON, 0, Serialized)  // _ON_: Power On
  {
      D8XH (Zero, 0x11)
      If ((TDGC == One))
      {
          If ((DGCX == 0x03))
          {
              _STA = One
              \_SB.PCI0.RP09.PEGP.GC6O ()
          }
          ElseIf ((DGCX == 0x04))
          {
              _STA = One
              \_SB.PCI0.RP09.PEGP.GC6O ()
          }

          TDGC = Zero
          DGCX = Zero
      }
      ElseIf ((OSYS != 0x07D9))
      {
          PCMR = 0x07
          PWRS = Zero
          Sleep (0x10)
          \_SB.PCI0.HGON () // &lt;---- This is where it turns on!
          _STA = One
      }

      D8XH (Zero, 0x12)
  }

</code></pre></div><p>When the <code>_ON</code> method is called, it calls <code>\_SB.PCI0.HGON ()</code> and that turns
on the card. There&rsquo;s another method called <code>\_SB.PCI0.HGOF ()</code> that turns off
the card.</p>
<p>Let&rsquo;s try changing any instances of <code>HGON</code> to <code>HGOF</code>. It&rsquo;s dirty, but it just
might work. There are two calls to <code>HGON</code> in <code>ssdt11.dsl</code> and I changed both
to <code>HSOF</code>. This should cause the card to be turned off when the system boots (and the <code>_INI</code> methods are called).</p>
<p>We need to make one more change so that the kernel will know our patched SSDT file is newer than the one in the BIOS. Look for this line at the top of <code>ssdt11.dsl</code>:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">DefinitionBlock (&#34;&#34;, &#34;SSDT&#34;, 2, &#34;LENOVO&#34;, &#34;SgRpSsdt&#34;, 0x00001000)
</code></pre></div><p>Change the number at the very end so that it is incremented by one:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">DefinitionBlock (&#34;&#34;, &#34;SSDT&#34;, 2, &#34;LENOVO&#34;, &#34;SgRpSsdt&#34;, 0x00001001)
</code></pre></div><p>Now we need to compile the SSDT</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">iasl -tc ssdt11.dsl
</code></pre></div><p>The easiest method for loading the SSDT table is to patch it during the initrd
step. We need to pack the file into a cpio archive:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">mkdir -p /tmp/fix-nvidia/kernel/firmware/acpi
cd /tmp/fix-nvidia
cp ~/dsdt/ssdt11.aml kernel/firmware/acpi
find kernel | cpio -H newc --create &gt; acpi_override
sudo cp acpi_override /boot/
</code></pre></div><p>Now we can carefully edit the bootloader options by adding
<code>initrd /acpi_override</code> to our current kernel entry. These are found in
<code>/boot/loader/entries</code> and are named based on the kernel they load. In my
case, the bootloader config for 5.4.12 is in
<code>/boot/loader/entries/d95743f260b941dcb518e3fcd3a02fa9-5.4.12-200.fc31.x86_64.conf</code>.</p>
<p>The file should look like this afterwards:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">title Fedora (5.4.12-200.fc31.x86_64) 31 (Thirty One)
version 5.4.12-200.fc31.x86_64
linux /vmlinuz-5.4.12-200.fc31.x86_64
initrd /acpi_override
initrd /initramfs-5.4.12-200.fc31.x86_64.img
options $kernelopts
grub_users $grub_users
grub_arg --unrestricted
grub_class kernel
</code></pre></div><p>The <code>initrd /acpi_override</code> line is the one I added.</p>
<p>Reboot your laptop. After the boot, look for the SSDT lines in <code>dmesg</code>:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">$ dmesg | egrep -i &#34;ssdt|dsdt&#34;
[    0.018597] ACPI: SSDT ACPI table found in initrd [kernel/firmware/acpi/ssdt11.aml][0xe28]
[    0.018813] ACPI: Table Upgrade: override [SSDT-LENOVO-SgRpSsdt]
[    0.018816] ACPI: SSDT 0x000000008780E000 Physical table override, new table: 0x0000000086781000
</code></pre></div><p>Now look for Nvidia:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">$ nvidia-smi
NVIDIA-SMI has failed because it couldn&#39;t communicate with the NVIDIA driver.
Make sure that the latest NVIDIA driver is installed and running.
</code></pre></div><p>Success! My laptop is now hovering around 4.5-5.5 watts. That&rsquo;s half of what
it was before! 🎊 🎉 🥳</p>
<h2 id="but-sometimes-i-want-my-dgpu">But sometimes I want my dGPU</h2>
<p>Okay, there <em>are</em> some times where the discrete GPU is nice. Let&rsquo;s edit the
SSDT table once more to add an option to enable it at boot time with a kernel
command line option.</p>
<p>Here are the changes needed for <code>ssdt11.dsl</code>:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-diff" data-lang="diff"><span style="color:#cb4b16">diff --git a/ssdt11.dsl b/ssdt11.dsl
</span><span style="color:#cb4b16">index fd9042f05376aa80e3b94c1d6313e69cbb495c34..f75b43f57655553c5ced7a2595ad2b48f26b2c10 100644
</span><span style="color:#cb4b16"></span><span style="color:#dc322f">--- a/ssdt11.dsl
</span><span style="color:#dc322f"></span><span style="color:#719e07">+++ b/ssdt11.dsl
</span><span style="color:#719e07"></span><span style="color:#268bd2">@@ -337,7 +337,17 @@ DefinitionBlock (&#34;&#34;, &#34;SSDT&#34;, 2, &#34;LENOVO&#34;, &#34;SgRpSsdt&#34;, 0x00001000)
</span><span style="color:#268bd2"></span>                             PCMR = 0x07
                             PWRS = Zero
                             Sleep (0x10)
<span style="color:#dc322f">-                            \_SB.PCI0.HGON ()
</span><span style="color:#dc322f"></span><span style="color:#719e07">+
</span><span style="color:#719e07">+                            // Set this ACPI OSI flag to enable the dGPU.
</span><span style="color:#719e07">+                            If (\_OSI (&#34;T490-Hybrid-Graphics&#34;))
</span><span style="color:#719e07">+                            {
</span><span style="color:#719e07">+                                \_SB.PCI0.HGON ()
</span><span style="color:#719e07">+                            }
</span><span style="color:#719e07">+                            Else
</span><span style="color:#719e07">+                            {
</span><span style="color:#719e07">+                                \_SB.PCI0.HGOF ()
</span><span style="color:#719e07">+                            }
</span><span style="color:#719e07">+
</span><span style="color:#719e07"></span>                             _STA = One
                         }

<span style="color:#268bd2">@@ -449,7 +459,15 @@ DefinitionBlock (&#34;&#34;, &#34;SSDT&#34;, 2, &#34;LENOVO&#34;, &#34;SgRpSsdt&#34;, 0x00001000)
</span><span style="color:#268bd2"></span>
                     Method (_ON, 0, Serialized)  // _ON_: Power On
                     {
<span style="color:#dc322f">-                        \_SB.PCI0.HGON ()
</span><span style="color:#dc322f"></span><span style="color:#719e07">+                        // Set this ACPI OSI flag to enable the dGPU.
</span><span style="color:#719e07">+                        If (\_OSI (&#34;T490-Hybrid-Graphics&#34;))
</span><span style="color:#719e07">+                        {
</span><span style="color:#719e07">+                            \_SB.PCI0.HGON ()
</span><span style="color:#719e07">+                        }
</span><span style="color:#719e07">+                        Else
</span><span style="color:#719e07">+                        {
</span><span style="color:#719e07">+                            \_SB.PCI0.HGOF ()
</span><span style="color:#719e07">+                        }
</span><span style="color:#719e07"></span>                         Return (Zero)
                     }
</code></pre></div><p>Follow the same steps as before to compile the SSDT, pack it into a cpio
archive, and copy it to <code>/boot/acpi_override</code>. Now you can add
<code>acpi_osi='T490-Hybrid-Graphics'</code> to your kernel command line whenever you
want to use your Nvidia card. You won&rsquo;t need to mess with SSDT tables again to
make it work.</p>
<p>I hope this guide was helpful! Keep in mind that future BIOS updates may
change your ACPI tables and this fix may stop working. You may need to look around for the changes and adjust your changes to match.</p>
]]></content></item><item><title>Bring Back Fedora's Beefy Miracle boot splash</title><link>https://major.io/2019/12/16/bring-back-fedora-beefy-miracle-boot-splash.md/</link><pubDate>Mon, 16 Dec 2019 00:00:00 +0000</pubDate><guid>https://major.io/2019/12/16/bring-back-fedora-beefy-miracle-boot-splash.md/</guid><description>Way back in 2012 when Fedora releases had names, there was one release that many of us in the Fedora community will never forget. Fedora 17&amp;rsquo;s code name was &amp;ldquo;Beefy Miracle&amp;rdquo; and it caused plenty of giggles and lots of consternation (especially in vegetarian and vegan circles).
No matter how you feel about the code name, the mascot was really good:
Major and the beefy miracle in 2012</description><content type="html"><![CDATA[<figure>
    <img src="/images/2019-12-16-beefy-miracle.png"
         alt="Beefy miracle"/> 
</figure>

<p>Way back in 2012 when Fedora releases had names, there was one release that
many of us in the Fedora community will never forget. Fedora 17&rsquo;s <a href="https://fedoraproject.org/wiki/History_of_Fedora_release_names#Fedora_17_.28Beefy_Miracle.29">code name</a>
was &ldquo;Beefy Miracle&rdquo; and it caused plenty of giggles and lots of consternation
(especially in vegetarian and vegan circles).</p>
<p>No matter how you feel about the code name, the mascot was really good:</p>
<figure>
    <img src="/images/2019-12-16-major-and-beefy-miracle.jpg"
         alt="Major and the beefy miracle in 2012"/> <figcaption>
            <p>Major and the beefy miracle in 2012</p>
        </figcaption>
</figure>

<h2 id="the-mustard">The mustard</h2>
<p>I was told several times that &ldquo;the mustard indicates progress.&rdquo; That didn&rsquo;t
make a lot of sense to me until I saw the Plymouth boot splash. During the
boot-up, the mustard moves from bottom to top to indicate how much of the boot
process has completed.</p>
<p>You can try out the hot dog boot splash yourself with a few quick commands on
Fedora.</p>
<p>First off, install the <code>hot-dog</code> plymouth theme:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">sudo dnf install plymouth-theme-hot-dog
</code></pre></div><p>Set the theme as the default and rebuild the initrd to ensure that the boot
screen is updated after you reboot:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">sudo plymouth-set-default-theme --rebuild-initrd hot-dog
</code></pre></div><p>This step takes a few moments to finish since it causes <code>dracut</code> to rebuild
the entire initrd with the new plymouth theme. Once it finishes, reboot your computer and you should get something like this:</p>
<figure>
    <img src="/images/2019-12-16-hot-dog-boot-splash.jpg"
         alt="Hot dog boot splash"/> <figcaption>
            <p>Hot dog boot splash</p>
        </figcaption>
</figure>

]]></content></item><item><title>Thinkpad T490 Fedora install tips</title><link>https://major.io/2019/12/12/thinkpad-t490-fedora-install-tips/</link><pubDate>Thu, 12 Dec 2019 00:00:00 +0000</pubDate><guid>https://major.io/2019/12/12/thinkpad-t490-fedora-install-tips/</guid><description>🔨 WORK IN PROGRESS! I&amp;rsquo;m still finding some additional issues and I&amp;rsquo;ll write those up here as soon as I find some solutions.
Thinkpad T490 With my 4th Gen X1 Carbon beginning to age (especially the battery), it was time for an upgrade. I now have a T490 with a 10th gen Intel CPU and a discrete NVIDIA MX250 GPU. This laptop spec was just released on Black Friday!</description><content type="html"><![CDATA[<blockquote>
<p>🔨 <strong>WORK IN PROGRESS!</strong> I&rsquo;m still finding some additional issues and I&rsquo;ll
write those up here as soon as I find some solutions.</p>
</blockquote>
<figure>
    <img src="/images/20191212-t490.png"/> <figcaption>
            <h4>Thinkpad T490</h4>
        </figcaption>
</figure>

<p>With my 4th Gen X1 Carbon beginning to age (especially the battery), it was
time for an upgrade. I now have a T490 with a 10th gen Intel CPU and a
discrete NVIDIA MX250 GPU. This laptop spec was just released on Black Friday!</p>
<p>As with any new technology, there are bound to be some quirks in Linux that
require some workarounds. This laptop is no exception!</p>
<p>This post will grow over time as I find more workarounds and fixes for this
laptop.</p>
<h2 id="installing-fedora">Installing Fedora</h2>
<p>Start by downloading whichever installation method of Fedora you prefer. Since
this laptop is fairly new, I went with the network installation (included in
the Server ISOs) and chose to apply updates during installation.</p>
<p>On the first boot, wait for the LUKS screen to appear and ask for your
password to decrypt the drive. Hit <code>CTRL-ALT-DEL</code> at the password prompt
and wait for the grub screen to appear on reboot.</p>
<blockquote>
<p><strong>Why are we issuing the three finger salute?</strong> If you allow the laptop to
fully boot, it will hang when it starts gdm. There are some nouveau issues
in the system journal that provide hints but I haven&rsquo;t made sense of them
yet. By preventing the system from fully booting, the grub success flag
won&rsquo;t be set and you will see the grub menu at the next boot that is
normally hidden from you.</p>
</blockquote>
<p>Press <code>e</code> on the first line of the grub menu. Find the longest line (it
usually has <code>rhgb quiet</code>) and add this text to the end:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">rd.driver.blacklist=nouveau
</code></pre></div><p>Press <code>CTRL-X</code> to boot the system. Enter your LUKS password when asked and you
should boot straight into gdm!</p>
<p>You have two options here:</p>
<ul>
<li>
<p><strong>Blacklist nouveau until bugs are fixed.</strong> <em>(Not recommended)</em> This will
force your laptop to use the integrated Intel GPU on the CPU, but it may or
may not shut off the NVIDIA GPU. This could cause a significant battery
drain.</p>
</li>
<li>
<p><strong>Install NVIDIA&rsquo;s proprietary drivers.</strong> <em>(Recommended)</em> You will have much
better control over the power state of the NVIDIA GPU and the installation
process will automatically blacklist nouveau for you.</p>
</li>
</ul>
<p>I&rsquo;m going to install NVIDIA&rsquo;s proprietary drivers that have power management
and optimus support built in already. All of these steps here come from <a href="https://rpmfusion.org/Howto/NVIDIA">RPM
Fusion&rsquo;s excellent NVIDIA documentation</a>.</p>
<p>Start by installing RPMFusion&rsquo;s repository configuration:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">sudo dnf install https://download1.rpmfusion.org/free/fedora/rpmfusion-free-release-$(rpm -E %fedora).noarch.rpm https://download1.rpmfusion.org/nonfree/fedora/rpmfusion-nonfree-release-$(rpm -E %fedora).noarch.rpm
</code></pre></div><p>Next, install the proprietary drivers.</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">sudo dnf install akmod-nvidia
</code></pre></div><p>When the packages are installed, run <code>akmods</code> to build the <code>nvidia</code> kernel
module (this took about a minute on my laptop):</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text"># sudo akmods
Checking kmods exist for 5.3.15-300.fc31.x86_64            [  OK  ]
</code></pre></div><p>Reboot your laptop. If all goes well, your laptop should boot right up without
interaction (except for entering your LUKS password).</p>
<h2 id="nvidia-power-management">NVIDIA power management</h2>
<p>You can enable some power management features for the NVIDIA GPU by following
the (somewhat lengthy) documentation about <a href="http://download.nvidia.com/XFree86/Linux-x86_64/435.17/README/dynamicpowermanagement.html">PCI-Express Runtime D3 (RTD3)
Power Management</a>. I&rsquo;ve enabled the most aggressive setting by adding the
following to <code>/etc/modprobe.d/nvidia.conf</code>:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">options nvidia &#34;NVreg_DynamicPowerManagement=0x02&#34;
</code></pre></div><p>Reboot your laptop for the change to take effect.</p>
<h2 id="bios-updates">BIOS Updates</h2>
<p>My laptop was shipped to me with the 1.04 BIOS, but 1.06 is the latest (as of
this writing). Follow these steps to update:</p>
<ul>
<li>Open the <em>Software</em> application</li>
<li>Go to the <em>Updates</em> tab</li>
<li>Look for a firmware update (usually at the end of the list)</li>
<li>Click update and wait for a notification</li>
<li>Reboot</li>
</ul>
<p>The firmware capsule is found on the first reboot and then the laptop reboots
to install the new firmware. You&rsquo;ll see some screens about backing up the BIOS
and some self-health related things. They take a while to complete, but my
laptop came right up on 1.06 without any problems!</p>
]]></content></item><item><title>Monitoring OpenShift cron jobs</title><link>https://major.io/2019/11/18/monitoring-openshift-cron-jobs/</link><pubDate>Mon, 18 Nov 2019 00:00:00 +0000</pubDate><guid>https://major.io/2019/11/18/monitoring-openshift-cron-jobs/</guid><description>Moving applications into an entirely containerized deployment, such as OpenShift or Kubernetes, requires care and attention. One aspect of both that is often overlooked is scheduled jobs, or cron jobs. ⏰
Cron jobs in OpenShift allow you to run certain containers on a regular basis and execute certain applications or scripts in those containers. You can use them to trigger GitLab CI pipelines, run certain housekeeping tasks in web applications, or run backups.</description><content type="html"><![CDATA[<p><img src="/images/2019-11-18-clock.jpg" alt="clock"></p>
<p>Moving applications into an entirely containerized deployment, such as
OpenShift or Kubernetes, requires care and attention. One aspect of both that
is often overlooked is scheduled jobs, or cron jobs. ⏰</p>
<p>Cron jobs in OpenShift allow you to run certain containers on a regular basis
and execute certain applications or scripts in those containers. You can use
them to trigger GitLab CI pipelines, run certain housekeeping tasks in web
applications, or run backups.</p>
<p>This post will cover a quick example of a cron job and how to monitor it.</p>
<p><em>Note: Almost all of these commands will work in a Kubernetes deployment by
changing <code>oc</code> to <code>kubectl</code>, but your mileage may vary based on your Kubernetes
version. All of these commands were tested on OpenShift 3.11.</em></p>
<h2 id="add-a-job">Add a job</h2>
<p>Here is a really simple cron job that gets the current date:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#586e75"># cronjob.yml</span>
<span style="color:#268bd2">apiVersion</span>: batch/v1beta1
<span style="color:#268bd2">kind</span>: CronJob
<span style="color:#268bd2">metadata</span>:
  <span style="color:#268bd2">name</span>: get-date
<span style="color:#268bd2">spec</span>:
  <span style="color:#268bd2">schedule</span>: <span style="color:#2aa198">&#34;*/1 * * * *&#34;</span>
  <span style="color:#268bd2">jobTemplate</span>:
    <span style="color:#268bd2">spec</span>:
      <span style="color:#268bd2">template</span>:
        <span style="color:#268bd2">spec</span>:
          <span style="color:#268bd2">containers</span>:
          - <span style="color:#268bd2">name</span>: get-date
            <span style="color:#268bd2">image</span>: docker.io/library/fedora:31
            <span style="color:#268bd2">command</span>:
              - date
</code></pre></div><p>The job definition says:</p>
<ul>
<li>Start a Fedora 31 container every minute</li>
<li>Run <code>date</code> in the container</li>
<li>Kill the container</li>
</ul>
<p>Load this into OpenShift with: <code>oc apply -f cronjob.yml</code></p>
<p>If you want to make more complex jobs, review the <a href="https://docs.openshift.com/container-platform/3.11/dev_guide/cron_jobs.html">OpenShift documentation on
cron job objects</a>. The <a href="https://docs.openshift.com/container-platform/3.11/rest_api/apis-batch/v2alpha1.CronJob.html">cron job API documentation</a> has much more detail.</p>
<h2 id="bad-things-happen-to-good-cron-jobs">Bad things happen to good cron jobs</h2>
<p>Cron jobs come with certain limitations and these are explained in the
<a href="https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/">Kuberntes documentation on cron jobs</a>. If a cron job is <em>missed</em> for a
certain period of time, the scheduler will think something has gone horribly
wrong and it won&rsquo;t schedule new jobs.</p>
<p>These situations include:</p>
<ul>
<li>
<p>the container takes too long to start
(check <code>.spec.startingDeadlineSeconds</code>)</p>
</li>
<li>
<p>one run of the job takes a very long time and another job can&rsquo;t start
(usually when <code>concurrencyPolicy</code> is set to <code>Forbid</code>)</p>
</li>
</ul>
<p>If 100 of the jobs are missed, the scheduler will not start any new jobs. This
could be a disaster for your application and it&rsquo;s a good place to add
monitoring.</p>
<h2 id="monitor-missed-cron-jobs-with-bash">Monitor missed cron jobs with bash</h2>
<p>Luckily, OpenShift makes an API available for checking on these situations
where cron jobs are missed. The API sits under the following URI:
<code>/apis/batch/v1beta1/namespaces/$NAMESPACE/cronjobs/$JOBNAME</code></p>
<p>For our <code>get-date</code> example above, this would be:
<code>/apis/batch/v1beta1/namespaces/$NAMESPACE/cronjobs/get-date</code></p>
<p>We can monitor this job using two handy tools: <code>curl</code> and <code>jq</code>.</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#719e07">#!/bin/bash
</span><span style="color:#719e07"></span>
<span style="color:#586e75"># Get unix time stamp of a last job run.</span>
<span style="color:#268bd2">LAST_RUN_DATE</span><span style="color:#719e07">=</span><span style="color:#719e07">$(</span>
  curl -s -H <span style="color:#2aa198">&#34;Authorization: Bearer </span><span style="color:#268bd2">$YOUR_BEARER_TOKEN</span><span style="color:#2aa198">&#34;</span> <span style="color:#cb4b16">\
</span><span style="color:#cb4b16"></span>    https://openshift.example.com/apis/batch/v1beta1/namespaces/<span style="color:#268bd2">$NAMESPACE</span>/cronjobs/get-date | <span style="color:#cb4b16">\
</span><span style="color:#cb4b16"></span>    jq  <span style="color:#2aa198">&#34;.status.lastScheduleTime | strptime(\&#34;%Y-%m-%dT%H:%M:%SZ\&#34;) | mktime&#34;</span>
<span style="color:#719e07">)</span>

<span style="color:#586e75"># Get current unix time stamp</span>
<span style="color:#268bd2">CURRENT_DATE</span><span style="color:#719e07">=</span><span style="color:#719e07">$(</span>date +%s<span style="color:#719e07">)</span>

<span style="color:#586e75"># How many minutes since the last run?</span>
<span style="color:#268bd2">MINUTES_SINCE_LAST_RUN</span><span style="color:#719e07">=</span><span style="color:#719e07">$((</span><span style="color:#719e07">(</span><span style="color:#268bd2">$CURRENT_DATE</span> <span style="color:#719e07">-</span> <span style="color:#268bd2">$LAST_RUN_DATE</span><span style="color:#719e07">)</span> <span style="color:#719e07">/</span> <span style="color:#2aa198">60</span><span style="color:#719e07">))</span>

<span style="color:#268bd2">DETAIL</span><span style="color:#719e07">=</span><span style="color:#2aa198">&#34;(last run </span><span style="color:#268bd2">$MINUTES_SINCE_LAST_RUN</span><span style="color:#2aa198"> minute(s) ago)&#34;</span>
<span style="color:#719e07">if</span> <span style="color:#719e07">[[</span> <span style="color:#268bd2">$MINUTES_SINCE_LAST_RUN</span> -ge <span style="color:#2aa198">2</span> <span style="color:#719e07">]]</span>; <span style="color:#719e07">then</span>
  <span style="color:#b58900">echo</span> -n <span style="color:#2aa198">&#34;FAIL </span><span style="color:#2aa198">${</span><span style="color:#268bd2">DETAIL</span><span style="color:#2aa198">}</span><span style="color:#2aa198">&#34;</span>
  <span style="color:#b58900">exit</span> <span style="color:#2aa198">1</span>
<span style="color:#719e07">else</span>
  <span style="color:#b58900">echo</span> -n <span style="color:#2aa198">&#34;OK </span><span style="color:#2aa198">${</span><span style="color:#268bd2">DETAIL</span><span style="color:#2aa198">}</span><span style="color:#2aa198">&#34;</span>
  <span style="color:#b58900">exit</span> <span style="color:#2aa198">0</span>
<span style="color:#719e07">fi</span>
</code></pre></div><p><em>Note: Getting tokens for the curl request is covered in <a href="https://docs.openshift.com/container-platform/3.11/rest_api/index.html#rest-api-authentication">OpenShift&rsquo;s
Authentication documentation</a>.</em></p>
<p>If the cron job is running normally, the script output should be:</p>
<pre><code class="language-test" data-lang="test">$ ./check-cron-job.sh
OK (last run 0 minute(s) ago)
$ echo $?
0
</code></pre><p>And when things go wrong:</p>
<pre><code class="language-test" data-lang="test">$ ./check-cron-job.sh
FAIL (last run 22 minute(s) ago)
$ echo $?
1
</code></pre><p><em>Photo credit: <a href="https://pxhere.com/en/photo/757871">pxhere</a></em></p>
]]></content></item><item><title>Monitor CyberPower UPS wattage</title><link>https://major.io/2019/11/08/monitor-cyberpower-ups-wattage/</link><pubDate>Fri, 08 Nov 2019 00:00:00 +0000</pubDate><guid>https://major.io/2019/11/08/monitor-cyberpower-ups-wattage/</guid><description>I have a CyberPower CP1350AVRLCD under my desk at home and I use it to run my computer, monitors, speakers, and a lamp. My new computer is a little more power hungry than my old one since I just moved to to a Ryzen 3700x and Nvidia GeForce 2060 and I like to keep tabs on how much energy it is consuming.
Some power supplies offer a monitoring interface where you can watch your power consumption in real time, but I&amp;rsquo;m not willing to spend that much money.</description><content type="html"><![CDATA[<p><img src="/images/2019-11-08-cyberpower-ups.png" alt="ups"></p>
<p>I have a CyberPower <a href="https://www.cyberpowersystems.com/product/ups/intelligent-lcd/cp1350avrlcd/">CP1350AVRLCD</a> under my desk at home and I use it to run my
computer, monitors, speakers, and a lamp. My new computer is a little more
power hungry than my old one since I just moved to to a Ryzen 3700x and Nvidia
GeForce 2060 and I like to keep tabs on how much energy it is consuming.</p>
<p>Some power supplies offer a monitoring interface where you can watch your
power consumption in real time, but I&rsquo;m not willing to spend that much money.
Most CyberPower UPS units offer some pretty decent power monitoring features
right out of the box, and fortunately for us, they work quite well in Linux.</p>
<p>In this post, we will set up the Linux communication with the UPS and make it
easy to monitor via scripts. Also, we will add it to an existing <a href="https://github.com/polybar/polybar">polybar</a>
configuration so we can monitor it right from the desktop environment.</p>
<h2 id="installing-powerpanel">Installing powerpanel</h2>
<p>CyberPower offers software called <a href="https://www.cyberpowersystems.com/products/software/power-panel-personal/">PowerPanel</a> that runs on most Linux
distributions. It has a daemon (<code>pwrstatd</code>) and a client (<code>pwrstat</code>) that
allows you to monitor the UPS and take actions automatically when the power is
disrupted.</p>
<p>Download the PowerPanel RPM and install it:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">sudo dnf install ~/Downloads/powerpanel-132-0x86_64.rpm
</code></pre></div><p>As I noted in my post called <a href="/2017/07/25/troubleshooting-cyberpower-powerpanel-issues-in-linux/">Troubleshooting CyberPower PowerPanel issues in
Linux</a>, we need to tell <code>pwrstatd</code> where it should communicate with the UPS.
If you skip this step, the daemon hangs without much explanation of what is
happening.</p>
<p>Open <code>/etc/pwrstatd.conf</code> with your favorite text editor and change the
<code>allowed_device_nodes</code> line to point to the right USB device:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ini" data-lang="ini"><span style="color:#586e75"># For example: restrict to use libusb device.</span>
<span style="color:#586e75"># allowed-device-nodes = libusb</span>
allowed-device-nodes <span style="color:#719e07">=</span> <span style="color:#2aa198">/dev/usb/hiddev0</span>
</code></pre></div><p>Unfortunately, CyberPower doesn&rsquo;t ship a systemd unit file for <code>pwrstatd</code>.
Write this unit file to <code>/etc/systemd/system/pwrstatd.service</code>:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ini" data-lang="ini"><span style="color:#719e07">[Unit]</span>
Description<span style="color:#719e07">=</span><span style="color:#2aa198">pwrstatd</span>

<span style="color:#719e07">[Service]</span>
Group<span style="color:#719e07">=</span><span style="color:#2aa198">wheel</span>
UMask<span style="color:#719e07">=</span><span style="color:#2aa198">0002</span>
ExecStart<span style="color:#719e07">=</span><span style="color:#2aa198">/usr/sbin/pwrstatd</span>

<span style="color:#719e07">[Install]</span>
WantedBy<span style="color:#719e07">=</span><span style="color:#2aa198">multi-user.target</span>
</code></pre></div><p>The <code>wheel</code> group should be fine here if your user is already in that group
and uses sudo. You can also change that to a different group, like <code>power</code>,
and then add your user to the <code>power</code> group.</p>
<p>Now we can reload systemd, start <code>pwrstatd</code>, and ensure it comes up at boot
time:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">systemctl daemon-reload
systemctl enable --now pwrstatd
</code></pre></div><h2 id="testing-the-client">Testing the client</h2>
<p>The <code>pwrstat</code> client is installed in <code>/usr/sbin</code> by default, but since this is
my home computer and I trust what happens there, I want to be able to run this
command as my regular user. Move the client to <code>/usr/bin</code> instead:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">mv /usr/sbin/pwrstat /usr/bin/pwrstat
</code></pre></div><p>Let&rsquo;s try getting a current status:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">$ pwrstat -status
The UPS information shows as following:

  Properties:
    Model Name...................  CP 1350C
    Firmware Number.............. BFE5107.B23
    Rating Voltage............... 120 V
    Rating Power................. 810 Watt

  Current UPS status:
    State........................ Normal
    Power Supply by.............. Utility Power
    Utility Voltage.............. 124 V
    Output Voltage............... 124 V
    Battery Capacity............. 100 %
    Remaining Runtime............ 38 min.
    Load......................... 137 Watt(17 %)
    Line Interaction............. None
    Test Result.................. Unknown
    Last Power Event............. None
</code></pre></div><h2 id="just-the-wattage-please">Just the wattage, please</h2>
<p>Awesome! Let&rsquo;s make a really short script that will dump just the wattage for
us:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#719e07">#!/bin/bash
</span><span style="color:#719e07"></span>pwrstat -status | grep -oP <span style="color:#2aa198">&#34;Load\.* \K([0-9]+)(?= Watt)&#34;</span>
</code></pre></div><p>Now we can test the script:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">$ ~/bin/ups_wattage.sh
137
</code></pre></div><p>My computer (and accessories) are using 137 watts.</p>
<h2 id="adding-it-to-polybar">Adding it to polybar</h2>
<p>I use polybar as my status bar, and it&rsquo;s easy to add a custom command to the
bar. Here&rsquo;s my configuration section for my <code>ups_wattage.sh</code> script:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ini" data-lang="ini"><span style="color:#719e07">[module/wattage]</span>
    type <span style="color:#719e07">=</span> <span style="color:#2aa198">custom/script
</span><span style="color:#2aa198">    exec = ~/bin/ups_wattage.sh
</span><span style="color:#2aa198">    label = &#34; %output%W&#34;
</span><span style="color:#2aa198">    interval = 15
</span><span style="color:#2aa198">    format-padding = 1</span>
</code></pre></div><p>Add that to your bar (mine is on the right side):</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ini" data-lang="ini"><span style="color:#719e07">[bar/primary]</span>
    ---SNIP---
    modules-right <span style="color:#719e07">=</span> <span style="color:#2aa198">weather cpu memory gpu filesystem wattage uptime
</span><span style="color:#2aa198">    ---SNIP---</span>
</code></pre></div><p>There&rsquo;s live power monitoring right there in my polybar!</p>
<p><img src="/images/2019-11-08-polybar-wattage.jpg" alt="polybar wattage"></p>
]]></content></item><item><title>Install Chromium with VAAPI on Fedora 30</title><link>https://major.io/2019/10/20/install-chromium-with-vaapi-on-fedora-30/</link><pubDate>Sun, 20 Oct 2019 00:00:00 +0000</pubDate><guid>https://major.io/2019/10/20/install-chromium-with-vaapi-on-fedora-30/</guid><description>UPDATE: The chromium-vaapi package is now chromium-freeworld. This post was updated on 2019-11-06 to include the change. See the end of the post for the update steps.
If you use a web browser to watch videos on a laptop, you&amp;rsquo;ve probably noticed that some videos play without much impact on the battery. Other videos cause the fans to spin wildly and your battery life plummets.
Intel designed a specification called VA API, often called VAAPI (without the space), and it offers up device drivers to applications running on your system.</description><content type="html"><![CDATA[<p><img src="/images/2019-10-20-film.jpg" alt="film"></p>
<p><em><strong>UPDATE:</strong></em> <em>The <code>chromium-vaapi</code> package is now <code>chromium-freeworld</code>.
This post was updated on 2019-11-06 to include the change. See the end of the
post for the update steps.</em></p>
<p>If you use a web browser to watch videos on a laptop, you&rsquo;ve probably noticed
that some videos play without much impact on the battery. Other videos cause
the fans to spin wildly and your battery life plummets.</p>
<p>Intel designed a specification called <a href="https://en.wikipedia.org/wiki/Video_Acceleration_API">VA API</a>, often called <em>VAAPI</em> (without
the space), and it offers up device drivers to applications running on your
system. It provides a pathway for those applications to access certain parts of
the graphics processing hardware directly. This increases performance, lowers
CPU usage, and increases battery life.</p>
<p>In this post, you will learn how to get VAAPI working on your Fedora 30 system
and how to use it along with a Chromium build that has VAAPI patches already
included. There are some DRM-related workarounds as well toward the end.</p>
<p><em>Note: Keep in mind that some videos are in formats that are difficult to
accelerate with a GPU and some applications support acceleration with some
formats but not others. You may find that your favorite site still uses the
same amount of CPU as it did before you completed this guide.</em> 😢</p>
<h2 id="getting-started-with-vaapi">Getting started with VAAPI</h2>
<p>You will need a few packages before you get started, and some of these depend
on the type of GPU that is present in your system. In my case, I&rsquo;m on a 4th
generation Lenovo X1 Carbon, and it has an Skylake GPU:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">$ lspci | grep VGA
00:02.0 VGA compatible controller: Intel Corporation Skylake GT2 [HD Graphics 520] (rev 07)
</code></pre></div><p>Fedora 30 has quite a few VAAPI packages available:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">$ sudo dnf list all | grep libva | awk &#39;{print $1}&#39;
libva.x86_64
libva-intel-driver.x86_64
libva-intel-hybrid-driver.x86_64
libva-utils.x86_64
libva-vdpau-driver.x86_64
libva.i686
libva-devel.i686
libva-devel.x86_64
libva-intel-driver.i686
libva-intel-hybrid-driver.i686
libva-vdpau-driver.i686
</code></pre></div><p>My Intel GPU requires these packages:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">$ sudo dnf install libva libva-intel-driver \
    libva-vdpau-driver \
    libva-utils
</code></pre></div><p>At this point, you should be able to run <code>vainfo</code> to ensure that everything is
working:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">$ vainfo
libva info: VA-API version 1.4.1
libva info: va_getDriverName() returns 0
libva info: Trying to open /usr/lib64/dri/i965_drv_video.so
libva info: Found init function __vaDriverInit_1_4
libva info: va_openDriver() returns 0
vainfo: VA-API version: 1.4 (libva 2.4.1)
vainfo: Driver version: Intel i965 driver for Intel(R) Skylake - 2.3.0
vainfo: Supported profile and entrypoints
      VAProfileMPEG2Simple            :	VAEntrypointVLD
      VAProfileMPEG2Simple            :	VAEntrypointEncSlice
      VAProfileMPEG2Main              :	VAEntrypointVLD
      VAProfileMPEG2Main              :	VAEntrypointEncSlice
      VAProfileH264ConstrainedBaseline:	VAEntrypointVLD
      VAProfileH264ConstrainedBaseline:	VAEntrypointEncSlice
      VAProfileH264ConstrainedBaseline:	VAEntrypointEncSliceLP
      VAProfileH264ConstrainedBaseline:	VAEntrypointFEI
      VAProfileH264ConstrainedBaseline:	VAEntrypointStats
      VAProfileH264Main               :	VAEntrypointVLD
      VAProfileH264Main               :	VAEntrypointEncSlice
      VAProfileH264Main               :	VAEntrypointEncSliceLP
      VAProfileH264Main               :	VAEntrypointFEI
      VAProfileH264Main               :	VAEntrypointStats
      VAProfileH264High               :	VAEntrypointVLD
      VAProfileH264High               :	VAEntrypointEncSlice
      VAProfileH264High               :	VAEntrypointEncSliceLP
      VAProfileH264High               :	VAEntrypointFEI
      VAProfileH264High               :	VAEntrypointStats
      VAProfileH264MultiviewHigh      :	VAEntrypointVLD
      VAProfileH264MultiviewHigh      :	VAEntrypointEncSlice
      VAProfileH264StereoHigh         :	VAEntrypointVLD
      VAProfileH264StereoHigh         :	VAEntrypointEncSlice
      VAProfileVC1Simple              :	VAEntrypointVLD
      VAProfileVC1Main                :	VAEntrypointVLD
      VAProfileVC1Advanced            :	VAEntrypointVLD
      VAProfileNone                   :	VAEntrypointVideoProc
      VAProfileJPEGBaseline           :	VAEntrypointVLD
      VAProfileJPEGBaseline           :	VAEntrypointEncPicture
      VAProfileVP8Version0_3          :	VAEntrypointVLD
      VAProfileVP8Version0_3          :	VAEntrypointEncSlice
      VAProfileHEVCMain               :	VAEntrypointVLD
      VAProfileHEVCMain               :	VAEntrypointEncSlice
      VAProfileVP9Profile0            :	VAEntrypointVLD

</code></pre></div><p>If you run into a problem like this one, try installing the
<code>libva-intel-hybrid-driver</code>:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">$ vainfo
libva info: VA-API version 1.4.1
libva info: va_getDriverName() returns 0
libva info: Trying to open /usr/lib64/dri/i965_drv_video.so
libva info: va_openDriver() returns -1
vaInitialize failed with error code -1 (unknown libva error),exit
</code></pre></div><h2 id="installing-chromium-with-vaapi-support">Installing Chromium with VAAPI support</h2>
<p>Now that we have a pathway for applications to talk to our GPU, we can install
Chromium with VAAPI support:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">$ sudo dnf -y install chromium-freeworld
</code></pre></div><p>Run <code>chromium-freeworld</code> to ensure Chromium starts properly. Visit
<a href="chrome://flags">chrome://flags</a> in the Chromium browser and search for
<code>ignore-gpu-blacklist</code>. Choose <strong>Enabled</strong> in the dropdown and press <strong>Relaunch
Now</strong> in the bottom right corner.</p>
<p>After the relaunch, check some common video sites, like <a href="https://youtube.com/">YouTube</a> or
<a href="http://dailymotion.com">DailyMotion</a>. The CPU usage may be a bit lower on these, but you can lower it
further by installing the <a href="https://github.com/erkserkserks/h264ify">h264ify</a> extension. It forces some sites to provide
h264 video rather than other CPU hungry formats.</p>
<h2 id="dealing-with-drm">Dealing with DRM</h2>
<p>The only remaining problem is DRM. Some sites, like Netflix or YouTube TV,
require that the browser can handle DRM content. The <code>Widevine</code> DRM module is
required for some of these sites, but it is automatically bundled with Chrome.
The regular Chrome (not Chromium) package contains the module at:
<code>/opt/google/chrome/libwidevinecdm.so</code>.</p>
<p>First, ensure Chromium is not running. Then copy that module over to Chromium&rsquo;s
directory:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">sudo cp /opt/google/chrome/libwidevinecdm.so /usr/lib64/chromium-freeworld/
</code></pre></div><p>Start <code>chromium-freeworld</code> one more time and try out some DRM-protected sites like
Netflix and they should be working properly.</p>
<p>As I mentioned at the start of the guide, some applications support
acceleration with certain video formats and not others, so your results may
vary.</p>
<hr>
<h2 id="new-package-chromium-freeworld">New package: <code>chromium-freeworld</code></h2>
<p>When this post was first written, the chromium package was called
<code>chromium-vaapi</code>. It now <code>chromium-freeworld</code>. The upgrade is seamless since
the new package obsoletes the old one, but you need one extra step to bring
over the DRM module to the new chromium library directory:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">sudo cp /usr/lib64/chromium-vaapi/libwidevinecdm.so /usr/lib64/chromium-freeworld
</code></pre></div><p>Restart <code>chromium-freeworld</code> and you&rsquo;re good to go again.</p>
<p><em>Photo credit: <a href="https://pixabay.com/photos/film-movie-cinema-reel-retro-2233692/">Pixabay</a></em></p>
]]></content></item><item><title>Customize GNOME from i3</title><link>https://major.io/2019/09/22/customize-gnome-from-i3/</link><pubDate>Sun, 22 Sep 2019 00:00:00 +0000</pubDate><guid>https://major.io/2019/09/22/customize-gnome-from-i3/</guid><description>i3 has been my window manager of choice for a while and I really enjoy its simplicity and ease of use. I use plenty of gtk applications, such as Firefox and Evolution, and configuring them within i3 can be confusing.
This post covers a few methods to change configurations for GNOME and gtk applications from i3.
lxappearance Almost all of the gtk theming settings are available in lxappearance. You can change fonts, mouse cursors, icons, and colors.</description><content type="html"><![CDATA[<p><img src="/images/2019-09-22-wrenches.jpg" alt="wrenches"></p>
<p>i3 has been my window manager of choice for a while and I really enjoy its
simplicity and ease of use. I use plenty of gtk applications, such as Firefox
and Evolution, and configuring them within i3 can be confusing.</p>
<p>This post covers a few methods to change configurations for GNOME and gtk
applications from i3.</p>
<h2 id="lxappearance">lxappearance</h2>
<p>Almost all of the gtk theming settings are available in <a href="https://wiki.lxde.org/en/LXAppearance">lxappearance</a>. You can
change fonts, mouse cursors, icons, and colors. The application makes the
changes easy to preview and you can install more icon sets if you wish.</p>
<p>Fedora already has lxappearance packaged and ready to go:</p>
<pre><code>$ sudo dnf install lxappearance
$ lxappearance
</code></pre><p>Although style changes are immediately applied in lxappearance, you need to
restart all gtk applications to see the style changes there.</p>
<p>lxappearance writes GTK 2.0 and GTK 3.0 configuration files:</p>
<ul>
<li>GTK 2.0: <code>~/.gtkrc-2.0</code></li>
<li>GTK 3.0: <code>~/.config/gtk-3.0/settings.ini</code></li>
</ul>
<h2 id="gnome-control-center">gnome-control-center</h2>
<p>Recent versions of GNOME bundle all of the system settings into a single
application called <code>gnome-control-center</code>. This normally starts right up in
GNOME, but i3 is a little trickier since it doesn&rsquo;t have some of the same
environment variables set:</p>
<pre><code>$ gnome-control-center 
**
ERROR:../shell/cc-shell-model.c:458:cc_shell_model_set_panel_visibility: assertion failed: (valid)
[1]    837 abort (core dumped)  gnome-control-center
</code></pre><p>The problem is a missing environment variable: <code>XDG_CURRENT_DESKTOP</code>. We can
set that on the command line and everything works:</p>
<pre><code>env XDG_CURRENT_DESKTOP=GNOME gnome-control-center
</code></pre><h2 id="gnome-tweaks">gnome-tweaks</h2>
<p>The <code>gnome-tweaks</code> application has been around for a long time and it works
well from i3. Install it in Fedora and run it:</p>
<pre><code>$ sudo dnf install gnome-tweaks
$ gnome-tweaks
</code></pre><p>Although many of the configurations inside gnome-tweaks match up with
lxappearance, gnome-tweaks offers an added benefit: it changes the
configuration inside GNOME&rsquo;s key-based configuration system (dconf). This is
required for some applications, such as Firefox.</p>
<p>You can also open up <code>dconf-editor</code> and make these changes manually in
<code>/org/gnome/desktop/interface</code>, but gnome-tweaks has a much more user-friendly
interface.</p>
<p><em>Photo credit: <a href="https://www.flickr.com/photos/julia_manzerova/932055546">Julia Manzerova</a></em></p>
]]></content></item><item><title>Deploy monit in OpenShift</title><link>https://major.io/2019/09/11/deploy-monit-in-openshift/</link><pubDate>Wed, 11 Sep 2019 00:00:00 +0000</pubDate><guid>https://major.io/2019/09/11/deploy-monit-in-openshift/</guid><description>Monit is a tried-and-true method for monitoring all kinds of systems, services, and network endpoints. Deploying monit is easy. There&amp;rsquo;s only one binary daemon to run and it reads monitoring configuration from files in a directory you specify.
Most Linux distributions have a package for monit and the package usually contains some basic configuration along with a systemd unit file to run the daemon reliably.
However, this post is all about how to deploy it inside OpenShift.</description><content type="html"><![CDATA[<p><img src="/images/2019-09-11-cctv-cameras.jpg" alt="cctv cameras"></p>
<p><a href="https://mmonit.com/monit/">Monit</a> is a tried-and-true method for monitoring all kinds of systems,
services, and network endpoints. Deploying monit is easy. There&rsquo;s only one
binary daemon to run and it reads monitoring configuration from files in a
directory you specify.</p>
<p>Most Linux distributions have a package for monit and the package usually
contains some basic configuration along with a systemd unit file to run the
daemon reliably.</p>
<p>However, this post is all about how to deploy it inside OpenShift. Deploying
monit inside OpenShift allows you to monitor services inside OpenShift that
might not have a route or a NodePort configured, but you can monitor systems
outside OpenShift, too.</p>
<h2 id="monit-in-a-container">Monit in a container</h2>
<p>Before we can put monit into a container, we need to think about what it
requires. At the most basic level, we will need:</p>
<ul>
<li>the monit daemon binary</li>
<li>a very basic config, the <code>.monitrc</code></li>
<li>a directory to hold lots of additional monitoring configs</li>
<li>any packages needed for running monitoring scripts</li>
</ul>
<p>In my case, some of the scripts I want to run require <code>curl</code>, <code>httpie</code> (for
complex HTTP/JSON requests), and <code>jq</code> (for parsing json). I&rsquo;ve added those,
along with some requirements for the monit binary, to my container build file:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-docker" data-lang="docker"><span style="color:#719e07">FROM</span><span style="color:#2aa198"> fedora:latest</span>

<span style="color:#586e75"># Upgrade packages and install monit.</span>
<span style="color:#719e07">RUN</span> dnf -y upgrade
<span style="color:#719e07">RUN</span> dnf -y install coreutils httpie jq libnsl libxcrypt-compat
<span style="color:#719e07">RUN</span> dnf clean all

<span style="color:#586e75"># Install monit.</span>
<span style="color:#719e07">RUN</span> curl -Lso /tmp/monit.tgz https://bitbucket.org/tildeslash/monit/downloads/monit-5.26.0-linux-x64.tar.gz
<span style="color:#719e07">RUN</span> <span style="color:#b58900">cd</span> /tmp <span style="color:#719e07">&amp;&amp;</span> tar xf monit.tgz
<span style="color:#719e07">RUN</span> mv /tmp/monit-*/bin/monit /usr/local/bin/monit
<span style="color:#719e07">RUN</span> rm -rf /tmp/monit*

<span style="color:#586e75"># Remove monit user/group.</span>
<span style="color:#719e07">RUN</span> sed -i <span style="color:#2aa198">&#39;/^monit/d&#39;</span> /etc/passwd
<span style="color:#719e07">RUN</span> sed -i <span style="color:#2aa198">&#39;/^monit/d&#39;</span> /etc/group

<span style="color:#586e75"># Work around OpenShift&#39;s arbitrary UID/GIDs.</span>
<span style="color:#719e07">RUN</span> chmod <span style="color:#268bd2">g</span><span style="color:#719e07">=</span>u /etc/passwd /etc/group

<span style="color:#586e75"># The monit server listens on 2812.</span>
<span style="color:#719e07">EXPOSE</span><span style="color:#2aa198"> 2812</span>

<span style="color:#586e75"># Set up a volume for /config.</span>
<span style="color:#719e07">VOLUME</span> [<span style="color:#2aa198">&#34;/config&#34;</span>]

<span style="color:#586e75"># Start monit when the container starts.</span>
<span style="color:#719e07">ENV</span> <span style="color:#268bd2">HOME</span><span style="color:#719e07">=</span>/tmp
<span style="color:#719e07">COPY</span> extras/start.sh /opt/start.sh
<span style="color:#719e07">RUN</span> chmod +x /opt/start.sh
<span style="color:#719e07">CMD</span> [<span style="color:#2aa198">&#34;/opt/start.sh&#34;</span>]
</code></pre></div><p>Let&rsquo;s break down what&rsquo;s here in the container build file:</p>
<ul>
<li>Install some basic packages that we need in the container</li>
<li>Download monit and install it to <code>/usr/local/bin/monit</code></li>
<li>Remove the <code>monit</code> user/group <em>(more on this later)</em></li>
<li>Make <code>/etc/passwd</code> and <code>/etc/group</code> writable by the root group <em>(more on this later)</em></li>
<li>Expose the default monit port</li>
<li>Run our special startup script</li>
</ul>
<p>The last three parts help us run with OpenShift&rsquo;s strict security requirements.</p>
<h2 id="startup-script">Startup script</h2>
<p>Monit has some strict security requirements for startup. It requires that the
monit daemon is started with the same user/group combination that owns the
initial configuration file (<code>.monitrc</code>). That&rsquo;s why we removed the <code>monit</code>
user/group <em>and</em> made <code>/etc/passwd</code> and <code>/etc/shadow</code> writable during the build
step. We need to add those back in once the container starts and we&rsquo;ve received
our arbitrary UID from OpenShift.</p>
<p><em>(For more on OpenShift&rsquo;s arbitrary UIDs, read my other post about <a href="/2019/03/22/running-ansible-in-openshift-with-arbitrary-uids/">Running
Ansible in OpenShift with arbitrary UIDs</a>.)</em></p>
<p>Here&rsquo;s the startup script:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#719e07">#!/bin/bash
</span><span style="color:#719e07"></span><span style="color:#b58900">set</span> -euxo pipefail

<span style="color:#b58900">echo</span> <span style="color:#2aa198">&#34;The home directory is: </span><span style="color:#2aa198">${</span><span style="color:#268bd2">HOME</span><span style="color:#2aa198">}</span><span style="color:#2aa198">&#34;</span>

<span style="color:#586e75"># Work around OpenShift&#39;s arbitrary UID/GIDs.</span>
<span style="color:#719e07">if</span> <span style="color:#719e07">[</span> -w <span style="color:#2aa198">&#39;/etc/passwd&#39;</span> <span style="color:#719e07">]</span>; <span style="color:#719e07">then</span>
    <span style="color:#b58900">echo</span> <span style="color:#2aa198">&#34;monit❌`id -u`:`id -g`:,,,:</span><span style="color:#2aa198">${</span><span style="color:#268bd2">HOME</span><span style="color:#2aa198">}</span><span style="color:#2aa198">:/bin/bash&#34;</span> &gt;&gt; /etc/passwd
<span style="color:#719e07">fi</span>
<span style="color:#719e07">if</span> <span style="color:#719e07">[</span> -w <span style="color:#2aa198">&#39;/etc/group&#39;</span> <span style="color:#719e07">]</span>; <span style="color:#719e07">then</span>
    <span style="color:#b58900">echo</span> <span style="color:#2aa198">&#34;monit❌</span><span style="color:#719e07">$(</span>id -G | cut -d<span style="color:#2aa198">&#39; &#39;</span> -f 2<span style="color:#719e07">)</span><span style="color:#2aa198">&#34;</span> &gt;&gt; /etc/group
<span style="color:#719e07">fi</span>

<span style="color:#586e75"># Make a basic monitrc.</span>
<span style="color:#b58900">echo</span> <span style="color:#2aa198">&#34;set daemon 30&#34;</span> &gt;&gt; <span style="color:#2aa198">&#34;</span><span style="color:#2aa198">${</span><span style="color:#268bd2">HOME</span><span style="color:#2aa198">}</span><span style="color:#2aa198">&#34;</span>/monitrc
<span style="color:#b58900">echo</span> <span style="color:#2aa198">&#34;include /config/*&#34;</span> &gt;&gt; <span style="color:#2aa198">&#34;</span><span style="color:#2aa198">${</span><span style="color:#268bd2">HOME</span><span style="color:#2aa198">}</span><span style="color:#2aa198">&#34;</span>/monitrc
chmod <span style="color:#2aa198">0700</span> <span style="color:#2aa198">&#34;</span><span style="color:#2aa198">${</span><span style="color:#268bd2">HOME</span><span style="color:#2aa198">}</span><span style="color:#2aa198">&#34;</span>/monitrc

<span style="color:#586e75"># Ensure the UID/GID mapping works.</span>
id

<span style="color:#586e75"># Run monit.</span>
/usr/local/bin/monit -v -I -c <span style="color:#2aa198">&#34;</span><span style="color:#2aa198">${</span><span style="color:#268bd2">HOME</span><span style="color:#2aa198">}</span><span style="color:#2aa198">&#34;</span>/monitrc
</code></pre></div><p>Let&rsquo;s talk about what is happening in the script:</p>
<ol>
<li>Add the <code>monit</code> user to <code>/etc/passwd</code> with the arbitrary UID</li>
<li>Do the same for the <code>monit</code> group in <code>/etc/group</code></li>
<li>Create a very basic <code>.monitrc</code> that is owned by the <code>monit</code> user and group</li>
<li>Run <code>monit</code> in verbose mode in the foreground with our <code>.monitrc</code></li>
</ol>
<p>OpenShift will make an emptyDir volume in <code>/config</code> that we can modify since we
specified a volume in the container build file.</p>
<h2 id="deploying-monit">Deploying monit</h2>
<p>Now that we have a container and a startup script, it&rsquo;s time to deploy monit in
OpenShift.</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#268bd2">apiVersion</span>: apps.openshift.io/v1
<span style="color:#268bd2">kind</span>: DeploymentConfig
<span style="color:#268bd2">metadata</span>:
  <span style="color:#268bd2">generation</span>: <span style="color:#2aa198">1</span>
  <span style="color:#268bd2">labels</span>:
    <span style="color:#268bd2">app</span>: monit
  <span style="color:#268bd2">name</span>: monit
<span style="color:#268bd2">spec</span>:
  <span style="color:#268bd2">replicas</span>: <span style="color:#2aa198">1</span>
  <span style="color:#268bd2">revisionHistoryLimit</span>: <span style="color:#2aa198">10</span>
  <span style="color:#268bd2">selector</span>:
    <span style="color:#268bd2">app</span>: monit
    <span style="color:#268bd2">deploymentconfig</span>: monit
  <span style="color:#268bd2">strategy</span>:
    <span style="color:#268bd2">activeDeadlineSeconds</span>: <span style="color:#2aa198">21600</span>
    <span style="color:#268bd2">resources</span>: {}
    <span style="color:#268bd2">rollingParams</span>:
      <span style="color:#268bd2">intervalSeconds</span>: <span style="color:#2aa198">1</span>
      <span style="color:#268bd2">maxSurge</span>: <span style="color:#2aa198">25</span>%
      <span style="color:#268bd2">maxUnavailable</span>: <span style="color:#2aa198">25</span>%
      <span style="color:#268bd2">timeoutSeconds</span>: <span style="color:#2aa198">600</span>
      <span style="color:#268bd2">updatePeriodSeconds</span>: <span style="color:#2aa198">1</span>
    <span style="color:#268bd2">type</span>: Rolling
  <span style="color:#268bd2">template</span>:
    <span style="color:#268bd2">metadata</span>:
      <span style="color:#268bd2">labels</span>:
        <span style="color:#268bd2">app</span>: monit
        <span style="color:#268bd2">deploymentconfig</span>: monit
    <span style="color:#268bd2">spec</span>:
      <span style="color:#268bd2">containers</span>:
      - <span style="color:#268bd2">image</span>: registry.gitlab.com/majorhayden/container-monit/monit:latest
        <span style="color:#268bd2">imagePullPolicy</span>: Always
        <span style="color:#268bd2">name</span>: monit
        <span style="color:#268bd2">resources</span>:
          <span style="color:#268bd2">limits</span>:
            <span style="color:#268bd2">cpu</span>: 100m
            <span style="color:#268bd2">memory</span>: 512Mi
          <span style="color:#268bd2">requests</span>:
            <span style="color:#268bd2">cpu</span>: 100m
            <span style="color:#268bd2">memory</span>: 512Mi
        <span style="color:#268bd2">terminationMessagePath</span>: /dev/termination-log
        <span style="color:#268bd2">terminationMessagePolicy</span>: File
        <span style="color:#268bd2">volumeMounts</span>:
        - <span style="color:#268bd2">mountPath</span>: /config
          <span style="color:#268bd2">name</span>: monit-config
        - <span style="color:#268bd2">mountPath</span>: /scripts
          <span style="color:#268bd2">name</span>: monit-scripts
      <span style="color:#268bd2">dnsPolicy</span>: ClusterFirst
      <span style="color:#268bd2">hostname</span>: monit-in-openshift
      <span style="color:#268bd2">restartPolicy</span>: Always
      <span style="color:#268bd2">schedulerName</span>: default-scheduler
      <span style="color:#268bd2">securityContext</span>: {}
      <span style="color:#268bd2">terminationGracePeriodSeconds</span>: <span style="color:#2aa198">30</span>
      <span style="color:#268bd2">volumes</span>:
      - <span style="color:#268bd2">configMap</span>:
          <span style="color:#268bd2">defaultMode</span>: <span style="color:#2aa198">0420</span>
          <span style="color:#268bd2">name</span>: monit-config
        <span style="color:#268bd2">name</span>: monit-config
      - <span style="color:#268bd2">configMap</span>:
          <span style="color:#268bd2">defaultMode</span>: <span style="color:#2aa198">0755</span>
          <span style="color:#268bd2">name</span>: monit-scripts
        <span style="color:#268bd2">name</span>: monit-scripts
  <span style="color:#268bd2">test</span>: <span style="color:#cb4b16">false</span>
  <span style="color:#268bd2">triggers</span>:
  - <span style="color:#268bd2">type</span>: ConfigChange
</code></pre></div><p>There is a lot of text here, but there are two important parts:</p>
<ul>
<li>The container <code>image</code> is pre-built from my <a href="https://gitlab.com/majorhayden/container-monit">monit GitLab repository</a>
(feel free to use it!)</li>
<li>The <code>volumes</code> refer to the OpenShift configmaps that hold the monit
configurations as well as the scripts that are called for monitoring</li>
</ul>
<p>Next comes the service (which allows the monit web port to be exposed inside
the OpenShift cluster):</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#268bd2">apiVersion</span>: v1
<span style="color:#268bd2">kind</span>: Service
<span style="color:#268bd2">metadata</span>:
  <span style="color:#268bd2">labels</span>:
    <span style="color:#268bd2">app</span>: monit
  <span style="color:#268bd2">name</span>: monit
<span style="color:#268bd2">spec</span>:
  <span style="color:#268bd2">ports</span>:
  - <span style="color:#268bd2">port</span>: <span style="color:#2aa198">2812</span>
    <span style="color:#268bd2">protocol</span>: TCP
    <span style="color:#268bd2">targetPort</span>: <span style="color:#2aa198">2812</span>
  <span style="color:#268bd2">selector</span>:
    <span style="color:#268bd2">app</span>: monit
    <span style="color:#268bd2">deploymentconfig</span>: monit
  <span style="color:#268bd2">sessionAffinity</span>: None
  <span style="color:#268bd2">type</span>: ClusterIP
</code></pre></div><p>And finally, the route (which exposes the monit web port service <em>outside</em> the
OpenShift cluster):</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#268bd2">apiVersion</span>: route.openshift.io/v1
<span style="color:#268bd2">kind</span>: Route
<span style="color:#268bd2">metadata</span>:
  <span style="color:#268bd2">labels</span>:
    <span style="color:#268bd2">app</span>: monit
  <span style="color:#268bd2">name</span>: monit
<span style="color:#268bd2">spec</span>:
  <span style="color:#268bd2">tls</span>:
    <span style="color:#268bd2">insecureEdgeTerminationPolicy</span>: Redirect
    <span style="color:#268bd2">termination</span>: edge
  <span style="color:#268bd2">host</span>: monit.openshift.example.com
  <span style="color:#268bd2">to</span>:
    <span style="color:#268bd2">kind</span>: Service
    <span style="color:#268bd2">name</span>: monit
    <span style="color:#268bd2">weight</span>: <span style="color:#2aa198">100</span>
  <span style="color:#268bd2">wildcardPolicy</span>: None
</code></pre></div><h2 id="monitoring-configuration-and-scripts">Monitoring configuration and scripts</h2>
<p>The deploymentConfig for monit refers to a configMap called <code>monit-config</code>.
This config map contains all of the additional monitoring configuration for
monit outside of the <code>.monitrc</code>. Here is a basic configMap for checking that
<code>icanhazheaders.com</code> is accessible:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#268bd2">apiVersion</span>: v1
<span style="color:#268bd2">kind</span>: ConfigMap
<span style="color:#268bd2">metadata</span>:
  <span style="color:#268bd2">name</span>: monit-config
<span style="color:#268bd2">data</span>:
  <span style="color:#268bd2">config</span>: |<span style="color:#2aa198">
</span><span style="color:#2aa198">    set daemon 30
</span><span style="color:#2aa198">    set httpd port 2812
</span><span style="color:#2aa198">      allow 0.0.0.0/0
</span><span style="color:#2aa198">    set alert me@example.com
</span><span style="color:#2aa198">    set mailserver smtp.example.com
</span><span style="color:#2aa198">
</span><span style="color:#2aa198">    check host &#34;icanhazheaders responding&#34; with address icanhazheaders.com
</span><span style="color:#2aa198">      if failed
</span><span style="color:#2aa198">        port 80
</span><span style="color:#2aa198">        for 2 cycles
</span><span style="color:#2aa198">      then alert
</span><span style="color:#2aa198">
</span><span style="color:#2aa198">    check program &#34;icanhazheaders header check&#34;
</span><span style="color:#2aa198">      with path &#34;/scripts/header-check.sh ACCEPT-ENCODING &#39;gzip&#39;&#34;
</span><span style="color:#2aa198">      if status gt 0
</span><span style="color:#2aa198">        then exec &#34;/scripts/irc-notification.sh&#34;
</span><span style="color:#2aa198">        else if succeeded then exec &#34;/scripts/irc-notification.sh&#34;</span>    

</code></pre></div><p>This configuration will check <code>icanhazheaders.com</code> and only alert if the check
fails for two check periods. Each check period is 30 seconds, so the site would
need to be inaccessible for 60 seconds before an alert would be sent.</p>
<p>Also, there is a second check that runs a script. Let&rsquo;s deploy the script to
OpenShift as well:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#268bd2">apiVersion</span>: v1
<span style="color:#268bd2">kind</span>: ConfigMap
<span style="color:#268bd2">metadata</span>:
  <span style="color:#268bd2">name</span>: monit-scripts
<span style="color:#268bd2">data</span>:
  <span style="color:#268bd2">header-check.sh</span>: |<span style="color:#2aa198">
</span><span style="color:#2aa198">    #!/bin/bash
</span><span style="color:#2aa198">    set -euo pipefail
</span><span style="color:#2aa198">
</span><span style="color:#2aa198">    URL=&#34;http://icanhazheaders.com&#34;
</span><span style="color:#2aa198">    HEADER=$1
</span><span style="color:#2aa198">    EXPECTED_VALUE=$2
</span><span style="color:#2aa198">
</span><span style="color:#2aa198">    HEADER_VALUE=$(curl -s ${URL} | jq -r ${HEADER})
</span><span style="color:#2aa198">
</span><span style="color:#2aa198">    if [[ $HEADER_VALUE == $EXPECTED_VALUE ]]; then
</span><span style="color:#2aa198">      exit 0
</span><span style="color:#2aa198">    else
</span><span style="color:#2aa198">      exit 1
</span><span style="color:#2aa198">    fi</span>    
</code></pre></div><p>Use <code>oc apply</code> to deploy all of these YAML files to your OpenShift cluster and
monit should be up and running within seconds!</p>
<p><a href="https://commons.wikimedia.org/wiki/File:CCTV_cameras_in_Mumbai.jpg"><em>Photo credit</em></a></p>
]]></content></item><item><title>Get faster GitLab runners with a ramdisk</title><link>https://major.io/2019/08/16/get-faster-gitlab-runners-with-a-ramdisk/</link><pubDate>Fri, 16 Aug 2019 00:00:00 +0000</pubDate><guid>https://major.io/2019/08/16/get-faster-gitlab-runners-with-a-ramdisk/</guid><description>When you build tons of kernels every day like my team does, you look for speed improvements anywhere you can. Caching repositories, artifacts, and compiled objects makes kernel builds faster and it reduces infrastructure costs.
Need for speed We use GitLab CI in plenty of places, and that means we have a lot of gitlab-runner configurations for OpenShift (using the kubernetes executor) and AWS (using the docker-machine executor). The runner&amp;rsquo;s built-in caching makes it easy to upload and download cached items from object storage repositories like Google Cloud Storage or Amazon S3.</description><content type="html"><![CDATA[<p><img src="/images/2019-08-16-jet-ski.jpg" alt="Jet ski moving fast"></p>
<p>When you build tons of kernels every day like my team does, you look for
speed improvements anywhere you can. Caching repositories, artifacts, and
compiled objects makes kernel builds faster and it reduces infrastructure
costs.</p>
<h2 id="need-for-speed">Need for speed</h2>
<p>We use GitLab CI in plenty of places, and that means we have a lot of
gitlab-runner configurations for OpenShift (using the kubernetes executor)
and AWS (using the docker-machine executor). The runner&rsquo;s built-in <a href="https://docs.gitlab.com/runner/configuration/advanced-configuration.html#the-runnerscache-section">caching</a>
makes it easy to upload and download cached items from object storage
repositories like Google Cloud Storage or Amazon S3.</p>
<p>However, there&rsquo;s an often overlooked feature hiding in the <a href="https://docs.gitlab.com/runner/configuration/advanced-configuration.html#the-runnersdocker-section">configuration for
the docker executor</a> that provides a great performance boost: mounting tmpfs
inside your container. Not familiar with tmpfs? Arch Linux has a <a href="https://wiki.archlinux.org/index.php/Tmpfs">great wiki
page for tmpfs</a> and James Coyle has a <a href="https://www.jamescoyle.net/knowledge/951-the-difference-between-a-tmpfs-and-ramfs-ram-disk">well-written blog post</a> about what
makes it unique from the older ramfs.</p>
<p>RAM is much faster than your average cloud provider&rsquo;s block storage. It also
has incredibly low latency relative to most storage media. There&rsquo;s a <a href="https://people.eecs.berkeley.edu/~rcs/research/interactive_latency.html">great
interactive latency page</a> that allows you to use a slider to travel back in
time to 1990 and compare all kinds of storage performance numbers. <em>(It&rsquo;s
really fun! Go drag the slider and be amazed.)</em></p>
<p>Better yet, many cloud providers give you lots of RAM per CPU on their
instances, so if your work isn&rsquo;t terribly memory intensive, you can use a lot
of this RAM for faster storage.</p>
<h2 id="enabling-tmpfs-in-docker-containers">Enabling tmpfs in Docker containers</h2>
<p>⚠️ <strong>Beware of the dangers of tmpfs before adjusting your runner
configuration!</strong> See the warnings at the end of this post.</p>
<p>This configuration is buried in the middle of the <a href="https://docs.gitlab.com/runner/executors/docker.html#mounting-a-directory-in-ram">docker executor
documentation</a>. You will need to add some extra configuration to your
<code>[runners.docker]</code> section to make it work:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-toml" data-lang="toml">[runners.docker]
  [runners.docker.tmpfs]
      <span style="color:#2aa198">&#34;/ramdisk&#34;</span> = <span style="color:#2aa198">&#34;rw,noexec&#34;</span>
</code></pre></div><p>This configuration mounts a tmpfs volume underneath <code>/ramdisk</code> inside the
container. By default, this directory will be mounted with <code>noexec</code>, but if
you need to execute scripts from that directory, change <code>noexec</code> to <code>exec</code>:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-toml" data-lang="toml">[runners.docker]
  [runners.docker.tmpfs]
      <span style="color:#2aa198">&#34;/ramdisk&#34;</span> = <span style="color:#2aa198">&#34;rw,exec&#34;</span>
</code></pre></div><p>In our case, compiling kernels requires executing scripts, so we use <code>exec</code>
for our tmpfs mounts.</p>
<p><strong>You must be specific for <code>exec</code>!</strong> As an example, this tmpfs volume will be
mounted with <code>noexec</code> since that is the default:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-toml" data-lang="toml">[runners.docker]
  [runners.docker.tmpfs]
      <span style="color:#2aa198">&#34;/ramdisk&#34;</span> = <span style="color:#2aa198">&#34;rw&#34;</span>
</code></pre></div><h2 id="extra-speed">Extra speed</h2>
<p>For even more speed, we moved the objects generated by <code>ccache</code> to the
ramdisk. The seek times are much lower and this allows <code>ccache</code> to look for
its cached objects much more quickly.</p>
<p>Git repositories are also great things to stash on tmpfs. Big kernel
repositories are usually 1.5GB to 2GB in size with tons of files. Checkouts
are really fast when they&rsquo;re done in tmpfs.</p>
<h2 id="dangers-are-lurking">Dangers are lurking</h2>
<p>⚠️ As mentioned earlier, <strong>beware of the dangers of tmpfs.</strong></p>
<ul>
<li>
<p>All of the containers on the machine will share the same amount of RAM for
their tmpfs volumes. Be sure to account for how much each container will use
and how many containers <em>could</em> be present on the same machine.</p>
</li>
<li>
<p>Be aware of how much memory your tests will use when they run. In our case,
kernel compiles can consume 2-4GB of RAM, depending on configuration, so we
try our best to leave some memory free.</p>
</li>
<li>
<p>These volumes also have no limits on how much data can go into the volume.
However, if you put too much data into the tmpfs volume and your system runs
critically low on available RAM, you could see a huge drop in performance,
system instability, or even a crash. 🔥</p>
</li>
</ul>
<p><em><a href="https://www.maxpixel.net/Water-Jet-Ski-Fast-Speed-Summer-Fun-Sport-1470072">Photo credit</a></em></p>
]]></content></item><item><title>buildah error: vfs driver does not support overlay.mountopt options</title><link>https://major.io/2019/08/13/buildah-error-vfs-driver-does-not-support-overlay-mountopt-options/</link><pubDate>Tue, 13 Aug 2019 00:00:00 +0000</pubDate><guid>https://major.io/2019/08/13/buildah-error-vfs-driver-does-not-support-overlay-mountopt-options/</guid><description>Buildah and podman make a great pair for building, managing and running containers on a Linux system. You can even use them with GitLab CI with a few small adjustments, namely the switch from the overlayfs to vfs storage driver.
I have some regularly scheduled GitLab CI jobs that attempt to build fresh containers each morning and I use these to get the latest packages and find out early when something is broken in the build process.</description><content type="html"><![CDATA[<p><img src="/images/2019-08-13-storage-bins.jpg" alt="Storage bins"></p>
<p>Buildah and podman make a great pair for building, managing and running
containers on a Linux system. You can even <a href="/2019/05/24/build-containers-in-gitlab-ci-with-buildah/">use them with GitLab CI</a> with a
few small adjustments, namely the switch from the overlayfs to vfs storage
driver.</p>
<p>I have some regularly scheduled GitLab CI jobs that attempt to build fresh
containers each morning and I use these to get the latest packages and find
out early when something is broken in the build process. A failed build
appeared in my inbox earlier this week with the following error:</p>
<pre><code>+ buildah bud -f builds/builder-fedora30 -t builder-fedora30 .
vfs driver does not support overlay.mountopt options
</code></pre><p>My <a href="https://gitlab.com/cki-project/containers/blob/master/build.sh">container build script</a> is fairly basic, but it does include a change to
use the vfs storage driver:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#586e75"># Use vfs with buildah. Docker offers overlayfs as a default, but buildah</span>
<span style="color:#586e75"># cannot stack overlayfs on top of another overlayfs filesystem.</span>
<span style="color:#b58900">export</span> <span style="color:#268bd2">STORAGE_DRIVER</span><span style="color:#719e07">=</span>vfs
</code></pre></div><p>The script doesn&rsquo;t change any mount options during the build process. A quick
glance at the <code>/etc/containers/storage.conf</code> revealed a possible problem:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ini" data-lang="ini"><span style="color:#719e07">[storage.options]</span>
<span style="color:#586e75"># Storage options to be passed to underlying storage drivers</span>

<span style="color:#586e75"># mountopt specifies comma separated list of extra mount options</span>
mountopt <span style="color:#719e07">=</span> <span style="color:#2aa198">&#34;nodev,metacopy=on&#34;</span>
</code></pre></div><p>These mount options make sense when used with an overlayfs filesystem, but
they are not used with vfs. I commented out the <code>mountopt</code> option, saved the
file, and ran a test build locally. <strong>Success!</strong></p>
<p>Fixing the build script involved a small change to the <code>storage.conf</code> just
before building the container:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#586e75"># Use vfs with buildah. Docker offers overlayfs as a default, but buildah</span>
<span style="color:#586e75"># cannot stack overlayfs on top of another overlayfs filesystem.</span>
<span style="color:#b58900">export</span> <span style="color:#268bd2">STORAGE_DRIVER</span><span style="color:#719e07">=</span>vfs

<span style="color:#586e75"># Newer versions of podman/buildah try to set overlayfs mount options when</span>
<span style="color:#586e75"># using the vfs driver, and this causes errors.</span>
sed -i <span style="color:#2aa198">&#39;/^mountopt =.*/d&#39;</span> /etc/containers/storage.conf
</code></pre></div><p>My containers are happily building again in GitLab.</p>
]]></content></item><item><title>Fedora 30 on Google Compute Engine</title><link>https://major.io/2019/08/07/fedora-30-on-google-compute-engine/</link><pubDate>Wed, 07 Aug 2019 00:00:00 +0000</pubDate><guid>https://major.io/2019/08/07/fedora-30-on-google-compute-engine/</guid><description>Fedora 30 is my primary operating system for desktops and servers, so I usually try to take it everywhere I go. I was recently doing some benchmarking for kernel compiles on different cloud plaforms and I noticed that Fedora isn&amp;rsquo;t included in Google Compute Engine&amp;rsquo;s default list of operating system images.
(Note: Fedora does include links to quick start an Amazon EC2 instance with their pre-built AMI&amp;rsquo;s. They are superb!)</description><content type="html"><![CDATA[<p><img src="/images/2019-08-07-google-hq.jpg" alt="Google building"></p>
<p>Fedora 30 is my primary operating system for desktops and servers, so I
usually try to take it everywhere I go. I was recently doing some
benchmarking for kernel compiles on different cloud plaforms and I noticed
that Fedora isn&rsquo;t included in Google Compute Engine&rsquo;s default list of
operating system images.</p>
<p><em>(Note: Fedora does include links to quick start an Amazon EC2 instance with
their <a href="https://alt.fedoraproject.org/cloud/">pre-built AMI&rsquo;s</a>. They are superb!)</em></p>
<h2 id="first-try">First try</h2>
<p>Fedora does offer cloud images in raw and qcow2 formats, so I decided to give
that a try. Start by downloading the image, decompressing it, and then
repackaging the image into a tarball.</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">$ wget http://mirrors.kernel.org/fedora/releases/30/Cloud/x86_64/images/Fedora-Cloud-Base-30-1.2.x86_64.raw.xz
$ xz -d Fedora-Cloud-Base-30-1.2.x86_64.raw.xz
$ mv Fedora-Cloud-Base-30-1.2.x86_64.raw disk.raw
$ tar cvzf fedora-30-google-cloud.tar.gz disk.raw
</code></pre></div><p>Once that&rsquo;s done, create a bucket on Google storage and upload the tarball.</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">$ gsutil mb gs://fedora-cloud-base-30-image
$ gsutil cp fedora-30-google-cloud.tar.gz gs://fedora-cloud-image-30/
</code></pre></div><p>Uploading 300MB on my 10mbit/sec uplink was a slow process. When that&rsquo;s done,
tell Google Compute Engine that we want a new image made from this raw
disk we uploaded:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">$ gcloud compute images create --source-uri <span style="color:#cb4b16">\
</span><span style="color:#cb4b16"></span>    gs://fedora-cloud-image-30/fedora-30-google-cloud.tar.gz <span style="color:#cb4b16">\
</span><span style="color:#cb4b16"></span>    fedora-30-google-cloud
</code></pre></div><p>After a few minutes, a new custom image called <code>fedora-30-google-cloud</code> will
appear in the list of images in Google Compute Engine.</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">$ gcloud compute images list | grep -i fedora
fedora-30-google-cloud   major-hayden-20150520    PENDING
$ gcloud compute images list | grep -i fedora
fedora-30-google-cloud   major-hayden-20150520    PENDING
$ gcloud compute images list | grep -i fedora
fedora-30-google-cloud   major-hayden-20150520    READY
</code></pre></div><p>I opened a browser, ventured to the <a href="https://console.cloud.google.com/compute/">Google Compute Engine console</a>, and
built a new VM with my image.</p>
<h2 id="problems-abound">Problems abound</h2>
<p>However, there are problems when the instance starts up. The serial console
has plenty of errors:</p>
<pre><code>DataSourceGCE.py[WARNING]: address &quot;http://metadata.google.internal/computeMetadata/v1/&quot; is not resolvable
</code></pre><p>Obviously something is wrong with DNS. It&rsquo;s apparent that <code>cloud-init</code> is
stuck in a bad loop:</p>
<pre><code>url_helper.py[WARNING]: Calling 'http://169.254.169.254/2009-04-04/meta-data/instance-id' failed [87/120s]: bad status code [404]
url_helper.py[WARNING]: Calling 'http://169.254.169.254/2009-04-04/meta-data/instance-id' failed [93/120s]: bad status code [404]
url_helper.py[WARNING]: Calling 'http://169.254.169.254/2009-04-04/meta-data/instance-id' failed [99/120s]: bad status code [404]
url_helper.py[WARNING]: Calling 'http://169.254.169.254/2009-04-04/meta-data/instance-id' failed [105/120s]: bad status code [404]
url_helper.py[WARNING]: Calling 'http://169.254.169.254/2009-04-04/meta-data/instance-id' failed [112/120s]: bad status code [404]
url_helper.py[WARNING]: Calling 'http://169.254.169.254/2009-04-04/meta-data/instance-id' failed [119/120s]: unexpected error [Attempted to set connect timeout to 0.0, but the timeout cannot be set to a value less than or equal to 0.]
DataSourceEc2.py[CRITICAL]: Giving up on md from ['http://169.254.169.254/2009-04-04/meta-data/instance-id'] after 126 seconds
</code></pre><p>Those are EC2-type metadata queries and they won&rsquo;t work here. The instance
also has no idea how to set up networking:</p>
<pre><code>Cloud-init v. 17.1 running 'init' at Wed, 07 Aug 2019 18:27:07 +0000. Up 17.50 seconds.
ci-info: +++++++++++++++++++++++++++Net device info++++++++++++++++++++++++++++
ci-info: +--------+-------+-----------+-----------+-------+-------------------+
ci-info: | Device |   Up  |  Address  |    Mask   | Scope |     Hw-Address    |
ci-info: +--------+-------+-----------+-----------+-------+-------------------+
ci-info: | eth0:  | False |     .     |     .     |   .   | 42:01:0a:f0:00:5f |
ci-info: |  lo:   |  True | 127.0.0.1 | 255.0.0.0 |   .   |         .         |
ci-info: |  lo:   |  True |     .     |     .     |   d   |         .         |
ci-info: +--------+-------+-----------+-----------+-------+-------------------+
</code></pre><p>This image is set up well for Amazon, but it needs some work to work at
Google.</p>
<h2 id="fixing-up-the-image">Fixing up the image</h2>
<p>Go back to the <code>disk.raw</code> that we made in the first step of the blog post. We
need to mount that disk, mount some additional filesystems, and chroot into
the Fedora 30 installation on the raw disk.</p>
<p>Start by making a loop device for the raw disk and enumerating its partitions:</p>
<pre><code>$ sudo losetup  /dev/loop0 disk.raw
$ kpartx -a /dev/loop0
</code></pre><p>Make a mountpoint and mount the first partition on that mountpoint:</p>
<pre><code>$ sudo mkdir /mnt/disk
$ sudo mount /dev/mapper/loop0p1 /mnt/disk
</code></pre><p>We need some extra filesystems mounted before we can run certain commands in
the chroot:</p>
<pre><code>$ sudo mount --bind /dev /mnt/disk/dev
$ sudo mount --bind /sys /mnt/disk/sys
$ sudo mount --bind /proc /mnt/disk/proc
</code></pre><p>Now we can hop into the chroot:</p>
<pre><code>$ sudo chroot /mnt/disk
</code></pre><p>From inside the chroot, remove <code>cloud-init</code> and install
<code>google-compute-engine-tools</code> to help with Google cloud:</p>
<pre><code>$ dnf -y remove cloud-init
$ dnf -y install google-compute-engine-tools
$ dnf clean all
</code></pre><p>The <code>google-compute-engine-tools</code> package has lots of services that help with
running on Google cloud. We need to enable each one to run at boot time:</p>
<pre><code>$ systemctl enable google-accounts-daemon google-clock-skew-daemon \
    google-instance-setup google-network-daemon \
    google-shutdown-scripts google-startup-scripts
</code></pre><p>To learn more about these daemons and what they do, head on over to the
<code>GitHub page</code> for the package.</p>
<p>Exit the chroot and get back to your main system. Now that we have this image
just like we want it, it&rsquo;s time to unmount the image and send it to the
cloud:</p>
<pre><code>$ sudo umount /mnt/disk/dev /mnt/disk/sys /mnt/disk/proc
$ sudo umount /mnt/disk
$ sudo losetup -d /dev/loop0
$ tar cvzf fedora-30-google-cloud-fixed.tar.gz disk.raw
$ gsutil cp fedora-30-google-cloud-fixed.tar.gz gs://fedora-cloud-image-30/
$ gcloud compute images create --source-uri \
    gs://fedora-cloud-image-30/fedora-30-google-cloud-fixed.tar.gz \
    fedora-30-google-cloud-fixed
</code></pre><p>Start a new instance with this fixed image and watch it boot in the serial
console:</p>
<pre><code>[   10.379253] RAPL PMU: API unit is 2^-32 Joules, 3 fixed counters, 10737418240 ms ovfl timer
[   10.381350] RAPL PMU: hw unit of domain pp0-core 2^-0 Joules
[   10.382487] RAPL PMU: hw unit of domain package 2^-0 Joules
[   10.383415] RAPL PMU: hw unit of domain dram 2^-16 Joules
[   10.503233] EDAC sbridge:  Ver: 1.1.2


Fedora 30 (Cloud Edition)
Kernel 5.1.20-300.fc30.x86_64 on an x86_64 (ttyS0)

instance-2 login:
</code></pre><p>Yes! A ten second boot with networking is exactly what I needed.</p>
]]></content></item><item><title>Texas Linux Fest 2019 Recap</title><link>https://major.io/2019/06/02/texas-linux-fest-2019-recap/</link><pubDate>Sun, 02 Jun 2019 00:00:00 +0000</pubDate><guid>https://major.io/2019/06/02/texas-linux-fest-2019-recap/</guid><description>Another Texas Linux Fest has come and gone! The 2019 Texas Linux Fest was held in Irving at the Irving Convention Center. It was a great venue surrounded by lots of shops and restaurants.
If you haven&amp;rsquo;t attended one of these events before, you really should! Attendees have varying levels of experience with Linux and the conference organizers (volunteers) work really hard to ensure everyone feels included.
The event usually falls on a Friday and Saturday.</description><content type="html"><![CDATA[<p><img src="/images/2019-06-02-irving-las-colinas.jpg" alt="Las Colinas in Irving"></p>
<p>Another Texas Linux Fest has come and gone! The <a href="https://2019.texaslinuxfest.org/">2019 Texas Linux Fest</a> was
held in Irving at the Irving Convention Center. It was a great venue surrounded
by lots of shops and restaurants.</p>
<p>If you haven&rsquo;t attended one of these events before, you really should!
Attendees have varying levels of experience with Linux and the conference
organizers (volunteers) work really hard to ensure everyone feels included.</p>
<p>The event usually falls on a Friday and Saturday. Fridays consist of longer,
deeper dive talks on various topics &ndash; technical and non-technical. Saturdays
are more of a typical conference format with a keynote in the morning and
45-minute talks through the day. Saturday nights have lightning talks as well
as &ldquo;Birds of a Feather&rdquo; events for people with similar interests.</p>
<h2 id="highlights">Highlights</h2>
<p><a href="https://twitter.com/linuxovens?lang=en">Steve Ovens</a> took us on a three hour journey on Friday to learn more about our
self-worth. His talk, &ldquo;<a href="https://2019.texaslinuxfest.org/presentations.html#trackf3_2">You&rsquo;re Worth More Than You Know, Matching your Skills
to Employers</a>&rdquo;, covered a myriad of concepts such as discovering what really
motivates you, understanding how to value yourself (and your skills), and how
to work well with different personality types.</p>
<p>I&rsquo;ve attended these types of talks before and they sometimes end up a bit
fluffy without items that you can begin using quickly. Steve&rsquo;s talk was the
opposite. He gave us concrete ways to change how we think about ourselves and
use that knowledge to advance ourselves at work. I learned a lot about
negotiation strategies for salary when getting hired or when pushing for a
raise. Steve stopped lots of times to answer questions and it was clear that he
was <em>really</em> interested in this topic.</p>
<p><a href="https://twitter.com/thomasdcameron">Thomas Cameron</a> kicked off Saturday with his &ldquo;Linux State of the Union&rdquo; talk.
He talked a lot about his personal journey and how he has changed along the
way. He noted quite a few changes to Linux (not the code, but the people around
it) that many of us had not noticed. We learned more about how we can make the
Linux community more diverse, inclusive, and welcoming. We also groaned through
some problems from the good old days with jumpers on SATA cards and the joys of
winmodems.</p>
<p><a href="https://twitter.com/themaxamillion">Adam Miller</a> threw us into a seat of a roller coaster and gave a whirlwind
talk about all the ways you can automate (nearly) everything with Ansible.</p>
<p><img src="/images/2019-06-02-adam-miller-ansible.jpg" alt="Adam Miller Ansible talk"></p>
<p>He covered everything from simple configuration management tasks to scaling up
software deployments over thousands of nodes. Adam also explained the OCI image
format as being &ldquo;sweet sweet tarballs with a little bit of metadata&rdquo; and the
audience was rolling with laughter. Adam&rsquo;s talks are always good and you&rsquo;ll be
energized all the way through.</p>
<p><a href="https://twitter.com/bureado">José Miguel Parrella</a> led a great lightning talk in the evening about how
Microsoft uses Linux in plenty of places:</p>
<p><img src="/images/2019-06-02-debian-at-microsoft.jpg" alt="Debian at Microsoft slide"></p>
<p>The audience was shocked by how much Debian was used at Microsoft and it made
it more clear that Microsoft is really making a big shift towards open source
well. Many of us knew that already but we didn&rsquo;t know the extent of the work
being done.</p>
<h2 id="my-talks">My talks</h2>
<p>My first talk was about my team at Red Hat, the <a href="https://cki-project.org/">Continuous Kernel Integration
team</a>. I shared some of the challenges involved with doing CI for the kernel at
scale and how difficult it is to increase test coverage of subsystems within
the kernel. There were two kernel developers in the audience and they had some
really good questions.</p>
<p>The discussion at the end was quite productive. The audience had plenty of
questions about how different pieces of the system worked, and how well GitLab
was working for us. We also talked a bit about how the kernel is developed and
if there is room for improvement. One attendee hoped that some of the work
we&rsquo;re doing will change the kernel development process for the better. I hope
so, too.</p>
<p>My second talk covered the topic of burnout. I have delivered plenty of talks
about impostor syndrome in the past and I was eager to share more ideas around
&ldquo;soft&rdquo; skills that become more important to technical career development over
time.</p>
<p>The best part of these types of talks for me is the honesty that people bring
when they share their thoughts after the talk. A few people from the audience
shared their own personal experiences (some were <em>very</em> personal) and you could
see people in the audience begin to understand how difficult burnout recovery
can be. Small conferences like these create environments where people can talk
honestly about difficult topics.</p>
<p>If you&rsquo;re looking for the slides from these talks, you can view them in
Google Slides (for the sake of the GIFs!):</p>
<ul>
<li><a href="https://docs.google.com/presentation/d/1T0JaRA0wtDU0aTWTyASwwy_ugtzjUcw_ZDmC5KFzw-A/edit">Continuous Kernel Integration</a></li>
<li><a href="https://docs.google.com/presentation/d/1uYY8Ezw8rMhMjRA_d-H8rnv4AWqROyBx86HTcGFEtdQ/edit">I was too burned out to name this talk</a></li>
</ul>
<p>Google Slides also allows you to download the slides as PDFs. Just choose
File &gt; Download as &gt; PDF.</p>
<h2 id="bof-ham-radio-and-oss">BoF: Ham Radio and OSS</h2>
<p>The BoFs were fairly late in the day and everyone was looking tired. However,
we had a great group assemble for the Ham Radio and OSS BoF. We had about 15-20
licensed hams and 5-6 people who were curious about the hobby.</p>
<p>We talked about radios, antennas, procedures, how to study, and the exams. The
ham-curious folks who joined us looked a bit overwhelmed by the help they were
getting, but they left the room with plenty of ideas on how to get started.</p>
<p>I also agreed to write a blog post about everything I&rsquo;ve learned so far that
has made the hobby easier for me and I hope to write that soon. There is so
much information out there for studying and finding equipment that it can
become really confusing for people new to the hobby.</p>
<h2 id="final-thoughts">Final thoughts</h2>
<p>If you get the opportunity to attend a local Linux fest in your state, do it!
The Texas one is always good and people joined us from Arkansas, Oklahoma,
Louisiana, and Arizona. Some people came as far as Connecticut and the United
Kingdom! These smaller events have a much higher signal to noise ratio and
there is more real discussion rather than marketing from industry giants.</p>
<p>Thanks to everyone who put the Texas Linux Fest together this year!</p>
]]></content></item><item><title>Build containers in GitLab CI with buildah</title><link>https://major.io/2019/05/24/build-containers-in-gitlab-ci-with-buildah/</link><pubDate>Fri, 24 May 2019 00:00:00 +0000</pubDate><guid>https://major.io/2019/05/24/build-containers-in-gitlab-ci-with-buildah/</guid><description>My team at Red Hat depends heavily on GitLab CI and we build containers often to run all kinds of tests. Fortunately, GitLab offers up CI to build containers and a container registry in every repository to hold the containers we build.
This is really handy because it keeps everything together in one place: your container build scripts, your container build infrastructure, and the registry that holds your containers. Better yet, you can put multiple types of containers underneath a single git repository if you need to build containers based on different Linux distributions.</description><content type="html"><![CDATA[<p><img src="/images/2019-05-24-cranes-skycrapers.jpg" alt="cranes and skyscrapers"></p>
<p>My team at Red Hat depends heavily on <a href="https://about.gitlab.com/product/continuous-integration/">GitLab CI</a> and we build containers
often to run all kinds of tests. Fortunately, GitLab offers up CI to build
containers and a <a href="https://about.gitlab.com/2016/05/23/gitlab-container-registry/">container registry</a> in every repository to hold the
containers we build.</p>
<p>This is really handy because it keeps everything together in one place: your
container build scripts, your container build infrastructure, and the
registry that holds your containers. Better yet, you can put multiple types
of containers underneath a single git repository if you need to build
containers based on different Linux distributions.</p>
<h2 id="building-with-docker-in-gitlab-ci">Building with Docker in GitLab CI</h2>
<p>By default, GitLab offers up a <a href="https://docs.gitlab.com/ee/ci/docker/using_docker_build.html">Docker builder</a> that works just fine. The CI
system clones your repository, builds your containers and pushes them
wherever you want. There&rsquo;s even a <a href="https://docs.gitlab.com/ee/ci/docker/using_docker_build.html#using-docker-caching">simple CI YAML file</a> that does everything
end-to-end for you.</p>
<p>However, I have two issues with the Docker builder:</p>
<ul>
<li>
<p><strong>Larger images:</strong> The Docker image layering is handy, but the images end up
being a bit larger, especially if you don&rsquo;t do a little cleanup in each
stage.</p>
</li>
<li>
<p><strong>Additional service:</strong> It requires an additional service inside the CI
runner for the <code>dind</code> (&ldquo;Docker in Docker&rdquo;) builder. This has caused some CI
delays for me several times.</p>
</li>
</ul>
<h2 id="building-with-buildah-in-gitlab-ci">Building with buildah in GitLab CI</h2>
<p>On my local workstation, I use <a href="https://podman.io/">podman</a> and <a href="https://buildah.io/">buildah</a> all the time to build,
run, and test containers. These tools are handy because I don&rsquo;t need to
remember to start the Docker daemon each time I want to mess with a
container. I also don&rsquo;t need sudo.</p>
<p>All of my containers are stored beneath my home directory. That&rsquo;s good for
keeping disk space in check, but it&rsquo;s especially helpful on shared servers
since each user has their own unique storage. My container pulls and builds
won&rsquo;t disrupt anyone else&rsquo;s work on the server and their work won&rsquo;t disrupt
mine.</p>
<p>Finally, buildah offers some nice options out of the box. First, when you
build a container with <code>buildah bud</code>, you end up with only three layers by
default:</p>
<ol>
<li>Original OS layer (example: <code>fedora:30</code>)</li>
<li>Everything you added on top of the OS layer</li>
<li>Tiny bit of metadata</li>
</ol>
<p>This is incredibly helpful if you use package managers like <code>dnf</code>, <code>apt</code>, and
<code>yum</code> that download a bunch of metadata before installing packages. You would
normally have to clear the metadata carefully for the package manager so that
your container wouldn&rsquo;t grow in size. Buildah takes care of that by squashing
all the stuff you add into one layer.</p>
<p>Of course, if you want to be more aggressive, buildah offers the <code>--squash</code>
option which squashes the whole image down into one layer. This can be
helpful if disk space is at a premium and you change the layers often.</p>
<h2 id="getting-started">Getting started</h2>
<p>I have a repository called <a href="https://gitlab.com/majorhayden/os-containers">os-containers</a> in GitLab that maintains fully
updated containers for Fedora 29 and 30. The <code>.gitlab-ci.yml</code> file calls
<code>build.sh</code> for two containers: <em>fedora29</em> and <em>fedora30</em>. Open the <code>build.sh</code>
file and follow along here:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#586e75"># Use vfs with buildah. Docker offers overlayfs as a default, but buildah</span>
<span style="color:#586e75"># cannot stack overlayfs on top of another overlayfs filesystem.</span>
<span style="color:#b58900">export</span> <span style="color:#268bd2">STORAGE_DRIVER</span><span style="color:#719e07">=</span>vfs
</code></pre></div><p>First off, we need to tell buildah to use the vfs storage driver. Docker uses
overlayfs by default and stacking overlay filesystems will definitely lead to
problems. Buildah won&rsquo;t let you try it.</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#586e75"># Write all image metadata in the docker format, not the standard OCI format.</span>
<span style="color:#586e75"># Newer versions of docker can handle the OCI format, but older versions, like</span>
<span style="color:#586e75"># the one shipped with Fedora 30, cannot handle the format.</span>
<span style="color:#b58900">export</span> <span style="color:#268bd2">BUILDAH_FORMAT</span><span style="color:#719e07">=</span>docker
</code></pre></div><p>By default, buildah uses the <a href="https://github.com/opencontainers/image-spec"><em>oci</em> container format</a>. This sometimes causes
issues with older versions of Docker that don&rsquo;t understand how to parse that
type of metadata. By setting the format to <code>docker</code>, we&rsquo;re using a format
that almost all container runtimes can understand.</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#586e75"># Log into GitLab&#39;s container repository.</span>
<span style="color:#b58900">export</span> <span style="color:#268bd2">REGISTRY_AUTH_FILE</span><span style="color:#719e07">=</span><span style="color:#2aa198">${</span><span style="color:#268bd2">HOME</span><span style="color:#2aa198">}</span>/auth.json
<span style="color:#b58900">echo</span> <span style="color:#2aa198">&#34;</span><span style="color:#268bd2">$CI_REGISTRY_PASSWORD</span><span style="color:#2aa198">&#34;</span> | buildah login -u <span style="color:#2aa198">&#34;</span><span style="color:#268bd2">$CI_REGISTRY_USER</span><span style="color:#2aa198">&#34;</span> --password-stdin <span style="color:#268bd2">$CI_REGISTRY</span>
</code></pre></div><p>Here we set a path for the <code>auth.json</code> that contains the credentials for
talking to the container repository. We also use buildah to authenticate to
GitLab&rsquo;s built-in container repository. GitLab automatically exports these
variables for us (and hides them in the job output), so we can use them here.</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">buildah bud -f builds/<span style="color:#2aa198">${</span><span style="color:#268bd2">IMAGE_NAME</span><span style="color:#2aa198">}</span> -t <span style="color:#2aa198">${</span><span style="color:#268bd2">IMAGE_NAME</span><span style="color:#2aa198">}</span> .
</code></pre></div><p>We&rsquo;re now building the container and storing it temporarily as the bare image
name, such as <em>fedora30</em>. This is roughly equivalent to <code>docker build</code>.</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#268bd2">CONTAINER_ID</span><span style="color:#719e07">=</span><span style="color:#719e07">$(</span>buildah from <span style="color:#2aa198">${</span><span style="color:#268bd2">IMAGE_NAME</span><span style="color:#2aa198">}</span><span style="color:#719e07">)</span>
buildah commit --squash <span style="color:#268bd2">$CONTAINER_ID</span> <span style="color:#268bd2">$FQ_IMAGE_NAME</span>
</code></pre></div><p>Now we are making a reference to our container with <code>buildah from</code> and using
that reference to squash that container down into a single layer. This keeps
the container as small as possible.</p>
<p>The <code>commit</code> step also tags the resulting image using our fully qualified
image name (in this case, it&rsquo;s
<code>registry.gitlab.com/majorhayden/os-containers/fedora30:latest</code>)</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">buildah push <span style="color:#2aa198">${</span><span style="color:#268bd2">FQ_IMAGE_NAME</span><span style="color:#2aa198">}</span>
</code></pre></div><p>This is the same as <code>docker push</code>. There&rsquo;s not much special to see here.</p>
<h2 id="maintaining-containers">Maintaining containers</h2>
<p>GitLab allows you to take things to the next level with CI schedules. In my
repository, there is a schedule to build my containers once a day to catch
the latest updates. I use these containers a lot and they need to be up to
date before I can run tests.</p>
<p>If the container build fails for some reason, GitLab will send me an email to
let me know.</p>
<p><a href="https://pxhere.com/en/photo/942096"><em>Photo Source</em></a></p>
]]></content></item><item><title>Inspecting OpenShift cgroups from inside the pod</title><link>https://major.io/2019/04/05/inspecting-openshift-cgroups-from-inside-the-pod/</link><pubDate>Fri, 05 Apr 2019 00:00:00 +0000</pubDate><guid>https://major.io/2019/04/05/inspecting-openshift-cgroups-from-inside-the-pod/</guid><description>My team at Red Hat builds a lot of kernels in OpenShift pods as part of our work with the Continuous Kernel Integration (CKI) project. We have lots of different pod sizes depending on the type of work we are doing and our GitLab runners spawn these pods based on the tags in our GitLab CI pipeline.
Compiling with make When you compile a large software project, such as the Linux kernel, you can use multiple CPU cores to speed up the build.</description><content type="html"><![CDATA[<p><img src="/images/2019-04-05-inspecting-cgroups.jpg" alt="walking_through_rock_valley"></p>
<p>My team at Red Hat builds a lot of kernels in OpenShift pods as part of our
work with the <a href="https://cki-project.org/">Continuous Kernel Integration (CKI)</a> project. We have lots of
different pod sizes depending on the type of work we are doing and our GitLab
runners spawn these pods based on the tags in our GitLab CI pipeline.</p>
<h2 id="compiling-with-make">Compiling with <code>make</code></h2>
<p>When you compile a large software project, such as the Linux kernel, you can
use multiple CPU cores to speed up the build. GNU&rsquo;s <code>make</code> does this with the
<code>-j</code> argument. Running <code>make</code> with <code>-j10</code> means that you want to run 10 jobs
while compiling. This would keep 10 CPU cores busy.</p>
<p>Setting the number too high causes more contention from the CPU and can
reduce performance. Setting the number too low means that you are spending
more time compiling than you would if you used all of your CPU cores.</p>
<p>Every once in a while, we adjusted our runners to use a different amount of
CPUs or memory and then we had to adjust our pipeline to reflect the new CPU
count. This was time consuming and error prone.</p>
<p>Many people just use <code>nproc</code> to determine the CPU core count. It works well
with make:</p>
<pre><code>make -j$(nproc)
</code></pre><h2 id="problems-with-containers">Problems with containers</h2>
<p>The handy <code>nproc</code> doesn&rsquo;t work well for OpenShift. If you start a pod on
OpenShift and limit it to a single CPU core, <code>nproc</code> tells you something very
wrong:</p>
<pre><code>$ nproc
32
</code></pre><p>We applied the single CPU limit with OpenShift, so what&rsquo;s the problem? The
issue is how <code>nproc</code> looks for CPUs. Here&rsquo;s a snippet of <code>strace</code> output:</p>
<pre><code>sched_getaffinity(0, 128, [0, 1, 2, 3, 4, 5]) = 8
fstat(1, {st_mode=S_IFCHR|0620, st_rdev=makedev(0x88, 0x6), ...}) = 0
write(1, &quot;6\n&quot;, 26
)                      = 2
</code></pre><p>The <a href="https://linux.die.net/man/2/sched_getaffinity">sched_getaffinity</a> syscall looks to see which CPUs are allowed to run
the process and returns a count of those. OpenShift doesn&rsquo;t prevent us from
seeing the CPUs of the underlying system (the VM or bare metal host
underneath our containers), but it uses cgroups to limit how much CPU time we
can use.</p>
<h2 id="reading-cgroups">Reading cgroups</h2>
<p>Getting cgroup data is easy! Just change into the <code>/sys/fs/cgroup/</code> directory
and look around:</p>
<pre><code>$ cd /sys/fs/cgroup/
$ ls -al cpu/
ls: cannot open directory 'cpu/': Permission denied
</code></pre><p><strong>Ouch.</strong> OpenShift makes this a little more challenging. We&rsquo;re not allowed to
wander around in the land of cgroups without a map to exactly what we want.</p>
<p>My Fedora workstation shows a bunch of CPU cgroup settings:</p>
<pre><code>$ ls -al /sys/fs/cgroup/cpu/
total 0
dr-xr-xr-x.  2 root root   0 Apr  5 01:40 .
drwxr-xr-x. 14 root root 360 Apr  5 01:40 ..
-rw-r--r--.  1 root root   0 Apr  5 13:08 cgroup.clone_children
-rw-r--r--.  1 root root   0 Apr  5 01:40 cgroup.procs
-r--r--r--.  1 root root   0 Apr  5 13:08 cgroup.sane_behavior
-r--r--r--.  1 root root   0 Apr  5 13:08 cpuacct.stat
-rw-r--r--.  1 root root   0 Apr  5 13:08 cpuacct.usage
-r--r--r--.  1 root root   0 Apr  5 13:08 cpuacct.usage_all
-r--r--r--.  1 root root   0 Apr  5 13:08 cpuacct.usage_percpu
-r--r--r--.  1 root root   0 Apr  5 13:08 cpuacct.usage_percpu_sys
-r--r--r--.  1 root root   0 Apr  5 13:08 cpuacct.usage_percpu_user
-r--r--r--.  1 root root   0 Apr  5 13:08 cpuacct.usage_sys
-r--r--r--.  1 root root   0 Apr  5 13:08 cpuacct.usage_user
-rw-r--r--.  1 root root   0 Apr  5 09:10 cpu.cfs_period_us
-rw-r--r--.  1 root root   0 Apr  5 13:08 cpu.cfs_quota_us
-rw-r--r--.  1 root root   0 Apr  5 09:10 cpu.shares
-r--r--r--.  1 root root   0 Apr  5 13:08 cpu.stat
-rw-r--r--.  1 root root   0 Apr  5 13:08 notify_on_release
-rw-r--r--.  1 root root   0 Apr  5 13:08 release_agent
-rw-r--r--.  1 root root   0 Apr  5 13:08 tasks
</code></pre><p>OpenShift uses the <a href="https://en.wikipedia.org/wiki/Completely_Fair_Scheduler">Completely Fair Scheduler (CFS)</a> to limit CPU time. Here&rsquo;s a quick excerpt from the <a href="https://www.kernel.org/doc/Documentation/scheduler/sched-bwc.txt">kernel documentation</a>:</p>
<blockquote>
<p>Quota and period are managed within the cpu subsystem via cgroupfs.</p>
<p>cpu.cfs_quota_us: the total available run-time within a period (in microseconds)
cpu.cfs_period_us: the length of a period (in microseconds)
cpu.stat: exports throttling statistics [explained further below]</p>
<p>The default values are:
cpu.cfs_period_us=100ms
cpu.cfs_quota=-1</p>
<p>A value of -1 for cpu.cfs_quota_us indicates that the group does not have any
bandwidth restriction in place, such a group is described as an unconstrained
bandwidth group.  This represents the traditional work-conserving behavior for
CFS.</p>
<p>Writing any (valid) positive value(s) will enact the specified bandwidth limit.
The minimum quota allowed for the quota or period is 1ms.  There is also an
upper bound on the period length of 1s.  Additional restrictions exist when
bandwidth limits are used in a hierarchical fashion, these are explained in
more detail below.</p>
<p>Writing any negative value to cpu.cfs_quota_us will remove the bandwidth limit
and return the group to an unconstrained state once more.</p>
<p>Any updates to a group&rsquo;s bandwidth specification will result in it becoming
unthrottled if it is in a constrained state.</p>
</blockquote>
<p>Let&rsquo;s see if inspecting <code>cpu.cfs_quota_us</code> can help us:</p>
<pre><code>$ cat /sys/fs/cgroup/cpu/cpu.cfs_quota_us
10000
</code></pre><p>Now we&rsquo;re getting somewhere. But what does <em>10000</em> mean here? OpenShift
operates on the concept of <em>millicores</em> of CPU time, or 1/1000 of a CPU. 500
millicores is half a CPU and 1000 millicores is a whole CPU.</p>
<p>The pod in this example is assigned 100 millicores. Now we know that we can
take the output of <code>/sys/fs/cgroup/cpu/cpu.cfs_quota_us</code>, divide by 100, and
get our millicores.</p>
<p>We can make a script like this:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#268bd2">CFS_QUOTA</span><span style="color:#719e07">=</span><span style="color:#719e07">$(</span>cat /sys/fs/cgroup/cpu/cpu.cfs_quota_us<span style="color:#719e07">)</span>
<span style="color:#719e07">if</span> <span style="color:#719e07">[</span> <span style="color:#268bd2">$CFS_QUOTA</span> -lt <span style="color:#2aa198">100000</span> <span style="color:#719e07">]</span>; <span style="color:#719e07">then</span>
  <span style="color:#268bd2">CPUS_AVAILABLE</span><span style="color:#719e07">=</span><span style="color:#2aa198">1</span>
<span style="color:#719e07">else</span>
  <span style="color:#268bd2">CPUS_AVAILABLE</span><span style="color:#719e07">=</span><span style="color:#719e07">$(</span>expr <span style="color:#2aa198">${</span><span style="color:#268bd2">CFS_QUOTA</span><span style="color:#2aa198">}</span> / <span style="color:#2aa198">100</span> / 1000<span style="color:#719e07">)</span>
<span style="color:#719e07">fi</span>
<span style="color:#b58900">echo</span> <span style="color:#2aa198">&#34;Found </span><span style="color:#2aa198">${</span><span style="color:#268bd2">CPUS_AVAILABLE</span><span style="color:#2aa198">}</span><span style="color:#2aa198"> CPUS&#34;</span>
make -j<span style="color:#2aa198">${</span><span style="color:#268bd2">CPUS_AVAILABLE</span><span style="color:#2aa198">}</span> ...
</code></pre></div><p>The script checks for the value of the quota and divides by 100,000 to get
the number of cores. If the share is set to something less than 100,000, then
a core count of 1 is assigned. <em>(Pro tip: <code>make</code> does not like being told to
compile with zero jobs.)</em></p>
<h2 id="reading-memory-limits">Reading memory limits</h2>
<p>There are other limits you can read and inspect in a pod, including the
available RAM. As we found with <code>nproc</code>, <code>free</code> is not very helpful:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#586e75"># An OpenShift pod with 200MB RAM</span>
$ free -m
              total        used        free      shared  buff/cache   available
Mem:          <span style="color:#2aa198">32008</span>       <span style="color:#2aa198">12322</span>         <span style="color:#2aa198">880</span>          <span style="color:#2aa198">31</span>       <span style="color:#2aa198">18805</span>       <span style="color:#2aa198">19246</span>
Swap:             <span style="color:#2aa198">0</span>           <span style="color:#2aa198">0</span>           <span style="color:#2aa198">0</span>
</code></pre></div><p>But the cgroups tell the truth:</p>
<pre><code>$ cat /sys/fs/cgroup/memory/memory.limit_in_bytes
209715200
</code></pre><p>If you run Java applications in a container, like Jenkins (or Jenkins
slaves), be sure to use the <code>-XX:+UseCGroupMemoryLimitForHeap</code> option. That
will cause Java to look at the cgroups to determine its heap size.</p>
<p><em>Photo credit: <a href="https://commons.wikimedia.org/wiki/File:Roca_de_la_Ley,_Parque_Nacional_de_%C3%9Eingvellir,_Su%C3%B0urland,_Islandia,_2014-08-16,_DD_022.JPG">Wikipedia</a></em></p>
]]></content></item><item><title>Running Ansible in OpenShift with arbitrary UIDs</title><link>https://major.io/2019/03/22/running-ansible-in-openshift-with-arbitrary-uids/</link><pubDate>Fri, 22 Mar 2019 00:00:00 +0000</pubDate><guid>https://major.io/2019/03/22/running-ansible-in-openshift-with-arbitrary-uids/</guid><description>My work at Red Hat involves testing lots and lots of kernels from various sources and we use GitLab CE to manage many of our repositories and run our CI jobs. Those jobs run in thousands of OpenShift containers that we spawn every day.
OpenShift has some handy security features that we like. First, each container is mounted read-only with some writable temporary space (and any volumes that you mount). Also, OpenShift uses arbitrarily assigned user IDs (UIDs) for each container.</description><content type="html"><![CDATA[<p><img src="/images/2019-03-22-blacksmith-anvil-hammer.jpg" alt="blacksmith_anvil_hammer"></p>
<p>My work at Red Hat involves testing lots and lots of kernels from various
sources and we use <a href="https://gitlab.com/gitlab-org/gitlab-ce/">GitLab CE</a> to manage many of our repositories and run our
CI jobs. Those jobs run in <em>thousands</em> of <a href="https://www.openshift.com/">OpenShift</a> containers that we
spawn every day.</p>
<p>OpenShift has some handy security features that we like. First, each
container is mounted read-only with some writable temporary space (and any
volumes that you mount). Also, OpenShift uses <a href="https://docs.openshift.com/container-platform/3.11/creating_images/guidelines.html#openshift-specific-guidelines">arbitrarily assigned user IDs</a>
(UIDs) for each container.</p>
<p>Constantly changing UIDs provide some good protection against container
engine vulnerabilities, but they can be a pain if you have a script or
application that depends on being able to resolve a UID or GID back to a real
user or group account.</p>
<h2 id="ansible-and-uids">Ansible and UIDs</h2>
<p>If you run an Ansible playbook within OpenShift, you will likely run into a
problem during the fact gathering process:</p>
<pre><code>$ ansible-playbook -i hosts playbook.yml

PLAY [all] *********************************************************************

TASK [Gathering Facts] *********************************************************
An exception occurred during task execution. To see the full traceback, use -vvv.
The error was: KeyError: 'getpwuid(): uid not found: 1000220000'
fatal: [localhost]: FAILED! =&gt; {&quot;msg&quot;: &quot;Unexpected failure during module execution.&quot;, &quot;stdout&quot;: &quot;&quot;}
	to retry, use: --limit @/major-ansible-messaround/playbook.retry

PLAY RECAP *********************************************************************
localhost                  : ok=0    changed=0    unreachable=0    failed=1
</code></pre><p>This exception is telling us that <a href="https://linux.die.net/man/3/getpwuid">getpwuid()</a> was not able to find an entry
in <code>/etc/passwd</code> for our UID (<code>1000220000</code> in this container).</p>
<p>One option would be to adjust the playbook so that we skip the fact gathering
process:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml">- <span style="color:#268bd2">hosts</span>: all
  <span style="color:#268bd2">gather_facts</span>: <span style="color:#cb4b16">no</span>
  <span style="color:#268bd2">tasks</span>:

    - <span style="color:#268bd2">name</span>: Run tests
      <span style="color:#268bd2">command</span>: ./run_tests.sh
</code></pre></div><p>However, this might not be helpful if you need facts to be gathered for your
playbook to run. In that case, you need to make some adjustments to your
container image first.</p>
<h2 id="updating-the-container">Updating the container</h2>
<p>Nothing in the container image is writable within OpenShift, but we can change
certain files to be group writable for the root user since every OpenShift
user has an effective GID of <code>0</code>.</p>
<p>When you build your container, add a line to your Dockerfile to allow the
container user to have group write access to <code>/etc/passwd</code> and <code>/etc/group</code>:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-dockerfile" data-lang="dockerfile"><span style="color:#586e75"># Make Ansible happy with arbitrary UID/GID in OpenShift.</span>
<span style="color:#719e07">RUN</span> chmod <span style="color:#268bd2">g</span><span style="color:#719e07">=</span>u /etc/passwd /etc/group
</code></pre></div><p>Once your container has finished building, the permissions on both files
should look like this:</p>
<pre><code>$ ls -al /etc/passwd /etc/group
-rw-rw-r--. 1 root root 514 Mar 20 18:12 /etc/group
-rw-rw-r--. 1 root root 993 Mar 20 18:12 /etc/passwd
</code></pre><h2 id="make-a-user-account">Make a user account</h2>
<p>Now that we&rsquo;ve made these files writable for our user in OpenShift, it&rsquo;s time
to change how we run our GitLab CI job. My job YAML currently looks like this:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#268bd2">ansible_test</span>:
  <span style="color:#268bd2">image</span>: docker.io/major/ansible:fedora29
  <span style="color:#268bd2">script</span>:
    - ansible-playbook -i hosts playbook.yml
</code></pre></div><p>We can add two lines that allow us to make a temporary user and group account
for our OpenShift user:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#268bd2">ansible_test</span>:
  <span style="color:#268bd2">image</span>: docker.io/major/ansible:fedora29
  <span style="color:#268bd2">script</span>:
    - echo &#34;tempuser❌$(id -u):$(id -g):,,,:${HOME}:/bin/bash&#34; &gt;&gt; /etc/passwd
    - echo &#34;tempuser❌$(id -G | cut -d&#39; &#39; -f 2)&#34; &gt;&gt; /etc/group
    - id
    - ansible-playbook -i hosts playbook.yml
</code></pre></div><p>Note that we want the second GID returned by <code>id</code> since the first one is <code>0</code>.
The <code>id</code> command helps us check our work when the container starts. When the
CI job starts, we should see some better output:</p>
<pre><code>$ echo &quot;tempuser❌$(id -u):$(id -g):,,,:${HOME}:/bin/bash&quot; &gt;&gt; /etc/passwd
$ echo &quot;tempuser❌$(id -G | cut -d' ' -f 2)&quot; &gt;&gt; /etc/group
$ id
uid=1000220000(tempuser) gid=0(root) groups=0(root),1000220000(tempuser)
$ ansible-playbook -i hosts playbook.yml

PLAY [all] *********************************************************************

TASK [Gathering Facts] *********************************************************
ok: [localhost]

TASK [Download kernel source] **************************************************
changed: [localhost]

PLAY RECAP *********************************************************************
localhost                  : ok=2    changed=1    unreachable=0    failed=0
</code></pre><p>Success!</p>
]]></content></item><item><title>Get a /56 from Spectrum using wide-dhcpv6</title><link>https://major.io/2019/03/19/get-a-slash-56-from-spectrum-using-wide-dhcpv6/</link><pubDate>Tue, 19 Mar 2019 00:00:00 +0000</pubDate><guid>https://major.io/2019/03/19/get-a-slash-56-from-spectrum-using-wide-dhcpv6/</guid><description>After writing my last post on my IPv6 woes with my Pixel 3, some readers asked how I&amp;rsquo;m handling IPv6 on my router lately. I wrote about this previously when Spectrum was Time Warner Cable and I was using Mikrotik network devices.
There is a good post from 2015 on the blog and it still works today:
Time Warner Road Runner, Linux, and large IPv6 subnets I am still using that same setup today, but some readers found it difficult to find the post since Time Warner Cable has renamed to Spectrum.</description><content type="html"><![CDATA[<p>After writing my <a href="/2019/03/17/pixel-3-wifi-drops-constantly/">last post</a> on my IPv6 woes with my Pixel 3, some readers
asked how I&rsquo;m handling IPv6 on my router lately. I <a href="/2014/09/11/howto-time-warner-cable-ipv6/">wrote about this
previously</a> when Spectrum was Time Warner Cable and I was using Mikrotik
network devices.</p>
<p>There is a good post from 2015 on the blog and it still works today:</p>
<ul>
<li><a href="/2015/09/11/time-warner-road-runner-linux-and-large-ipv6-subnets/">Time Warner Road Runner, Linux, and large IPv6 subnets</a></li>
</ul>
<p>I am still using that same setup today, but some readers found it difficult
to find the post since Time Warner Cable has renamed to Spectrum. Don&rsquo;t worry
&ndash; the old post still works. :)</p>
]]></content></item><item><title>Pixel 3 Wi-Fi drops constantly</title><link>https://major.io/2019/03/17/pixel-3-wifi-drops-constantly/</link><pubDate>Sun, 17 Mar 2019 00:00:00 +0000</pubDate><guid>https://major.io/2019/03/17/pixel-3-wifi-drops-constantly/</guid><description>We have two Google Pixel phones in our house: a Pixel 2 and a Pixel 3. Both of them drop off our home wireless network regularly. It causes lots of problems with various applications on the phones, especially casting video via Chromecast.
At the time when I first noticed the drops, I was using a pair of wireless access points (APs) from Engenius:
EAP600 EAP1200H Also, here&amp;rsquo;s what I knew at the time:</description><content type="html"><![CDATA[<p><img src="/images/2019-03-17-google-pixel-phones.jpg" alt="pixel_phones"></p>
<p>We have two Google Pixel phones in our house: a Pixel 2 and a Pixel 3. Both
of them drop off our home wireless network regularly. It causes lots of
problems with various applications on the phones, especially casting video
via Chromecast.</p>
<p>At the time when I first noticed the drops, I was using a pair of wireless
access points (APs) from Engenius:</p>
<ul>
<li><a href="https://www.engeniustech.com/engenius-products/indoor-wireless-ceiling-ap-eap600/">EAP600</a></li>
<li><a href="https://www.engeniustech.com/engenius-products/indoor-wireless-ceiling-ap-eap1200h/">EAP1200H</a></li>
</ul>
<p>Also, here&rsquo;s what I knew at the time:</p>
<ul>
<li>Mac and Linux computers had no Wi-Fi issues at all</li>
<li>The signal level from both APs was strong</li>
<li>Disabling one AP made no improvement</li>
<li>Disabling one band (2.4 or 5GHz) on the APs made no improvement</li>
<li>Clearing the bluetooth/Wi-Fi data on the Pixel had no effect</li>
<li>Assigning a static IP address on the Pixel made no improvement</li>
<li>Using unencrypted SSIDs made no improvement</li>
</ul>
<p>At this point, I felt strongly that the APs had nothing to do with it. I
ordered a new <a href="https://www.netgear.com/support/product/RBR50.aspx">NetGear Orbi</a> mesh router and satellite anyway. The Pixels
still dropped off the wireless network even with the new Orbi APs.</p>
<h2 id="reading-logs">Reading logs</h2>
<p>I started reading logs from every source I could find:</p>
<ul>
<li>dhcpd logs from my router</li>
<li>syslogs from my APs (which forwarded into the router)</li>
<li>output from tcpdump on my router</li>
</ul>
<p>Several things became apparent after reading the logs:</p>
<ul>
<li>The Wi-Fi drop occurred usually every 30-60 seconds</li>
<li>The DHCP server received requests for a new IP address after every drop</li>
<li>None of the network traffic from the phones was being blocked at the router</li>
<li>The logs from the APs showed the phone disconnecting itself from the
network; the APs were not forcing the phones off the network</li>
</ul>
<p>All of the wireless and routing systems in my house seemed to point to a
problem in the phones themselves. They were voluntarily dropping from the
network without being bumped off by APs or the router.</p>
<h2 id="getting-logs-from-the-phone">Getting logs from the phone</h2>
<p>It was time to get some logs from the phone itself. That would require
connecting the phone via USB to a computer and enabling USB debugging on the
phone.</p>
<p>First, I downloaded the <a href="https://developer.android.com/studio">Android SDK</a>. The full studio release isn&rsquo;t needed
&ndash; scroll down and find the <em>Command line tools only</em> section. Unzip the
download and find the <code>tools/bin/sdkmanager</code> executable. Run it like this:</p>
<pre><code># Fedora 29 systems may need to choose the older Java version for sdkmanager
# to run properly.
export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.201.b09-2.fc29.x86_64/jre
# Install the android-28 platform tools
./sdkmanager &quot;platform-tools&quot; &quot;platforms;android-28&quot;
</code></pre><p>Now we need to enable USB debugging on the phone itself. <strong>Be sure to disable
USB debugging when you are done!</strong> Follow these steps:</p>
<ol>
<li>Go into the phone&rsquo;s settings and choose <em>About Phone</em> from the bottom of
the list.</li>
<li>Scroll to the bottom and tap the <em>Build number</em> section repeatedly until
a message appears saying that you are now a developer.</li>
<li>Go back one screen and tap <em>System</em>.</li>
<li>Click <em>Advanced</em> to show the additional options and tap <em>Developer Options</em>.</li>
<li>In the <em>Debugging</em> section, tap <em>USB Debugging</em> to enable USB debugging.</li>
</ol>
<p>Connect the phone to your computer via USB and run:</p>
<pre><code>sudo platform-tools/adb logcat
</code></pre><p>Your screen will fill with logs from your phone.</p>
<h2 id="nuggets-in-the-log">Nuggets in the log</h2>
<p>I watched the logs and waited for the Wi-Fi to drop. As soon as it dropped, I
saw some interesting log messages:</p>
<pre><code>I wpa_supplicant: wlan0: CTRL-EVENT-AVOID-FREQ ranges=5785-5825
I chatty  : uid=1000(system) IpClient.wlan0 expire 3 lines
I chatty  : uid=1000 system_server expire 1 line
D CommandListener: Setting iface cfg
E cnss-daemon: wlan_service_update_sys_param: unable to open /proc/sys/net/ipv4/tcp_use_userconfig
I chatty  : uid=1000(system) android.fg expire 1 line
I wpa_supplicant: wlan0: CTRL-EVENT-DISCONNECTED bssid=88:dc:96:4a:b6:75 reason=3 locally_generated=1
I chatty  : uid=10025 com.google.android.gms.persistent expire 7 lines
V NativeCrypto: Read error: ssl=0x7b349e2d08: I/O error during system call, Software caused connection abort
V NativeCrypto: Write error: ssl=0x7b349e2d08: I/O error during system call, Broken pipe
V NativeCrypto: Write error: ssl=0x7b349e2d08: I/O error during system call, Broken pipe
V NativeCrypto: SSL shutdown failed: ssl=0x7b349e2d08: I/O error during system call, Success
D ConnectivityService: reportNetworkConnectivity(158, false) by 10025
</code></pre><p>The line with <code>CTRL-EVENT-AVOID-FREQ</code> isn&rsquo;t relevant because it&rsquo;s simply a
hint to the wireless drivers to avoid certain frequencies not used in the
USA. The <code>CTRL-EVENT-DISCONNECTED</code> shows where wpa_supplicant received the
disconnection message. The last line with <code>ConnectivityService</code> was very
interesting. Something in the phone believes there is a network connectivity
issue. That could be why the Pixel is hopping off the wireless network.</p>
<p>From there, I decided to examine only the <code>ConnectivityService</code> logs:</p>
<pre><code>sudo platform-tools/adb logcat 'ConnectivityService:* *:S'
</code></pre><p>This logcat line tells adb that I want all logs from all log levels about the
<code>ConnectivityService</code>, but all of the other logs should be silenced. I
started seeing some interesting details:</p>
<pre><code>D ConnectivityService: NetworkAgentInfo [WIFI () - 148] validation failed
D ConnectivityService: Switching to new default network: NetworkAgentInfo{ ni{[type: MOBILE[LTE]...
D ConnectivityService: Sending DISCONNECTED broadcast for type 1 NetworkAgentInfo [WIFI () - 148] isDefaultNetwork=true
D ConnectivityService: Sending CONNECTED broadcast for type 0 NetworkAgentInfo [MOBILE (LTE) - 100] isDefaultNetwork=true
D ConnectivityService: handleNetworkUnvalidated NetworkAgentInfo [WIFI () - 148] ...
</code></pre><p>Wait, what is this &ldquo;validation failed&rdquo; message? The Pixel was making network
connections successfully the entire time as shown by tcpdump. This is part of
Android&rsquo;s [network connecivity checks] for various networks.</p>
<p>The last few connections just before the disconnect were to
<code>connectivitycheck.gstatic.com</code> (based on tcpdump logs) and that&rsquo;s Google&rsquo;s
way of verifying that the wireless network is usable and that there are no
captive portals. I connected to it from my desktop on IPv4 and IPv6 to verify:</p>
<pre><code>$ curl -4 -i https://connectivitycheck.gstatic.com/generate_204
HTTP/2 204
date: Sun, 17 Mar 2019 15:00:30 GMT
alt-svc: quic=&quot;:443&quot;; ma=2592000; v=&quot;46,44,43,39&quot;
$ curl -6 -i https://connectivitycheck.gstatic.com/generate_204
HTTP/2 204
date: Sun, 17 Mar 2019 15:00:30 GMT
alt-svc: quic=&quot;:443&quot;; ma=2592000; v=&quot;46,44,43,39&quot;
</code></pre><p>Everything looked fine.</p>
<h2 id="heading-to-google">Heading to Google</h2>
<p>After a bunch of searching on Google, I kept finding posts talking about
disabling IPv6 to fix the Wi-Fi drop issues. I shrugged it off and kept
searching. Finally, I decided to disable IPv6 and see if that helped.</p>
<p>I stopped <code>radvd</code> on the router, disabled Wi-Fi on the phone, and then
re-enabled it. As I watched, the phone stayed on the wireless network for two
minutes. Three minutes. Ten minutes. <strong>There were no drops.</strong></p>
<p>At this point, this is still an unsolved mystery for me. Disabling IPv6 is a
terrible idea, but it keeps my phones online. I plan to put the phones on
their own VLAN without IPv6 so I can still keep IPv6 addresses for my other
computers, but this is not a good long term fix. If anyone has any input on
why this helps and how I can get IPv6 re-enabled, please <a href="mailto:major@mhtx.net">let me know</a>!</p>
<h2 id="update-2019-03-18">Update 2019-03-18</h2>
<p>Several readers wanted to see what was happening just before the Wi-Fi drop,
so here&rsquo;s a small snippet from tcpdump:</p>
<pre><code>07:26:06.736900 IP6 2607:f8b0:4000:80d::2003.443 &gt; phone.41310: Flags [F.], seq 3863, ack 511, win 114, options [nop,nop,TS val 1288800272 ecr 66501414], length 0
07:26:06.743101 IP6 2607:f8b0:4000:80d::2003.443 &gt; phone.41312: Flags [F.], seq 3864, ack 511, win 114, options [nop,nop,TS val 1778536228 ecr 66501414], length 0
07:26:06.765444 IP6 phone.41312 &gt; 2607:f8b0:4000:80d::2003.443: Flags [R], seq 4183481455, win 0, length 0
07:26:06.765454 IP6 phone.41310 &gt; 2607:f8b0:4000:80d::2003.443: Flags [R], seq 3279990707, win 0, length 0
07:26:07.487180 IP6 2607:f8b0:4000:80d::2003.443 &gt; phone.41316: Flags [F.], seq 3863, ack 511, win 114, options [nop,nop,TS val 4145292968 ecr 66501639], length 0
07:26:07.537080 IP6 phone.41316 &gt; 2607:f8b0:4000:80d::2003.443: Flags [R], seq 4188442452, win 0, length 0
</code></pre><p>That IPv6 address is at a Google PoP in Dallas, TX:</p>
<pre><code>$ host 2607:f8b0:4000:80d::2003
3.0.0.2.0.0.0.0.0.0.0.0.0.0.0.0.d.0.8.0.0.0.0.4.0.b.8.f.7.0.6.2.ip6.arpa domain name pointer dfw06s49-in-x03.1e100.net.
</code></pre><p>I haven&rsquo;t been able to intercept the traffic via man-in-the-middle since
Google&rsquo;s certificate checks are very strict. However, checks from my own
computer work without an issue:</p>
<pre><code>$ curl -ki &quot;https://[2607:f8b0:4000:80d::2003]/generate_204&quot;
HTTP/2 204
date: Mon, 18 Mar 2019 12:35:18 GMT
alt-svc: quic=&quot;:443&quot;; ma=2592000; v=&quot;46,44,43,39&quot;
</code></pre>]]></content></item><item><title>Stop audio pops on Intel HD Audio</title><link>https://major.io/2019/03/04/stop-audio-pops-on-intel-hd-audio/</link><pubDate>Mon, 04 Mar 2019 00:00:00 +0000</pubDate><guid>https://major.io/2019/03/04/stop-audio-pops-on-intel-hd-audio/</guid><description>I recently picked up a Dell Optiplex 7060 and I&amp;rsquo;m using it as my main workstation now. The Fedora installation was easy, but I noticed a variety of &amp;ldquo;pop&amp;rdquo; or clicking sounds when audio played, especially terminal bells.
If everything was quiet and I triggered a terminal bell, I would hear a loud pop just before the terminal bell sound. However, if I played music and then triggered a terminal bell, the pop was gone.</description><content type="html"><![CDATA[<p><img src="/images/2019-03-04-headphones.jpg" alt="headphones"></p>
<p>I recently picked up a Dell Optiplex 7060 and I&rsquo;m using it as my main
workstation now. The Fedora installation was easy, but I noticed a variety of
&ldquo;pop&rdquo; or clicking sounds when audio played, especially terminal bells.</p>
<p>If everything was quiet and I triggered a terminal bell, I would hear a loud
pop just before the terminal bell sound. However, if I played music and then
triggered a terminal bell, the pop was gone.</p>
<p>A quick Google search told me that the likely culprit was power saving
settings on my Intel HD Audio chipset:</p>
<pre><code>$ lspci | grep Audio
00:1f.3 Audio device: Intel Corporation Cannon Lake PCH cAVS (rev 10)
</code></pre><h2 id="fixing-it">Fixing it</h2>
<p>There&rsquo;s a handy power saving tunable available at
<code>/sys/module/snd_hda_intel/parameters/power_save</code> that can be usd to adjust
the timeout or disable power savings entirely. In my case, the delay was set
to one second.</p>
<pre><code>$ cat /sys/module/snd_hda_intel/parameters/power_save
1
</code></pre><p>That would be good for a laptop use case, but my workstation is always
plugged in. I disabled it by setting it to zero:</p>
<pre><code># echo 0 &gt; /sys/module/snd_hda_intel/parameters/power_save
$ cat /sys/module/snd_hda_intel/parameters/power_save
0
</code></pre><p>And the pops are gone! My Klipsch speakers have a built in amplifier and it
was likely the abrupt changes in current that was causing the popping noises.</p>
<p>This setting will last until you reboot. You can make it permanent by adding
this text to <code>/etc/modprobe.d/audio_disable_powersave.conf</code>:</p>
<pre><code>options snd_hda_intel power_save=0
</code></pre><p>If you&rsquo;re a laptop user and you want power savings but fewer pops, you could
increase the delay to a more acceptable number. For example, setting it to
<code>60</code> would mean that the card will power down after 60 seconds of silence.
Just remember that you&rsquo;ll get a nice pop when the 60 seconds has passed and a
new sound is played.</p>
<h2 id="learning-more">Learning more</h2>
<p>Diving into the kernel code reveals the tunable in
<code>/sound/pci/hda/hda_intel.c</code>:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-c" data-lang="c"><span style="color:#719e07">static</span> <span style="color:#dc322f">int</span> power_save <span style="color:#719e07">=</span> CONFIG_SND_HDA_POWER_SAVE_DEFAULT;
module_param(power_save, xint, <span style="color:#2aa198">0644</span>);
MODULE_PARM_DESC(power_save, <span style="color:#2aa198">&#34;Automatic power-saving timeout &#34;</span>
		 <span style="color:#2aa198">&#34;(in second, 0 = disable).&#34;</span>);
</code></pre></div><p>The default comes from a kernel config option:
<code>CONFIG_SND_HDA_POWER_SAVE_DEFAULT</code>. Most kernel packages on most
distributions provide access to the kernel config file that was used to build
the kernel originally. It&rsquo;s often found in <code>/boot</code> (named the same as the
kernel version) or it might be available at <code>/proc/config.gz</code>.</p>
<p>For Fedora, the kernel config is provided in <code>/boot</code> whenever a new kernel is
is installed. I inspected mine and found:</p>
<pre><code>$ grep HDA_POWER_SAVE_DEFAULT /boot/config-4.20.13-200.fc29.x86_64
CONFIG_SND_HDA_POWER_SAVE_DEFAULT=1
</code></pre><p>The <code>power_save</code> setting is applied in <code>/sound/pci/hda/hda_codec.c</code>:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-c" data-lang="c"><span style="color:#586e75">/**
</span><span style="color:#586e75"> * snd_hda_set_power_save - reprogram autosuspend for the given delay
</span><span style="color:#586e75"> * @bus: HD-audio bus
</span><span style="color:#586e75"> * @delay: autosuspend delay in msec, 0 = off
</span><span style="color:#586e75"> *
</span><span style="color:#586e75"> * Synchronize the runtime PM autosuspend state from the power_save option.
</span><span style="color:#586e75"> */</span>
<span style="color:#dc322f">void</span> <span style="color:#268bd2">snd_hda_set_power_save</span>(<span style="color:#719e07">struct</span> hda_bus <span style="color:#719e07">*</span>bus, <span style="color:#dc322f">int</span> delay)
{
	<span style="color:#719e07">struct</span> hda_codec <span style="color:#719e07">*</span>c;

	list_for_each_codec(c, bus)
		codec_set_power_save(c, delay);
}
EXPORT_SYMBOL_GPL(snd_hda_set_power_save);
</code></pre></div><p>We can look where <code>codec_set_power_save</code> is defined in the same file to learn
more:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-c" data-lang="c"><span style="color:#719e07">#ifdef CONFIG_PM
</span><span style="color:#719e07"></span><span style="color:#719e07">static</span> <span style="color:#dc322f">void</span> <span style="color:#268bd2">codec_set_power_save</span>(<span style="color:#719e07">struct</span> hda_codec <span style="color:#719e07">*</span>codec, <span style="color:#dc322f">int</span> delay)
{
	<span style="color:#719e07">struct</span> device <span style="color:#719e07">*</span>dev <span style="color:#719e07">=</span> hda_codec_dev(codec);

	<span style="color:#719e07">if</span> (delay <span style="color:#719e07">==</span> <span style="color:#2aa198">0</span> <span style="color:#719e07">&amp;&amp;</span> codec<span style="color:#719e07">-&gt;</span>auto_runtime_pm)
		delay <span style="color:#719e07">=</span> <span style="color:#2aa198">3000</span>;

	<span style="color:#719e07">if</span> (delay <span style="color:#719e07">&gt;</span> <span style="color:#2aa198">0</span>) {
		pm_runtime_set_autosuspend_delay(dev, delay);
		pm_runtime_use_autosuspend(dev);
		pm_runtime_allow(dev);
		<span style="color:#719e07">if</span> (<span style="color:#719e07">!</span>pm_runtime_suspended(dev))
			pm_runtime_mark_last_busy(dev);
	} <span style="color:#719e07">else</span> {
		pm_runtime_dont_use_autosuspend(dev);
		pm_runtime_forbid(dev);
	}
}
</code></pre></div><p>This logic looks to see if <code>CONFIG_PM</code> is set to know if power management is
desired at all. From there, it checks if we disabled power saving but there&rsquo;s
a discrete graphics card involved (<code>codec-&gt;auto_runtime_pm</code>). This check is
important because the discrete graphics card cannot power down unless the HDA
card suspends at the same time.</p>
<p>Next, there&rsquo;s a check to see if the delay is greater than 0. This would be
the case if <code>CONFIG_SND_HDA_POWER_SAVE_DEFAULT</code> was set to <code>1</code> (Fedora&rsquo;s
default). If so, the proper auto suspend delays are set.</p>
<p>If the delay is 0, then autosuspend is disabled and removed from power
management entirely. This is the option I chose and it&rsquo;s working great.</p>
<p><em>Photo source: <a href="https://www.maxpixel.net/Headphone-Caption-Music-Sound-Listing-Music-2694489">Max Pixel</a></em></p>
]]></content></item><item><title>Automatic floating windows in i3</title><link>https://major.io/2019/02/08/automatic-floating-windows-in-i3/</link><pubDate>Fri, 08 Feb 2019 00:00:00 +0000</pubDate><guid>https://major.io/2019/02/08/automatic-floating-windows-in-i3/</guid><description>The i3 window manager is a fast window manager that helps you keep all of your applications in the right place. It automatically tiles windows and can manage those tiles across multiple virtual desktops.
However, there are certain applications that I really prefer in a floating window. Floating windows do not get tiled and they can easily be dragged around with your mouse. They&amp;rsquo;re the type of windows you expect to see on other non-tiling desktops such as GNOME or KDE.</description><content type="html"><![CDATA[<p><img src="/images/2019-02-08-floating-leaf.jpg" alt="floating leaf"></p>
<p>The <a href="https://i3wm.org/">i3 window manager</a> is a fast window manager that helps you keep all of
your applications in the right place. It automatically tiles windows and can
manage those tiles across multiple virtual desktops.</p>
<p>However, there are certain applications that I really prefer in a floating
window. Floating windows do not get tiled and they can easily be dragged
around with your mouse. They&rsquo;re the type of windows you expect to see on
other non-tiling desktops such as GNOME or KDE.</p>
<h2 id="convert-a-window-to-floating-temporarily">Convert a window to floating temporarily</h2>
<p>If you have an existing window that you prefer to float, select that window
and press <em>Mod + Shift + Space bar</em>. The window will pop up in front of the
tiled windows and you can easily move it with your mouse.</p>
<p>Depending on your configuration, you may be able to resize it by grabbing a
corner of the window with your mouse. You can also assign a key combination
for resizing in your i3 configuration file (usually <code>~/.config/i3/config</code>):</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text"># resize window (you can also use the mouse for that)
mode &#34;resize&#34; {
        bindsym Left resize shrink width 10 px or 10 ppt
        bindsym Down resize grow height 10 px or 10 ppt
        bindsym Up resize shrink height 10 px or 10 ppt
        bindsym Right resize grow width 10 px or 10 ppt
        bindsym Return mode &#34;default&#34;
        bindsym Escape mode &#34;default&#34;
        bindsym $mod+r mode &#34;default&#34;
}
bindsym $mod+r mode &#34;resize&#34;
</code></pre></div><p>With this configuration, simply press <em>Mod + r</em> and use the arrow keys to
grow or shrink the window&rsquo;s borders.</p>
<h2 id="always-float-certain-windows">Always float certain windows</h2>
<p>For those windows that you always want to be floating no matter what, i3 has
a solution for that, too. Just tell i3 how to identify your windows and
ensure <code>floating enable</code> appears in the i3 config:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">for_window [window_role=&#34;About&#34;] floating enable
for_window [class=&#34;vlc&#34;] floating enable
for_window [title=&#34;Authy&#34;] floating enable
</code></pre></div><p>In the example above, I have a few windows always set to be floating:</p>
<ul>
<li><code>[window_role=&quot;About&quot;]</code> - Any of the &ldquo;About&rdquo; windows in various applications
that are normally opened by <em>Help -&gt; About</em>.</li>
<li><code>[class=&quot;vlc&quot;]</code> - The VLC media player can be a good one to float if you
need to stuff it away in a corner.</li>
<li><code>[title=&quot;Authy&quot;]</code> - Authy&rsquo;s chrome extension looks downright silly as a
tiled window.</li>
</ul>
<p>Any time these windows are spawned, they will automatically appear as
floating windows. You can always switch them back to tiled manually by
pressing <em>Mod + Shift + Space bar</em>.</p>
<h2 id="identifying-windows">Identifying windows</h2>
<p>Identifying windows in the way that i3 cares about can be challenging.
Knowing when to use <code>window_role</code> or <code>class</code> for a window isn&rsquo;t very
intuitive. Fortunately, there&rsquo;s a great script from an <a href="https://faq.i3wm.org/question/2172/how-do-i-find-the-criteria-for-use-with-i3-config-commands-like-for_window-eg-to-force-splashscreens-and-dialogs-to-show-in-floating-mode.1.html">archived i3 faq
thread</a> that makes this easy:</p>
<!-- raw HTML omitted -->
<p>Download this script to your system, make it executable (<code>chmod +x i3-get-window-criteria</code>), and run it. As soon as you do that, a plus (+) icon
will replace your normal mouse cursor. Click on the window you care about and
look for the output in your terminal where you ran the
<code>i3-get-window-criteria</code> script.</p>
<p>On my system, clicking on a terminator terminal window gives me:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">[class=&#34;Terminator&#34; id=37748743 instance=&#34;terminator&#34; title=&#34;major@indium:~&#34;]
</code></pre></div><p>If I wanted to float all terminator windows, I could add this to my i3
configuration file:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">for_window [class=&#34;Terminator&#34;] floating enable
</code></pre></div><h2 id="float-in-a-specific-workspace">Float in a specific workspace</h2>
<p>Do you need a window to always float on a specific workspace? i3 can do that,
too!</p>
<p>Let&rsquo;s go back to the example with VLC. Let&rsquo;s consider that we have a really
nice 4K display where we always want to watch movies and that&rsquo;s where
workspace 2 lives. We can tell i3 to always float the VLC window on workspace
2 with this configuration:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">set $ws1 &#34;1: main&#34;
set $ws2 &#34;2: 4kdisplay&#34;
for_window [class=&#34;vlc&#34;] floating enable
for_window [class=&#34;vlc&#34;] move to workspace $ws2
</code></pre></div><p>Restart i3 to pick up the new changes (usually <em>Mod + Shift + R</em>) and start
VLC. It should appear on workspace 2 as a floating window!</p>
<p><em><a href="https://www.maxpixel.net/Floating-Sea-Leaf-Water-Cute-Leaf-Floating-2438419">Photo source</a></em></p>
]]></content></item><item><title>DevConf.CZ 2019 Recap</title><link>https://major.io/2019/01/31/devconf.cz-2019-recap/</link><pubDate>Thu, 31 Jan 2019 00:00:00 +0000</pubDate><guid>https://major.io/2019/01/31/devconf.cz-2019-recap/</guid><description>DevConf.CZ 2019 wrapped up last weekend and it was a great event packed with lots of knowledgeable speakers, an engaging hallway track, and delicious food. This was my first trip to any DevConf and it was my second trip to Brno.
Lots of snow showed up on the second day and more snow arrived later in the week!
First talk of 2019 I co-presented a talk with one of my teammates, Nikolai, about some of the fun work we&amp;rsquo;ve been doing at Red Hat to improve the quality of the Linux kernel in an automated way.</description><content type="html"><![CDATA[<p><a href="http://devconf.info/cz">DevConf.CZ 2019</a> wrapped up last weekend and it was a great event packed
with lots of knowledgeable speakers, an engaging hallway track, and delicious
food. This was my first trip to any DevConf and it was my second trip to
<a href="https://en.wikipedia.org/wiki/Brno">Brno</a>.</p>
<p>Lots of snow showed up on the second day and more snow arrived later in the
week!</p>
<p><a href="/images/20190131-devconf-2019-snow.jpg"><img src="/images/20190131-devconf-2019-snow-720.jpg" alt="devconf-snow-small"></a></p>
<h2 id="first-talk-of-2019">First talk of 2019</h2>
<p>I co-presented a talk with one of my teammates, Nikolai, about some of the
fun work we&rsquo;ve been doing at Red Hat to improve the quality of the Linux
kernel in an automated way. The room was full and we had lots of good
questions at the end of the talk. We also received some feedback that we
could take back to the team to change how we approached certain parts of the
kernel testing.</p>
<p><img src="/images/20190131-devconf-major-nikolai.jpg" alt="devconf-major-nikolai"></p>
<p>Our project, called Continuous Kernel Integration (CKI), has a goal of
reducing the amount of bugs that are merged into the Linux kernel. This
requires lots of infrastructure, automation, and testing capabilities. We
shared information about our setup, the problems we&rsquo;ve found, and where we
want to go in the future.</p>
<p>Feel free to <a href="https://www.slideshare.net/MajorHayden/cookies-for-kernel-developers">view our slides</a> and watch the video (which should be up soon.)</p>
<h2 id="great-talks-from-devconf">Great talks from DevConf</h2>
<p>My favorite talk of the conference was <a href="https://twitter.com/openlabbott">Laura Abbott&rsquo;s</a> &ldquo;Monsters, Ghosts, and
Bugs.&rdquo;</p>
<p><img src="/images/20190131-devconf-laura-abbott-720.jpg" alt="devconf-laura-abbott"></p>
<p>It&rsquo;s the most informative, concise, and sane review of how all the Linux
kernels on the planet fit together. From the insanity of linux-next to the
wild world of being a Linux distribution kernel maintainer, she helped us all
understand the process of how kernels are maintained. She also took time to
help the audience understand which kernels are most important to them and how
they can make the right decisions about the kernel that will suit their
needs. There are <a href="https://twitter.com/majorhayden/status/1089507679977046017">plenty of good points in my Twitter thread</a> about her talk.</p>
<p>Dan Walsh gave a detailed overview of how to use Podman instead of Docker. He
talked about the project&rsquo;s origins and some of the incorrect assumptions that
many people have (that running containers means only running Docker). Running
containers without root has plenty of benefits. In addition, a significant
amount of work has been done to speed up container pulls and pushes in
Podman. I took some <a href="https://twitter.com/majorhayden/status/1088715555635220481">notes on Dan&rsquo;s talk</a> in a thread on Twitter.</p>
<p>The firewalld package has gained some new features recently and it&rsquo;s poised
to fully take advantage of nftables in Fedora 31! Using nftables means that
firewall updates are done faster with fewer hiccups in busy environments
(think OpenStack and Kubernetes). In addition, nftables can apply rules to
IPv4 and IPv6 simultaneously, depeending on your preferences. My <a href="https://twitter.com/majorhayden/status/1088807448817876992">firewalld
Twitter thread</a> has more details from the talk.</p>
<p>The cgroups v2 subsystem was a popular topic in a few of the talks I visited,
including the lightning talks. There are plenty of issues to get it working
with Kubernetes and container management systems. It&rsquo;s also missing the
<a href="https://www.kernel.org/doc/Documentation/cgroup-v1/freezer-subsystem.txt">freezer capability</a> from the original cgroups implementation. Without that,
pausing a container, or using technology like <a href="https://criu.org/Main_Page">CRIU</a>, simply won&rsquo;t work.
Nobody could name a Linux distribution that has cgroups v2 enabled at the
moment, and that&rsquo;s not helping the effort move forward. Look for more news on
this soon.</p>
<p><img src="/images/20190131-devconf-openshif-pun-720.jpg" alt="devconf-openshift-pun"></p>
<p>OpenShift is quickly moving towards offering multiple architectures as a
first class product feature. That would incluve aarch64, ppc64le, and s390x
in addition to the existing x86_64 support. Andy McCrae and Jeff Young had a
talk detailing many of the challenges along with lots of punny references to
various &ldquo;arches&rdquo;. I made a <a href="https://twitter.com/majorhayden/status/1089470907607928833">Twitter thread of the main points</a> from the
OpenShift talk.</p>
<p>Some of the other news included:</p>
<ul>
<li><a href="https://wiki.linuxfoundation.org/realtime/start">real-time linux patches</a> are likely going to be merged into mainline.
(only 15 years in the making!)</li>
<li>Fedora, CentOS, RHEL and EPEL communities are eager to bring more of their
processes together and make it easier for contributors to join in.</li>
<li>Linux 5.0 is no more exciting than 4.20. It would have been 4.21 if Linus
had an <a href="https://www.theregister.co.uk/2019/01/07/linux_reaches_the_big_five_point_oh/">extra finger or toe</a>.</li>
</ul>
<h2 id="devconfus-boston-2019">DevConf.US Boston 2019</h2>
<p>The next DevConf.US is in Boston, USA this summer. I hope to see you there!</p>
]]></content></item><item><title>Using the pressure stall information interface in kernel 4.20</title><link>https://major.io/2019/01/27/using-the-pressure-stall-information-interface-in-kernel-4.20/</link><pubDate>Sun, 27 Jan 2019 00:00:00 +0000</pubDate><guid>https://major.io/2019/01/27/using-the-pressure-stall-information-interface-in-kernel-4.20/</guid><description>Fedora 29 now has kernel 4.20 available and it has lots of new features. One of the more interesting and easy to use features is the pressure stall information interface.
Load average We&amp;rsquo;re all familiar with the load average measurement on Linux machines, even if the numbers do seem a bit cryptic:
$ w 10:55:46 up 11 min, 1 user, load average: 0.42, 0.39, 0.26 The numbers denote how many processes were active over the last one, five and 15 minutes.</description><content type="html"><![CDATA[<p><img src="/images/pressure_cooker.jpg" alt="pressure_cooker"></p>
<p>Fedora 29 now has kernel 4.20 available and it has <a href="https://kernelnewbies.org/Linux_4.20">lots of new features</a>.
One of the more interesting and easy to use features is the <a href="https://lwn.net/Articles/759658/">pressure stall
information</a> interface.</p>
<h2 id="load-average">Load average</h2>
<p>We&rsquo;re all familiar with the <a href="https://en.wikipedia.org/wiki/Load_(computing)">load average</a> measurement on Linux machines,
even if the numbers do seem a bit cryptic:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">$ w
 10:55:46 up 11 min,  1 user,  load average: 0.42, 0.39, 0.26
</code></pre></div><p>The numbers denote how many processes were active over the last one, five and
15 minutes. In my case, I have a system with four cores. My numbers above
show that less than one process was active in the last set of intervals. That
means that my system isn&rsquo;t doing very much and processes are not waiting in
the queue.</p>
<p>However, if I begin compiling a kernel with eight threads (double my core
count), the numbers change dramatically:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">$ w
 11:00:28 up 16 min,  1 user,  load average: 4.15, 1.89, 0.86
</code></pre></div><p>The one minute load average is now over four, which means some processes are
waiting to be served on the system. This makes sense because I am using eight
threads to compile a kernel on a system with four cores.</p>
<h2 id="more-detail">More detail</h2>
<p>We assume that the CPU is the limiting factor in the system since we know
that compiling a kernel takes lots of CPU time. We can verify (and quantify) that with the pressure stall information available in 4.20.</p>
<p>We start by taking a look in <code>/proc/pressure</code>:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">$ head /proc/pressure/*
==&gt; /proc/pressure/cpu &lt;==
some avg10=71.37 avg60=57.25 avg300=23.83 total=100354487

==&gt; /proc/pressure/io &lt;==
some avg10=0.17 avg60=0.13 avg300=0.24 total=8101378
full avg10=0.00 avg60=0.01 avg300=0.16 total=5866706

==&gt; /proc/pressure/memory &lt;==
some avg10=0.00 avg60=0.00 avg300=0.00 total=0
full avg10=0.00 avg60=0.00 avg300=0.00 total=0
</code></pre></div><p>But what do these numbers mean? The shortest explanation is in the patch
itself:</p>
<blockquote>
<p>PSI aggregates and reports the overall wallclock time in which the
tasks in a system (or cgroup) wait for contended hardware resources.</p>
</blockquote>
<p>The numbers here are percentages, not time itself:</p>
<blockquote>
<p>The averages give the percentage of walltime in which one or more
tasks are delayed on the runqueue while another task has the
CPU. They&rsquo;re recent averages over 10s, 1m, 5m windows, so you can tell
short term trends from long term ones, similarly to the load average.</p>
</blockquote>
<p>We can try to apply some I/O pressure by making a big tarball of a kernel
source tree:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">$ head /proc/pressure/*
==&gt; /proc/pressure/cpu &lt;==
some avg10=1.33 avg60=10.07 avg300=26.83 total=262389574

==&gt; /proc/pressure/io &lt;==
some avg10=40.53 avg60=13.27 avg300=3.46 total=20451978
full avg10=37.44 avg60=12.40 avg300=3.21 total=16659637

==&gt; /proc/pressure/memory &lt;==
some avg10=0.00 avg60=0.00 avg300=0.00 total=0
full avg10=0.00 avg60=0.00 avg300=0.00 total=0
</code></pre></div><p>The CPU is still under some stress here, but the I/O is now the limiting
factor.</p>
<p>The output also shows a <code>total=</code> number, and that is explained in the patch
as well:</p>
<blockquote>
<p>The total= value gives the absolute stall time in microseconds. This
allows detecting latency spikes that might be too short to sway the
running averages. It also allows custom time averaging in case the
10s/1m/5m windows aren&rsquo;t adequate for the usecase (or are too coarse
with future hardware).</p>
</blockquote>
<p>The total number can be helpful for machines that run for a long time,
especially when you graph them and you monitor them for trends.</p>
]]></content></item><item><title>Running Home Assistant in a Docker container with a Z-Wave USB stick</title><link>https://major.io/2019/01/14/running-home-assistant-in-a-docker-container-with-zwave-usb-stick/</link><pubDate>Mon, 14 Jan 2019 00:00:00 +0000</pubDate><guid>https://major.io/2019/01/14/running-home-assistant-in-a-docker-container-with-zwave-usb-stick/</guid><description>The Home Assistant project provides a great open source way to get started with home automtion that can be entirely self-contained within your home. It already has plenty of integrations with external services, but it can also monitor Z-Wave devices at your home or office.
Here are my devices:
Monoprice Z-Wave Garade Door Sensor Aeotec Z-Stick Gen5 (ZW090) Fedora Linux server with Docker installed Install the Z-Wave stick Start by plugging the Z-Stick into your Linux server.</description><content type="html"><![CDATA[<p>The <a href="https://www.home-assistant.io/">Home Assistant</a> project provides a great open source way to get started
with home automtion that can be entirely self-contained within your home. It
already has plenty of <a href="https://www.home-assistant.io/components/">integrations</a> with external services, but it can also
monitor <a href="https://en.wikipedia.org/wiki/Z-Wave">Z-Wave</a> devices at your home or office.</p>
<p>Here are my devices:</p>
<ul>
<li><a href="https://www.amazon.com/Monoprice-Z-Wave-Garage-Door-Sensor/dp/B00V5IQ8E8">Monoprice Z-Wave Garade Door Sensor</a></li>
<li><a href="https://aeotec.com/z-wave-usb-stick">Aeotec Z-Stick Gen5 (ZW090)</a></li>
<li>Fedora Linux server with Docker installed</li>
</ul>
<h2 id="install-the-z-wave-stick">Install the Z-Wave stick</h2>
<p>Start by plugging the Z-Stick into your Linux server. Run <code>lsusb</code> and it should appear in the list:</p>
<pre><code># lsusb | grep Z-Stick
Bus 003 Device 006: ID 0658:0200 Sigma Designs, Inc. Aeotec Z-Stick Gen5 (ZW090) - UZB
</code></pre><p>The system journal should also tell you which TTY is assigned to the USB
stick (run <code>journalctl --boot</code> and search for <code>ACM</code>):</p>
<pre><code>kernel: usb 3-3.2: USB disconnect, device number 4
kernel: usb 3-1: new full-speed USB device number 6 using xhci_hcd
kernel: usb 3-1: New USB device found, idVendor=0658, idProduct=0200, bcdDevice= 0.00
kernel: usb 3-1: New USB device strings: Mfr=0, Product=0, SerialNumber=0
kernel: cdc_acm 3-1:1.0: ttyACM0: USB ACM device
kernel: usbcore: registered new interface driver cdc_acm
kernel: cdc_acm: USB Abstract Control Model driver for USB modems and ISDN adapters
</code></pre><p>In my case, my device is <code>/dev/ttyACM0</code>. If you have other serial devices
attached to your system, your Z-Stick may show up as <code>ttyACM1</code> or <code>ttyACM2</code>.</p>
<h2 id="using-z-wave-in-the-docker-container">Using Z-Wave in the Docker container</h2>
<p>If you use <code>docker-compose</code>, simply add a <code>devices</code> section to your existing
YAML file:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#268bd2">version</span>: <span style="color:#2aa198">&#39;2&#39;</span>
<span style="color:#268bd2">services</span>:
  <span style="color:#268bd2">home-assistant</span>:
    <span style="color:#268bd2">ports</span>:
      - <span style="color:#2aa198">&#34;8123:8123/tcp&#34;</span>
    <span style="color:#268bd2">network_mode</span>: <span style="color:#2aa198">&#34;host&#34;</span>
    <span style="color:#268bd2">devices</span>:
      - /dev/ttyACM0
    <span style="color:#268bd2">volumes</span>:
      - /etc/localtime:/etc/localtime:ro
      - /mnt/raid/hass/:/config:Z
    <span style="color:#268bd2">image</span>: homeassistant/home-assistant
    <span style="color:#268bd2">restart</span>: always
</code></pre></div><p>You can add the device to manual <code>docker run</code> commands by adding <code>--device /dev/ttyACM0</code> to your existing command line.</p>
<h2 id="pairing">Pairing</h2>
<p>For this step, always refer to the instructions that came with your Z-Wave
device since some require different pairing steps. In my case, I installed
the battery, pressed the button inside the sensor, and paired the device:</p>
<ul>
<li>Go to the Home Assistant web interface</li>
<li>Click <strong>Configuration</strong> on the left</li>
<li>Click <strong>Z-Wave</strong> on the right</li>
<li>Click <strong>Add Node</strong> and follow the steps on screen</li>
</ul>
<h2 id="understanding-how-the-sensor-works">Understanding how the sensor works</h2>
<p>Now that the sensor has been added, we need to understand how it works. One
of the entities the sensor provides is an <code>alarm_level</code>. It has two possible
values:</p>
<ul>
<li><code>0</code>: the sensor is tilted vertically (garage door is closed)</li>
<li><code>255</code>: the sensor is tilted horizontally (garage door is open)</li>
</ul>
<p>If the sensor changes from <code>0</code> to <code>255</code>, then someone opened the garage door.
Closing the door would result in the sensor changing from <code>255</code> to <code>0</code>.</p>
<h2 id="adding-automation">Adding automation</h2>
<p>Let&rsquo;s add automation to let us know when the door is open:</p>
<ul>
<li>Click <strong>Configuration</strong> on the left</li>
<li>Click <strong>Automation</strong> on the right</li>
<li>Click the plus (+) at the bottom right</li>
<li>Set a good name (like &ldquo;Garage door open&rdquo;)</li>
<li>Under triggers, look for <code>Vision ZG8101 Garage Door Detector Alarm Level</code>
and select it</li>
<li>Set <strong>From</strong> to <code>0</code></li>
<li>Set <strong>To</strong> to <code>255</code></li>
<li>Leave the <strong>For</strong> spot empty</li>
</ul>
<p>Now that we can detect the garage door being open, we need a notification
action. I love <a href="https://www.pushbullet.com/">PushBullet</a> and I have an action set up for PushBullet
notifications already. Here&rsquo;s how to use an action:</p>
<ul>
<li>Select <strong>Call Service</strong> for <strong>Action Type</strong> in the <strong>Actions</strong> section</li>
<li>Select a service to call when the trigger occurs</li>
<li><strong>Service data</strong> should contain the json that contains the notification
message and title</li>
</ul>
<p>Here&rsquo;s an example of my service data:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json">{
  <span style="color:#268bd2">&#34;message&#34;</span>: <span style="color:#2aa198">&#34;Someone opened the garage door at home.&#34;</span>,
  <span style="color:#268bd2">&#34;title&#34;</span>: <span style="color:#2aa198">&#34;Garage door opened&#34;</span>
}
</code></pre></div><p>Press the orange and white save icon at the bottom right and you are ready to
go! You can tilt the sensor in your hand to test it or attach it to your
garage door and test it there.</p>
<p>If you want to know when the garage door is closed, follow the same steps
above, but use <code>255</code> for <strong>From</strong> and <code>0</code> for <strong>To</strong>.</p>
]]></content></item><item><title>Allow a port range with firewalld</title><link>https://major.io/2019/01/04/allow-port-range-with-firewalld/</link><pubDate>Fri, 04 Jan 2019 00:00:00 +0000</pubDate><guid>https://major.io/2019/01/04/allow-port-range-with-firewalld/</guid><description>Managing iptables gets a lot easier with firewalld. You can manage rules for the IPv4 and IPv6 stacks using the same commands and it provides fine-grained controls for various &amp;ldquo;zones&amp;rdquo; of network sources and destinations.
Quick example Here&amp;rsquo;s an example of allowing an arbitrary port (for netdata) through the firewall with iptables and firewalld on Fedora:
## iptables iptables -A INPUT -j ACCEPT -p tcp --dport 19999 ip6tables -A INPUT -j ACCEPT -p tcp --dport 19999 service iptables save service ip6tables save ## firewalld firewall-cmd --add-port=19999/tcp --permanent In this example, firewall-cmd allows us to allow a TCP port through the firewall with a much simpler interface and the change is made permanent with the --permanent argument.</description><content type="html"><![CDATA[<p>Managing iptables gets a lot easier with <a href="https://firewalld.org/">firewalld</a>. You can manage rules for
the IPv4 and IPv6 stacks using the same commands and it provides fine-grained
controls for various &ldquo;zones&rdquo; of network sources and destinations.</p>
<h2 id="quick-example">Quick example</h2>
<p>Here&rsquo;s an example of allowing an arbitrary port (for <a href="https://github.com/netdata/netdata">netdata</a>) through the
firewall with iptables and firewalld on Fedora:</p>
<pre><code>## iptables
iptables -A INPUT -j ACCEPT -p tcp --dport 19999
ip6tables -A INPUT -j ACCEPT -p tcp --dport 19999
service iptables save
service ip6tables save

## firewalld
firewall-cmd --add-port=19999/tcp --permanent
</code></pre><p>In this example, <code>firewall-cmd</code> allows us to allow a TCP port through the
firewall with a much simpler interface and the change is made permanent with
the <code>--permanent</code> argument.</p>
<p>You can always test a change with firewalld without making it permanent:</p>
<pre><code>firewall-cmd --add-port=19999/tcp
## Do your testing to make sure everything works.
firewall-cmd --runtime-to-permanent
</code></pre><p>The <code>--runtime-to-permanent</code> argument tells firewalld to write the currently
active firewall configuration to disk.</p>
<h2 id="adding-a-port-range">Adding a port range</h2>
<p>I use <a href="https://mosh.org/">mosh</a> with most of my servers since it allows me to reconnect to an
existing session from anywhere in the world and it makes higher latency
connections less painful. Mosh requires a range of UDP ports (60000 to 61000)
to be opened.</p>
<p>We can do that easily in firewalld:</p>
<pre><code>firewall-cmd --add-port=60000-61000/udp --permanent
</code></pre><p>We can also see the rule it added to the firewall:</p>
<pre><code># iptables-save | grep 61000
-A IN_public_allow -p udp -m udp --dport 60000:61000 -m conntrack --ctstate NEW,UNTRACKED -j ACCEPT
# ip6tables-save | grep 61000
-A IN_public_allow -p udp -m udp --dport 60000:61000 -m conntrack --ctstate NEW,UNTRACKED -j ACCEPT
</code></pre><p>If you haven&rsquo;t used firewalld yet, give it a try! There&rsquo;s a lot more documentation on common use cases in the <a href="https://docs.fedoraproject.org/en-US/quick-docs/firewalld/">Fedora firewalld documentation</a>.</p>
]]></content></item><item><title>Disable autoplay for videos in Firefox 65</title><link>https://major.io/2018/12/18/disable-autoplay-for-videos-in-firefox-65/</link><pubDate>Tue, 18 Dec 2018 00:00:00 +0000</pubDate><guid>https://major.io/2018/12/18/disable-autoplay-for-videos-in-firefox-65/</guid><description>Firefox has some great features, but one of my favorites is the ability to disable autoplay for videos. We&amp;rsquo;ve all had one of those moments: your speakers are turned up and you browse to a website with an annoying advertisement that plays immediately.
This feature stopped working for me somewhere in the Firefox 65 beta releases. Also, the usual setting in the preference page (under Privacy &amp;amp; Security) seems to be missing.</description><content type="html"><![CDATA[<p>Firefox has some great features, but one of my favorites is the ability to
disable autoplay for videos. We&rsquo;ve all had one of those moments: your
speakers are turned up and you browse to a website with an annoying
advertisement that plays immediately.</p>
<!-- raw HTML omitted -->
<p>This feature stopped working for me somewhere in the Firefox 65 beta
releases. Also, the usual setting in the preference page (under <em>Privacy &amp;
Security</em>) seems to be missing.</p>
<p>Luckily we can edit Firefox&rsquo;s configuration directly to get this feature
working again. Open up a new browser tab, go to <code>about:config</code>, and adjust
these settings:</p>
<ul>
<li>
<p>Set <code>media.autoplay.default</code> to <code>1</code> to disable video autoplay for all sites</p>
</li>
<li>
<p>Set <code>media.autoplay.allow-muted</code> to <code>false</code> to disable video autoplay <em>even for muted videos</em></p>
</li>
</ul>
<p>Those changes take effect for any new pages that you open after making the change.</p>
]]></content></item><item><title>Getting started with ham radio repeaters</title><link>https://major.io/2018/12/13/getting-started-with-ham-radio-repeaters/</link><pubDate>Thu, 13 Dec 2018 00:00:00 +0000</pubDate><guid>https://major.io/2018/12/13/getting-started-with-ham-radio-repeaters/</guid><description>Amateur radio is a fun way to mess around with technology, meet new people, and communicate off the grid. Talking directly to another radio on a single frequency (also called simplex) is the easiest way to get started. However, it can be difficult to communicate over longer distances without amplifiers, proper wiring, and antennas. This is where a radio repeater can help.
What&amp;rsquo;s in scope This post is focused on fairly local communication on VHF/UHF bands.</description><content type="html"><![CDATA[<p><a href="https://en.wikipedia.org/wiki/Amateur_radio">Amateur radio</a> is a fun way to mess around with technology, meet new people,
and communicate off the grid. Talking directly to another radio on a single
frequency (also called <em>simplex</em>) is the easiest way to get started. However,
it can be difficult to communicate over longer distances without amplifiers,
proper wiring, and antennas. This is where a radio repeater can help.</p>
<h2 id="whats-in-scope">What&rsquo;s in scope</h2>
<p>This post is focused on fairly local communication on VHF/UHF bands. The most
common frequencies for local communication in these bands are:</p>
<ul>
<li>2 meters (~144-148MHz)*</li>
<li>70 centimeters (~420-450MHz)*</li>
</ul>
<p><em>* NOTE: Always consult the <a href="http://www.arrl.org/band-plan">band plan</a> for your area to see which part of the
frequency band you could and should use.</em></p>
<p>Of course, you can do some amazing things with weak signal VHF (which can be
used to commuinicate over <strong>great</strong> distances), but we&rsquo;re not talking about
that here. The <a href="http://www.144200.net/about.html">HAMSter Amateur Radio Group</a> is a great place to get started
with that.</p>
<p>We&rsquo;re also not talking about radio bands longer than 2 meters (which includes
high frequency (HF) bands). Some of those bands require advanced FCC
licensing that takes additional studying and practice.</p>
<h2 id="keeping-it-simplex">Keeping it simple(x)</h2>
<p><a href="https://en.wikipedia.org/wiki/Simplex_communication">Simplex radio</a> involves communication where radios are tuned to a single
frequency and only one radio can transmit at a time. This is like a simple
walkie-talkie. If one person is transmitting, everyone else listens. If
someone else tries to transmit at the same time, then the waves will be
garbled and nobody will be able to hear either person. This is often called
&ldquo;doubling up&rdquo;.</p>
<p>This method works well when radios are in range of each other without a bunch
of objects in between. However, it&rsquo;s difficult to talk via simplex over great
distances or around big obstables, such as mountains or hills.</p>
<h2 id="repeaters">Repeaters</h2>
<p><a href="https://en.wikipedia.org/wiki/Repeater">Repeaters</a> are a little more complex to use, but they provide some great
benefits. A repeater usually consists of one or two radios, one or two
antennas, duplexers, and some other basic equipment. They receive a signel on
one frequency and broadcast that same signal on another frequency. They often
are mounted high on towers and this gives them a much better reach than
antennas on your car or home.</p>
<p>I enjoy using a repeater here in San Antonio called <a href="https://repeaterbook.com/repeaters/details.php?state_id=48&amp;ID=11397">KE5HBB</a>. The repeater
has this configuration:</p>
<ul>
<li>Downlink: 145.370</li>
<li>Uplink: 144.770</li>
<li>Offset: -0.6 MHz</li>
<li>Uplink Tone: 114.8</li>
<li>Downlink Tone: 114.8</li>
</ul>
<p>Let&rsquo;s make sense of this data:</p>
<ul>
<li>
<p>Downlink: This is the frequency that the repeater uses to <em>transmit</em>. In
other words, when people talk on this repeater, this is the frequency you
use to hear them.</p>
</li>
<li>
<p>Uplink: The receiver <em>listens</em> on this frequency. If you want to talk to
people who are listening to this repeater, you need to transmit on this
frequency.</p>
</li>
<li>
<p>Offset: This tells you how to calculate the uplink frequency if it is not
shown. This repeater has a negative 0.6 offset, so we can calculate the
uplink frequency if it was not provided:</p>
</li>
</ul>
<pre><code>145.370 - 0.600 = 144.770
</code></pre><ul>
<li>Uplink/Downlink Tones: Your radio must transmit this tone to <em>open the
squelch</em> on the repeater (more on this in a moment). The repeater will use
the same tone to transmit, so we can configure our radio to listen for that
tone and only open our squelch when it is detected.</li>
</ul>
<h2 id="opening-the-squelch">Opening the squelch</h2>
<p>Transmitting radio waves uses a lot of power and it creates a lot of heat.
There are parts of a radio that will wear out much more quickly if a radio is
transmitting constantly. This is why receivers have a <em>squelch</em>. This means
that a radio must transmit something strong enough on the frequency (or use a
tone) to let the repeater know that it needs to repeat something.</p>
<p>You may come across repeaters with no tones listed (sometimes shown as <em>PL</em>).
This means that you can just transmit on the uplin frequency and the repeater
will repeat your signal. These repeaters are easy to use, but they can create
problems.</p>
<p>Imagine if you&rsquo;re traveling through an area and you&rsquo;re using a frequency to
talk to a friend in another car. As you&rsquo;re driving, you move in range of a
repeater that is listening on that frequeny. Suddenly your conversation is
now being broadcasted through the repeater and everyone listening to that
repeater must listen to you. This isn&rsquo;t what you expected and it could be
annoying to other listeners.</p>
<p>Also, in crowded urban areas, there&rsquo;s always a chance that signals might end
up on the repeater&rsquo;s listening frequency unintentionally. That would cause
the repeater to start transmitting and it would increase wear.</p>
<p>Two repeaters might be relatively close (or just out of range) and the tone
helps each repeater identify its own valid radio traffic.</p>
<h2 id="tuning-the-tones">Tuning the tones</h2>
<p>Most repeaters have a <em>tone squelch</em>. That means you can blast them with 100
watts of radio waves and they won&rsquo;t repeat a thing until you transmit an
inaudible tone at the beginning of your transmission.</p>
<p>As an example, in the case of KE5HBB, this tone is 114.8. You must configure
a <a href="https://en.wikipedia.org/wiki/Continuous_Tone-Coded_Squelch_System">CTCSS</a> tone on your radio so that the tone is transmitted as soon as you
begin transmitting. That signals the repeater that it&rsquo;s time to repeat. These
signals aren&rsquo;t audible to humans.</p>
<p>If you know you&rsquo;re tuned to the right frequency to transmit (the uplink
frequency), but the repeater won&rsquo;t repeat your traffic, then you are most
likely missing a tone. There&rsquo;s also a chance that you programmed the uplink
and downlink tones into your radio in reverse, so check that, too.</p>
<h2 id="repeater-transmit-tone">Repeater transmit tone</h2>
<p>Some receivers will transmit a tone when they broadcast back to you, but some
won&rsquo;t. If you can transmit but you can&rsquo;t hear anyone else when they talk,
double check your radio&rsquo;s settings for a tone squelch on the receiving side.
Your radio can also listen for these tones and only open its squelch when it
hears them.</p>
<p>I usually disable receiver squelch for tones on my radio since the repeater
operator could disable that feature at any time and I wouldn&rsquo;t be able to
hear any transmissions since my radio would be waiting for the tone.</p>
<h2 id="testing-a-repeater">Testing a repeater</h2>
<p>First off, please don&rsquo;t test a repeater unless you have a proper amateur
radio license in your jurisdiction. In the United States, that&rsquo;s the FCC.
Don&rsquo;t skip this step.</p>
<p>Once you get your repeater&rsquo;s frequencies programmed into your radio properly
and you&rsquo;ve double checked the settings for sending tones, you can try
&ldquo;breaking the squelch.&rdquo;</p>
<p>Press the transmit button on your radio briefly for about half second and
release. You should hear something when you do this. For some repeaters, you
may hear a <em>KERRRCHUNK</em> noise. That&rsquo;s the sound of the repeater squelch
closing the transmission now that you&rsquo;re done with your transmission. On
other repeaters, you may hear some audible tones or beeps as soon as you
release the transmit button.</p>
<p>Once you have it working properly, stop breakng the squelch and introduce
yourself! For example, when I&rsquo;m in my car, I might say: <em>&ldquo;W5WUT mobile and
monitoring.&quot;</em> That lets people on the repeater know that I&rsquo;m there and that
I&rsquo;m moving (so I might not be on for a very long time).</p>
<p>Good luck on the radio waves! 73&rsquo;s from W5WUT.</p>
]]></content></item><item><title>Use a secret as an environment variable in OpenShift deployments</title><link>https://major.io/2018/12/06/use-secret-as-environment-variable-in-openshift-deployments/</link><pubDate>Thu, 06 Dec 2018 00:00:00 +0000</pubDate><guid>https://major.io/2018/12/06/use-secret-as-environment-variable-in-openshift-deployments/</guid><description>OpenShift deployments allow you to take a container image and run it within a cluster. You can easily add extra items to the deployment, such as environment variables or volumes.
The best practice for sensitive environment variables is to place them into a secret object rather than directly in the deployment configuration itself. Although this keeps the secret data out of the deployment, the environment variable is still exposed to the running application inside the container.</description><content type="html"><![CDATA[<p>OpenShift <a href="https://docs.openshift.com/container-platform/3.9/dev_guide/deployments/how_deployments_work.html">deployments</a> allow you to take a container image and run it within a
cluster. You can easily add extra items to the deployment, such as
environment variables or volumes.</p>
<p>The best practice for sensitive environment variables is to place them into a
<a href="https://docs.openshift.com/container-platform/3.9/dev_guide/secrets.html">secret object</a> rather than directly in the deployment configuration itself.
Although this keeps the secret data out of the deployment, the environment
variable is still exposed to the running application inside the container.</p>
<h2 id="creating-a-secret">Creating a secret</h2>
<p>Let&rsquo;s start with a snippet of a <code>deploymentConfig</code> that has a sensitive
environment variable in plain text:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yml" data-lang="yml"><span style="color:#268bd2">spec</span>:
    <span style="color:#268bd2">containers</span>:
    - <span style="color:#268bd2">env</span>:
        - <span style="color:#268bd2">name</span>: MYAPP_SECRET_TOKEN
          <span style="color:#268bd2">value</span>: vPWps5E7KO8KPlljaD3eXED3f6jmLsV5mQ
    <span style="color:#268bd2">image</span>: <span style="color:#2aa198">&#34;fedora:29&#34;</span>
    <span style="color:#268bd2">name</span>: my_app
</code></pre></div><p>The first step is to create a secret object that contains our sensitive
environment variable:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yml" data-lang="yml"><span style="color:#268bd2">apiVersion</span>: v1
<span style="color:#268bd2">kind</span>: Secret
<span style="color:#268bd2">metadata</span>:
  <span style="color:#268bd2">name</span>: secret-token-for-my-app
<span style="color:#268bd2">stringData</span>:
  <span style="color:#268bd2">SECRET_TOKEN</span>: vPWps5E7KO8KPlljaD3eXED3f6jmLsV5mQ
</code></pre></div><p>Save this file as <code>secret-token.yml</code> and deploy it to OpenShift:</p>
<pre><code>oc apply -f secret-token.yml
</code></pre><p>Query OpenShift to ensure the secret is ready to use:</p>
<pre><code>$ oc get secret/secret-token-for-my-app
NAME                            TYPE      DATA      AGE
secret-token-for-my-app         Opaque    1         1h
</code></pre><h2 id="using-the-secret">Using the secret</h2>
<p>We can adjust the deployment configuration to use this new secret:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yml" data-lang="yml"><span style="color:#268bd2">spec</span>:
    <span style="color:#268bd2">containers</span>:
    - <span style="color:#268bd2">env</span>:
      - <span style="color:#268bd2">name</span>: MYAPP_SECRET_TOKEN
        <span style="color:#268bd2">valueFrom</span>:
          <span style="color:#268bd2">secretKeyRef</span>:
            <span style="color:#268bd2">key</span>: SECRET_TOKEN
            <span style="color:#268bd2">name</span>: secret-token-for-my-app
    <span style="color:#268bd2">image</span>: <span style="color:#2aa198">&#34;fedora:29&#34;</span>
    <span style="color:#268bd2">name</span>: my_app
</code></pre></div><p>This configuration tells OpenShift to look inside the secret object called
<code>secret-token-for-my-app</code> for a key matching <code>SECRET_TOKEN</code>. The value will
be passed into the <code>MYAPP_SECRET_TOKEN</code> environment variable and it will be
available to the application running in the container.</p>
<p><strong>Security note:</strong> If someone has access to change the deployment
configuration object, they could get access to the value of the secret
without having direct access to the secret object itself. It would be trivial
to change the startup command in the container to print all of the
environment variables in the container (using the <code>env</code> command) and view them
in the container logs.</p>
]]></content></item><item><title>Make alt-arrow keys work with terminator and weechat</title><link>https://major.io/2018/09/05/make-alt-arrow-keys-work-with-terminator-and-weechat/</link><pubDate>Thu, 06 Sep 2018 03:43:30 +0000</pubDate><guid>https://major.io/2018/09/05/make-alt-arrow-keys-work-with-terminator-and-weechat/</guid><description>&lt;p>As I make the move from the world of GNOME to i3, I found myself digging deeper into the &lt;a href="https://terminator-gtk3.readthedocs.io/en/latest/">terminator&lt;/a> preferences to make it work more like &lt;a href="https://help.gnome.org/users/gnome-terminal/stable/">gnome-terminal&lt;/a>.&lt;/p></description><content type="html"><![CDATA[<p>As I make the move from the world of GNOME to i3, I found myself digging deeper into the <a href="https://terminator-gtk3.readthedocs.io/en/latest/">terminator</a> preferences to make it work more like <a href="https://help.gnome.org/users/gnome-terminal/stable/">gnome-terminal</a>.</p>
<p>I kept running into an issue where I couldn&rsquo;t move up and down between buffers using alt and arrow keys. My workaround was to call the buffer directly with alt-8 (for buffer #8) or alt-j 18 (buffer #18). However, that became tedious. Sometimes I just wanted to quickly hop up or down one or two buffers.</p>
<p>To fix this problem, right click anywhere inside the terminal and choose <em>Preferences</em>. Click on the <em>Keybindings</em> tab and look for <code>go_up</code> and <code>go_down</code>. These are almost always set to <em>Alt-Up</em> and <em>Alt-Down</em> by default. That&rsquo;s the root of the problem: terminator is grabbing those keystrokes before they can make it down into weechat.</p>
<p>Unfortunately, it&rsquo;s not possible to clear a keybinding within the preferences dialog. Close the window and open <code>~/.config/terminator/config</code> in a terminal.</p>
<p>If you&rsquo;re new to terminator, you might not have a <code>[keybindings]</code> section in your configuration file. If that&rsquo;s the case, add the whole section below the <code>[global_config]</code> section. Otherwise, just ensure your <code>[keybindings]</code> section contains these lines:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ini" data-lang="ini"><span style="color:#719e07">[keybindings]</span>
  go_down <span style="color:#719e07">=</span> <span style="color:#2aa198">None
</span><span style="color:#2aa198">  go_up = None</span>
</code></pre></div><p>Close <em>all</em> of the terminator windows (on all of your workspaces). <strong>This is a critical step!</strong> Terminator only loads the config file when it is first started, not when additional terminals are opened.</p>
<p>Open a terminator terminal, start weechat, and test your alt-arrow keys! You should be moving up and down between buffers easily. If that doesn&rsquo;t work, check your window manager&rsquo;s settings to ensure that another application hasn&rsquo;t stolen that keybinding from your terminals.</p>]]></content></item><item><title>How to thrive at a technical conference</title><link>https://major.io/2018/05/09/how-to-thrive-at-a-tech-conference/</link><pubDate>Wed, 09 May 2018 23:54:28 +0000</pubDate><guid>https://major.io/2018/05/09/how-to-thrive-at-a-tech-conference/</guid><description>I&amp;rsquo;m at the 2018 Red Hat Summit this week in San Francisco and I am enjoying the interactions between developers, executives, vendors, and engineers. It&amp;rsquo;s my seventh Summit (but my first as a Red Hat employee!), but I regularly meet people who are attending their first technical conference.
The question inevitably comes up: &amp;ldquo;I&amp;rsquo;m so tired. How do you survive these events?&amp;rdquo;
One attendee asked me to write a blog post on my tips and tricks.</description><content type="html"><![CDATA[<p><img src="/wp-content/uploads/2018/05/IMG_20180508_202748.jpg" alt="1"></p>
<p>I&rsquo;m at the <a href="https://www.redhat.com/es/summit/2018">2018 Red Hat Summit</a> this week in San Francisco and I am enjoying the interactions between developers, executives, vendors, and engineers. It&rsquo;s my seventh Summit (but my first as a Red Hat employee!), but I regularly meet people who are attending their first technical conference.</p>
<p>The question inevitably comes up: &ldquo;I&rsquo;m so tired. How do you survive these events?&rdquo;</p>
<p>One attendee asked me to write a blog post on my tips and tricks. This is the post that explains how to thrive, not just survive, at conferences. Beware - these tips are based on my experiences and your mileage may vary depending on your personality, the event itself, and your caffeine intake.</p>
<h2 id="discover-the-area">Discover the area</h2>
<p>Traveling to a conference is awesome way to experience more of the world! Take time to enjoy the tourist sites but also find out where the locals like to go. Any hotel concierge should be able to give you advice on where to go to truly experience the location.</p>
<p>Take some time to learn the area around your hotel and the venue. Be sure you can navigate between the two and find some important spots nearby, like pharmacies and coffee shops.</p>
<h2 id="food-water-and-sleep">Food, water, and sleep</h2>
<p>These conferences can often feel overwhelming and you may find yourself forgetting to eat the right foods, stay hydrated, and get some rest.</p>
<p>Take every opportunity to eat healthier foods during the week that will give you energy without weighing you down. All the stuff that your Mom told you to eat is a good idea. My rule of thumb is to eat a heavy breakfast, a medium sized lunch, and then whatever I want for dinner. Evening events often have free food (more on those events next), and that fits my travel budget well. It also allows me to splurge a bit on foods that I might not eat back home.</p>
<p>Take along a water container when you travel. You can&rsquo;t always depend on the conference for making water available and you&rsquo;ll often need more than they offer anyway. I&rsquo;m a big fan of <a href="https://www.nalgene.com/">Nalgene&rsquo;s</a> products since they take a beating and they have really good seals.</p>
<p>Sleeping is a real challenge. Early morning keynotes and late night events put a strain on anyone&rsquo;s sleep schedule. Lots of people have trouble sleeping in hotels or in cities where the noise level remains high all night long. The best remedy is to be choosy about the events you attend and the time you spend there. Think about what is more valuable: more time listening to blasting music at a party or more time with your head on the pillow.</p>
<p>Consider using an application on your phone that provides various types of noises, such as <a href="https://en.wikipedia.org/wiki/White_noise">white noise</a>. I love the <a href="https://play.google.com/store/apps/details?id=com.tmsoft.whitenoise.full">White Noise</a> app on Android since it has tons of options for various sounds. In my experience, brown noise works best for sleeping. Pink noise can help in extremely noisy environments (like downtown San Francisco) but it&rsquo;s often too loud for me.</p>
<h2 id="keep-your-devices-charged">Keep your devices charged</h2>
<p>Find a way to keep your devices charged, especially your phone. I use <a href="https://www.anker.com/">Anker</a> battery packs to keep my phone topped up during the day when I can&rsquo;t get to a plug. A dead phone disconnects you from your friends, maps, and conference details.</p>
<h2 id="dress-for-success">Dress for success</h2>
<p>Your clothing selection really depends on the type of conference and the company you represent. If you need to dress formally each day, then your choices are already made for you.</p>
<p>Pack layers of clothing so you can add or remove layers as needed. The walk to the conference center may be warm, but the keynote auditorium could feel like a freezer. This also prepares you for evening events which might be outdoors.</p>
<p>Wear clothing that makes you feel comfortable. You&rsquo;ll find a wide range of outfits at most tech conferences and you&rsquo;ll find that nobody really cares how formal or informal you are. If you&rsquo;re there to listen, learn, and contribute, then dress casually. If you&rsquo;re looking for a new job, doing a talk, or if you&rsquo;ll be on camera, choose something a little more formal.</p>
<h2 id="the-hallway-track">The hallway track</h2>
<p>You won&rsquo;t find the hallway track on any agenda, but it is often the most valuable part of any gathering. The hallway track encompasses those brief encounters you have with other people at the event. Turn those mundane events, such as waiting in line, eating lunch, or between talks, into opportunities to meet other people.</p>
<p>Yes, this does mean that you must do something to come out of your shell and start a conversation. This is still difficult for me. Here are some good ways to start a conversation with someone around you:</p>
<ul>
<li>&ldquo;Hello, my name is Major&rdquo; (put out your hand for a handshake)</li>
<li>&ldquo;Where do you work?&rdquo;</li>
<li>&ldquo;What do you work on?&rdquo;</li>
<li>&ldquo;Man, this line is really long.&rdquo;</li>
<li>&ldquo;vim or emacs?&rdquo; <em>(just kidding)</em></li>
</ul>
<p>The secret is to find something that makes you and the other person feel comfortable. There are situations where you might be met with a cold shoulder, and that&rsquo;s okay. I&rsquo;ve found that sometimes people need some space or the issue could be a language barrier. Making the attempt is what matters.</p>
<p>These are excellent opportunities for learning, for listening, and for sharing. These new contacts will show up again and again at the event (more on parties/networking next), and you can talk to them again when you feel the tendency to become a <a href="https://en.wikipedia.org/wiki/Wallflower_(people)">wallflower</a> again.</p>
<h2 id="parties-and-networking-events">Parties and networking events</h2>
<p>Evening events at conferences are a great way to keep the hallway track going while taking some time to relax as well. Some of the best conversations I&rsquo;ve had at conferences were during evening events or vendor parties. People are more candid since the conference demands are often reduced.</p>
<p>However, it&rsquo;s incredibly easy to make some spectacularly bad decisions at these events. This list should help you navigate these events and get value from them:</p>
<h3 id="enjoy-an-open-bar-responsibly">Enjoy an open bar responsibly</h3>
<p>Early in my career, I looked at an open bar as a magical oasis. Free drinks! As many as I want! This is heaven! (Narrator: <em>It was not heaven. It was something else.</em>)</p>
<p>I think about open bars much like I think about a trip to Las Vegas. Before I go, I think about how much money I feel like losing, and I only bet that much. <strong>Once the money is gone, I&rsquo;m done.</strong></p>
<p>Go into the event knowing how much or how little you want to consume. <strong>Zero is an entirely valid answer.</strong> Keep in mind that the answer to &ldquo;Why aren&rsquo;t you drinking anything?&rdquo; does not have to be &ldquo;I guess I&rsquo;ll get something.&rdquo; <strong>Nobody needs to know why you&rsquo;re not drinking and you shouldn&rsquo;t feel pressured to do something you don&rsquo;t want to do.</strong></p>
<p>Think about how you want to feel in the morning. Is a massive hangover worth another round of shots? Is it worth it to ruin your talk the next day? Is it worth it to get belligerent and say something that may be difficult to take back? Think about these things ahead of time and make a plan before you begin drinking.</p>
<h3 id="leave-when-you-want">Leave when you want</h3>
<p>Some evening events can last much too late and this could derail your plans for the morning. If the party runs from 7-10PM, don&rsquo;t feel obligated to stay until 10PM. If you&rsquo;re not meeting the right people or if you&rsquo;re not having a good time: leave. It&rsquo;s better to abandon an event early than suffer through it and crawl through the next morning.</p>
<h3 id="turn-down-an-uninteresting-invitation">Turn down an uninteresting invitation</h3>
<p>The conference may host various events or a vendor may invite you to an event. These are just invitations and your attendance is not required (unless you work for the vendor throwing the party). Feel free to do something else with your time if the event or the venue seem uninteresting or unsafe. (More on safety next.)</p>
<h3 id="get-a-party-buddy">Get a party buddy</h3>
<p>Remember those people you talked to in the hallway and during lunch? Find those people at the event and tell them you enjoyed the conversation from earlier. I&rsquo;ve been to conferences before where I&rsquo;ve been the only one from my company and after letting the other person know that, they invited me to hang out with them or their group at the event.</p>
<p>This is a good idea for two reasons. First, it gives you someone to talk to. More importantly, it helps you stay safe.</p>
<h2 id="dealing-with-harassment">Dealing with harassment</h2>
<p>This gets its own section. It has happened to me and it will likely happen to you.</p>
<p>Nobody ever wants it to happen, but people are often harassed in one way or another at these events. It&rsquo;s inevitable: there are drinks, people are away from home, and they&rsquo;re enjoying time away from work. For some people, this is a combination of factors that leads them to make bad choices at these events.</p>
<p>Harassment comes in many forms, but nobody should put up with it. If you see someone being treated badly, step in. If you&rsquo;re being treated badly, get help. If you&rsquo;re treating someone badly, apologize and remove yourself from the situation. This is where a party buddy can be extremely helpful.</p>
<p>Harassment is not a women-only or men-only problem. I have been touched in unwelcome ways and verbally harassed at evening events. It is not fun. In my experience, telling the other person to &ldquo;Please stop&rdquo; or &ldquo;That is not okay&rdquo; is usually enough to diffuse the situation.</p>
<p>This may not always work. Grab your buddy and get help from conference staffers or a security guard if a situation continues to escalate.</p>
<h2 id="more-ideas">More ideas</h2>
<p>These are some ideas that help me thrive at conferences and make the most of my time traveling. Feel free to leave some of your ideas below in the comments section!</p>
]]></content></item><item><title>Reaching the fork in the road</title><link>https://major.io/2018/03/07/reaching-the-fork-in-the-road/</link><pubDate>Wed, 07 Mar 2018 16:18:51 +0000</pubDate><guid>https://major.io/2018/03/07/reaching-the-fork-in-the-road/</guid><description>Walt Disney said it best:
We keep moving forward, opening new doors, and doing new things, because we&amp;rsquo;re curious and curiosity keeps leading us down new paths.
The world of technology is all about change. We tear down the old things that get in our way and we build new technology that takes us to new heights. Tearing down these old things can often be difficult and that forces us to make difficult choices.</description><content type="html"><![CDATA[<p><img src="/wp-content/uploads/2018/03/1024px-Mountain_trail_in_Panachaiko_mountains_Greece.jpg" alt="1"></p>
<p>Walt Disney said it best:</p>
<blockquote>
<p>We keep moving forward, opening new doors, and doing new things, because we&rsquo;re curious and curiosity keeps leading us down new paths.</p>
</blockquote>
<p>The world of technology is all about change. We tear down the old things that get in our way and we build new technology that takes us to new heights. Tearing down these old things can often be difficult and that forces us to make difficult choices.</p>
<p>Rackspace has been a great home for me for over 11 years. I&rsquo;ve made the incredibly difficult choice to leave Rackspace on March 9th to pursue new challenges.</p>
<h2 id="humble-beginnings">Humble beginnings</h2>
<p>I came to Rackspace as an entry-level Linux administrator and was amazed by the culture generated by Rackers. The dedication to customers, technology, and quality was palpable from the first few minutes I spent with my team. Although I didn&rsquo;t know it at the time, I had landed at the epicenter of a sink-or-swim technology learning experience. My team had some very demanding customers with complex infrastructures and it forced me to take plenty of notes (and hard knocks). My manager and teammates supported me through it all.</p>
<p>From there, I served in several different roles. I was a manager of technicians on a support team and had the opportunity to learn how to mentor. One of my favorite leaders said that &ldquo;good managers know when to put their arm around to people and when to put a boot in their rear.&rdquo; I reluctantly learned how to do both and I watched my people grow into senior engineers and great leaders.</p>
<p><img src="/wp-content/uploads/2018/03/6519121761_ab65bab3c1_b.jpg" alt="/wp-content/uploads/2018/03/6519121761_ab65bab3c1_b.jpg">
Datapoint office closing in 2011</p>
<p>I was pulled to Mosso, Rackspace&rsquo;s first cloud offering, shortly after that and discovered an entirely new world. Rackers force-fed me &ldquo;Why&rsquo;s (Poignant) Guide to Ruby&rdquo; and I started building scripts and web front-ends for various services. Rackspace acquired Slicehost after that and I jumped at the chance to work as an operations engineer on the new infrastructure. That led to a lot of late nights diagnosing problems with Xen hypervisors and rails applications. I met some amazing people and began to realize that St. Louis has some pretty good barbecue (but Texas still has them beat).</p>
<p><img src="/wp-content/uploads/2018/03/4171091103_7150ded95f_b.jpg" alt="/wp-content/uploads/2018/03/4171091103_7150ded95f_b.jpg">
Slicehost humor in 2009</p>
<p>Not long after that, I found myself managing an operations team that cared for Slicehost&rsquo;s infrastructure and Rackspace&rsquo;s growing Cloud Servers infrastructure. OpenStack appeared later and I jumped at the chance to do operations there. It was an extremely rough experience in the Diablo release, but it taught me a lot. My start with OpenStack involved fixing lots of broken Keystone tests that didn&rsquo;t run on Python 2.6.</p>
<p><img src="/wp-content/uploads/2018/03/7730840100_01257c5fa4_b.jpg" alt="/wp-content/uploads/2018/03/7730840100_01257c5fa4_b.jpg">
Working on OpenStack in 2012</p>
<p>If you&rsquo;ve attended some of my talks on impostor syndrome, you may know what came next. We had a security issue and I sent some direct feedback to our CSO about how it was handled. I expected to be told to &ldquo;pack a box&rdquo; after that, but I was actually asked to lead a security architecture team in the corporate security group. It was definitely a surprise. I accepted and joined the team as Chief Security Architect. My coworkers called it &ldquo;joining the dark side&rdquo;, but I did my best to build bridges between security teams and the rest of the company.</p>
<p><img src="/wp-content/uploads/2018/03/24142777780_5196ca622b_h.jpg" alt="/wp-content/uploads/2018/03/24142777780_5196ca622b_h.jpg">
Talking at Rackspace::Solve in 2015</p>
<p>This role really challenged me. I had never operated at the Director level before and our team had a ton of work to do. I found myself stumbling (and floundering) fairly often and I leaned on other leaders in the business for advice. This led me to take some courses on critical thinking, accounting, finance, and tough conversations. I&rsquo;ve never had a role as difficult as this one.</p>
<p>Our cloud team came calling and asked me to come back and help with some critical projects in the public cloud. We worked on some awesome skunkworks projects that could really change the business. Although they didn&rsquo;t get deployed in one piece, we found ways to take chunks of the work and optimize different areas of our work. An opportunity came up to bring public cloud experience to the private cloud team and I jumped on that one. I discovered the awesome OpenStack-Ansible project and a strong set of Rackers who were dedicated to bringing high-touch service to customers who wanted OpenStack in their own datacenter.</p>
<p><img src="/wp-content/uploads/2018/03/imposter-syndrome_hayden.jpg" alt="/wp-content/uploads/2018/03/imposter-syndrome_hayden.jpg">
Impostor syndrome talk at the Boston OpenStack Summit in 2017</p>
<p>During this time, I had the opportunity to deliver several conference talks about OpenStack, Fedora, security, and Ansible. My favorite topic was impostor syndrome and I set out on a mission to help people understand it. My first big talk was at the Fedora Flock conference in Rochester in 2015. This led to deep conversations with technical people in conference hallways, evening events, and even airport terminals about how impostor syndrome affects them. I took those conversations and refined my message several times over.</p>
<p><img src="/wp-content/uploads/2018/03/DSCF0425.jpg" alt="/wp-content/uploads/2018/03/DSCF0425.jpg">
Talking about impostor syndrome at Fedora Flock 2015 (Photo credit: Kushal Das)</p>
<h2 id="gratitude">Gratitude</h2>
<p>I couldn&rsquo;t even begin to name a list of Rackers who have helped me along the way. I wouldn&rsquo;t be where I am now without the help of hundreds of Rackers. They&rsquo;ve taught me how to build technology, how to navigate a business, and how to be a better human. They have made me who I am today and I&rsquo;m eternally grateful. I&rsquo;ve had an incredible amount of hugs this week at the office and I&rsquo;ve tried my best not to get a face full of tears in the process.</p>
<p>I&rsquo;d also like to thank all of the people who have allowed me to mentor them and teach them something along the way. One of the best ways to understand something is to teach it to someone else. I relish any opportunity to help someone avoid a mistake I made, or at least be able to throw something soft under them to catch their fall. These people put up with my thick Texas accent, my erratic whiteboard diagrams, and worse of all, my dad jokes.</p>
<p>Another big &ldquo;thank you&rdquo; goes out to all of the members of the open source communities who have mentored me and dealt with my patches.</p>
<p>The first big community I joined was the Fedora Linux community. I&rsquo;ve been fortunate to serve on the board and participate in different working groups. Everyone has been helpful and accommodating, even when I pushed broken package builds. I plan to keep working in the community as long as they will have me!</p>
<p>The OpenStack community has been like family. Everyone - from developers to foundation leaders - has truly been a treat to work with over several years. My work on Rackspace&rsquo;s public and private clouds has pushed me into various projects within the OpenStack ecosystem and I&rsquo;ve found everyone to be responsive. OpenStack events are truly inspiring and it is incredible to see so many people from so many places who dedicate themselves to the software and the people that make cloud infrastructure work.</p>
<h2 id="the-next-adventure">The next adventure</h2>
<p>I plan to talk more on this later, but I will be working from home on some projects that are entirely different from what I&rsquo;m working on now. That adventure starts on March 19th after a week of &ldquo;funemployment.&rdquo; I&rsquo;m incredibly excited about the new opportunity and I&rsquo;ll share more details when I can.</p>
<p><em>Top photo credit: <a href="https://commons.wikimedia.org/wiki/File:Mountain_trail_in_Panachaiko_mountains,_Greece.jpg">Wikipedia</a></em></p>
]]></content></item><item><title>Install testing kernels in Fedora</title><link>https://major.io/2018/02/28/install-testing-kernels-in-fedora/</link><pubDate>Wed, 28 Feb 2018 13:53:48 +0000</pubDate><guid>https://major.io/2018/02/28/install-testing-kernels-in-fedora/</guid><description>If you&amp;rsquo;re on the latest Fedora release, you&amp;rsquo;re already running lots of modern packages. However, there are those times when you may want to help with testing efforts or try out a new feature in a newer package.
Most of my systems have the updates-testing repository enabled in one way or another. This repository contains packages that package maintainers have submitted to become the next stable package in Fedora. For example, if there is a bug fix for nginx, the package maintainer submits the changes and publish a release.</description><content type="html"><![CDATA[<p><img src="/wp-content/uploads/2018/02/120928-F-YV474-917.jpg" alt="1"></p>
<p>If you&rsquo;re on the latest Fedora release, you&rsquo;re already running lots of modern packages. However, there are those times when you may want to help with testing efforts or try out a new feature in a newer package.</p>
<p>Most of my systems have the <code>updates-testing</code> repository enabled in one way or another. This repository contains packages that package maintainers have submitted to become the next stable package in Fedora. For example, if there is a bug fix for nginx, the package maintainer submits the changes and publish a release. That release goes into the testing repositories and must sit for a waiting period or receive sufficient karma (&ldquo;works for me&rdquo; responses) to move into stable repositories.</p>
<h2 id="getting-started">Getting started</h2>
<p>One of the easiest ways to get started is to allow a small amount of packages to be installed from the testing repository on a regular basis. Fully enabling the testing repository for all packages can lead to trouble on occasion, especially if a package maintainer discovers a problem and submits a new testing package.</p>
<p>To get started, open <code>/etc/yum.repos.d/fedora-updates-testing.repo</code> in your favorite text editor (using <code>sudo</code>). This file tells yum and dnf where it should look for packages. The stock testing repository configuration looks like this:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ini" data-lang="ini"><span style="color:#719e07">[updates-testing]</span>
name<span style="color:#719e07">=</span><span style="color:#2aa198">Fedora $releasever - $basearch - Test Updates</span>
failovermethod<span style="color:#719e07">=</span><span style="color:#2aa198">priority</span>
<span style="color:#586e75">#baseurl=http://download.fedoraproject.org/pub/fedora/linux/updates/testing/$releasever/$basearch/</span>
metalink<span style="color:#719e07">=</span><span style="color:#2aa198">https://mirrors.fedoraproject.org/metalink?repo=updates-testing-f$releasever&amp;arch=$basearch</span>
enabled<span style="color:#719e07">=</span><span style="color:#2aa198">0</span>
repo_gpgcheck<span style="color:#719e07">=</span><span style="color:#2aa198">0</span>
type<span style="color:#719e07">=</span><span style="color:#2aa198">rpm</span>
gpgcheck<span style="color:#719e07">=</span><span style="color:#2aa198">1</span>
metadata_expire<span style="color:#719e07">=</span><span style="color:#2aa198">6h</span>
gpgkey<span style="color:#719e07">=</span><span style="color:#2aa198">file:///etc/pki/rpm-gpg/RPM-GPG-KEY-fedora-$releasever-$basearch</span>
skip_if_unavailable<span style="color:#719e07">=</span><span style="color:#2aa198">False</span>
</code></pre></div><p>By default, the repository is not enabled (<code>enabled=0</code>).</p>
<p>In this example, let&rsquo;s consider a situation where you want to test the latest kernel packages as soon as they reach the testing repository. We need to make two edits to the repository configuration:</p>
<ul>
<li><code>enabled=1</code> - Allow yum/dnf to use the repository</li>
<li><code>includepkgs=kernel*</code> - Only allow packages matching <code>kernel*</code> to be installed from the testing repository</li>
</ul>
<p>The repository configuration should now look like this:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ini" data-lang="ini"><span style="color:#719e07">[updates-testing]</span>
name<span style="color:#719e07">=</span><span style="color:#2aa198">Fedora $releasever - $basearch - Test Updates</span>
failovermethod<span style="color:#719e07">=</span><span style="color:#2aa198">priority</span>
<span style="color:#586e75">#baseurl=http://download.fedoraproject.org/pub/fedora/linux/updates/testing/$releasever/$basearch/</span>
metalink<span style="color:#719e07">=</span><span style="color:#2aa198">https://mirrors.fedoraproject.org/metalink?repo=updates-testing-f$releasever&amp;arch=$basearch</span>
enabled<span style="color:#719e07">=</span><span style="color:#2aa198">1</span>
repo_gpgcheck<span style="color:#719e07">=</span><span style="color:#2aa198">0</span>
type<span style="color:#719e07">=</span><span style="color:#2aa198">rpm</span>
gpgcheck<span style="color:#719e07">=</span><span style="color:#2aa198">1</span>
metadata_expire<span style="color:#719e07">=</span><span style="color:#2aa198">6h</span>
gpgkey<span style="color:#719e07">=</span><span style="color:#2aa198">file:///etc/pki/rpm-gpg/RPM-GPG-KEY-fedora-$releasever-$basearch</span>
skip_if_unavailable<span style="color:#719e07">=</span><span style="color:#2aa198">False</span>
includepkgs<span style="color:#719e07">=</span><span style="color:#2aa198">kernel*</span>
</code></pre></div><h2 id="getting-testing-packages">Getting testing packages</h2>
<p>Running <code>dnf upgrade kernel*</code> should now pull a kernel from the <code>updates-testing</code> repository. You can verify this by checking the <code>Repository</code> column in the dnf output.</p>
<p>If you feel more adventurous later, you can add additional packages (separated by spaces) to the <code>includepkgs</code> line. The truly adventurous users can leave the repo enabled but remove <code>includepkgs</code> altogether. This will pull all available packages from the testing repository as soon as they are available.</p>
<h2 id="package-maintainers-need-feedback">Package maintainers need feedback!</h2>
<p>One final note: <strong>package maintainers need your feedback</strong> on packages. Positive or negative feedback is very helpful. You can search for the package on <a href="https://bodhi.fedoraproject.org/">Bodhi</a> and submit feedback there, or use the <code>fedora-easy-karma</code> script via the <code>fedora-easy-karma</code> package. The script will look through your installed package list and query you for feedback on each one.</p>
<p>Submitting lots of feedback can earn you some <a href="https://badges.fedoraproject.org/badge/in-search-of-the-bull-tester-i">awesome Fedora Badges</a>!</p>
<p><em>Photo credit: <a href="http://www.arpc.afrc.af.mil/News/Article-Display/Article/365815/a-wish-come-true-colorado-native-becomes-cadet-for-a-day/">US Air Force</a></em></p>
]]></content></item><item><title>Takeaways from my foray into amateur radio</title><link>https://major.io/2018/01/06/takeaways-from-my-foray-into-amateur-radio/</link><pubDate>Sat, 06 Jan 2018 19:26:53 +0000</pubDate><guid>https://major.io/2018/01/06/takeaways-from-my-foray-into-amateur-radio/</guid><description>The Overland Expo in Asheville last year was a great event, and one of my favorite sessions covered the basics about radio communications while overlanding. The instructors shared their radios with us and taught us some tips and tricks for how to save power and communicate effectively on the trail.
Back at the office, I was surprised to discover how many of my coworkers had an FCC license already. They gave me tips on getting started and how to learn the material for the exam.</description><content type="html"><![CDATA[<p><img src="/wp-content/uploads/2018/01/kenwood_mobile_radio.jpg" alt="1"></p>
<p>The <a href="https://www.overlandexpo.com/">Overland Expo in Asheville</a> last year was a great event, and one of my favorite sessions covered the basics about radio communications while overlanding. The instructors shared their radios with us and taught us some tips and tricks for how to save power and communicate effectively on the trail.</p>
<p>Back at the office, I was surprised to discover how many of my coworkers had an FCC license already. They gave me tips on getting started and how to learn the material for the exam. I took some of my questions to Twitter and had plenty of help pouring in quickly.</p>
<p>This post covers how I studied, what the exam was like, and what I&rsquo;ve learned after getting on the air.</p>
<h2 id="the-basics">The basics</h2>
<p>FCC licenses in the US for amateur radio operators have multiple levels. Everything starts with the Technician level and you get the most basic access to radio frequencies. From there, you can upgrade (with another exam) to General, and Extra. Each license upgrade opens up more frequencies and privileges.</p>
<h2 id="studying">Studying</h2>
<p>A coworker recommended the <a href="http://www.arrl.org/ham-radio-license-manual">official ARRL book</a> for the Technician exam and I picked up a paper copy. The content is extremely dry. It was difficult to remain focused for long periods.</p>
<p>The entire exam is available in the public domain, so you can actually go straight to the questions that you&rsquo;ll see on the exam and study those. I flipped to the question section in the ARRL book and found the questions I could answer easily (mostly about circuits and electrical parts). For each one that was new or difficult, I flipped back in the ARRL book to the discussion in each chapter and learned the material.</p>
<p>I also used <a href="https://hamstudy.org/">HamStudy.org</a> to quickly practice and keep track of my progress. The site has some handy graphs that show you how many questions you&rsquo;ve seen and what your knowledge level of different topics really is. I kept working through questions on the site until I was regularly getting 90% or higher on the practice tests.</p>
<h2 id="testing">Testing</h2>
<p>Before you test, be sure to <a href="https://www.fcc.gov/help/getting-fcc-registration-number-frn-universal-licensing-system-uls">get a FCC Registration Number</a> (commonly called a FRN). They are free to get and it ensures that you get your license (often called your &lsquo;ticket&rsquo;) as soon as possible. I was told that some examiners won&rsquo;t offer you a test if you don&rsquo;t have your FRN already.</p>
<p>The next step is to <a href="http://www.arrl.org/find-an-amateur-radio-license-exam-session">find an amateur radio exam</a> in your area. Exams are available in the San Antonio area every weekend and they are held by different groups. I took mine with the <a href="http://www.w5ros.org/">Radio Operators of South Texas</a> and the examiners were great! Some examiners require you to check in with them so they know you are coming to test, but it&rsquo;s a good idea to do this anyway. Ask how they want to be paid (cash, check, etc), too.</p>
<p>Be sure to take a couple of pencils, a basic calculator, your government issued ID, your payment, and your FRN to the exam. I forgot the calculator but the examiners had a few extras. The examiners complete some paperwork before your exam, and you select one of the available test versions. Each test contains a randomly selected set of 35 questions from the pool of 350.</p>
<p>Go through the test, carefully read each question, and fill in the answer sheet. Three examiners will grade it when you turn it in, and they will fill out your Certificate of Successful Completion of Examination (CSCE). Hold onto this paper just in case something happens with your FCC paperwork.</p>
<p>The examiners will send your paperwork to the FCC and you should receive a license within two weeks. Mine took about 11-12 business days, but I took it just before Thanksgiving. The FCC will send you a generic email stating that there is a new license available and you can download it directly from the FCC&rsquo;s website.</p>
<h2 id="lessons-learned-on-the-air">Lessons learned on the air</h2>
<p>Once I passed the exam and keyed up for the first transmission, I feared a procedural misstep more than anything. What if I say my callsign incorrectly? What if I&rsquo;m transmitting at a power level that is too high? What power level is too high? What am I doing?!</p>
<p>Everyone has to start somewhere and you&rsquo;re going to make mistakes. Almost 99.9% of my radio contacts so far have been friendly, forgiving, and patient. I&rsquo;ve learned a lot from listening to other people and from the feedback I get from radio contacts. Nobody will yell at you for using a repeater when simplex should work. Nobody will yell at you if you blast a repeater with 50 watts when 5 would be fine.</p>
<p>I&rsquo;m on VHF most often and I&rsquo;ve found many local repeaters on <a href="https://www.repeaterbook.com/">RepeaterBook</a>. Most of the repeaters in the San Antonio area are busiest during commute times (morning and afternoon) as well as lunchtime. I&rsquo;ve announced my callsign when the repeater has been quiet for a while and often another radio operator will call back. It&rsquo;s a good idea to mention that you&rsquo;re new to amateur radio since that will make it easier for others to accept your mistakes and provide feedback.</p>
<p>when I&rsquo;m traveling long distances, I monitor the national simplex calling frequency (146.520). That&rsquo;s the CB equivalent of channel 19 where you can announce yourself and have conversations. In busy urban areas, it&rsquo;s best to work out another frequency with your contact to keep the calling frequency clear.</p>
<h2 id="my-equipment">My equipment</h2>
<p>My first purchase was a (cheap) <a href="https://baofengtech.com/uv-5x3">BTECH UV-5X3</a>. The price is fantastic, but the interface is rough to use. Editing saved channels is nearly impossible and navigating the menus requires a good manual to decipher the options. The manual that comes with it is surprisingly brief. There are some helpful how-to guides from other radio operators on various blogs that can help.</p>
<p>I picked up a <a href="http://www.kenwood.com/usa/com/amateur/tm-d710ga/">Kenwood TM-D710G</a> mobile radio from a coworker and mounted it in the car. I wired it up with <a href="https://powerwerx.com/anderson-power-powerpole-sb-connectors">Anderson Powerpole connectors</a> and that makes things incredibly easy (and portable). The interface on the Kenwood is light years ahead of the BTECH, but the price is 10x more.</p>
<p>My car has the <a href="http://www.cometantenna.com/amateur-radio/mobile-antennas/ma-dual-band/">Comet SBB-5NMO</a> antenna mounted with a <a href="http://www.cometantenna.com/amateur-radio/no-holes-mobile-mounts/mounts-with-coax/">Comet CP-5NMO</a> lip mount. It fits well on the rear of the 4Runner.</p>
<p>Managing a lot of repeater frequencies is challenging with both radios (exponentially more so with the BTECH), but the open source <a href="https://chirp.danplanet.com/projects/chirp/wiki/Home">CHIRP</a> software works well. I installed it on my Fedora laptop and could manage both radios easily. The BTECH radio requires you to download the entire current configuration, edit it, and upload it to the radio. The Kenwood allows you to make adjustments to the radio in real time (which is excellent for testing).</p>
<h2 id="more-questions">More questions?</h2>
<p>If you have more questions about any part of the process, let me know!</p>
]]></content></item><item><title>Ensuring keepalived starts after the network is ready</title><link>https://major.io/2017/12/15/ensuring-keepalived-starts-network-ready/</link><pubDate>Fri, 15 Dec 2017 21:18:37 +0000</pubDate><guid>https://major.io/2017/12/15/ensuring-keepalived-starts-network-ready/</guid><description>After a recent OpenStack-Ansible (OSA) deployment on CentOS, I found that keepalived was not starting properly at boot time:
Keepalived_vrrp[801]: Cant find interface br-mgmt for vrrp_instance internal !!! Keepalived_vrrp[801]: Truncating auth_pass to 8 characters Keepalived_vrrp[801]: VRRP is trying to assign ip address 172.29.236.11/32 to unknown br-mgmt interface !!! go out and fix your conf !!! Keepalived_vrrp[801]: Cant find interface br-mgmt for vrrp_instance external !!! Keepalived_vrrp[801]: Truncating auth_pass to 8 characters Keepalived_vrrp[801]: VRRP is trying to assign ip address 192.</description><content type="html"><![CDATA[<p><img src="/wp-content/uploads/2017/12/wait.jpg" alt="1"></p>
<p>After a recent <a href="https://github.com/openstack/openstack-ansible">OpenStack-Ansible (OSA)</a> deployment on CentOS, I found that keepalived was not starting properly at boot time:</p>
<pre><code>Keepalived_vrrp[801]: Cant find interface br-mgmt for vrrp_instance internal !!!
Keepalived_vrrp[801]: Truncating auth_pass to 8 characters
Keepalived_vrrp[801]: VRRP is trying to assign ip address 172.29.236.11/32 to unknown br-mgmt interface !!! go out and fix your conf !!!
Keepalived_vrrp[801]: Cant find interface br-mgmt for vrrp_instance external !!!
Keepalived_vrrp[801]: Truncating auth_pass to 8 characters
Keepalived_vrrp[801]: VRRP is trying to assign ip address 192.168.250.11/32 to unknown br-mgmt interface !!! go out and fix your conf !!!
Keepalived_vrrp[801]: VRRP_Instance(internal) Unknown interface !
systemd[1]: Started LVS and VRRP High Availability Monitor.
Keepalived_vrrp[801]: Stopped
Keepalived[799]: Keepalived_vrrp exited with permanent error CONFIG. Terminating
</code></pre><p>OSA deployments have a management bridge for traffic between containers. These containers run the OpenStack APIs and other support services. By default, this bridge is called <code>br-mgmt</code>.</p>
<p>The keepalived daemon is starting before NetworkManager can bring up the <code>br-mgmt</code> bridge and that is causing keepalived to fail. We need a way to tell systemd to wait on the network before bringing up keepalived.</p>
<h2 id="waiting-on-networkmanager">Waiting on NetworkManager</h2>
<p>There is a special systemd target, <code>network-online.target</code>, that is not reached until all networking is properly configured. NetworkManager comes with a handy service called <code>NetworkManager-wait-online.service</code> that must be complete before the <code>network-online</code> target can be reached:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#586e75"># rpm -ql NetworkManager | grep network-online</span>
/usr/lib/systemd/system/network-online.target.wants
/usr/lib/systemd/system/network-online.target.wants/NetworkManager-wait-online.service
</code></pre></div><p>Start by ensuring that the <code>NetworkManager-wait-online</code> service starts at boot time:</p>
<pre><code>systemctl enable NetworkManager-wait-online.service
</code></pre><h2 id="using-network-onlinetarget">Using network-online.target</h2>
<p>Next, we tell the keepalived service to wait on <code>network-online.target</code>. Bring up an editor for overriding the <code>keepalived.service</code> unit:</p>
<pre><code>systemctl edit keepalived.service
</code></pre><p>Once the editor appears, add the following text:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ini" data-lang="ini"><span style="color:#719e07">[Unit]</span>
Wants<span style="color:#719e07">=</span><span style="color:#2aa198">network-online.target</span>
After<span style="color:#719e07">=</span><span style="color:#2aa198">network-online.target</span>
</code></pre></div><p>Save the file in the editor and reboot the server. The keepalived service should come up successfully after NetworkManager signals that all of the network devices are online.</p>
<p>Learn more by reading the upstream <a href="https://www.freedesktop.org/wiki/Software/systemd/NetworkTarget/">NetworkTarget</a> documentation.</p>
]]></content></item><item><title>Changes in RHEL 7 Security Technical Implementation Guide Version 1, Release 3</title><link>https://major.io/2017/11/02/changes-in-rhel-7-security-technical-implementation-guide-version-1-release-3/</link><pubDate>Thu, 02 Nov 2017 15:00:25 +0000</pubDate><guid>https://major.io/2017/11/02/changes-in-rhel-7-security-technical-implementation-guide-version-1-release-3/</guid><description>The latest release of the Red Hat Enterprise Linux Security Technical Implementation Guide (STIG) was published last week. This release is Version 1, Release 3, and it contains four main changes:
V-77819 - Multifactor authentication is required for graphical logins V-77821 - Datagram Congestion Control Protocol (DCCP) kernel module must be disabled V-77823 - Single user mode must require user authentication V-77825 - Address space layout randomization (ASLR) must be enabled Deep dive Let&amp;rsquo;s break down this list to understand what each one means.</description><content type="html"><![CDATA[<p><a href="/wp-content/uploads/2017/06/2.jpg"><!-- raw HTML omitted --></a>The latest release of the Red Hat Enterprise Linux Security Technical Implementation Guide (STIG) <a href="https://iase.disa.mil/stigs/os/unix-linux/Pages/index.aspx">was published last week</a>. This release is Version 1, Release 3, and it contains four main changes:</p>
<ul>
<li>V-77819 - Multifactor authentication is required for graphical logins</li>
<li>V-77821 - Datagram Congestion Control Protocol (DCCP) kernel module must be disabled</li>
<li>V-77823 - Single user mode must require user authentication</li>
<li>V-77825 - Address space layout randomization (ASLR) must be enabled</li>
</ul>
<h2 id="deep-dive">Deep dive</h2>
<p>Let&rsquo;s break down this list to understand what each one means.</p>
<h3 id="v-77819---multifactor-authentication-is-required-for-graphical-logins">V-77819 - Multifactor authentication is required for graphical logins</h3>
<p>This requirement improves security for graphical logins and extends the existing requirements for multifactor authentication for logins (see V-71965, V-72417, and V-72427). The STIG recommends smartcards (since the US Government often uses <a href="https://en.wikipedia.org/wiki/Common_Access_Card">CAC cards</a> for multifactor authentication), and this is a good idea for high security systems.</p>
<p>I use <a href="https://www.yubico.com/products/yubikey-hardware/yubikey4/">Yubikey 4&rsquo;s</a> as smartcards in most situations and they work anywhere you have available USB slots.</p>
<h3 id="v-77821---datagram-congestion-control-protocol-dccp-kernel-module-must-be-disabled">V-77821 - Datagram Congestion Control Protocol (DCCP) kernel module must be disabled</h3>
<p><a href="https://en.wikipedia.org/wiki/Datagram_Congestion_Control_Protocol">DCCP</a> is often used as a congestion control mechanism for UDP traffic, but it isn&rsquo;t used that often in modern networks. There have been <a href="https://threatpost.com/impact-of-new-linux-kernel-dccp-vulnerability-limited/123863/">vulnerabilities</a> in the past that are mitigated by disabling DCCP, so it&rsquo;s a good idea to disable it unless you have a strong reason for keeping it enabled.</p>
<p>The ansible-hardening role has been updated to <a href="https://docs.openstack.org/ansible-hardening/latest/rhel7/domains/kernel.html#v-77821">disable the DCCP kernel module by default</a>.</p>
<h3 id="v-77823---single-user-mode-must-require-user-authentication">V-77823 - Single user mode must require user authentication</h3>
<p>Single user mode is often used in emergency situations where the server cannot boot properly or an issue must be repaired without a fully booted server. This mode can only be used at the server&rsquo;s physical console, serial port, or via out-of-band management (DRAC, iLO, and IPMI). Allowing single-user mode access without authentication is a serious security risk.</p>
<p>Fortunately, every distribution supported by the ansible-hardening role already has authentication requirements for single user mode in place. The ansible-hardening role does not make any adjustments to the single user mode unit file since any untested adjustment could cause a system to have problems booting.</p>
<h3 id="v-77825---address-space-layout-randomization-aslr-must-be-enabled">V-77825 - Address space layout randomization (ASLR) must be enabled</h3>
<p><a href="https://en.wikipedia.org/wiki/Address_space_layout_randomization">ASLR</a> is a handy technology that makes it more difficult for attackers to guess where a particular program is storing data in memory. It&rsquo;s not perfect, but it certainly raises the difficulty for an attacker. There are multiple settings for this variable and the <a href="https://www.kernel.org/doc/Documentation/sysctl/kernel.txt">kernel documentation for sysctl</a> has some brief explanations for each setting (search for <code>randomize_va_space</code> on the page).</p>
<p>Every distribution supported by the ansible-hardening role is already setting <code>kernel.randomize_va_space=2</code> by default, which applies randomization for the basic parts of process memory (such as shared libraries and the stack) as well as the heap. The ansible-hardening role will ensure that the default setting is maintained.</p>
<h2 id="ansible-hardening-is-already-up-to-date">ansible-hardening is already up to date</h2>
<p>If you&rsquo;re already using the ansible-hardening role&rsquo;s master branch, these changes are <a href="https://github.com/openstack/ansible-hardening/commit/782bb48c14c03aedaefcaf421fd5935ef5f561b8">already in place</a>! Try out the new updates and <a href="https://bugs.launchpad.net/openstack-ansible/+filebug">open a bug report</a> if you find any problems.</p>
]]></content></item><item><title>Import RPM repository GPG keys from other keyservers temporarily</title><link>https://major.io/2017/09/20/import-rpm-repository-keys-from-other-keyservers-temporarily/</link><pubDate>Wed, 20 Sep 2017 15:24:13 +0000</pubDate><guid>https://major.io/2017/09/20/import-rpm-repository-keys-from-other-keyservers-temporarily/</guid><description>I&amp;rsquo;ve been working through some patches to OpenStack-Ansible lately to optimize how we configure yum repositories in our deployments. During that work, I ran into some issues where pgp.mit.edu was returning 500 errors for some requests to retrieve GPG keys.
Ansible was returning this error:
curl: (22) The requested URL returned error: 502 Proxy Error error: http://pgp.mit.edu:11371/pks/lookup?op=get&amp;amp;search=0x61E8806C: import read failed(2) How does the rpm command know which keyserver to use? Let&amp;rsquo;s use the --showrc argument to show how it is configured:</description><content type="html"><![CDATA[<p><a href="/wp-content/uploads/2017/09/Close-up_of_keys-e1505920978611.jpg"><!-- raw HTML omitted --></a>I&rsquo;ve been working through some patches to <a href="https://github.com/openstack/openstack-ansible">OpenStack-Ansible</a> lately to optimize how we configure yum repositories in our deployments. During that work, I ran into some issues where pgp.mit.edu was returning 500 errors for some requests to retrieve GPG keys.</p>
<p>Ansible was returning this error:</p>
<pre><code>curl: (22) The requested URL returned error: 502 Proxy Error
error: http://pgp.mit.edu:11371/pks/lookup?op=get&amp;search=0x61E8806C: import read failed(2)
</code></pre><p>How does the <code>rpm</code> command know which keyserver to use? Let&rsquo;s use the <code>--showrc</code> argument to show how it is configured:</p>
<pre><code>$ rpm --showrc | grep hkp
-14: _hkp_keyserver http://pgp.mit.edu
-14: _hkp_keyserver_query   %{_hkp_keyserver}:11371/pks/lookup?op=get&amp;search=0x
</code></pre><p>How do we change this value temporarily to test a GPG key retrieval from a different server? There&rsquo;s an argument for that as well: <code>--define</code>:</p>
<pre><code>$ rpm --help | grep define
  -D, --define='MACRO EXPR'        define MACRO with value EXPR
</code></pre><p>We can assemble that on the command line to set a different keyserver temporarily:</p>
<pre><code># rpm -vv --define=&quot;%_hkp_keyserver http://pool.sks-keyservers.net&quot; --import 0x61E8806C
-- SNIP --
D: adding &quot;63deac79abe7ad80e147d671c2ac5bd1c8b3576e&quot; to Sha1header index.
-- SNIP --
</code></pre><p>Let&rsquo;s verify that our new key is in place:</p>
<pre><code># rpm -qa | grep -i gpg-pubkey-61E8806C
gpg-pubkey-61e8806c-5581df56
# rpm -qi gpg-pubkey-61e8806c-5581df56
Name        : gpg-pubkey
Version     : 61e8806c
Release     : 5581df56
Architecture: (none)
Install Date: Wed 20 Sep 2017 10:17:11 AM CDT
Group       : Public Keys
Size        : 0
License     : pubkey
Signature   : (none)
Source RPM  : (none)
Build Date  : Wed 17 Jun 2015 03:57:58 PM CDT
Build Host  : localhost
Relocations : (not relocatable)
Packager    : CentOS Virtualization SIG (http://wiki.centos.org/SpecialInterestGroup/Virtualization) &lt;security@centos.org&gt;
Summary     : gpg(CentOS Virtualization SIG (http://wiki.centos.org/SpecialInterestGroup/Virtualization) &lt;security@centos.org&gt;)
Description :
-----BEGIN PGP PUBLIC KEY BLOCK-----
Version: rpm-4.11.3 (NSS-3)

mQENBFWB31YBCAC4dFmTzBDOcq4R1RbvQXLkyYfF+yXcsMA5kwZy7kjxnFqBoNPv
aAjFm3e5huTw2BMZW0viLGJrHZGnsXsE5iNmzom2UgCtrvcG2f65OFGlC1HZ3ajA
8ZIfdgNQkPpor61xqBCLzIsp55A7YuPNDvatk/+MqGdNv8Ug7iVmhQvI0p1bbaZR
0GuavmC5EZ/+mDlZ2kHIQOUoInHqLJaX7iw46iLRUnvJ1vATOzTnKidoFapjhzIt
i4ZSIRaalyJ4sT+oX4CoRzerNnUtIe2k9Hw6cEu4YKGCO7nnuXjMKz7Nz5GgP2Ou
zIA/fcOmQkSGcn7FoXybWJ8DqBExvkJuDljPABEBAAG0bENlbnRPUyBWaXJ0dWFs
aXphdGlvbiBTSUcgKGh0dHA6Ly93aWtpLmNlbnRvcy5vcmcvU3BlY2lhbEludGVy
ZXN0R3JvdXAvVmlydHVhbGl6YXRpb24pIDxzZWN1cml0eUBjZW50b3Mub3JnPokB
OQQTAQIAIwUCVYHfVgIbAwcLCQgHAwIBBhUIAgkKCwQWAgMBAh4BAheAAAoJEHrr
voJh6IBsRd0H/A62i5CqfftuySOCE95xMxZRw8+voWO84QS9zYvDEnzcEQpNnHyo
FNZTpKOghIDtETWxzpY2ThLixcZOTubT+6hUL1n+cuLDVMu4OVXBPoUkRy56defc
qkWR+UVwQitmlq1ngzwmqVZaB8Hf/mFZiB3B3Jr4dvVgWXRv58jcXFOPb8DdUoAc
S3u/FLvri92lCaXu08p8YSpFOfT5T55kFICeneqETNYS2E3iKLipHFOLh7EWGM5b
Wsr7o0r+KltI4Ehy/TjvNX16fa/t9p5pUs8rKyG8SZndxJCsk0MW55G9HFvQ0FmP
A6vX9WQmbP+ml7jsUxtEJ6MOGJ39jmaUvPc=
=ZzP+
-----END PGP PUBLIC KEY BLOCK-----

</code></pre><p>Success!</p>
<p>If you want to override the value permanently, create a <code>~/.rpmmacros</code> file and add the following line to it:</p>
<pre><code>%_hkp_keyserver http://pool.sks-keyservers.net
</code></pre><p><em>Photo credit: <a href="https://commons.wikimedia.org/wiki/File:Close-up_of_keys.jpg">Wikipedia</a></em></p>
]]></content></item><item><title>Thunderbird changes fonts in some messages, not all</title><link>https://major.io/2017/08/02/thunderbird-changes-fonts-messages-not/</link><pubDate>Wed, 02 Aug 2017 12:54:38 +0000</pubDate><guid>https://major.io/2017/08/02/thunderbird-changes-fonts-messages-not/</guid><description>Thunderbird is a great choice for a mail client on Linux systems if you prefer a GUI, but I had some problems with fonts in the most recent releases. The monospace font used for plain text messages was difficult to read.
I opened Edit &amp;gt; Preferences &amp;gt; Display and clicked Advanced to the right of Fonts &amp;amp; Colors. The default font for monospace text was &amp;ldquo;Monospace&amp;rdquo;, and that one isn&amp;rsquo;t terribly attractive.</description><content type="html"><![CDATA[<p><a href="https://www.mozilla.org/en-US/thunderbird/">Thunderbird</a> is a great choice for a mail client on Linux systems if you prefer a GUI, but I had some problems with fonts in the most recent releases. The monospace font used for plain text messages was difficult to read.</p>
<p>I opened <strong>Edit &gt; Preferences &gt; Display</strong> and clicked <strong>Advanced</strong> to the right of <strong>Fonts &amp; Colors</strong>. The default font for monospace text was &ldquo;Monospace&rdquo;, and that one isn&rsquo;t terribly attractive. I chose &ldquo;DejaVu Sans Mono&rdquo; instead, and closed the dialog boxes.</p>
<p>The fonts in monospace messages didn&rsquo;t change. I quit Thunderbird, opened it again, and still didn&rsquo;t see a change. The strange part is that a small portion of my monospaced messages were opening with the updated font while the majority were not.</p>
<p>I went back into Thunderbird&rsquo;s preferences and took another look:</p>
<p><a href="/wp-content/uploads/2017/08/Screenshot-from-2017-08-02-07-48-18.png"><!-- raw HTML omitted --></a></p>
<p>Everything was set as I expected. I started with some Google searches and stumbled upon a Mozilla Bug: <a href="https://bug1234567.bugzilla.mozilla.org/show_bug.cgi?id=546877">Changing monospace font doesn&rsquo;t affect all messages</a>. One of the participants in the bug mentioned that any emails received without ISO-8859-1 encoding would be unaffected since Thunderbird allows you set fonts for each encoding.</p>
<p>I clicked the dropdown where &ldquo;Latin&rdquo; was selected and I selected &ldquo;Other Writing Systems&rdquo;. After changing the monospace font there, the changes went into effect for all of my monospaced messages!</p>
]]></content></item><item><title>Troubleshooting CyberPower PowerPanel issues in Linux</title><link>https://major.io/2017/07/25/troubleshooting-cyberpower-powerpanel-issues-in-linux/</link><pubDate>Tue, 25 Jul 2017 18:16:11 +0000</pubDate><guid>https://major.io/2017/07/25/troubleshooting-cyberpower-powerpanel-issues-in-linux/</guid><description>I have a CyberPower BRG1350AVRLCD at home and I&amp;rsquo;ve just connected it to a new device. However, the pwrstat command doesn&amp;rsquo;t retrieve any useful data on the new system:
# pwrstat -status The UPS information shows as following: Current UPS status: State........................ Normal Power Supply by.............. Utility Power Last Power Event............. None I disconnected the USB cable and ran pwrstat again. Same output. I disconnected power from the UPS itself and ran pwrstat again.</description><content type="html"><![CDATA[<p><img src="/wp-content/uploads/2017/07/1024px-Sierra_Blanca_and_electricity_pole-e1501006440664.jpg" alt="1"></p>
<p>I have a <a href="https://www.cyberpowersystems.com/product/ups/brg1350avrlcd/">CyberPower BRG1350AVRLCD</a> at home and I&rsquo;ve just connected it to a new device. However, the <code>pwrstat</code> command doesn&rsquo;t retrieve any useful data on the new system:</p>
<pre><code># pwrstat -status

The UPS information shows as following:


    Current UPS status:
        State........................ Normal
        Power Supply by.............. Utility Power
        Last Power Event............. None
</code></pre><p>I disconnected the USB cable and ran <code>pwrstat</code> again. <strong>Same output.</strong> I disconnected power from the UPS itself and ran <code>pwrstat</code> again. <strong>Same output.</strong> This can&rsquo;t be right.</p>
<h2 id="checking-the-basics">Checking the basics</h2>
<p>A quick look at <code>dmesg</code> output shows that the UPS is connected and the kernel recognizes it:</p>
<pre><code>[   65.661489] usb 3-1: new full-speed USB device number 7 using xhci_hcd
[   65.830769] usb 3-1: New USB device found, idVendor=0764, idProduct=0501
[   65.830771] usb 3-1: New USB device strings: Mfr=3, Product=1, SerialNumber=2
[   65.830772] usb 3-1: Product: BRG1350AVRLCD
[   65.830773] usb 3-1: Manufacturer: CPS
[   65.830773] usb 3-1: SerialNumber: xxxxxxxxx
[   65.837801] hid-generic 0003:0764:0501.0004: hiddev0,hidraw0: USB HID v1.10 Device [CPS BRG1350AVRLCD] on usb-0000:00:14.0-1/input0
</code></pre><p>I checked the <code>/var/log/pwrstatd.log</code> file to see if there were any errors:</p>
<pre><code>2017/07/25 12:01:17 PM  Daemon startups.
2017/07/25 12:01:24 PM  Communication is established.
2017/07/25 12:01:27 PM  Low Battery capacity is restored.
2017/07/25 12:05:19 PM  Daemon stops its service.
2017/07/25 12:05:19 PM  Daemon startups.
2017/07/25 12:05:19 PM  Communication is established.
2017/07/25 12:05:22 PM  Low Battery capacity is restored.
2017/07/25 12:06:27 PM  Daemon stops its service.
</code></pre><p>The <code>pwrstatd</code> daemon can see the device and communicate with it. This is unusual.</p>
<h2 id="digging-into-the-daemon">Digging into the daemon</h2>
<p>If the daemon can truly see the UPS, then what is it talking to? I used <code>lsof</code> to examine what the <code>pwrstatd</code> daemon is doing:</p>
<pre><code># lsof -p 3975
COMMAND   PID USER   FD   TYPE             DEVICE SIZE/OFF      NODE NAME
pwrstatd 3975 root  cwd    DIR               8,68      224        96 /
pwrstatd 3975 root  rtd    DIR               8,68      224        96 /
pwrstatd 3975 root  txt    REG               8,68   224175 134439879 /usr/sbin/pwrstatd
pwrstatd 3975 root  mem    REG               8,68  2163104 134218946 /usr/lib64/libc-2.25.so
pwrstatd 3975 root  mem    REG               8,68  1226368 134218952 /usr/lib64/libm-2.25.so
pwrstatd 3975 root  mem    REG               8,68    19496 134218950 /usr/lib64/libdl-2.25.so
pwrstatd 3975 root  mem    REG               8,68   187552 134218939 /usr/lib64/ld-2.25.so
pwrstatd 3975 root    0r   CHR                1,3      0t0      1028 /dev/null
pwrstatd 3975 root    1u  unix 0xffff9e395e137400      0t0     37320 type=STREAM
pwrstatd 3975 root    2u  unix 0xffff9e395e137400      0t0     37320 type=STREAM
pwrstatd 3975 root    3u  unix 0xffff9e392f0c0c00      0t0     39485 /var/pwrstatd.ipc type=STREAM
pwrstatd 3975 root    4u   CHR             180,96      0t0     50282 /dev/ttyS1
</code></pre><p><strong>Wait a minute.</strong> The last line of the <code>lsof</code> output shows that <code>pwrstatd</code> is talking to <code>/dev/ttyS1</code>, but the device is supposed to be a <code>hiddev</code> device over USB. If you remember, we had this line in <code>dmesg</code> when the UPS was plugged in:</p>
<pre><code>hid-generic 0003:0764:0501.0004: hiddev0,hidraw0: USB HID v1.10 Device [CPS BRG1350AVRLCD] on usb-0000:00:14.0-1/input0
</code></pre><p>Things are beginning to make more sense now. I have a USB-to-serial device that allows my server to talk to the console port on my Cisco switch:</p>
<pre><code>[   80.389533] usb 3-1: new full-speed USB device number 9 using xhci_hcd
[   80.558025] usb 3-1: New USB device found, idVendor=067b, idProduct=2303
[   80.558027] usb 3-1: New USB device strings: Mfr=1, Product=2, SerialNumber=0
[   80.558028] usb 3-1: Product: USB-Serial Controller D
[   80.558029] usb 3-1: Manufacturer: Prolific Technology Inc.
[   80.558308] pl2303 3-1:1.0: pl2303 converter detected
[   80.559937] usb 3-1: pl2303 converter now attached to ttyUSB0
</code></pre><p>It appears that <code>pwrstatd</code> is trying to talk to my Cisco switch (through the USB-to-serial adapter) rather than my UPS! I&rsquo;m sure they could have a great conversation together, but it&rsquo;s hardly productive.</p>
<h2 id="fixing-it">Fixing it</h2>
<p>The <code>/etc/pwrstatd.conf</code> has a relevant section:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ini" data-lang="ini"><span style="color:#586e75"># The pwrstatd accepts four types of device node which includes the &#39;ttyS&#39;,</span>
<span style="color:#586e75"># &#39;ttyUSB&#39;, &#39;hiddev&#39;, and &#39;libusb&#39; for communication with UPS. The pwrstatd</span>
<span style="color:#586e75"># defaults to enumerate all acceptable device nodes and pick up to use an</span>
<span style="color:#586e75"># available device node automatically. But this may cause a disturbance to the</span>
<span style="color:#586e75"># device node which is occupied by other software. Therefore, you can restrict</span>
<span style="color:#586e75"># this enumerate behave by using allowed-device-nodes option. You can assign</span>
<span style="color:#586e75"># the single device node path or multiple device node paths divided by a</span>
<span style="color:#586e75"># semicolon at this option. All groups of &#39;ttyS&#39;, &#39;ttyUSB&#39;, &#39;hiddev&#39;, or</span>
<span style="color:#586e75"># &#39;libusb&#39; device node are enumerated without a suffix number assignment.</span>
<span style="color:#586e75"># Note, the &#39;libusb&#39; does not support suffix number only.</span>
<span style="color:#586e75">#</span>
<span style="color:#586e75"># For example: restrict to use ttyS1, ttyS2 and hiddev1 device nodes at /dev</span>
<span style="color:#586e75"># path only.</span>
<span style="color:#586e75"># allowed-device-nodes = /dev/ttyS1;/dev/ttyS2;/dev/hiddev1</span>
<span style="color:#586e75">#</span>
<span style="color:#586e75"># For example: restrict to use ttyS and ttyUSB two groups of device node at</span>
<span style="color:#586e75"># /dev,/dev/usb, and /dev/usb/hid paths(includes ttyS0 to ttySN and ttyUSB0 to</span>
<span style="color:#586e75"># ttyUSBN, N is number).</span>
<span style="color:#586e75"># allowed-device-nodes = ttyS;ttyUSB</span>
<span style="color:#586e75">#</span>
<span style="color:#586e75"># For example: restrict to use hiddev group of device node at /dev,/dev/usb,</span>
<span style="color:#586e75"># and /dev/usb/hid paths(includes hiddev0 to hiddevN, N is number).</span>
<span style="color:#586e75"># allowed-device-nodes = hiddev</span>
<span style="color:#586e75">#</span>
<span style="color:#586e75"># For example: restrict to use libusb device.</span>
<span style="color:#586e75"># allowed-device-nodes = libusb</span>
allowed-device-nodes <span style="color:#719e07">=</span>
</code></pre></div><p>We need to explicitly tell <code>pwrstatd</code> to talk to the UPS on <code>/dev/hid/hiddev0</code>:</p>
<pre><code>allowed-device-nodes = /dev/usb/hiddev0
</code></pre><p>Let&rsquo;s restart the <code>pwrstatd</code> daemon and see what we get:</p>
<pre><code># systemctl restart pwrstatd
# pwrstat -status

The UPS information shows as following:

    Properties:
        Model Name................... BRG1350AVRLCD
        Firmware Number..............
        Rating Voltage............... 120 V
        Rating Power................. 810 Watt(1350 VA)

    Current UPS status:
        State........................ Normal
        Power Supply by.............. Utility Power
        Utility Voltage.............. 121 V
        Output Voltage............... 121 V
        Battery Capacity............. 100 %
        Remaining Runtime............ 133 min.
        Load......................... 72 Watt(9 %)
        Line Interaction............. None
        Test Result.................. Unknown
        Last Power Event............. None
</code></pre><p>Success!</p>
<p><em>Photo credit: <a href="https://commons.wikimedia.org/wiki/File%3ASierra_Blanca_and_electricity_pole.jpg">Wikipedia</a></em></p>
]]></content></item><item><title>Apply the STIG to even more operating systems with ansible-hardening</title><link>https://major.io/2017/07/21/apply-stig-operating-systems-ansible-hardening/</link><pubDate>Fri, 21 Jul 2017 17:38:46 +0000</pubDate><guid>https://major.io/2017/07/21/apply-stig-operating-systems-ansible-hardening/</guid><description>Tons of improvements made their way into the ansible-hardening role in preparation for the OpenStack Pike release next month. The role has a new name, new documentation and extra tests.
The role uses the Security Technical Implementation Guide (STIG) produced by the Defense Information Systems Agency (DISA) and applies the guidelines to Linux hosts using Ansible. Every control is configurable via simple Ansible variables and each control is thoroughly documented.</description><content type="html"><![CDATA[<p><img src="/wp-content/uploads/2017/07/1024px-Samuils_Fortress_and_Ohrid_Lake.jpg" alt="1"></p>
<p>Tons of improvements made their way into the <a href="https://github.com/openstack/ansible-hardening">ansible-hardening</a> role in preparation for the OpenStack Pike release <a href="https://releases.openstack.org/pike/schedule.html">next month</a>. The role has a <a href="/2017/06/27/old-role-new-name-ansible-hardening/">new name</a>, new <a href="https://docs.openstack.org/ansible-hardening/latest/">documentation</a> and extra tests.</p>
<p>The role uses the Security Technical Implementation Guide (STIG) produced by the Defense Information Systems Agency (DISA) and applies the guidelines to Linux hosts using Ansible. Every control is configurable via simple Ansible variables and each control is thoroughly documented.</p>
<p>These controls are now applied to an even wider variety of Linux distributions:</p>
<ul>
<li>CentOS 7</li>
<li>Debian 8 Jessie <em>(new for Pike)</em></li>
<li>Fedora 25 <em>(new for Pike)</em></li>
<li>openSUSE Leap 42.2+ <em>(new for Pike)</em></li>
<li>Red Hat Enterprise Linux 7</li>
<li>SUSE Linux Enterprise 12 <em>(new for Pike)</em></li>
<li>Ubuntu 14.04 Trusty</li>
<li>Ubuntu 16.04 Xenial</li>
</ul>
<p>Any patches to the ansible-hardening role are tested against all of these operating systems (except RHEL 7 and SUSE Linux Enterprise). Support for openSUSE testing <a href="https://github.com/openstack-infra/project-config/commit/0795a7414ca8f06931877919d7ecb0b2d4e5f6e0">landed this week</a>.</p>
<p><strong>Work is underway to put the finishing touches on the master branch before the Pike release and we need your help!</strong></p>
<p>If you have any of these operating systems deployed, please test the role on your systems! This is pre-release software, so it&rsquo;s best to apply it only to a new server. Read the <a href="https://docs.openstack.org/ansible-hardening/latest/getting-started.html">&ldquo;Getting Started&rdquo;</a> documentation to get started with <code>ansible-galaxy</code> or <code>git</code>.</p>
<p><em>Photo credit: <a href="https://commons.wikimedia.org/wiki/File%3ASamuil's_Fortress_and_Ohrid_Lake.JPG">Wikipedia</a></em></p>
]]></content></item><item><title>Customize LDAP autocompletion format in Thunderbird</title><link>https://major.io/2017/07/18/customize-ldap-autocompletion-format-in-thunderbird/</link><pubDate>Tue, 18 Jul 2017 18:08:42 +0000</pubDate><guid>https://major.io/2017/07/18/customize-ldap-autocompletion-format-in-thunderbird/</guid><description>Thunderbird can connect to an LDAP server and autocomplete email addresses as you type, but it needs some adjustment for some LDAP servers. One of the LDAP servers that I use regularly returns email addresses like this in the thunderbird interface:
username &amp;lt;firstname.lastname@domain.tld&amp;gt; The email address looks fine, but I&amp;rsquo;d much rather have the person&amp;rsquo;s full name instead of the username. Here&amp;rsquo;s what I&amp;rsquo;m looking for:
Firstname Lastname &amp;lt;firstname.lastname@domain.tld&amp;gt; In older Thunderbird versions, setting ldap_2.</description><content type="html"><![CDATA[<p><img src="/wp-content/uploads/2017/07/1280px-Mailbox_USA-e1500401199427.jpg" alt="1"></p>
<p>Thunderbird can connect to an LDAP server and autocomplete email addresses as you type, but it needs some adjustment for some LDAP servers. One of the LDAP servers that I use regularly returns email addresses like this in the thunderbird interface:</p>
<pre><code>username &lt;firstname.lastname@domain.tld&gt;
</code></pre><p>The email address looks fine, but I&rsquo;d much rather have the person&rsquo;s full name instead of the username. Here&rsquo;s what I&rsquo;m looking for:</p>
<pre><code>Firstname Lastname &lt;firstname.lastname@domain.tld&gt;
</code></pre><p>In older Thunderbird versions, setting <code>ldap_2.servers.SERVER_NAME.autoComplete.nameFormat</code> to <code>displayName</code> was enough. However, this option isn&rsquo;t used in recent versions of Thunderbird.</p>
<h2 id="digging-in">Digging in</h2>
<p>After a fair amount of searching the Thunderbird source code with <code>awk</code>, I found a mention of <code>DisplayName</code> in <code>nsAbLDAPAutoCompleteSearch.js</code> that looked promising:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-javascript" data-lang="javascript"><span style="color:#586e75">// Create a minimal map just for the display name and primary email.
</span><span style="color:#586e75"></span>      <span style="color:#719e07">this</span>._attributes <span style="color:#719e07">=</span>
        Components.classes[<span style="color:#2aa198">&#34;@mozilla.org/addressbook/ldap-attribute-map;1&#34;</span>]
                  .createInstance(Components.interfaces.nsIAbLDAPAttributeMap);
      <span style="color:#719e07">this</span>._attributes.setAttributeList(<span style="color:#2aa198">&#34;DisplayName&#34;</span>,
        <span style="color:#719e07">this</span>._book.attributeMap.getAttributeList(<span style="color:#2aa198">&#34;DisplayName&#34;</span>, {}), <span style="color:#cb4b16">true</span>);
      <span style="color:#719e07">this</span>._attributes.setAttributeList(<span style="color:#2aa198">&#34;PrimaryEmail&#34;</span>,
        <span style="color:#719e07">this</span>._book.attributeMap.getAttributeList(<span style="color:#2aa198">&#34;PrimaryEmail&#34;</span>, {}), <span style="color:#cb4b16">true</span>);
    }
</code></pre></div><p>Something is unusual here. The LDAP field is called <code>displayName</code>, but this attribute is called <code>DisplayName</code> (note the capitalization of the <em>D</em>). Just before that line, I see a lookup in an attributes map of some sort. There may be a configuration option that is called <code>DisplayName</code>.</p>
<p>In Thunderbird, I selected <strong>Edit &gt; Preferences</strong>. I clicked the <strong>Advanced</strong> tab and then <strong>Config Editor</strong>. A quick search for <em>DisplayName</em> revealed an interesting configuration option:</p>
<pre><code>ldap_2.servers.default.attrmap.DisplayName: cn,commonname
</code></pre><h2 id="fixing-it">Fixing it</h2>
<p>That&rsquo;s the problem! This needs to map to <code>displayName</code> in my case, and not <code>cn,commonname</code> (which returns a user&rsquo;s username). There are two different ways to fix this:</p>
<pre><code># Change it for just one LDAP server
ldap_2.servers.SERVER_NAME.attrmap.DisplayName: displayName
# Change it for all LDAP servers by default (careful)
ldap_2.servers.default.attrmap.DisplayName: displayName
</code></pre><p>After making the change, quit Thunderbird and relaunch it. Compose a new email and start typing in the email address field. The user&rsquo;s first and last name should appear!</p>
]]></content></item><item><title>Old role, new name: ansible-hardening</title><link>https://major.io/2017/06/27/old-role-new-name-ansible-hardening/</link><pubDate>Tue, 27 Jun 2017 20:49:44 +0000</pubDate><guid>https://major.io/2017/06/27/old-role-new-name-ansible-hardening/</guid><description>The interest in the openstack-ansible-security role has taken off faster than I expected, and one piece of constant feedback I received was around the name of the role. Some users were unsure if they needed to use the role in an OpenStack cloud or if the OpenStack-Ansible project was required.
The role works everywhere - OpenStack cloud or not. I started a mailing list thread on the topic and we eventually settled on a new name: ansible-hardening!</description><content type="html"><![CDATA[<p><img src="/wp-content/uploads/2017/06/2.jpg" alt="2"></p>
<p>The interest in the <a href="https://github.com/openstack/openstack-ansible-security">openstack-ansible-security</a> role has taken off faster than I expected, and one piece of constant feedback I received was around the name of the role. Some users were unsure if they needed to use the role in an OpenStack cloud or if the OpenStack-Ansible project was required.</p>
<p>The role works everywhere - OpenStack cloud or not. I started a <a href="http://lists.openstack.org/pipermail/openstack-dev/2017-May/116922.html">mailing list thread</a> on the topic and we eventually settled on a new name: <a href="https://github.com/openstack/ansible-hardening">ansible-hardening</a>! The updated documentation is <a href="https://docs.openstack.org/developer/ansible-hardening/">already available</a>.</p>
<p>The old openstack-ansible-security role is being retired and it will not receive any additional updates. Moving to the new role is easy:</p>
<ol>
<li>Install <em>ansible-hardening</em> with <code>ansible-galaxy</code> (or <code>git clone</code>)</li>
<li>Change your playbooks to use the ansible-hardening role</li>
</ol>
<p>There&rsquo;s no need to change any variable names or tags - they are all kept the same in the new role.</p>
<p>As always, if you have questions or comments about the role, drop by <code>#openstack-ansible</code> on Freenode IRC or <a href="https://bugs.launchpad.net/openstack-ansible/+filebug">open a bug in Launchpad</a>.</p>
]]></content></item><item><title>Enable AppArmor on a Debian Jessie cloud image</title><link>https://major.io/2017/05/24/enable-apparmor-on-a-debian-jessie-cloud-image/</link><pubDate>Wed, 24 May 2017 16:14:03 +0000</pubDate><guid>https://major.io/2017/05/24/enable-apparmor-on-a-debian-jessie-cloud-image/</guid><description>I merged some initial Debian support into the openstack-ansible-security role and ran into an issue enabling AppArmor. The apparmor service failed to start and I found this output in the system journal:
kernel: AppArmor: AppArmor disabled by boot time parameter Digging in That was unexpected. I was using the Debian jessie cloud image and it uses extlinux as the bootloader. The file didn&amp;rsquo;t reference AppArmor at all:
# cat /boot/extlinux/extlinux.conf default linux timeout 1 label linux kernel boot/vmlinuz-3.</description><content type="html"><![CDATA[<p><img src="/wp-content/uploads/2017/05/MaxPixel.freegreatpicture.com-Knights-Glove-Middle-Ages-Knight-Historically-Armor-2010659-e1495641903942.jpg" alt="1"></p>
<p>I merged some <a href="https://github.com/openstack/openstack-ansible-security/commit/4e9a8a1d6ab556628555063402dd5f491814b9db">initial Debian support</a> into the openstack-ansible-security role and ran into an issue enabling AppArmor. The <code>apparmor</code> service failed to start and I found this output in the system journal:</p>
<pre><code>kernel: AppArmor: AppArmor disabled by boot time parameter
</code></pre><h2 id="digging-in">Digging in</h2>
<p>That was unexpected. I was using the <a href="https://cdimage.debian.org/cdimage/openstack/">Debian jessie cloud image</a> and it uses extlinux as the bootloader. The file didn&rsquo;t reference AppArmor at all:</p>
<pre><code># cat /boot/extlinux/extlinux.conf
default linux
timeout 1
label linux
kernel boot/vmlinuz-3.16.0-4-amd64
append initrd=boot/initrd.img-3.16.0-4-amd64 root=/dev/vda1 console=tty0 console=ttyS0,115200 ro quiet
</code></pre><p>I <a href="https://wiki.debian.org/AppArmor/HowToUse#Enable_AppArmor">learned</a> that AppArmor is <strong>disabled by default</strong> in Debian unless you <strong>explicitly enable it</strong>. In contrast, SELinux is enabled unless you turn it off. To make matters worse, Debian&rsquo;s cloud image doesn&rsquo;t have any facilities or scripts to automatically update the extlinux configuration file when new kernels are installed.</p>
<h2 id="making-a-repeatable-fix">Making a repeatable fix</h2>
<p>My two goals here were to:</p>
<ol>
<li>Ensure AppArmor is enabled on the next boot</li>
<li>Ensure that AppArmor remains enabled when new kernels are installed</li>
</ol>
<p>The first step is to install grub2:</p>
<pre><code>apt-get -y install grub2
</code></pre><p>During the installation, a package configuration window will appear that asks about where grub should be installed. I selected <code>/dev/vda</code> from the list and waited for apt to finish the package installation.</p>
<p>The next step is to edit <code>/etc/default/grub</code> and add in the AppArmor configuration. Adjust the <code>GRUB_CMDLINE_LINUX_DEFAULT</code> line to look like the one below:</p>
<pre><code>GRUB_DEFAULT=0
GRUB_TIMEOUT=5
GRUB_DISTRIBUTOR=`lsb_release -i -s 2&gt; /dev/null || echo Debian`
GRUB_CMDLINE_LINUX_DEFAULT=&quot;quiet apparmor=1 security=apparmor&quot;
GRUB_CMDLINE_LINUX=&quot;&quot;
</code></pre><p>Ensure that the required AppArmor packages are installed:</p>
<pre><code>apt-get -y install apparmor apparmor-profiles apparmor-utils
</code></pre><p>Enable the AppArmor service upon reboot:</p>
<pre><code>systemctl enable apparmor
</code></pre><p>Run <code>update-grub</code> and reboot. After the reboot, run <code>apparmor_status</code> and you should see lots of AppArmor profiles loaded:</p>
<pre><code># apparmor_status
apparmor module is loaded.
38 profiles are loaded.
3 profiles are in enforce mode.
   /usr/lib/chromium-browser/chromium-browser//browser_java
   /usr/lib/chromium-browser/chromium-browser//browser_openjdk
   /usr/lib/chromium-browser/chromium-browser//sanitized_helper
35 profiles are in complain mode.
   /sbin/klogd
   /sbin/syslog-ng
   /sbin/syslogd
   /usr/lib/chromium-browser/chromium-browser
   /usr/lib/chromium-browser/chromium-browser//chromium_browser_sandbox
   /usr/lib/chromium-browser/chromium-browser//lsb_release
   /usr/lib/chromium-browser/chromium-browser//xdgsettings
   /usr/lib/dovecot/anvil
   /usr/lib/dovecot/auth
   /usr/lib/dovecot/config
   /usr/lib/dovecot/deliver
   /usr/lib/dovecot/dict
   /usr/lib/dovecot/dovecot-auth
   /usr/lib/dovecot/dovecot-lda
   /usr/lib/dovecot/imap
   /usr/lib/dovecot/imap-login
   /usr/lib/dovecot/lmtp
   /usr/lib/dovecot/log
   /usr/lib/dovecot/managesieve
   /usr/lib/dovecot/managesieve-login
   /usr/lib/dovecot/pop3
   /usr/lib/dovecot/pop3-login
   /usr/lib/dovecot/ssl-params
   /usr/sbin/avahi-daemon
   /usr/sbin/dnsmasq
   /usr/sbin/dovecot
   /usr/sbin/identd
   /usr/sbin/mdnsd
   /usr/sbin/nmbd
   /usr/sbin/nscd
   /usr/sbin/smbd
   /usr/sbin/smbldap-useradd
   /usr/sbin/smbldap-useradd///etc/init.d/nscd
   /usr/{sbin/traceroute,bin/traceroute.db}
   /{usr/,}bin/ping
0 processes have profiles defined.
0 processes are in enforce mode.
0 processes are in complain mode.
0 processes are unconfined but have a profile defined.
</code></pre><h2 id="final-thoughts">Final thoughts</h2>
<p>I&rsquo;m still unsure about why AppArmor is disabled by default. There aren&rsquo;t that many profiles shipped by default (38 on my freshly installed jessie system versus 417 SELinux policies in Fedora 25) and many of them affect services that wouldn&rsquo;t cause significant disruptions on the system.</p>
<p>There is a <a href="https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=702030">discussion that ended last year</a> around how to automate the AppArmor enablement process when the AppArmor packages are installed. This would be a great first step to make the process easier, but it would probably make more sense to take the step of enabling it by default.</p>
<p><em>Photo credit: <a href="http://maxpixel.freegreatpicture.com/Knights-Glove-Middle-Ages-Knight-Historically-Armor-2010659">Max Pixel</a></em></p>
]]></content></item><item><title>Fixing OpenStack noVNC consoles that ignore keyboard input</title><link>https://major.io/2017/05/18/fixing-openstack-novnc-consoles-that-ignore-keyboard-input/</link><pubDate>Thu, 18 May 2017 16:58:56 +0000</pubDate><guid>https://major.io/2017/05/18/fixing-openstack-novnc-consoles-that-ignore-keyboard-input/</guid><description>I opened up a noVNC console to a virtual machine today in my OpenStack cloud but found that the console wouldn&amp;rsquo;t take keyboard input. The Send Ctrl-Alt-Del button in the top right of the window worked just fine, but I couldn&amp;rsquo;t type anywhere in the console. This happened on an Ocata OpenStack cloud deployed with OpenStack-Ansible on CentOS 7.
Test the network path The network path to the console is a little deep for this deployment, but here&amp;rsquo;s a quick explanation:</description><content type="html"><![CDATA[<p><img src="/wp-content/uploads/2017/05/Televideo925Terminal-e1495126632469.jpg" alt="1"></p>
<p>I opened up a noVNC console to a virtual machine today in my OpenStack cloud but found that the console wouldn&rsquo;t take keyboard input. The <strong>Send Ctrl-Alt-Del</strong> button in the top right of the window worked just fine, but I couldn&rsquo;t type anywhere in the console. This happened on an Ocata OpenStack cloud deployed with <a href="https://github.com/openstack/openstack-ansible">OpenStack-Ansible</a> on CentOS 7.</p>
<h2 id="test-the-network-path">Test the network path</h2>
<p>The network path to the console is a little deep for this deployment, but here&rsquo;s a quick explanation:</p>
<ul>
<li>My laptop connects to HAProxy</li>
<li>HAProxy sends the traffic to the nova-novncproxy service</li>
<li>nova-novncproxy connects to the correct VNC port on the right hypervisor</li>
</ul>
<p>If all of that works, I get a working console! I knew the network path was set up correctly because I could see the console in my browser.</p>
<p>My next troubleshooting step was to dump network traffic with <code>tcpdump</code> on the hypervisor itself. I dumped the traffic on port 5900 (which was the VNC port for this particular instance) and watched the output. Whenever I wiggled the mouse over the noVNC console in my browser, I saw a flurry of network traffic. The same thing happened if I punched lots of keys on the keyboard. At this point, it was clear that the keyboard input was making it to the hypervisor, but it wasn&rsquo;t being handled correctly.</p>
<h2 id="test-the-console">Test the console</h2>
<p>Next, I opened up <code>virt-manager</code>, connected to the hypervisor, and opened a connection to the instance. The keyboard input worked fine there. I opened up <code>remmina</code> and connected via plain old VNC. The keyboard input worked fine there, too!</p>
<h2 id="investigate-in-the-virtual-machine">Investigate in the virtual machine</h2>
<p>The system journal in the virtual machine had some interesting output:</p>
<pre><code>kernel: atkbd serio0: Unknown key released (translated set 2, code 0x0 on isa0060/serio0).
kernel: atkbd serio0: Use 'setkeycodes 00 &lt;keycode&gt;' to make it known.
kernel: atkbd serio0: Unknown key released (translated set 2, code 0x0 on isa0060/serio0).
kernel: atkbd serio0: Use 'setkeycodes 00 &lt;keycode&gt;' to make it known.
kernel: atkbd serio0: Unknown key pressed (translated set 2, code 0x0 on isa0060/serio0).
kernel: atkbd serio0: Use 'setkeycodes 00 &lt;keycode&gt;' to make it known.
kernel: atkbd serio0: Unknown key pressed (translated set 2, code 0x0 on isa0060/serio0).
kernel: atkbd serio0: Use 'setkeycodes 00 &lt;keycode&gt;' to make it known.
kernel: atkbd serio0: Unknown key released (translated set 2, code 0x0 on isa0060/serio0).
kernel: atkbd serio0: Use 'setkeycodes 00 &lt;keycode&gt;' to make it known.
kernel: atkbd serio0: Unknown key released (translated set 2, code 0x0 on isa0060/serio0).
kernel: atkbd serio0: Use 'setkeycodes 00 &lt;keycode&gt;' to make it known.
</code></pre><p>It seems like my keyboard input was being lost in translation - literally. I have a US layout keyboard (Thinkpad X1 Carbon) and the virtual machine was configured with the <code>en-us</code> keymap:</p>
<pre><code># virsh dumpxml 4 | grep vnc
    &lt;graphics type='vnc' port='5900' autoport='yes' listen='192.168.250.41' keymap='en-us'&gt;
</code></pre><p>A thorough Googling session revealed that it is <a href="https://github.com/novnc/noVNC/issues/666#issuecomment-248303186">not recommended to set a keymap for virtual machines</a> in libvirt in most situations. I set the <code>nova_console_keymap</code> variable in <code>/etc/openstack_deploy/user_variables.yml</code> to an empty string:</p>
<pre><code>nova_console_keymap: ''
</code></pre><p>I redeployed the nova service using the OpenStack-Ansible playbooks:</p>
<pre><code>openstack-ansible os-nova-install.yml
</code></pre><p>Once that was done, I powered off the virtual machine and powered it back on. (This is needed to ensure that the libvirt changes go into effect for the virtual machine.)</p>
<p><strong>Great success!</strong> The keyboard was working in the noVNC console once again!</p>
<p><em>Photo credit: <a href="https://commons.wikimedia.org/wiki/File:Televideo925Terminal.jpg">Wikipedia</a></em></p>
]]></content></item><item><title>OpenStack Summit Boston 2017 Recap</title><link>https://major.io/2017/05/11/openstack-summit-boston-2017-recap/</link><pubDate>Fri, 12 May 2017 00:25:55 +0000</pubDate><guid>https://major.io/2017/05/11/openstack-summit-boston-2017-recap/</guid><description>The OpenStack Summit wrapped up today in Boston and it was a great week! There were plenty of informative breakouts and some interesting keynotes.
Keynotes Beth Cohen shared some of the work that Verizon has done with software-defined WAN on customer-premises equipment (CPE). She showed a demo of how customers could easily provision virtual network hardware, such as firewalls or intrusion detection systems, without waiting for hardware or cabling changes. I&amp;rsquo;m less familiar with the world of telcos, so I found this really interesting.</description><content type="html"><![CDATA[<p><img src="/wp-content/uploads/2017/05/1280px-Bunker_Hill_Monument_by_night_dfv-e1494544221347.jpg" alt="1"></p>
<p>The OpenStack Summit wrapped up today in Boston and it was a great week! There were plenty of informative breakouts and some interesting keynotes.</p>
<h2 id="keynotes">Keynotes</h2>
<p>Beth Cohen <a href="https://www.openstack.org/videos/boston-2017/taking-openstack-out-to-the-network-edges">shared some of the work that Verizon has done</a> with software-defined WAN on customer-premises equipment (CPE). She showed a demo of how customers could easily provision virtual network hardware, such as firewalls or intrusion detection systems, without waiting for hardware or cabling changes. I&rsquo;m less familiar with the world of telcos, so I found this really interesting.</p>
<p>Daniela Rus <a href="https://www.openstack.org/videos/boston-2017/democratizing-robotics-origami-inspired-printed-robots">gave an amazing keynote</a> about the democratization of robotics. She showed videos of tiny robots doing some amazing things, including robots which could be swallowed. Those robots could help children who swallow dangerous things (like batteries) without painful surgery.</p>
<p>The big surprise on the second day was the <a href="https://www.openstack.org/videos/boston-2017/mark-collier-q-and-a-with-edward-snowden">Q&amp;A with Edward Snowden</a>. At first, I was skeptical about it being a publicity stunt, but it turned out to be a really good conversation about the value of open source.</p>
<p>My <a href="https://www.openstack.org/videos/boston-2017/ge-healthcare-accelerates-to-cloud-on-openstack-platform">favorite keynote</a> was from Patrick Weeks of GE. He talked about their IT transformation goals and how they selected OpenStack to solve them. They chose a solution from Rackspace and their engineers love it!</p>
<h2 id="breakouts">Breakouts</h2>
<p>Here are some links to my favorite breakouts:</p>
<ul>
<li>Adam Young&rsquo;s <a href="https://www.openstack.org/videos/boston-2017/per-api-role-based-access-control">&ldquo;Per API Role Based Access Control&rdquo;</a></li>
<li>Ansible&rsquo;s <a href="https://www.openstack.org/summit/boston-2017/summit-schedule/events/18900/ansible-ops-bof-management-upgrades-and-operations">BoF about Management, Upgrades and Operations</a></li>
<li>Forum: <a href="https://www.openstack.org/summit/boston-2017/summit-schedule/events/18774/compliancesecurity-certification-for-upstream-openstack">Compliance/Security Certification for Upstream Openstack</a></li>
</ul>
<h2 id="openstack-ansible">OpenStack-Ansible</h2>
<p>Although I couldn&rsquo;t make it to all of the OpenStack-Ansible sessions, we had a great turnout for the ones I attended! Every seat was taken during the developer onboarding session and we had some helpful comments from new contributors.</p>
<p><img src="/wp-content/uploads/2017/05/IMG_20170510_092327-e1494547255505.jpg" alt="16">
Andy McCrae leads the OpenStack-Ansible onboarding session</p>
<h2 id="my-talks">My talks</h2>
<p>The week was a long one for me! I shared two full-length talks, helped with a lightning talk, and joined a panel. Here are some quick links to the videos and slides:</p>
<ul>
<li>Grow Your Community: Inspire an Impostor
<ul>
<li><a href="https://www.openstack.org/videos/boston-2017/grow-your-community-inspire-an-impostor">Video</a></li>
<li><a href="https://www.slideshare.net/MajorHayden/grow-your-community-inspire-an-impostor">Slides</a></li>
</ul>
</li>
<li>Securing OpenStack Cloud and Beyond with Ansible
<ul>
<li><a href="https://www.openstack.org/videos/boston-2017/securing-openstack-clouds-and-beyond-with-ansible">Video</a></li>
<li><a href="https://www.slideshare.net/MajorHayden/securing-openstack-and-beyond-with-ansible">Slides</a></li>
</ul>
</li>
<li>The Open Open Open Open Cloud
<ul>
<li><a href="https://www.openstack.org/videos/boston-2017/the-open-open-open-cloud">Video</a></li>
</ul>
</li>
<li>OpenStack Security Team Update
<ul>
<li><a href="https://www.openstack.org/videos/boston-2017/openstack-security-team-update">Video</a></li>
</ul>
</li>
</ul>
<p><em>Photo credit: <a href="https://commons.wikimedia.org/wiki/File:Bunker_Hill_Monument_by_night_dfv.jpg">Luciot</a></em></p>
]]></content></item><item><title>OpenStack-Ansible networking on CentOS 7 with systemd-networkd</title><link>https://major.io/2017/04/13/openstack-ansible-on-centos-7-with-systemd-networkd/</link><pubDate>Thu, 13 Apr 2017 13:18:09 +0000</pubDate><guid>https://major.io/2017/04/13/openstack-ansible-on-centos-7-with-systemd-networkd/</guid><description>Although OpenStack-Ansible doesn&amp;rsquo;t fully support CentOS 7 yet, the support is almost ready. I have a four node Ocata cloud deployed on CentOS 7, but I decided to change things around a bit and use systemd-networkd instead of NetworkManager or the old rc scripts.
This post will explain how to configure the network for an OpenStack-Ansible cloud on CentOS 7 with systemd-networkd.
Each one of my OpenStack hosts has four network interfaces and each one has a specific task:</description><content type="html"><![CDATA[<p><img src="https://major.io/wp-content/uploads/2017/04/MaxPixel.freegreatpicture.com-Chip-Nsa-Distributor-Switch-It-Network-Ethernet-490027-e1492089463954.jpg" alt="1"></p>
<p>Although OpenStack-Ansible doesn&rsquo;t fully support CentOS 7 yet, the support is almost ready. I have a four node Ocata cloud deployed on CentOS 7, but I decided to change things around a bit and use systemd-networkd instead of NetworkManager or the old rc scripts.</p>
<p>This post will explain how to configure the network for an OpenStack-Ansible cloud on CentOS 7 with systemd-networkd.</p>
<p>Each one of my OpenStack hosts has four network interfaces and each one has a specific task:</p>
<ul>
<li><code>enp2s0</code> – regular network interface, carries inter-host LAN traffic</li>
<li><code>enp3s0</code> – carries <code>br-mgmt</code> bridge for LXC container communication</li>
<li><code>enp4s0</code> – carries <code>br-vlan</code> bridge for VM public network connectivity</li>
<li><code>enp5s0</code> – carries <code>br-vxlan</code> bridge for VM private network connectivity</li>
</ul>
<h2 id="adjusting-services">Adjusting services</h2>
<p>First off, we need to get systemd-networkd and systemd-resolved ready to take over networking:</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<h2 id="lan-interface">LAN interface</h2>
<p>My <code>enp2s0</code> network interface carries traffic between hosts and handles regular internal LAN traffic.</p>
<p><code>/etc/systemd/network/enp2s0.network</code></p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ini" data-lang="ini"><span style="color:#719e07">[Match]</span>
Name<span style="color:#719e07">=</span><span style="color:#2aa198">enp2s0</span>

<span style="color:#719e07">[Network]</span>
Address<span style="color:#719e07">=</span><span style="color:#2aa198">192.168.250.21/24</span>
Gateway<span style="color:#719e07">=</span><span style="color:#2aa198">192.168.250.1</span>
DNS<span style="color:#719e07">=</span><span style="color:#2aa198">192.168.250.1</span>
DNS<span style="color:#719e07">=</span><span style="color:#2aa198">8.8.8.8</span>
DNS<span style="color:#719e07">=</span><span style="color:#2aa198">8.8.4.4</span>
IPForward<span style="color:#719e07">=</span><span style="color:#2aa198">yes</span>
</code></pre></div><p>This one is quite simple, but the rest get a little more complicated.</p>
<h2 id="management-bridge">Management bridge</h2>
<p>The management bridge (<code>br-mgmt</code>) carries traffic between LXC containers. We start by creating the bridge device itself:</p>
<p><code>/etc/systemd/network/br-mgmt.netdev</code></p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ini" data-lang="ini"><span style="color:#719e07">[NetDev]</span>
Name<span style="color:#719e07">=</span><span style="color:#2aa198">br-mgmt</span>
Kind<span style="color:#719e07">=</span><span style="color:#2aa198">bridge</span>
</code></pre></div><p>Now we configure the network on the bridge (I use OpenStack-Ansible&rsquo;s defaults here):</p>
<p><code>/etc/systemd/network/br-mgmt.network</code></p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ini" data-lang="ini"><span style="color:#719e07">[Match]</span>
Name<span style="color:#719e07">=</span><span style="color:#2aa198">br-mgmt</span>

<span style="color:#719e07">[Network]</span>
Address<span style="color:#719e07">=</span><span style="color:#2aa198">172.29.236.21/22</span>
</code></pre></div><p>I run the management network on VLAN 10, so I need a network device and network configuration for the VLAN as well. This step adds the <code>br-mgmt</code> bridge to the VLAN 10 interface:</p>
<p><code>/etc/systemd/network/vlan10.netdev</code></p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ini" data-lang="ini"><span style="color:#719e07">[NetDev]</span>
Name<span style="color:#719e07">=</span><span style="color:#2aa198">vlan10</span>
Kind<span style="color:#719e07">=</span><span style="color:#2aa198">vlan</span>

<span style="color:#719e07">[VLAN]</span>
Id<span style="color:#719e07">=</span><span style="color:#2aa198">10</span>
</code></pre></div><p><code>/etc/systemd/network/vlan10.network</code></p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ini" data-lang="ini"><span style="color:#719e07">[Match]</span>
Name<span style="color:#719e07">=</span><span style="color:#2aa198">vlan10</span>

<span style="color:#719e07">[Network]</span>
Bridge<span style="color:#719e07">=</span><span style="color:#2aa198">br-mgmt</span>
</code></pre></div><p>Finally, we add the VLAN 10 interface to <code>enp3s0</code> to tie it all together:</p>
<p><code>/etc/systemd/network/enp3s0.network</code></p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ini" data-lang="ini"><span style="color:#719e07">[Match]</span>
Name<span style="color:#719e07">=</span><span style="color:#2aa198">enp3s0</span>

<span style="color:#719e07">[Network]</span>
VLAN<span style="color:#719e07">=</span><span style="color:#2aa198">vlan10</span>
</code></pre></div><h2 id="public-instance-connectivity">Public instance connectivity</h2>
<p>My router offers up a few different VLANs for OpenStack instances to use for their public networks. We start by creating a <code>br-vlan</code> network device and its configuration:</p>
<p><code>/etc/systemd/network/br-vlan.netdev</code></p>
<pre><code>[NetDev]
Name=br-vlan
Kind=bridge
</code></pre><p><code>/etc/systemd/network/br-vlan.network</code></p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ini" data-lang="ini"><span style="color:#719e07">[Match]</span>
Name<span style="color:#719e07">=</span><span style="color:#2aa198">br-vlan</span>

<span style="color:#719e07">[Network]</span>
DHCP<span style="color:#719e07">=</span><span style="color:#2aa198">no</span>
</code></pre></div><p>We can add this bridge onto the <code>enp4s0</code> physical interface:</p>
<p><code>/etc/systemd/network/enp4s0.network</code></p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ini" data-lang="ini"><span style="color:#719e07">[Match]</span>
Name<span style="color:#719e07">=</span><span style="color:#2aa198">enp4s0</span>

<span style="color:#719e07">[Network]</span>
Bridge<span style="color:#719e07">=</span><span style="color:#2aa198">br-vlan</span>
</code></pre></div><h2 id="vxlan-private-instance-connectivity">VXLAN private instance connectivity</h2>
<p>This step is similar to the previous one. We start by defining our <code>br-vxlan</code> bridge:</p>
<p><code>/etc/systemd/network/br-vxlan.netdev</code></p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ini" data-lang="ini"><span style="color:#719e07">[NetDev]</span>
Name<span style="color:#719e07">=</span><span style="color:#2aa198">br-vxlan</span>
Kind<span style="color:#719e07">=</span><span style="color:#2aa198">bridge</span>
</code></pre></div><p><code>/etc/systemd/network/br-vxlan.network</code></p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ini" data-lang="ini"><span style="color:#719e07">[Match]</span>
Name<span style="color:#719e07">=</span><span style="color:#2aa198">br-vxlan</span>

<span style="color:#719e07">[Network]</span>
Address<span style="color:#719e07">=</span><span style="color:#2aa198">172.29.240.21/22</span>
</code></pre></div><p>My VXLAN traffic runs over VLAN 11, so we need to define that VLAN interface:</p>
<p><code>/etc/systemd/network/vlan11.netdev</code></p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ini" data-lang="ini"><span style="color:#719e07">[NetDev]</span>
Name<span style="color:#719e07">=</span><span style="color:#2aa198">vlan11</span>
Kind<span style="color:#719e07">=</span><span style="color:#2aa198">vlan</span>

<span style="color:#719e07">[VLAN]</span>
Id<span style="color:#719e07">=</span><span style="color:#2aa198">11</span>
</code></pre></div><p><code>/etc/systemd/network/vlan11.network</code></p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ini" data-lang="ini"><span style="color:#719e07">[Match]</span>
Name<span style="color:#719e07">=</span><span style="color:#2aa198">vlan11</span>

<span style="color:#719e07">[Network]</span>
Bridge<span style="color:#719e07">=</span><span style="color:#2aa198">br-vxlan</span>
</code></pre></div><p>We can hook this VLAN interface into the <code>enp5s0</code> interface now:</p>
<p><code>/etc/systemd/network/enp5s0.network</code></p>
<pre><code>[Match]
Name=enp5s0

[Network]
VLAN=vlan11
</code></pre><h2 id="checking-our-work">Checking our work</h2>
<p>The cleanest way to apply all of these configurations is to reboot. The <em>Adjusting services</em> step from the beginning of this post will ensure that systemd-networkd and systemd-resolved come up after a reboot.</p>
<p>Run <code>networkctl</code> to get a current status of your network interfaces:</p>
<pre><code># networkctl
IDX LINK             TYPE               OPERATIONAL SETUP
  1 lo               loopback           carrier     unmanaged
  2 enp2s0           ether              routable    configured
  3 enp3s0           ether              degraded    configured
  4 enp4s0           ether              degraded    configured
  5 enp5s0           ether              degraded    configured
  6 lxcbr0           ether              routable    unmanaged
  7 br-vxlan         ether              routable    configured
  8 br-vlan          ether              degraded    configured
  9 br-mgmt          ether              routable    configured
 10 vlan11           ether              degraded    configured
 11 vlan10           ether              degraded    configured
</code></pre><p>You should have <code>configured</code> in the <code>SETUP</code> column for all of the interfaces you created. Some interfaces will show as <code>degraded</code> because they are missing an IP address (which is intentional for most of these interfaces).</p>
]]></content></item><item><title>RHEL 7 STIG v1 updates for openstack-ansible-security</title><link>https://major.io/2017/04/05/rhel-7-stig-v1-updates-for-openstack-ansible-security/</link><pubDate>Wed, 05 Apr 2017 17:46:17 +0000</pubDate><guid>https://major.io/2017/04/05/rhel-7-stig-v1-updates-for-openstack-ansible-security/</guid><description>DISA&amp;rsquo;s final release of the Red Hat Enterprise Linux (RHEL) 7 Security Technical Implementation Guide (STIG) came out a few weeks ago and it has plenty of improvements and changes. The openstack-ansible-security role has already been updated with these changes.
Quite a few duplicated STIG controls were removed and a few new ones were added. Some of the controls in the pre-release were difficult to implement, especially those that changed parameters for PKI-based authentication.</description><content type="html"><![CDATA[<p><a href="/wp-content/uploads/2017/04/OpenStack-Logo-Horizontal-e1491414195297.png"><!-- raw HTML omitted --></a>DISA&rsquo;s final release of the Red Hat Enterprise Linux (RHEL) 7 Security Technical Implementation Guide (STIG) <a href="http://iase.disa.mil/stigs/os/unix-linux/Pages/index.aspx">came out a few weeks ago</a> and it has plenty of improvements and changes. The openstack-ansible-security role has already been updated with these changes.</p>
<p>Quite a few duplicated STIG controls were removed and a few new ones were added. Some of the controls in the pre-release were difficult to implement, especially those that changed parameters for PKI-based authentication.</p>
<p>The biggest challenge overall was the renumbering. The pre-release STIG used an unusual numbering convention: RHEL-07-123456. The final version used the more standardized &ldquo;V&rdquo; numbers, such as V-72225. This change required a <a href="https://github.com/openstack/openstack-ansible-security/commit/dccce1d5cc06985a58f0ecba4fd0d977388592b2">substantial patch</a> to bring the Ansible role inline with the new STIG release.</p>
<p>All of the <a href="https://docs.openstack.org/developer/openstack-ansible-security/controls-rhel7.html">role&rsquo;s documentation</a> is now updated to reflect the new numbering scheme and STIG changes. The key thing to remember is that you&rsquo;ll need to use <code>--skip-tag</code> with the new STIG numbers if you need to skip certain tasks.</p>
<p><strong>Note:</strong> These changes won&rsquo;t be backported to the <code>stable/ocata</code> branch, so you need to use the <code>master</code> branch to get these changes.</p>
<p>Have feedback? Found a bug? Let us know!</p>
<ul>
<li>IRC: <code>#openstack-ansible</code> on Freenode IRC</li>
<li>Bugs: <a href="https://bugs.launchpad.net/openstack-ansible">LaunchPad</a></li>
<li>E-mail: <a href="mailto:openstack-dev@lists.rackspace.com">openstack-dev@lists.rackspace.com</a> with the subject line <code>[openstack-ansible][security]</code></li>
</ul>
]]></content></item><item><title>Takeaways from Bruce Schneier’s talk: “Security and Privacy in a Hyper-connected World”</title><link>https://major.io/2017/03/21/takeaways-from-bruce-schneiers-talk-security-and-privacy-in-a-hyper-connected-world/</link><pubDate>Wed, 22 Mar 2017 01:31:52 +0000</pubDate><guid>https://major.io/2017/03/21/takeaways-from-bruce-schneiers-talk-security-and-privacy-in-a-hyper-connected-world/</guid><description>&lt;p>&lt;a href="../wp-content/uploads/2017/03/IMG_20170321_113057-e1490144858438.jpg">&lt;!-- raw HTML omitted -->&lt;/a>Bruce Schneier is one of my favorite speakers when it comes to the topic of all things security. His talk from IBM Interconnect 2017, &amp;ldquo;&lt;a href="https://myibm.ibm.com/events/interconnect/all-sessions/session/7350A">Security and Privacy in a Hyper-connected World&lt;/a>&amp;rdquo;, covered a wide range of security concerns.&lt;/p>
&lt;p>There were plenty of great quotes from the talk (scroll to the end for those) and I will summarize the main takeaways in this post.&lt;/p>
&lt;h2 id="people-process-and-technology">People, process, and technology&lt;/h2>
&lt;p>Bruce hits this topic a lot and for good reason: a weak link in any of the three could lead to a breach and a loss of data. He talked about the concept of security as a product and a process. Security is part of every product we consume. Whether it&amp;rsquo;s the safety of the food that makes it into our homes or the new internet-connected thermostat on the wall, security is part of the product.&lt;/p>
&lt;p>The companies that sell these products have a wide variety of strategies for managing security issues. Vulnerabilities in an internet-connected teapot are not worth much since there isn&amp;rsquo;t a lot of value there. It&amp;rsquo;s probably safe to assume that a teapot will have many more vulnerabilities than your average Apple or Android mobile device. Vulnerabilities in those devices are extremely valuable because the data we carry on those devices is valuable.&lt;/p>
&lt;h2 id="certainty-vs-uncertainty">Certainty vs. uncertainty&lt;/h2>
&lt;p>The talk moved into incident response and how to be successful when the worst happens. Automation only works when there&amp;rsquo;s a high degree of certainty in the situation. If there are variables that can be plugged into an algorithm and a result comes out the other end, automation is fantastic.&lt;/p>
&lt;p>Bruce recommended using orchestration when tackling uncertain situations, such as security incident responses. Orchestration involves people following processes and using technology where it makes sense.&lt;/p>
&lt;p>He talked about going through TSA checkpoints where metal detectors and x-ray scanners essentially run the show. Humans are around when these pieces of technology detect a problem. If you put a weapon into your carry on, the x-ray scanner will notify a human and that human can take an appropriate response to escalate the problem. If a regular passenger has a firearm in a carry-on bag, the police should be alerted. If an Air Marshal has one, then the situation is handled entirely differently - by a human.&lt;/p>
&lt;p>One other aspect he noted was around the uncertainty surrounding our data. Our control over our data, and our control over the systems that hold our data, is decreasing. Bruce remarked that he has more control over what his laptop does than his thermostat.&lt;/p>
&lt;h2 id="ooda-loop">OODA loop&lt;/h2>
&lt;p>Bruce raised awareness around the &lt;a href="https://en.wikipedia.org/wiki/OODA_loop">OODA loop&lt;/a> and its value when dealing with security incidents. Savvy readers will remember that the OODA loop was the crux of my &amp;ldquo;&lt;a href="https://www.slideshare.net/MajorHayden/be-an-inspiration-not-an-impostor-texas-linux-fest-2015">Be an inspiration, not an impostor&lt;/a>&amp;rdquo; talk about impostor syndrome.&lt;/p>
&lt;p>His point was that the OODA loop is a great way to structure a response during a stressful situation. When the orchestration works well, the defenders can complete an OODA loop faster than their adversaries can. When it works really well, the defenders can find ways to disrupt the adversaries' OODA loops and thwart the attack.&lt;/p></description><content type="html"><![CDATA[<p><a href="/wp-content/uploads/2017/03/IMG_20170321_113057-e1490144858438.jpg"><!-- raw HTML omitted --></a>Bruce Schneier is one of my favorite speakers when it comes to the topic of all things security. His talk from IBM Interconnect 2017, &ldquo;<a href="https://myibm.ibm.com/events/interconnect/all-sessions/session/7350A">Security and Privacy in a Hyper-connected World</a>&rdquo;, covered a wide range of security concerns.</p>
<p>There were plenty of great quotes from the talk (scroll to the end for those) and I will summarize the main takeaways in this post.</p>
<h2 id="people-process-and-technology">People, process, and technology</h2>
<p>Bruce hits this topic a lot and for good reason: a weak link in any of the three could lead to a breach and a loss of data. He talked about the concept of security as a product and a process. Security is part of every product we consume. Whether it&rsquo;s the safety of the food that makes it into our homes or the new internet-connected thermostat on the wall, security is part of the product.</p>
<p>The companies that sell these products have a wide variety of strategies for managing security issues. Vulnerabilities in an internet-connected teapot are not worth much since there isn&rsquo;t a lot of value there. It&rsquo;s probably safe to assume that a teapot will have many more vulnerabilities than your average Apple or Android mobile device. Vulnerabilities in those devices are extremely valuable because the data we carry on those devices is valuable.</p>
<h2 id="certainty-vs-uncertainty">Certainty vs. uncertainty</h2>
<p>The talk moved into incident response and how to be successful when the worst happens. Automation only works when there&rsquo;s a high degree of certainty in the situation. If there are variables that can be plugged into an algorithm and a result comes out the other end, automation is fantastic.</p>
<p>Bruce recommended using orchestration when tackling uncertain situations, such as security incident responses. Orchestration involves people following processes and using technology where it makes sense.</p>
<p>He talked about going through TSA checkpoints where metal detectors and x-ray scanners essentially run the show. Humans are around when these pieces of technology detect a problem. If you put a weapon into your carry on, the x-ray scanner will notify a human and that human can take an appropriate response to escalate the problem. If a regular passenger has a firearm in a carry-on bag, the police should be alerted. If an Air Marshal has one, then the situation is handled entirely differently - by a human.</p>
<p>One other aspect he noted was around the uncertainty surrounding our data. Our control over our data, and our control over the systems that hold our data, is decreasing. Bruce remarked that he has more control over what his laptop does than his thermostat.</p>
<h2 id="ooda-loop">OODA loop</h2>
<p>Bruce raised awareness around the <a href="https://en.wikipedia.org/wiki/OODA_loop">OODA loop</a> and its value when dealing with security incidents. Savvy readers will remember that the OODA loop was the crux of my &ldquo;<a href="https://www.slideshare.net/MajorHayden/be-an-inspiration-not-an-impostor-texas-linux-fest-2015">Be an inspiration, not an impostor</a>&rdquo; talk about impostor syndrome.</p>
<p>His point was that the OODA loop is a great way to structure a response during a stressful situation. When the orchestration works well, the defenders can complete an OODA loop faster than their adversaries can. When it works really well, the defenders can find ways to disrupt the adversaries' OODA loops and thwart the attack.</p>
<h2 id="quotes">Quotes</h2>
<p>I tried to capture as many of the memorable quotes on Twitter as they happened. It&rsquo;s certainly possible - perhaps likely - that I&rsquo;ve missed a few words in the quotes. <em>I apologize in advance to Bruce if I&rsquo;ve mangled any of his words here.</em></p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->]]></content></item><item><title>Five reasons why I’m excited about POWER9</title><link>https://major.io/2017/03/21/five-reasons-why-im-excited-about-power9/</link><pubDate>Tue, 21 Mar 2017 18:38:22 +0000</pubDate><guid>https://major.io/2017/03/21/five-reasons-why-im-excited-about-power9/</guid><description>There&amp;rsquo;s plenty to like about the POWER8 architecture: high speed interconnections, large (and flexible) core counts, and support for lots of memory. POWER9 provides improvements in all of these areas and it has learned some entirely new tricks as well.
Here are my top five reasons for getting excited about POWER9:
NVLink 2.0 In the simplext terms, NVLink provides a very high speed interface between CPUs and GPUs with very low latency.</description><content type="html"><![CDATA[<p>There&rsquo;s plenty to like about the POWER8 architecture: high speed interconnections, large (and flexible) core counts, and support for lots of memory. POWER9 provides improvements in all of these areas and it has learned some entirely new tricks as well.</p>
<p>Here are my top five reasons for getting excited about POWER9:</p>
<h2 id="nvlink-20">NVLink 2.0</h2>
<p>In the simplext terms, NVLink provides a very high speed interface between CPUs and GPUs with very low latency. This is quite handy for software that needs to exchange large amounts of data with GPUs. Machine learning can get a significant performance boost with NVLink.</p>
<p>NVLink 2.0 connects CPUs and GPUS with a 25GB/sec link (per lane). That&rsquo;s not all - GPUs can communicate with each other over their own independent lanes. Drop in a few NVIDIA&rsquo;s Tesla P100 GPUs and you will have an extremely powerful accelerated system. NVIDIA&rsquo;s next generation GPUs, codenamed &ldquo;Volta&rdquo;, will take this to the next level.</p>
<h2 id="capi-20">CAPI 2.0</h2>
<p>The Coherent Accelerator Processor Interface (CAPI) allows the CPU to quickly access accelerators (think ASICs and FPGAs) over a high bandwidth interface with very low latency. CAPI 2.0 gets a 4x performance bump in POWER9 since it uses PCI-Express Gen 4.</p>
<p>The OpenCAPI 3.0 interface is also available, but it doesn&rsquo;t use PCI-Express like CAPI does. It has an open interface with 25GB/sec of bandwidth and it uses direct memory access to perform operations very quickly.</p>
<h2 id="on-chip-acceleration">On-chip acceleration</h2>
<p>POWER9 provides more acceleration for common tasks right on the chip itself. This includes the common functions, like cryptography, but it also accelerates compression. The chip will accelerate gzip compression, 842 compression and AES/SHA. It also has a true random number generator built in.</p>
<p>Another nice on-chip benefit is the virtualization acceleration. No hypervisor calls are needed (this depends on your hypervisor choice) and this allows for user mode invocation of virtualization actions.</p>
<h2 id="multiple-core-options">Multiple core options</h2>
<p>POWER9 comes in two flavors: SMT8 and SMT4. SMT8 is geared towards the PowerVM platform and provides the strongest individual threads. This makes it great for larger PowerVM partitions that need lots of cores. SMT4 is designed more for Linux workloads.</p>
<p>The chip can handle 64 instructions per cycle on the SMT4 and 128 instructions on the SMT8. There are also some compiler benefits that can improve performance for modern codebases.</p>
<h2 id="openpower-zaius">OpenPOWER Zaius</h2>
<p>I&rsquo;d be remiss if I didn&rsquo;t mention Rackspace&rsquo;s contributions to the <a href="https://cloudplatform.googleblog.com/2016/10/introducing-Zaius-Google-and-Rackspaces-open-server-running-IBM-POWER9.html">Zaius P9 server</a>! Zaius is a spec for an Open Compute POWER9 server. Google, Rackspace, IBM and Ingrasys have been working together to build this server for the masses.</p>
]]></content></item><item><title>IBM Interconnect 2017 first day keynote recap</title><link>https://major.io/2017/03/20/ibm-interconnect-first-day-keynote-recap/</link><pubDate>Mon, 20 Mar 2017 20:18:47 +0000</pubDate><guid>https://major.io/2017/03/20/ibm-interconnect-first-day-keynote-recap/</guid><description>The &amp;ldquo;Welcome to Interconnect&amp;rdquo; keynote kicked off the first day of IBM Interconnect 2017. The Mandalay Bay Event Center was quite full (which was evident as we began to file out!) and the speakers were engaging.
Some of the most memorable talks came from Indiegogo, Delos, and of course, Will Smith.
Indiegogo Indiegogo is probably a familiar name for most people who know about crowdfunding. Over $1.1B has been raised by over 8 million backers since Indiegogo got started.</description><content type="html"><![CDATA[<p><img src="/wp-content/uploads/2017/03/IMG_20170320_104424-e1490039929657.jpg" alt="1"></p>
<p>The &ldquo;Welcome to Interconnect&rdquo; keynote kicked off the first day of IBM Interconnect 2017. The Mandalay Bay Event Center was quite full (which was evident as we began to file out!) and the speakers were engaging.</p>
<p>Some of the most memorable talks came from Indiegogo, Delos, and of course, Will Smith.</p>
<h2 id="indiegogo">Indiegogo</h2>
<p>Indiegogo is probably a familiar name for most people who know about crowdfunding. Over $1.1B has been raised by over 8 million backers since Indiegogo got started. They showcased three crowdfunded projects during the keynote and allowed audience members to vote for each one! The talk moved quickly, but it sounds like IBM committed to contribute one dollar per vote given to a project!</p>
<p>One of the most interesting one was <a href="https://www.indiegogo.com/projects/waterbot-world-s-first-smart-water-quality-monitor-health-technology--2">Waterbot</a>. It detects subtle changes in water quality and can alert people to the nature of the change.</p>
<p>Another project was <a href="https://www.indiegogo.com/projects/smart-plate-topview-your-personal-nutritionist-fitness">SmartPlate</a>. It detects the quantity of food on the plate and what type of food is on the plate. It can make estimates on the contents of the food (calories, fat, etc) and could potentially warn someone if they&rsquo;re about to eat something on their list of allergies.</p>
<h2 id="delos">Delos</h2>
<p><a href="http://delos.com/">Delos</a> is in the business of making buildings work better for the people who work in them. They dig deep into everything from lighting to temperature to window placement and understand the best combination of environmental factors that enable people to do great work. It&rsquo;s challenging work because changing buildings is costly and different types of industries may do better in different environments.</p>
<h2 id="will-smith">Will Smith</h2>
<p>The &ldquo;one more thing&rdquo; moment of the morning was an interview with Will Smith. It may seem odd in the context of a technical conference, but he provided lots of good advice for people who are trying to transform themselves and their companies. He shared a lot about the early days of his career and how he transformed himself from a &ldquo;clean rapper&rdquo; to a TV and movie star.</p>
<p>There were <strong>several</strong> quotable (and often hilarious) moments:</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>Two things stuck with me the most: what it was like to portray a hero in film (like Muhammad Ali) and how important it is to always have a simple mission statement:</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>He talked about how he approached new challenges and forks in the road. Each time he approached one, he asked himself: &ldquo;Will this improve people&rsquo;s lives?&rdquo; If the answer was yes, he signed on and put his maximum effort behind it. It&rsquo;s a simple question, but he talked about how it helped him get through some highly complex decisions.</p>
]]></content></item><item><title>Reflecting on 10 years of (mostly) technical blogging</title><link>https://major.io/2017/03/10/reflecting-on-10-years-of-mostly-technical-blogging/</link><pubDate>Fri, 10 Mar 2017 14:11:51 +0000</pubDate><guid>https://major.io/2017/03/10/reflecting-on-10-years-of-mostly-technical-blogging/</guid><description>It all started shortly after I joined Rackspace in December of 2006. I needed a place to dump the huge amounts of information I was learning as an entry-level Linux support technician and I wanted to store everything in a place where it could be easily shared. The blog was born!
The blog now has over 700 posts on topics ranging from Linux system administration to job interview preparation. I&amp;rsquo;ll get an email or a tweet once every few weeks from someone saying: &amp;ldquo;I ran into a problem, Googled for it, and found your blog!</description><content type="html"><![CDATA[<p><img src="/wp-content/uploads/2017/03/typewriter-1462562129n95-e1489154490478.jpg" alt="1"></p>
<p>It all started shortly after I joined Rackspace in December of 2006. I needed a place to dump the huge amounts of information I was learning as an entry-level Linux support technician and I wanted to store everything in a place where it could be easily shared. The blog was born!</p>
<p>The blog now has over 700 posts on topics ranging from Linux system administration to job interview preparation. I&rsquo;ll get an email or a tweet once every few weeks from someone saying: &ldquo;I ran into a problem, Googled for it, and found your blog!&rdquo; Comments like that keep me going and allow me to push through the deepest writer&rsquo;s block moments.</p>
<p>The post titled &ldquo;<a href="https://major.io/2012/03/30/why-technical-people-should-blog-but-dont/">Why technical people should blog (but don&rsquo;t)</a>&rdquo; is one of my favorites and I get a lot of feedback about it. Many people still feel like there&rsquo;s no audience out there for the things they write. Just remember that someone, somewhere, can learn something from you and from your experiences. Write from the heart about what interests you and the readers will gradually appear. It&rsquo;s a <a href="http://www.imdb.com/title/tt0097351/">Field of Dreams</a> moment.</p>
<p>Thanks to everyone who has given me support over the years to keep the writing going!</p>
]]></content></item><item><title>OpenStack isn’t dead. It’s boring. That’s a good thing.</title><link>https://major.io/2017/02/24/openstack-isnt-dead-its-boring-thats-a-good-thing/</link><pubDate>Fri, 24 Feb 2017 16:06:24 +0000</pubDate><guid>https://major.io/2017/02/24/openstack-isnt-dead-its-boring-thats-a-good-thing/</guid><description>NOTE: The opinions shared in this post are mine alone and are not related to my employer in any way.
The first OpenStack Project Teams Gathering (PTG) event was held this week in Atlanta. The week was broken into two parts: cross-project work on Monday and Tuesday, and individual projects Wednesday through Friday. I was there for the first two days and heard a few discussions that started the same way.</description><content type="html"><![CDATA[<p><img src="/wp-content/uploads/2017/02/1024px-Midtown_HDR_Atlanta-e1487943722280.jpg" alt="1"></p>
<p><em>NOTE: The opinions shared in this post are mine alone and are not related to my employer in any way.</em></p>
<hr>
<p>The first <a href="https://www.openstack.org/ptg/">OpenStack Project Teams Gathering (PTG)</a> event was held this week in Atlanta. The week was broken into two parts: cross-project work on Monday and Tuesday, and individual projects Wednesday through Friday. I was there for the first two days and heard a few discussions that started the same way.</p>
<blockquote>
<p>Everyone keeps saying OpenStack is dead.</p>
</blockquote>
<blockquote>
<p>Is it?</p>
</blockquote>
<p><strong>OpenStack isn&rsquo;t dead.</strong> It&rsquo;s boring.</p>
<h2 id="the-report-of-my-death-was-an-exaggeration">&ldquo;The report of my death was an exaggeration&rdquo;</h2>
<p>Mark Twain <a href="http://www.thisdayinquotes.com/2010/06/reports-of-my-death-are-greatly.html">said it best</a>, but it works for OpenStack as well. The news has plenty of negative reports that cast a shadow over OpenStack&rsquo;s future. You don&rsquo;t have to look far to find them:</p>
<ul>
<li><a href="http://fortune.com/2016/12/19/openstack-public-cloud/">HPE and Cisco Moves Hurt OpenStack&rsquo;s Public Cloud Story (Fortune)</a></li>
<li><a href="http://www.cbronline.com/news/cloud/public/cisco-killing-off-openstack-public-cloud/">Is Cisco killing off its OpenStack public cloud? (Computer Business Review)</a></li>
<li><a href="http://www.itworld.com/article/2699624/open-source-tools/openstack-still-has-an-enterprise-problem.html">OpenStack still has an enterprise problem</a></li>
</ul>
<p>This isn&rsquo;t evidence of OpenStack&rsquo;s demise, but rather a transformation. Gartner called OpenStack a <a href="https://www.theregister.co.uk/2015/05/18/openstack_private_clouds_are_science_projects_says_gartner/">&ldquo;science project&rdquo;</a> in 2015 and now 451 Research Group is <a href="http://www.informationweek.com/cloud/what-you-need-to-know-about-openstack/a/d-id/1328252">saying something very different</a>:</p>
<blockquote>
<p>451 Research Group estimates OpenStack&rsquo;s ecosystem to grow nearly five-fold in revenue, from US$1.27 billion market size in 2015 to US$5.75 billion by 2020.</p>
</blockquote>
<p>A 35% CAGR sounds pretty good for a product in the middle of a transformation. In Texas, we&rsquo;d say that&rsquo;s more than enough to <a href="http://english.stackexchange.com/questions/92393/origin-of-more-x-than-you-can-shake-a-stick-at">&ldquo;shake a stick at&rdquo;</a>.</p>
<h2 id="the-transformation">The transformation</h2>
<p>You can learn a lot about the transformation going on within OpenStack by reading analyst reports and other news online. I won&rsquo;t go into that here since that data is readily available.</p>
<p>Instead, I want to take a look at how OpenStack has changed from the perspective of a developer. My involvement with OpenStack started in the Diablo release in 2011 and my first OpenStack Summit was the Folsom summit in San Francisco.</p>
<p>Much of the discussion at that time was around the &ldquo;minutiae&rdquo; of developing software in its early forms. We discussed topics like how to test, how to handle a myriad of requirements that constantly change, and which frameworks to use in which projects. The list of projects was quite short at that time (there were only 7 main services in Grizzly). Lots of effort certainly poured into feature development, but there was a ton of work being done to keep the wheels from falling off entirely.</p>
<p>The discussions at this week&rsquo;s PTG were very different.</p>
<p>Most of the discussion was around adding new integrations, improving reliability, and increasing scale. Questions were asked about how to integrate OpenStack into existing enterprise processes and applications. Reliability discussions were centered less around making the OpenStack services reliable, but more around how to increase overall resiliency when other hardware or software is misbehaving.</p>
<p>Discussions or arguments about minutiae were difficult to find.</p>
<h2 id="boring-is-good">Boring is good</h2>
<p><strong>I&rsquo;m not trying to say that working with OpenStack is boring.</strong> Developing software within the OpenStack community is an enjoyable experience. The rules and regulations within most projects are there to prevent design mistakes that have appeared before and many of these sets of rules are aligned between projects. Testing code and providing meaningful reviews is also straightforward.</p>
<p>However, the drama, both unproductive and productive, that plagued the project in the past is diminishing. It still exists in places, especially when it comes to vendor relationships. (That&rsquo;s where most open source projects see their largest amounts of friction, anyway.)</p>
<p>This transformation may make OpenStack appear &ldquo;dead&rdquo; to some. The OpenStack community is solving different problems now. Many of them are larger and more difficult to solve. Sometimes these challenges take more than one release to overcome. Either way, many OpenStack developers are up for these new challenges, even if they don&rsquo;t make the headlines.</p>
<p>As for me: <strong>bring on the boring</strong>. Let&rsquo;s crush the hard stuff.</p>
<hr>
<p><em>Photo credit: By Mike (Flickr: DSC_6831_2_3_tonemapped) [<a href="http://creativecommons.org/licenses/by/2.0">CC BY 2.0</a>], <a href="https://commons.wikimedia.org/wiki/File%3AMidtown_HDR_Atlanta.jpg">via Wikimedia Commons</a></em></p>
]]></content></item><item><title>What I’m looking forward to at IBM Interconnect 2017</title><link>https://major.io/2017/02/23/what-im-looking-forward-to-at-ibm-interconnect-2017/</link><pubDate>Thu, 23 Feb 2017 16:35:02 +0000</pubDate><guid>https://major.io/2017/02/23/what-im-looking-forward-to-at-ibm-interconnect-2017/</guid><description>IBM Interconnect 2017 is coming up next month in Las Vegas. Last year&amp;rsquo;s conference was a whirlwind of useful talks, inspiring hallway conversations, and great networking opportunities. I was exhausted by the week&amp;rsquo;s end, but it was totally worth it.
One of my favorite sessions from last year was Tanmay Bakshi&amp;rsquo;s keynote. It was truly inspiring to see someone so young take command of such a large stage and teach us all something.</description><content type="html"><![CDATA[<p><img src="/wp-content/uploads/2017/02/Mandalay_Bay_4067278713-e1487860423502.jpg" alt="1"></p>
<p>IBM Interconnect 2017 is coming up next month in Las Vegas. Last year&rsquo;s
conference was a whirlwind of useful talks, inspiring hallway conversations,
and great networking opportunities. I was exhausted by the week&rsquo;s end, but it
was totally worth it.</p>
<p>One of my favorite sessions from last year was <a href="https://twitter.com/tajymany">Tanmay Bakshi&rsquo;s</a> keynote.
It was truly inspiring to see someone so young take command of such a large
stage and teach us all something. I can&rsquo;t wait to hear another of his talks.</p>
<h2 id="open-technology-summit-ots">Open Technology Summit (OTS)</h2>
<p>This year, I&rsquo;m interested to see the talks at the Open Technology Summit. It&rsquo;s
essentially a group of lightning talks from industry experts on a variety of
topics. This year&rsquo;s agenda has talks on OpenStack, Cloud Foundry, Blockchain,
and others. Although the event is three hours long, it feels a lot shorter
than that since each talk is anywhere from 10-20 minutes long.</p>
<p>The OTS starts on Sunday afternoon, so be sure to arrive in Las Vegas early
enough to attend.</p>
<h2 id="security">Security</h2>
<p>Information security is always an interesting topic and I have my eye on the
following sessions:</p>
<ul>
<li>Watson &amp; Cyber Security: Bringing AI to the Battle <em>(it should be
interesting to hear how machine learning can change the game)</em></li>
<li>Transforming Security Organizations: Case Studies and Lessons Learned]
<em>(I&rsquo;m always interested in new ways to improve security outside of the
systems themselves)</em></li>
<li>Turning Traffic Lights Green: Defense and National Security in Cyberspace</li>
</ul>
<h2 id="it-transformation">IT Transformation</h2>
<ul>
<li>The Austin Convention Center Gets Agile in the Cloud][8] <em>(hearing the
&lsquo;how&rsquo; and &lsquo;why&rsquo; about customers moving into cloud is valuable)</em></li>
</ul>
<h2 id="open-source">Open Source</h2>
<ul>
<li>Working in Open Communities is the Next Differentiator for Your Solution
<em>(Walmart has done a lot in this space and I&rsquo;m eager to learn more)</em></li>
<li>Linux on the Mainframe: Linux Foundation and the Open Mainframe Project</li>
<li>Open Cloud Demystified: Open Source Leaders Tell All</li>
</ul>
<p>I hope to see you there!</p>
<p><em>Photo credit: By Ronnie Macdonald from Chelmsford, United Kingdom (Mandalay
Bay) [<a href="http://creativecommons.org/licenses/by/2.0">CC BY 2.0</a>], <a href="https://commons.wikimedia.org/wiki/File%3AMandalay_Bay_(4067278713).jpg">via Wikimedia Commons</a></em></p>
]]></content></item><item><title>systemd-networkd on Ubuntu 16.04 LTS (Xenial)</title><link>https://major.io/2017/01/15/systemd-networkd-on-ubuntu-16-04-lts-xenial/</link><pubDate>Sun, 15 Jan 2017 15:24:40 +0000</pubDate><guid>https://major.io/2017/01/15/systemd-networkd-on-ubuntu-16-04-lts-xenial/</guid><description>My OpenStack cloud depends on Ubuntu, and the latest release of OpenStack-Ansible (what I use to deploy OpenStack) requires Ubuntu 16.04 at a minimum. I tried upgrading the servers in place from Ubuntu 14.04 to 16.04, but that didn&amp;rsquo;t work so well. Those servers wouldn&amp;rsquo;t boot and the only recourse was a re-install.
Once I finished re-installing them (and wrestling with several installer bugs in Ubuntu 16.04), it was time to set up networking.</description><content type="html"><![CDATA[<p>My OpenStack cloud depends on Ubuntu, and the latest release of OpenStack-Ansible (what I use to deploy OpenStack) requires Ubuntu 16.04 at a minimum. I tried upgrading the servers in place from Ubuntu 14.04 to 16.04, but that didn&rsquo;t work so well. Those servers wouldn&rsquo;t boot and the only recourse was a re-install.</p>
<p>Once I finished re-installing them (and wrestling with several installer bugs in Ubuntu 16.04), it was time to set up networking. The traditional network configurations in <code>/etc/network/interfaces</code> are fine, but they weren&rsquo;t working the same way they were in 14.04. The VLAN configuration syntax appears to be different now.</p>
<p>But wait - 16.04 has systemd 229! I can use systemd-networkd to configure the network in a way that is a lot more familiar to me. I&rsquo;ve made posts about systemd-networkd before and the simplicity in the configurations.</p>
<p>I started with some simple configurations:</p>
<pre><code>root@hydrogen:~# cd /etc/systemd/network
root@hydrogen:/etc/systemd/network# cat enp3s0.network
[Match]
Name=enp3s0

[Network]
VLAN=vlan10
root@hydrogen:/etc/systemd/network# cat vlan10.netdev
[NetDev]
Name=vlan10
Kind=vlan

[VLAN]
Id=10
root@hydrogen:/etc/systemd/network# cat vlan10.network
[Match]
Name=vlan10

[Network]
Bridge=br-mgmt
root@hydrogen:/etc/systemd/network# cat br-mgmt.netdev
[NetDev]
Name=br-mgmt
Kind=bridge
root@hydrogen:/etc/systemd/network# cat br-mgmt.network
[Match]
Name=br-mgmt

[Network]
Address=172.29.236.21/22
</code></pre><p>Here&rsquo;s a summary of the configurations:</p>
<ul>
<li>Physical network interface is <code>enp3s0</code></li>
<li>VLAN 10 is trunked down from a switch to that interface</li>
<li>Bridge <code>br-mgmt</code> should be on VLAN 10 (only send/receive traffic tagged with VLAN 10)</li>
</ul>
<p>Once that was done, I restarted systemd-networkd to put the change into effect:</p>
<pre><code># systemctl restart systemd-networkd
</code></pre><p>Great! Let&rsquo;s check our work:</p>
<pre><code>root@hydrogen:~# brctl show
bridge name bridge id       STP enabled interfaces
br-mgmt     8000.0a30a9a949d9   no
root@hydrogen:~# networkctl
IDX LINK             TYPE               OPERATIONAL SETUP
  1 lo               loopback           carrier     unmanaged
  2 enp2s0           ether              routable    configured
  3 enp3s0           ether              degraded    configured
  4 enp4s0           ether              off         unmanaged
  5 enp5s0           ether              off         unmanaged
  6 br-mgmt          ether              no-carrier  configuring
  7 vlan10           ether              degraded    unmanaged

7 links listed.
</code></pre><p>So the bridge has no interfaces and it&rsquo;s in a <code>no-carrier</code> status. Why? Let&rsquo;s check the journal:</p>
<pre><code># journalctl --boot -u systemd-networkd
Jan 15 09:16:46 hydrogen systemd[1]: Started Network Service.
Jan 15 09:16:46 hydrogen systemd-networkd[1903]: br-mgmt: netdev exists, using existing without changing its parameters
Jan 15 09:16:46 hydrogen systemd-networkd[1903]: br-mgmt: Could not append VLANs: Operation not permitted
Jan 15 09:16:46 hydrogen systemd-networkd[1903]: br-mgmt: Failed to assign VLANs to bridge port: Operation not permitted
Jan 15 09:16:46 hydrogen systemd-networkd[1903]: br-mgmt: Could not set bridge vlan: Operation not permitted
Jan 15 09:16:59 hydrogen systemd-networkd[1903]: enp3s0: Configured
Jan 15 09:16:59 hydrogen systemd-networkd[1903]: enp2s0: Configured
</code></pre><p>The <code>Could not append VLANs: Operation not permitted</code> error is puzzling. After some searching on Google, I found a <a href="https://lists.freedesktop.org/archives/systemd-devel/2016-August/037385.html">thread from Lennart</a>:</p>
<pre><code> After an upgrade, systemd-networkd is broken, exactly the way descibed
&gt; in this issue #3876[0]

Please upgrade to 231, where this should be fixed.

Lennart
</code></pre><p>But Ubuntu 16.04 has systemd 229:</p>
<pre><code># dpkg -l | grep systemd
ii  libpam-systemd:amd64                229-4ubuntu13                      amd64        system and service manager - PAM module
ii  libsystemd0:amd64                   229-4ubuntu13                      amd64        systemd utility library
ii  python3-systemd                     231-2build1                        amd64        Python 3 bindings for systemd
ii  systemd                             229-4ubuntu13                      amd64        system and service manager
ii  systemd-sysv                        229-4ubuntu13                      amd64        system and service manager - SysV links
</code></pre><p>I haven&rsquo;t found a solution for this quite yet. Keep an eye on this post and I&rsquo;ll update it once I know more!</p>
]]></content></item><item><title>ICC color profile for Lenovo ThinkPad X1 Carbon 4th generation</title><link>https://major.io/2017/01/11/icc-color-profile-lenovo-thinkpad-x1-carbon-4th-generation/</link><pubDate>Wed, 11 Jan 2017 18:42:26 +0000</pubDate><guid>https://major.io/2017/01/11/icc-color-profile-lenovo-thinkpad-x1-carbon-4th-generation/</guid><description>My new ThinkPad arrived this week and it is working well! The Fedora 25 installation was easy and all of the hardware was recognized immediately.
However, there was a downside. The display looked washed out and had a strange tint. It seemed to be more pale than the previous ThinkPad. The default ICC profile in GNOME didn&amp;rsquo;t help much.
There&amp;rsquo;s a helpful review over at NotebookCheck that has a link to an ICC profile generated from a 4th generation ThinkPad X1 Carbon.</description><content type="html"><![CDATA[<p>My new ThinkPad arrived this week and it is working well! The Fedora 25 installation was easy and all of the hardware was recognized immediately.</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>However, there was a downside. The display looked washed out and had a strange tint. It seemed to be more pale than the previous ThinkPad. The default <a href="https://en.wikipedia.org/wiki/ICC_profile">ICC profile</a> in GNOME didn&rsquo;t help much.</p>
<p>There&rsquo;s a <a href="http://www.notebookcheck.net/Lenovo-ThinkPad-X1-Carbon-2016-Core-i7-WQHD-Ultrabook-Review.162631.0.html">helpful review over at NotebookCheck</a> that has a link to an <a href="http://www.notebookcheck.net/uploads/tx_nbc2/X1_Carbon_WQHD_VVX14T058J00.icm">ICC profile generated</a> from a 4th generation ThinkPad X1 Carbon. This profile was marginally better than GNOME&rsquo;s default, but it still looked a bit more washed out than what it should be.</p>
<p>I picked up a <a href="http://www.xrite.com/categories/calibration-profiling/colormunki-display">ColorMunki Display</a> and went through a fast calibration in <a href="https://help.gnome.org/users/gnome-help/3.14/color.html.en">GNOME&rsquo;s Color Manager</a>. The low quality run finished in under 10 minutes and the improvement was definitely noticeable. Colors look much deeper and less washed out. The display looks <em>very</em> similar to the previous generation ThinkPad X1 Carbon.</p>
]]></content></item><item><title>Display auditd messages with journalctl</title><link>https://major.io/2017/01/05/display-auditd-messages-with-journalctl/</link><pubDate>Thu, 05 Jan 2017 15:53:13 +0000</pubDate><guid>https://major.io/2017/01/05/display-auditd-messages-with-journalctl/</guid><description>All systems running systemd come with a powerful tool for reviewing the system journal: journalctl. It allows you to get a quick look at the system journal while also allowing you to heavily customize your view of the log.
I logged into a server recently that was having a problem and I found that the audit logs weren&amp;rsquo;t going into syslog. That&amp;rsquo;s no problem - they&amp;rsquo;re in the system journal. The system journal was filled with tons of other messages, so I decided to limit the output only to messages from the auditd unit:</description><content type="html"><![CDATA[<p>All systems running <code>systemd</code> come with a powerful tool for reviewing the system journal: <a href="https://www.freedesktop.org/software/systemd/man/journalctl.html"><code>journalctl</code></a>. It allows you to get a quick look at the system journal while also allowing you to heavily customize your view of the log.</p>
<p>I logged into a server recently that was having a problem and I found that the audit logs weren&rsquo;t going into syslog. That&rsquo;s no problem - they&rsquo;re in the system journal. The system journal was filled with tons of other messages, so I decided to limit the output only to messages from the <code>auditd</code> unit:</p>
<pre><code>$ sudo journalctl -u auditd --boot
-- Logs begin at Thu 2015-11-05 09:20:01 CST, end at Thu 2017-01-05 09:38:49 CST. --
Jan 05 07:47:04 arsenic systemd[1]: Starting Security Auditing Service...
Jan 05 07:47:04 arsenic auditd[937]: Started dispatcher: /sbin/audispd pid: 949
Jan 05 07:47:04 arsenic audispd[949]: priority_boost_parser called with: 4
Jan 05 07:47:04 arsenic audispd[949]: max_restarts_parser called with: 10
Jan 05 07:47:04 arsenic audispd[949]: audispd initialized with q_depth=150 and 1 active plugins
Jan 05 07:47:04 arsenic augenrules[938]: /sbin/augenrules: No change
Jan 05 07:47:04 arsenic augenrules[938]: No rules
Jan 05 07:47:04 arsenic auditd[937]: Init complete, auditd 2.7 listening for events (startup state enable)
Jan 05 07:47:04 arsenic systemd[1]: Started Security Auditing Service.
</code></pre><p>This isn&rsquo;t helpful. I&rsquo;m seeing messages about the <code>auditd</code> daemon itself. I want the actual output from the audit rules.</p>
<p>Then I remembered: the kernel is the one that sends messages about audit rules to the system journal. Let&rsquo;s just look at what&rsquo;s coming from the kernel instead:</p>
<pre><code>$ sudo journalctl -k --boot
-- Logs begin at Thu 2015-11-05 09:20:01 CST, end at Thu 2017-01-05 09:40:44 CST. --
Jan 05 07:46:47 arsenic kernel: Linux version 4.8.15-300.fc25.x86_64 (mockbuild@bkernel01.phx2.fedoraproject.org) (gcc version 6.2.1 20160916 (Red Hat 6.2.1-2
Jan 05 07:46:47 arsenic kernel: Command line: BOOT_IMAGE=/vmlinuz-4.8.15-300.fc25.x86_64 root=/dev/mapper/luks-e... ro rd.luks
Jan 05 07:46:47 arsenic kernel: x86/fpu: Supporting XSAVE feature 0x001: 'x87 floating point registers'
Jan 05 07:46:47 arsenic kernel: x86/fpu: Supporting XSAVE feature 0x002: 'SSE registers'
Jan 05 07:46:47 arsenic kernel: x86/fpu: Supporting XSAVE feature 0x004: 'AVX registers'
Jan 05 07:46:47 arsenic kernel: x86/fpu: xstate_offset[2]:  576, xstate_sizes[2]:  256
</code></pre><p><strong>This is worse!</strong> Luckily, the system journal keeps a lot more data about what it receives than just the text of the log line. We can dig into that extra data with the <code>verbose</code> option:</p>
<pre><code>$ sudo journalctl --boot -o verbose
</code></pre><p>After running that command, search for one of the audit log lines in the output:</p>
<pre><code>_UID=0
_BOOT_ID=...
_MACHINE_ID=...
_HOSTNAME=arsenic
_TRANSPORT=audit
SYSLOG_FACILITY=4
SYSLOG_IDENTIFIER=audit
AUDIT_FIELD_HOSTNAME=?
AUDIT_FIELD_ADDR=?
AUDIT_FIELD_RES=success
_AUDIT_TYPE=1105
AUDIT_FIELD_OP=PAM:session_open
_SELINUX_CONTEXT=unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c1023
_AUDIT_LOGINUID=1000
_AUDIT_SESSION=3
AUDIT_FIELD_ACCT=root
AUDIT_FIELD_EXE=/usr/bin/sudo
AUDIT_FIELD_GRANTORS=pam_keyinit,pam_limits,pam_keyinit,pam_limits,pam_systemd,pam_unix
AUDIT_FIELD_TERMINAL=/dev/pts/4
_PID=2666
_SOURCE_REALTIME_TIMESTAMP=1483631103122000
_AUDIT_ID=385
MESSAGE=USER_START pid=2666 uid=0 auid=1000 ses=3 subj=unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c1023 msg='op=PAM:session_open grantors=pam_keyinit,pam_limits,pam_keyinit,pam_limits,pam_systemd,pam_unix acct=&quot;root&quot; exe=&quot;/usr/bin/sudo&quot; hostname=? addr=? terminal=/dev/pts/4 res=success'
</code></pre><p>One of the identifiers we can use is <code>_TRANSPORT=audit</code>. Let&rsquo;s pass that to <code>journalctl</code> and see what we get:</p>
<pre><code>$ sudo journalctl --boot _TRANSPORT=audit
-- Logs begin at Thu 2015-11-05 09:20:01 CST. --
Jan 05 09:47:24 arsenic audit[3028]: USER_END pid=3028 uid=0 auid=1000 ses=3 subj=unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c1023 msg='op=PAM:session_close grantors=pam_keyinit,pam_limits,pam_keyinit,pam_limits,pam_systemd,pam_unix acct=&quot;root&quot; exe=&quot;/usr/bin/sudo&quot; hostname=? addr=? terminal=/dev/pts/4 res=success'
... more log lines snipped ...
</code></pre><p>Success! You can get live output of the audit logs by tailing the output:</p>
<pre><code>sudo journalctl -af _TRANSPORT=audit
</code></pre><p>For more details on <code>journalctl</code>, refer to the <a href="https://www.freedesktop.org/software/systemd/man/journalctl.html">online documentation</a>.</p>
]]></content></item><item><title>augenrules fails with “rule exists” when loading rules into auditd</title><link>https://major.io/2017/01/03/augenrules-fails-with-rule-exists-when-loading-rules-into-auditd/</link><pubDate>Tue, 03 Jan 2017 19:01:46 +0000</pubDate><guid>https://major.io/2017/01/03/augenrules-fails-with-rule-exists-when-loading-rules-into-auditd/</guid><description>When I came back from the holiday break, I found that the openstack-ansible-security role wasn&amp;rsquo;t passing tests any longer. The Ansible playbook stopped when augenrules ran to load the new audit rules. The error wasn&amp;rsquo;t terribly helpful:
/usr/sbin/augenrules: No change Error sending add rule data request (Rule exists) There was an error in line 5 of /etc/audit/audit.rules A duplicated rule? I&amp;rsquo;ve been working on lots of changes to implement the Red Hat Enterprise Linux 7 Security Technical Implementation Guide (STIG) and I assumed I put in the same rule twice with an errant copy and paste.</description><content type="html"><![CDATA[<p>When I came back from the holiday break, I found that the <a href="https://github.com/openstack/openstack-ansible-security">openstack-ansible-security</a> role wasn&rsquo;t passing tests any longer. The Ansible playbook stopped when <code>augenrules</code> ran to load the new audit rules. The error wasn&rsquo;t terribly helpful:</p>
<pre><code>/usr/sbin/augenrules: No change
Error sending add rule data request (Rule exists)
There was an error in line 5 of /etc/audit/audit.rules
</code></pre><h2 id="a-duplicated-rule">A duplicated rule?</h2>
<p>I&rsquo;ve been <a href="http://specs.openstack.org/openstack/openstack-ansible-specs/specs/newton/security-rhel7-stig.html">working on lots of changes</a> to implement the Red Hat Enterprise Linux 7 <a href="http://iase.disa.mil/stigs/os/unix-linux/Pages/index.aspx">Security Technical Implementation Guide (STIG)</a> and I assumed I put in the same rule twice with an errant copy and paste.</p>
<p>That wasn&rsquo;t the case. I checked the input rule file in <code>/etc/audit/rules.d/</code> and found that all of the rules were unique.</p>
<h2 id="is-something-missing">Is something missing?</h2>
<p>The <code>augenrules</code> command works by taking files from <code>/etc/audit/rules.d/</code> and joining them together into <code>/etc/audit/audit.rules</code>. Based on the output from <code>augenrules</code>, the rule file checks out fine and it determined that the existing rule doesn&rsquo;t need to be updated. However, <code>augenrules</code> is still unable to load the new rules into auditd.</p>
<p>I decided to check the first several lines of <code>/etc/audit/rules.d/</code> to see if line 5 had a problem:</p>
<pre><code>## This file is automatically generated from /etc/audit/rules.d


-f 1
-a always,exit -F path=/usr/bin/chsh -F perm=x -F auid&gt;=1000 -F auid!=4294967295 -k RHEL-07-030525
-a always,exit -F path=/usr/bin/chage -F perm=x -F auid&gt;=1000 -F auid!=4294967295 -k RHEL-07-030513
</code></pre><p>Two things looked strange to me:</p>
<ul>
<li>Line 5 is correct and it is unique</li>
<li>Why are lines 2 and 3 blank?</li>
</ul>
<p>I checked another CentOS 7 server and found the following in lines 2 and 3:</p>
<pre><code>-D
-b 320
</code></pre><p>The <code>-D</code> <a href="https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/6/html/Security_Guide/sec-Defining_Audit_Rules_and_Controls.html">deletes all previously loaded rules</a> and <code>-b</code> increases the buffer size for busy periods. My rules weren&rsquo;t loading properly because the <code>-D</code> was missing! Those two lines normally come from <code>/etc/audit/rules.d/audit.rules</code>, but that default file was not present.</p>
<p>Here&rsquo;s what was going wrong:</p>
<ul>
<li><code>augenrules</code> read rules from <code>rules.d/</code></li>
<li><code>augenrules</code> found that the rules in <code>rules.d/</code> were already in the main <code>audit.rules</code> file and didn&rsquo;t need to be updated</li>
<li><code>augenrules</code> attempted to load the rules into <code>auditd</code>, but that failed</li>
<li><code>auditd</code> was rejecting the rules because at least one of them (line 5) already existed in the running rule set</li>
</ul>
<p>All of this happened because the <code>-D</code> wasn&rsquo;t handled first before new rules were loaded.</p>
<h2 id="fixing-it">Fixing it</h2>
<p>I decided to add the <code>-D</code> line explicitly in my rules file within <code>rules.d/</code> to catch those situations when the <code>audit.rules</code> default file is missing. The <code>augenrules</code> command ensures that the line appears at the top of the rules when they are loaded into <code>auditd</code>.</p>
]]></content></item><item><title>OpenPOWER Summit Europe 2016 Recap</title><link>https://major.io/2016/11/04/openpower-summit-europe-2016-recap/</link><pubDate>Fri, 04 Nov 2016 14:07:17 +0000</pubDate><guid>https://major.io/2016/11/04/openpower-summit-europe-2016-recap/</guid><description>I was in Barcelona last week for two big events: the OpenStack Summit and the OpenPOWER Summit. Luckily, the events were separated only by a five minute walk. Many of the slides from the OpenPOWER Summit are already available online.
One of my favorite talks was from Prof. Mateo Valero, the director of the Barcelona Supercomputer Center (Centro Nacional de Supercomputación). He was a great speaker and he talked a lot about how OpenPOWER has given them a new edge.</description><content type="html"><![CDATA[<p><img src="/wp-content/uploads/2016/11/barcelona_morning.jpg" alt="1"></p>
<p>I was in Barcelona last week for two big events: the <a href="https://www.openstack.org/summit/barcelona-2016/">OpenStack Summit</a> and the <a href="https://openpowerfoundation.org/openpower-summit-europe/">OpenPOWER Summit</a>. Luckily, the events were separated only by a five minute walk. Many of the slides from the OpenPOWER Summit are already available <a href="https://openpowerfoundation.org/openpower-summit-europe/">online</a>.</p>
<p>One of my <a href="https://openpowerfoundation.org/wp-content/uploads/2016/10/3-Mateo-Barcelona-SuperComputing-Center.pdf">favorite talks</a> was from Prof. Mateo Valero, the director of the Barcelona Supercomputer Center (Centro Nacional de Supercomputación). He was a great speaker and he talked a lot about how OpenPOWER has given them a new edge. It&rsquo;s part of what helps them stay on the forefront of supercomputer technology.</p>
<p>On Friday, I was honored to be on a panel with some other leaders in the OpenPOWER community:</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>We talked about what makes open source a game changer in our businesses and our worlds. I talked about how open source software and hardware gives everyone a seat at the table. Every seat is a little different - some people offer software improvements, some offer use cases, and others do real-world testing. Every seat has intrinsic value in the process and everyone has a say in where the community goes.</p>
<p>Open source also provides mobility in multiple ways. For businesses, open source allows them to stand on the shoulders of others and make meaningful improvements in their product much more quickly. Open source projects also allows individual people to increase their skills, learn patterns from others, and level up their capabilities.</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>Several vendors were showing off technology during the reception on Thursday night and I had the opportunity to talk with <a href="https://twitter.com/robtaylor78">Rob Taylor</a> from <a href="https://reconfigure.io/">reconfigure.io</a>:</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>They&rsquo;re doing some amazing work with accelerators that makes them more approachable for developers. Learning <a href="https://en.wikipedia.org/wiki/VHDL">VHDL</a> isn&rsquo;t terribly easy and developers prefer to work in the languages that they&rsquo;re using to build their applications. The folks at Reconfigure.io have software that allows a developer to write software in C, C++, or Go and then translate certain pieces of that code to VHDL. This makes accelerators more approachable for more developers and reduces the cycle time for improvements.</p>
<p>The conference was fairly small, but it was nice to get extra face time with developers, executives, and partners in the OpenPOWER market. I&rsquo;ll be looking forward to the next wave of developments in accelerators and Power 9!</p>
]]></content></item><item><title>Talk Recap: Holistic Security for OpenStack Clouds</title><link>https://major.io/2016/10/31/talk-recap-holistic-security-for-openstack-clouds/</link><pubDate>Mon, 31 Oct 2016 15:52:47 +0000</pubDate><guid>https://major.io/2016/10/31/talk-recap-holistic-security-for-openstack-clouds/</guid><description>Thanks to everyone who attended my talk at the OpenStack Summit in Barcelona! I really enjoyed sharing some tips with the audience and it was great to meet some attendees in person afterwards.
If you weren&amp;rsquo;t able to make it, don&amp;rsquo;t fret! This post will cover some of the main points of the talk and link to the video and slides.
Purpose OpenStack clouds are inherently complex. Operating a cloud involves a lot of moving pieces in software, hardware, and networking.</description><content type="html"><![CDATA[<p><img src="/wp-content/uploads/2016/10/Holistic-Security-for-OpenStack-Clouds-OpenStack-Summit-Barcelona-2.png" alt="1"></p>
<p>Thanks to everyone who attended my talk at the OpenStack Summit in Barcelona! I really enjoyed sharing some tips with the audience and it was great to meet some attendees in person afterwards.</p>
<p>If you weren&rsquo;t able to make it, don&rsquo;t fret! This post will cover some of the main points of the talk and link to the video and slides.</p>
<h2 id="purpose">Purpose</h2>
<p>OpenStack clouds are inherently complex. Operating a cloud involves a lot of moving pieces in software, hardware, and networking. Securing complex systems can be a real challenge, especially for newcomers to the information security realm. One wrong turn can knock critical infrastructure online or lead to lengthy debugging sessions.</p>
<p>However, securing OpenStack clouds doesn&rsquo;t need to be a tremendously stressful experience. It requires a methodical, thoughful, and strategic approach. The goal of the talk is give the audience a reliable strategy that is easy to start with and that scales easily over time.</p>
<h2 id="why-holistic">Why holistic?</h2>
<p>The dictionary definition of <em>holistic</em> is:</p>
<blockquote>
<p>characterized by comprehension of the parts of something as intimately connected and explicable only by reference to the whole</p>
</blockquote>
<p>To simplify things a bit, thinking about something holistically means that you understand that there are small parts that are valuable on their own, but they make much more value when combined together. Also, it&rsquo;s difficult to talk about the individual parts and get a real understanding of the whole.</p>
<p>In holistic medicine, humans are considered to be a body, mind, and spirit. OpenStack clouds involve servers, software, and a business goal. Security consists of people, process, and technology. To truly understand what&rsquo;s going on, you need to take a look at something with all of its parts connected.</p>
<h2 id="security-refresher">Security refresher</h2>
<p>Get into the mindset that <strong>attackers will get in eventually</strong>. Just change each instance of <em>if</em> to <em>when</em> in your conversations. Attackers can be wrong many times, but the defenders only need to be wrong once to allow a breach to occur.</p>
<p>Simply building a huge moat and tall castle walls around the outside isn&rsquo;t sufficient. Attackers will have free reign to move around inside and take what they want. Multiple layers are needed, and this is the backbone of a <em>defense-in-depth</em> strategy.</p>
<p>Cloud operators need to work from the outside inwards, much like you do with utensils at a fancy dinner. Make a good wall around the outside and work on tightening down the internals at multiple levels.</p>
<h2 id="four-layers-for-openstack">Four layers for OpenStack</h2>
<p>During the talk, I divided OpenStack clouds into four main layers:</p>
<ul>
<li>outer perimeter</li>
<li>control and data planes</li>
<li>OpenStack services and backend services in the control plane</li>
<li>OpenStack services</li>
</ul>
<p>For the best explanation of what to do at this level, I highly recommend reviewing the slides or the presentation video (keep scrolling).</p>
<h2 id="links-and-downloads">Links and downloads</h2>
<p>The slides are on <a href="http://www.slideshare.net/MajorHayden/holistic-security-for-openstack-clouds">SlideShare</a> and they are licensed <a href="https://creativecommons.org/licenses/by-sa/2.0/?">CC-BY-SA</a>. Feel free to share anything from the slide deck as you wish, but please share it via a similar license and attribute the source!</p>
<p>The video of the talk (including Q&amp;A) is up on YouTube:</p>
<p><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --></p>
<h2 id="feedback">Feedback</h2>
<p>I love feedback about my talks! I&rsquo;m still a novice presenter and every little bit of feedback - positive or negative - really helps. Feel free to <a href="mailto:major@mhtx.net">email me</a> or talk to me on <a href="https://twitter.com/majorhayden">Twitter</a>.</p>
]]></content></item><item><title>Why should students learn to write code?</title><link>https://major.io/2016/10/10/why-should-students-learn-to-write-code/</link><pubDate>Tue, 11 Oct 2016 04:08:19 +0000</pubDate><guid>https://major.io/2016/10/10/why-should-students-learn-to-write-code/</guid><description>There are lots of efforts underway to get students (young and old) to learn to write code. There are far-reaching efforts, like the Hour of Code, and plenty of smaller, more focused projects, such as the Design and Technology Academy (part of Northeast ISD here in San Antonio, Texas). These are excellent programs that enrich the education of many students.
I often hear a question from various people about these programs:</description><content type="html"><![CDATA[<p><img src="/wp-content/uploads/2016/10/computers-332238_1920-e1476155887912.jpg" alt="1"></p>
<p>There are lots of efforts underway to get students (young and old) to learn to write code. There are far-reaching efforts, like the <a href="https://code.org/learn">Hour of Code</a>, and plenty of smaller, more focused projects, such as the <a href="http://www.neisd.net/dataHS/">Design and Technology Academy</a> (part of <a href="http://www.neisd.net/">Northeast ISD</a> here in San Antonio, Texas). These are excellent programs that enrich the education of many students.</p>
<p>I often hear a question from various people about these programs:</p>
<blockquote>
<p>Why should a student learn to write code if they aren&rsquo;t going to become a software developer or IT professional?</p>
</blockquote>
<p>It&rsquo;s a completely legitimate question and I hope to provide a helpful response in this post.</p>
<h2 id="some-students-will-actually-enter-the-it-field">Some students will actually enter the IT field</h2>
<p>This may seem obvious, but it&rsquo;s important to note that many students may choose to enter the IT field. They may not become full-time software developers, but the experience is useful for many different IT jobs. For example, knowing some basic principles about how software works is critical for system administrators, network administrators, project managers, and people managers. These skills could give students an edge later in their IT career.</p>
<h2 id="students-learn-to-measure-twice-and-cut-once">Students learn to measure twice and cut once</h2>
<p>The concept of thorough planning before execution shows up in many different fields. You can find it in general engineering, architecture, medicine, and criminal justice. A failure to plan often leads to bigger challenges down the line.</p>
<p>In software development, planning is key. <em>Where will you store your data? How much data will you need to store? Does the user need a graphical interface? What if the amount of users on the system increases by a factor of ten? How do we keep the system secure?</em></p>
<p>I talk to computer science students at <a href="http://www.utsa.edu/">UTSA</a> on a regular basis. One of their most challenging courses involves a group project where the students must build a fully functional application that solves a need. The students run head-first into a myriad of problems during development. They almost always learn that it&rsquo;s much easier to talk through how everything will fit together before they write one line of software.</p>
<h2 id="students-learn-to-think-logically">Students learn to think logically</h2>
<p>Dale Carnegie said:</p>
<blockquote>
<p>When dealing with people, remember you are not dealing with creatures of logic, but creatures of emotion.</p>
</blockquote>
<p>Humans are swayed easily by emotional appeals and logic is often tossed aside. Computers are the total opposite. You can scream for joy, cry uncontrollably, or yell angrily at a computer and it keeps doing the same thing. It doesn&rsquo;t understand that you just told it to eat your data. It doesn&rsquo;t understand that you can&rsquo;t figure out why an error is occurring.</p>
<p>Students can learn a lot from the way computers work. As one of my journalism professors said in school:</p>
<blockquote>
<p>You put garbage in? You&rsquo;ll get garbage out.</p>
</blockquote>
<p>Students will learn to be explicit in their instructions and contemplate how to handle treacherous situations in software. Some smaller failures might result in an error or a poor user experience. Others could result in a buffer overflow that leads to a costly security breach. In the end, computers can only do what they&rsquo;re told and it&rsquo;s up to the developer to tell to the computer - in the computer&rsquo;s language - in all of these situations.</p>
<p>Outside of the world of computers, learning to be explicit has its benefits. It reduces confusion in conversations, leads to better results in group projects, and it encourage students to structure their thoughts into more organized communication.</p>
<h2 id="theres-more-to-it-than-writing-code">There&rsquo;s more to IT than writing code</h2>
<p>Software doesn&rsquo;t run without computer hardware. We live in the age of easily accessible, inexpensive cloud infrastructure where you can have a server online in seconds, but it&rsquo;s still someone&rsquo;s computer. Within large businesses, software developers are often asked to justify the resources they need to deploy their applications.</p>
<p>There is obviously some technical knowledge involved here, especially around the topic of sizing a server for a particular software workload. However, there are plenty of non-technical questions to ask.</p>
<p><em>Can we re-use depreciated hardware to reduce capital expenditures? Is our datacenter space limited by power, space, or something else? Can we select a lower-wattage server to reduce power consumption (if that&rsquo;s the bigger expense)? Will ordering in a larger batch allow us to drive down the hardware cost?</em></p>
<p>Software developers wield significant power if they can use their technical skills to branch into the world of accounting, finance, and basic engineering. A well-rounded approach could also allow developers to get more hardware than they planned to get if they make the purchases in a smarter way.</p>
<h2 id="basic-understanding-of-computers-is-useful">Basic understanding of computers is useful</h2>
<p>Almost every technical person has fielded that awkward question from a family member at a gathering or during the holidays:</p>
<blockquote>
<p>I heard you do computer stuff? I think I have a virus - could you look at it?</p>
</blockquote>
<p>All students should have a basic understanding of how a computer works, even if they never write software or work in IT. This knowledge helps keep all of us a bit safer online and it helps to diagnose some issues before they become a serious problem. Approaching computers with an observant and inquisitive mind will reduce security breaches and increase confidence. We will also flood the market with people who can teach others the basics about their technology</p>
<h2 id="summary">Summary</h2>
<p>All students could learn some important life lessons simply from learning how to write some code. Does this mean that all students must write some software before they graduate? Definitely not.</p>
<p>Many of the lessons learned from writing software will easily transfer into other fields and disciplines. Gaining these skills is a lot like learning a foreign language. If you use them frequently, you&rsquo;ll have a strong foundation of knowledge to build upon over time. Even if you don&rsquo;t use them frequently, they could give you that small edge that you need later on in your professional career.</p>
<p><em>Photo credit: <a href="https://pixabay.com/p-332238/?no_redirect">Pixabay</a></em></p>
]]></content></item><item><title>What’s Happening in OpenStack-Ansible (WHOA) – September 2016</title><link>https://major.io/2016/09/30/whats-happening-in-openstack-ansible-whoa-september-2016/</link><pubDate>Fri, 30 Sep 2016 14:33:33 +0000</pubDate><guid>https://major.io/2016/09/30/whats-happening-in-openstack-ansible-whoa-september-2016/</guid><description>Welcome to the fourth post in the series of What&amp;rsquo;s Happening in OpenStack-Ansible (WHOA) posts that I&amp;rsquo;m assembling each month. OpenStack-Ansible is a flexible framework for deploying enterprise-grade OpenStack clouds. In fact, I use OpenStack-Ansible to deploy the OpenStack cloud underneath the virtual machine that runs this blog!
My goal with these posts is to inform more people about what we&amp;rsquo;re doing in the OpenStack-Ansible community and bring on more contributors to the project.</description><content type="html"><![CDATA[<p><img src="/wp-content/uploads/2011/11/openstack-justheo.png" alt="1"></p>
<p>Welcome to the fourth post in the series of <a href="/tags/whoa/">What&rsquo;s Happening in OpenStack-Ansible (WHOA)</a> posts that I&rsquo;m assembling each month. <a href="https://wiki.openstack.org/wiki/OpenStackAnsible">OpenStack-Ansible</a> is a flexible framework for deploying enterprise-grade <a href="http://openstack.org">OpenStack</a> clouds. In fact, I use OpenStack-Ansible to deploy the OpenStack cloud underneath the virtual machine that runs this blog!</p>
<p>My goal with these posts is to inform more people about what we&rsquo;re doing in the OpenStack-Ansible community and bring on more contributors to the project.</p>
<p>There are plenty of updates since the <a href="/2016/08/23/whats-happening-in-openstack-ansible-whoa-august-2016/">last post from August</a>. The race is on to finish up the Newton release and start new developments for the Ocata release! We hope to see lots of contributors in Barcelona!</p>
<p><img src="/wp-content/uploads/2016/09/1024px-Arc_de_Triomf_Barcelona.jpg" alt="6"></p>
<h2 id="new-releases">New releases</h2>
<p>The OpenStack-Ansible releases are announced on the OpenStack development mailing list. Here are the things you need to know:</p>
<h3 id="newton">Newton</h3>
<p>The OpenStack-Ansible Newton release is still being finalized this week. The <code>stable/newton</code> branches were <a href="https://review.openstack.org/#/c/379590/">created yesterday</a> and stabilization work is ongoing.</p>
<h3 id="mitaka">Mitaka</h3>
<p>Mitaka is the latest stable release available and the latest version is 13.3.4. This release includes some upgrade improvements for <a href="https://review.openstack.org/#/c/366169/">ml2 ports</a> and <a href="https://review.openstack.org/#/c/360539/">container hostname adjustments</a>.</p>
<ul>
<li><a href="https://review.openstack.org/379499">Version 13.3.4</a></li>
<li><a href="http://docs.openstack.org/releasenotes/openstack-ansible/mitaka.html">Release notes</a></li>
<li><a href="https://gist.github.com/anonymous/3880b24f381d7d4d31be036ef820c21e">Detailed changelog</a> <em>(from 13.3.1 to 13.3.4)</em></li>
</ul>
<h3 id="liberty">Liberty</h3>
<p>The latest Liberty release, 12.2.4, contains lots of updates and fixes. The updates include a <a href="https://review.openstack.org/#/c/360385/">fix for picking up where you left off on a failed upgrade</a> and a <a href="https://review.openstack.org/#/c/368066/">fix for duplicated log lines</a>. The security role received <a href="https://gist.github.com/anonymous/9fe1f5110e2d8f7c1d148d4b6968b5a9#openstack-ansible-security">some updates</a> to improve performance and reduce unnecessary logging.</p>
<ul>
<li><a href="https://review.openstack.org/379505/">Version 12.2.4</a></li>
<li><a href="http://docs.openstack.org/releasenotes/openstack-ansible/liberty.html">Release notes</a></li>
<li><a href="https://gist.github.com/anonymous/9fe1f5110e2d8f7c1d148d4b6968b5a9">Detailed changelog</a> <em>(from 12.2.1 to 12.2.4)</em></li>
</ul>
<h2 id="notable-discussions">Notable discussions</h2>
<p>This section covers discussions from the OpenStack-Ansible weekly meetings, IRC channels, mailing lists, or in-person events.</p>
<h3 id="newton-branch">Newton branch</h3>
<p>As mentioned earlier, the <code>stable/newton</code> branches have arrived for OpenStack-Ansible! This will allow us to finish stabilizing the Newton release and look ahead to Ocata.</p>
<h3 id="octavia">Octavia</h3>
<p>Michael Johnson and Jorge Miramontes stopped by our weekly meeting to talk about how Octavia could be implemented in OpenStack-Ansible. Recent Octavia releases have some new features that should be valuable to OpenStack-Ansible deployers.</p>
<p>There is a <a href="http://specs.openstack.org/openstack/openstack-ansible-specs/specs/mitaka/lbaasv2.html">spec from the Liberty release</a> for deploying Octavia, but we were only able to get LBaaSv2 with the agent deployed. Jorge and Michael are working on a new spec to get Octavia deployed with OpenStack-Ansible.</p>
<h3 id="testing-repo">Testing repo</h3>
<p>There&rsquo;s now a <a href="https://github.com/openstack/openstack-ansible-tests">centralized testing repository</a> for all OpenStack-Ansible roles. This allows the developers to share variables, scripts, and test cases between multiple roles. Developers can begin testing new roles with much less effort since the scaffolding for a basic test environment is available in the repository.</p>
<p>You can follow along with the development by watching the <a href="https://review.openstack.org/#/q/topic:central-test-config">central-test-config</a> topic in Gerrit.</p>
<h3 id="mailing-list">Mailing list</h3>
<p>The OpenStack-Ansible tag was fairly quiet on the OpenStack Development mailing list during the time frame of this report, but there were a few threads:</p>
<ul>
<li><a href="http://lists.openstack.org/pipermail/openstack-dev/2016-September/103671.html">cinder volume lxc and iscsi</a></li>
<li><a href="http://lists.openstack.org/pipermail/openstack-dev/2016-September/104813.html">Blueprint discussion</a> <em>(for the Ocata OpenStack Summit)</em></li>
<li><a href="http://lists.openstack.org/pipermail/openstack-dev/2016-September/104807.html">Newton RC2 available</a></li>
</ul>
<h2 id="notable-developments">Notable developments</h2>
<p>This section covers some of the improvements coming to Newton, the upcoming OpenStack release.</p>
<h3 id="openstack-ansible-training">OpenStack-Ansible Training</h3>
<p><a href="/wp-content/uploads/2016/09/hastexo-logo-e1475245310720.png"><!-- raw HTML omitted --></a><a href="https://www.hastexo.com/">Hastexo</a> is now offering a <a href="https://www.hastexo.com/resources/news-releases/hx201-ansible">self-paced course</a> for learning OpenStack-Ansible! It&rsquo;s called HX201 Cloud Fundamentals for OpenStack-Ansible and it is available now.</p>
<p>Thanks to Florian Haas and Adolfo Brandes for assembling this course!</p>
<h3 id="openstack-ansible-powers-the-osic-cloud">OpenStack-Ansible powers the OSIC cloud</h3>
<p><a href="/wp-content/uploads/2016/09/OSIC-Logo.png"><!-- raw HTML omitted --></a>One of the clouds operated by the <a href="https://osic.org/">OpenStack Innovation Center (OSIC)</a> is powered by OpenStack-Ansible. It&rsquo;s a dual-stack (IPv4 and IPv6) environment and it provides the most nodes for the OpenStack CI service! If you need to test an application on a large OpenStack cloud, <a href="https://osic.org/clusters">apply for access</a> to the OSIC cluster.</p>
<h3 id="inventory-improvements">Inventory improvements</h3>
<p>The backbone of OpenStack-Ansible is its inventory. The dynamic inventory defines where each service should be deployed, configured and managed. Some recent improvements include <a href="https://review.openstack.org/#/c/371798/">exporting inventory</a> for use by other scripts or applications. Ocata should bring even more improvements to the dynamic inventory.</p>
<p>Thanks to Nolan Brubaker for leading this effort!</p>
<h3 id="install-guide">Install guide</h3>
<p>The installation guide has been completely overhauled! It has a more concise, opinionated approach to deployments and this should make the first deployment a little easier for newcomers. OpenStack can be a complex system to deploy and our goal is to provide the cleanest path to a successful deployment.</p>
<p>Thanks to Alex Settle for leading this effort!</p>
<h2 id="feedback">Feedback?</h2>
<p>The goal of this newsletter is three fold:</p>
<ul>
<li>
<p>Keep OpenStack-Ansible developers updated with new changes</p>
</li>
<li>
<p>Inform operators about new features, fixes, and long-term goals</p>
</li>
<li>
<p>Bring more people into the OpenStack-Ansible community to share their use</p>
<p>cases, bugs, and code</p>
</li>
</ul>
<p>Please let me know if you spot any errors, areas for improvement, or items that I missed altogether. I&rsquo;m <code>mhayden</code> on Freenode IRC and you can find me on <a href="https://twitter.com/majorhayden">Twitter</a> anytime.</p>
<p><em>Photo credit: Mattia Felice Palermo (Own work) <a href="http://creativecommons.org/licenses/by-sa/3.0/es/deed.en">CC BY-SA 3.0 es</a>, via Wikimedia Commons</em></p>
]]></content></item><item><title>Power 8 to the people</title><link>https://major.io/2016/09/21/power-8-to-the-people/</link><pubDate>Thu, 22 Sep 2016 00:00:21 +0000</pubDate><guid>https://major.io/2016/09/21/power-8-to-the-people/</guid><description>IBM Edge 2016 is almost over and I&amp;rsquo;ve learned a lot about Power 8 this week. I&amp;rsquo;ve talked about some of the learnings in my recaps of days one and two. The performance arguments sound really interesting and some of the choices in AIX&amp;rsquo;s design seem to make a lot of sense.
However, there&amp;rsquo;s one remaining barrier for me: Power 8 isn&amp;rsquo;t really accessible for a tinkerer.
Tinkering? Google defines tinkering as:</description><content type="html"><![CDATA[<p>IBM Edge 2016 is almost over and I&rsquo;ve learned a lot about Power 8 this week. I&rsquo;ve talked about some of the learnings in my recaps of days <a href="/2016/09/20/ibm-edge-2016-day-1-recap/">one</a> and <a href="https://major.io/2016/09/21/ibm-edge-2016-day-2-recap/">two</a>. The performance arguments sound really interesting and some of the choices in AIX&rsquo;s design seem to make a lot of sense.</p>
<p>However, there&rsquo;s one remaining barrier for me: <strong>Power 8 isn&rsquo;t really accessible for a tinkerer</strong>.</p>
<p><img src="/wp-content/uploads/2016/09/IBMPowerSystemE870-2.jpg" alt="3"></p>
<h2 id="tinkering">Tinkering?</h2>
<p>Google defines <em>tinkering</em> as:</p>
<blockquote>
<p>attempt to repair or improve something in a casual or desultory way,</p>
</blockquote>
<blockquote>
<p>often to no useful effect.</p>
</blockquote>
<blockquote>
<p>&ldquo;he spent hours tinkering with the car&rdquo;</p>
</blockquote>
<p>When I come across a new piece of technology, I really enjoy learning how it works. I like to find its strengths and its limitations. I use that information to figure out how I might use the technology later and when I would recommend the technology for someone else to use it.</p>
<p>To me, tinkering is simply messing around with something until I have a better understanding of how it works. Tinkering doesn&rsquo;t have a finish line. Tinkering may not have a well-defined goal. However, it&rsquo;s tinkering that leads to a more robust community around a particular technology.</p>
<p>For example, take a look at the Raspberry Pi. There were plenty of other ARM systems on the market before the Pi and there are still a lot of them now. What makes the Pi different is that it&rsquo;s highly accessible. You can get the newest model for $35 and there are tons of guides for running various operating systems on it. There are even more guides for how to integrate it with other items, such as sprinkler systems, webcams, door locks, and automobiles.</p>
<p>Another example is the Intel NUC. Although the NUC isn&rsquo;t the most cost-effective way to get an Intel chip on your desk, it&rsquo;s powerful enough to be a small portable server that you can take with you. This opens up the door for software developers to test code wherever they are (we use them for OpenStack development), run demos at a customer location, or make multi-node clusters that fit in a laptop bag.</p>
<h3 id="what-makes-power-8-inaccessible-to-tinkerers">What makes Power 8 inaccessible to tinkerers?</h3>
<p>One of the first aspects that most people notice is the cost. The <a href="http://www-03.ibm.com/systems/power/hardware/s821lc/index.html">S821LC</a> currently starts at around $6,000 on IBM&rsquo;s site, which is a bit steep for someone who wants to learn a platform.</p>
<p>I&rsquo;m not saying this server should cost less - the pricing seems quite reasonable when you consider that it comes with dual 8-core Power 8 processors in a 1U form factor. It also has plenty of high speed interconnects ready for GPUs and CAPI chips. With all of that considered, $6,000 for a server like this <strong>sounds very reasonable</strong>.</p>
<p>There are other considerations as well. A stripped down S821LC with two 8-core CPUs will <a href="http://www-912.ibm.com/see/EnergyEstimator">consume about 406 Watts at 50% utilization</a>. That&rsquo;s a fair amount of power draw for a tinkerer and I&rsquo;d definitely think twice about running something like that at home. When you consider the cooling that&rsquo;s required, it&rsquo;s even more difficult to justify.</p>
<h3 id="what-about-aix">What about AIX?</h3>
<p>AIX provides some nice benefits on Power 8 systems, but it&rsquo;s difficult to access as well. Put &ldquo;learning AIX&rdquo; into a Google search and <a href="https://www.google.com/search?q=learning+AIX">look at the results</a>. The first link is a <a href="http://www.linuxquestions.org/questions/aix-43/cheapest-way-to-learn-aix-4175534982/">thread on LinuxQuestions.org</a> where the original poster is given a few options:</p>
<ul>
<li>Buy some IBM hardware</li>
<li>Get in some legal/EULA gray areas with VMware</li>
<li>Find an old Power 5/6 server that is coming offline at a business that is doing a refresh</li>
</ul>
<p>Having access to AIX is definitely useful for tinkering, but it could be very useful for software developers. For example, if I write a script in Python and I want to add AIX support, I&rsquo;ll need access to a system running AIX. It wouldn&rsquo;t necessarily need to be a system with tons of performance, but it would need the functionality of a basic AIX environment.</p>
<h2 id="potential-solutions">Potential solutions</h2>
<p>I&rsquo;d suggest two solutions:</p>
<ol>
<li>Get AIX into an accessible format, perhaps on a public cloud</li>
<li>Make a more tinker-friendly Power 8 hardware platform</li>
</ol>
<p>Let&rsquo;s start with AIX. I&rsquo;d gladly work with AIX in a public cloud environment where I pay some amount for the virtual machine itself plus additional licensing for AIX. It would still be valuable even if the version of AIX had limiters so that it couldn&rsquo;t be used for production workloads. I would be able to access the full functionality of a running AIX environment.</p>
<p>The hardware side leads to challenges. However, if it&rsquo;s possible to do a single Power 8 SMT2 CPU in a smaller form factor, this could become possible. Perhaps these could even be CPUs with some type of defect where one or more cores are disabled. That could reduce cost while still providing the full functionality to someone who wants to tinker with Power 8.</p>
<p>Some might argue that this defeats the point of Power 8 since it&rsquo;s a high performance, purpose-built chip that crunches through some of the world&rsquo;s biggest workloads. That&rsquo;s a totally valid argument.</p>
<p>However, that&rsquo;s not the point.</p>
<p>The point is to get a fully-functional Power 8 CPU - even if it has serious performance limitations - into the hands of developers who want to do amazing things with it. My hope would be that these small tests will later turn into new ways to utilize POWER systems.</p>
<p>It could also be a way for more system administrators and developers to get experience with AIX. Companies would be able to find more people with a base level of AIX knowledge as well.</p>
<h2 id="final-thoughts">Final thoughts</h2>
<p>IBM has something truly unique with Power 8. The raw performance of the chip itself is great and the door is open for even more performance through NVlink and CAPI accelerators. These features are game changers for businesses that are struggling to keep up with customer demands. A wider audience could learn about this game-changing technology if it becomes more accessible for tinkering.</p>
<p><em>Photo credit: <a href="https://commons.wikimedia.org/wiki/File:IBMPowerSystemE870-2.jpg">Wikipedia</a></em></p>
]]></content></item><item><title>IBM Edge 2016: Day 2 Recap</title><link>https://major.io/2016/09/21/ibm-edge-2016-day-2-recap/</link><pubDate>Wed, 21 Sep 2016 17:27:53 +0000</pubDate><guid>https://major.io/2016/09/21/ibm-edge-2016-day-2-recap/</guid><description>Day two of IBM Edge 2016 is all done, and the focus has shifted to the individual. Let&amp;rsquo;s get right to the recap:
General session One of the more memorable talks during the general session was Hortonworks. They&amp;rsquo;ve helped a transport company do more than simply track drivers. They assemble and analyze lots of information about each driver, the truck, the current road conditions, and other factors. From there, they apply a risk rating to that particular truck and provide updates to the driver about potential hazards.</description><content type="html"><![CDATA[<p>Day two of <a href="http://www-03.ibm.com/systems/edge/">IBM Edge 2016</a> is all done, and the focus has shifted to the individual. Let&rsquo;s get right to the recap:</p>
<h1 id="general-session">General session</h1>
<p>One of the more memorable talks during the general session was Hortonworks. They&rsquo;ve helped a transport company do more than simply track drivers. They assemble and analyze lots of information about each driver, the truck, the current road conditions, and other factors. From there, they apply a risk rating to that particular truck and provide updates to the driver about potential hazards. It reduced their insurance costs by 10%.</p>
<p>Florida Blue shared some insights from their POWER deployments and how they were able to get customers serviced faster. One of the more memorable quotes was:</p>
<blockquote>
<p>The best way to get a customer happy is to get them off the phone.</p>
</blockquote>
<p>They were able to rework how the backend systems retrieved data for their customer service personnel and cut average phone call durations from 9 minutes to 6.</p>
<p><a href="https://twitter.com/jason_pontin">Jason Pontin</a> came on stage with three technology innovators under 35. They shared some of their latest work with the audience and it was amazing to see the problems they&rsquo;re trying to solve. Lisa DeLuca introduced her new children&rsquo;s book that helps to explain technology in new ways:</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<h1 id="breakouts">Breakouts</h1>
<p>My first breakout session was Getting Started with Linux Performance on IBM POWER8 from Steve Nasypany. This was a highly informative session and you&rsquo;ll definitely want to grab the slides from this talk whether you use POWER or not.</p>
<p>Steve dove into how to measure and adjust performance on POWER systems. He also gave some insight on how AIX and Linux differ when it comes to performance measurements. There are quite a few differences in how AIX and Linux refer to processors and how they measure memory usage. He took quite a bit of time to explain not only the what, but the why. It was a great session.</p>
<p>My second breakout was Bringing the Deep Learning Revolution into the Enterprise from Michael Gschwind. He kicked off with the basics of machine learning and how it matches up with the functions of a human brain. He provided some examples of objects that the human brain can quickly identify but a computer cannot.</p>
<p>The math is deep. Really deep. One of the interesting topics was <a href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent">stochastic gradient descent</a> (warning: highly nerdy territory). It measures how well the computer has been trained on a particular machine learning task. The goal is to reduce errors and do less brute-force training with the computer so it can begin working independently. It&rsquo;s oddly similar to raising children.</p>
<h1 id="thecube-interview">theCUBE interview</h1>
<p>My breakouts were cut a little short because I was invited to be on <a href="http://siliconangle.tv/">theCUBE</a>! It was completely nerve-wracking, but I had a great time. The hosts were fun to work with and the conversation seemed to flow quite well.</p>
<p>We talked about OpenStack, OpenPOWER, and Rackspace. You can watch my interview below if you can put up with my Texas accent:</p>
<p><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --></p>
<h1 id="train-concert">Train concert</h1>
<p>We headed outside in the evening for a poolside reception. The weather was in the 80&rsquo;s and it felt great outside!</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>Everyone made their way inside to see <a href="https://twitter.com/train">Train</a> perform live!</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>The concert was great. They played plenty of their older hits and shared a new single that hasn&rsquo;t been released yet. We even heard some covers of Led Zeppelin and Rolling Stones songs! Some attendees were dragged up on stage to help with the singing and they loved it.</p>
]]></content></item><item><title>IBM Edge 2016: Day 1 Recap</title><link>https://major.io/2016/09/20/ibm-edge-2016-day-1-recap/</link><pubDate>Tue, 20 Sep 2016 20:19:48 +0000</pubDate><guid>https://major.io/2016/09/20/ibm-edge-2016-day-1-recap/</guid><description>I am here in Las Vegas for IBM Edge 2016 to learn about the latest developments in POWER, machine learning, and OpenStack. It isn&amp;rsquo;t just about learning - I&amp;rsquo;m sharing some of our own use cases and challenges from my daily work at Rackspace.
I kicked off the day with a great run down the Las Vegas strip. There are many more staircases and escalators than I remember, but it was still a fun run.</description><content type="html"><![CDATA[<p><a href="/wp-content/uploads/2016/09/IMG_20160919_090354077_TOP-e1474402617332.jpg"><!-- raw HTML omitted --></a>I am here in Las Vegas for <a href="http://www-03.ibm.com/systems/edge/">IBM Edge 2016</a> to learn about the latest developments in POWER, machine learning, and OpenStack. It isn&rsquo;t just about learning - I&rsquo;m sharing some of our own use cases and challenges from my daily work at <a href="http://www.rackspace.com/">Rackspace</a>.</p>
<p>I kicked off the day with a great run down the Las Vegas strip. There are many more staircases and escalators than I remember, but it was still a fun run. The sunrise was awesome to watch as all of the colors began to change:</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>Without further ado, let&rsquo;s get to the recap.</p>
<h2 id="general-session">General Session</h2>
<p>Two of the talks in the general session caught my attention: OpenPOWER and Red Bull Racing.</p>
<p>The OpenPOWER talk showcased the growth in the OpenPOWER ecosystem. It started with only five members, but it&rsquo;s now over 250. IBM is currently offering OpenPOWER-based servers and Rackspace&rsquo;s Barreleye design is available for purchase from various vendors.</p>
<p>Red Bull Racing kicked off with an amazing video about the car itself, the sensors, and what&rsquo;s involved with running in 21 races per year. The highlight of the video for me was seeing the F1 car round corners on a mountain while equipped with snow chains.</p>
<p>The car itself has 100,000 components and the car is disassembled and reassembled for each race based on the race conditions. Due to restrictions on how often they can practice, they run over 50,000 virtual simulations per year to test out different configurations and parts. Each race generates 8 petabytes of data and it is live-streamed to the engineers at the track as well as an engineering team in the UK. They can make split second choices on what to do during the race based on this data.</p>
<p>They gave an example of a situation where something was wrong with the car and the driver needed to make a pit stop. The engineers looked over the data that was coming from the car and identified the problem. Luckily, the driver could fix the issue by flipping a switch on the steering wheel. The car won the race by less than a second.</p>
<h2 id="breakouts">Breakouts</h2>
<p>My first stop on breakouts was Trends and Directions in IBM Power Systems. We had a high-level look at some of the advancements in POWER8 and OpenPOWER. Two customers shared their stories around why POWER was a better choice for them than other platforms, and everyone made sure to beat up on Moore&rsquo;s Law at every available opportunity. Rackspace was applauded for its leadership on Barreleye!</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>The most interesting session of the day was the IBM POWER9 Technology Advanced Deep Dive. Jeff covered the two chips in detail and talked about some of the new connections between the CPU and various components. I&rsquo;m interested in the hardware GZIP acceleration, NVLINK, and CAPI advancements. The connections to CAPI will be faster, thanks to the Power Service Layer (PSL) moving from the CAPI chip to the CPU itself. This reduces latency when communicating with the accelerator chip.</p>
<p>POWER9 has 192GB/sec on the PCIe Gen4 bus (that&rsquo;s 48 lanes) and there&rsquo;s 300GB/sec (25Gbit/sec x 48 lanes) of duplex bandwidth available for what&rsquo;s called <em>Common Link</em>. Common Link is used to communicate with accelerators or remote SMP and it will likely be called &ldquo;Blue Link&rdquo; at a later date. Very clever, IBM.</p>
<p>I wrapped the day with Calista Redmond&rsquo;s OpenPower Revolution in the Datacenter. She talked about where the OpenPOWER foundation is today and where it&rsquo;s going in the future.</p>
<h2 id="expo">EXPO</h2>
<p>As you might expect, IBM has most of the EXPO floor set aside for themselves and they&rsquo;re showing off new advances in POWER, System z, and LinuxONE. I spent a while looking at some of the new POWER8 chassis offerings and had a good conversation with some LinuxONE experts about some blockchain use cases.</p>
<p>IBM hired DJ Andrew Hypes and DJ Tim Exile to make some unique music by sampling sounds in a datacenter. They sampled sounds from IBM servers and storage devices and created some really unique music. It doesn&rsquo;t sound anything like a datacenter, though (thank goodness for that).</p>
<p><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --></p>
<p>The <a href="https://twitter.com/redbullracing">Red Bull Racing</a> booth drew a fairly large crowd throughout the evening. They had one of their F1 cars on site with its 100+ sensors:</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<h2 id="summary">Summary</h2>
<p>The big emphasis for the first day was on using specialized hardware for specialized workloads. Moore&rsquo;s Law took a beating throughout the day as each presenter assured the audience that 2x performance gains won&rsquo;t come in the chip itself for much longer.</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>It won&rsquo;t be possible to achieve the performance we want in the future on the backs of software projects alone. We will need to find ways to be smarter about how we run software on our servers. When something is ripe for acceleration, especially CPU-intensive, repetitive workloads, we should find a way to accelerate it in hardware. There are tons of examples of this already, like AES encryption acceleration, but we will need more acceleration capabilities soon.</p>
]]></content></item><item><title>Getting ready for IBM Edge 2016</title><link>https://major.io/2016/09/14/getting-ready-for-ibm-edge-2016/</link><pubDate>Wed, 14 Sep 2016 13:00:28 +0000</pubDate><guid>https://major.io/2016/09/14/getting-ready-for-ibm-edge-2016/</guid><description>IBM Edge 2016 starts next week in Las Vegas with an emphasis on changing how we think about technology. I&amp;rsquo;ll definitely be interested in the sessions on open source technology and advances in OpenPOWER.
Here are a few of the sessions from my must-see list:
OpenPower Revolution in the Datacenter IBM Power Systems - Made for the future of Cloud - Tech and Strategy Overview Scalable TensorFlow Deep Learning Deploying an OpenStack Cloud on POWER8 Using Ubuntu Metal-as-a-Service and Juju Power Cloud: PowerVM in a Heterogenous OpenStack Cloud (from members of the OpenStack-Ansible community!</description><content type="html"><![CDATA[<p><img src="/wp-content/uploads/2016/09/welcome-to-las-vegas-1086412_1920-e1473775447598.jpg" alt="1"></p>
<p><a href="http://ibm.co/2aAfTwk">IBM Edge 2016</a> starts next week in Las Vegas with an emphasis on changing how we think about technology. I&rsquo;ll definitely be interested in the sessions on open source technology and advances in OpenPOWER.</p>
<p>Here are a few of the sessions from my must-see list:</p>
<ul>
<li><a href="https://www-950.ibm.com/events/global/edge/sessions/preview.html?sessionid=IOP-2084">OpenPower Revolution in the Datacenter</a></li>
<li><a href="https://www-950.ibm.com/events/global/edge/sessions/preview.html?sessionid=KEY-2109">IBM Power Systems - Made for the future of Cloud - Tech and Strategy Overview</a></li>
<li><a href="https://www-950.ibm.com/events/global/edge/sessions/preview.html?sessionid=PPR-1605">Scalable TensorFlow Deep Learning</a></li>
<li><a href="https://www-950.ibm.com/events/global/edge/sessions/preview.html?sessionid=PPT-1797">Deploying an OpenStack Cloud on POWER8 Using Ubuntu Metal-as-a-Service and Juju</a></li>
<li><a href="https://www-950.ibm.com/events/global/edge/sessions/preview.html?sessionid=PPT-1574">Power Cloud: PowerVM in a Heterogenous OpenStack Cloud</a> <em>(from members of the OpenStack-Ansible community!)</em></li>
</ul>
<p>As always, I love the hallway track the most. I enjoy the opportunity to meet new people and hear about new challenges and use cases. It&rsquo;s also a great time to reconnect with people I haven&rsquo;t seen in a long time.</p>
<p>If you&rsquo;re going to be there and want to talk about open source, OpenStack, POWER, or technology in general, send me a note on <a href="https://twitter.com/majorhayden/">twitter</a>!</p>
<p><em>Photo credit: <a href="https://pixabay.com/p-1086412/">lindsayascott at pixabay</a></em></p>
]]></content></item><item><title>HTTP/2 for the blog and icanhazip.com</title><link>https://major.io/2016/09/13/http2-for-the-blog-and-icanhazip-com/</link><pubDate>Tue, 13 Sep 2016 13:47:05 +0000</pubDate><guid>https://major.io/2016/09/13/http2-for-the-blog-and-icanhazip-com/</guid><description>I&amp;rsquo;ve recently updated this blog and icanhazip.com to enable HTTP/2! This probably won&amp;rsquo;t have much of an effect on users who query icanhazip.com with automated tools, but it should deliver the content on this blog a little faster. If you&amp;rsquo;re using an older, non-HTTP/2 client - don&amp;rsquo;t worry. All of the sites will continue working for you as they always have.
Head on over to Wikipedia to learn more about HTTP/2 and its benefits.</description><content type="html"><![CDATA[<p>I&rsquo;ve recently updated this blog and <a href="https://icanhazip.com/">icanhazip.com</a> to enable HTTP/2! This probably won&rsquo;t have much of an effect on users who query icanhazip.com with automated tools, but it should deliver the content on this blog a <a href="https://http2.github.io/faq/#what-are-the-key-differences-to-http1x">little faster</a>. If you&rsquo;re using an older, non-HTTP/2 client - don&rsquo;t worry. All of the sites will continue working for you as they always have.</p>
<p>Head on over to Wikipedia to <a href="https://en.wikipedia.org/wiki/HTTP/2">learn more about HTTP/2 and its benefits</a>.</p>
<p>Also, I&rsquo;ve revised the list of allowed TLS ciphers so that stronger ciphers are required. If you&rsquo;re having issues with a particular client, please let me know.</p>
]]></content></item><item><title>What’s Happening in OpenStack-Ansible (WHOA) – August 2016</title><link>https://major.io/2016/08/23/whats-happening-in-openstack-ansible-whoa-august-2016/</link><pubDate>Tue, 23 Aug 2016 20:35:19 +0000</pubDate><guid>https://major.io/2016/08/23/whats-happening-in-openstack-ansible-whoa-august-2016/</guid><description>Welcome to the third post in the series of What&amp;rsquo;s Happening in OpenStack-Ansible (WHOA) posts that I&amp;rsquo;m assembling each month. OpenStack-Ansible is a flexible framework for deploying enterprise-grade OpenStack clouds. In fact, I use OpenStack-Ansible to deploy the OpenStack cloud underneath the virtual machine that runs this blog!
My goal with these posts is to inform more people about what we&amp;rsquo;re doing in the OpenStack-Ansible community and bring on more contributors to the project.</description><content type="html"><![CDATA[<p><a href="/wp-content/uploads/2011/11/openstack-justheo.png"><!-- raw HTML omitted --></a>Welcome to the third post in the series of <a href="/tags/whoa/">What&rsquo;s Happening in OpenStack-Ansible (WHOA)</a> posts that I&rsquo;m assembling each month. <a href="https://wiki.openstack.org/wiki/OpenStackAnsible">OpenStack-Ansible</a> is a flexible framework for deploying enterprise-grade <a href="http://openstack.org">OpenStack</a> clouds. In fact, I use OpenStack-Ansible to deploy the OpenStack cloud underneath the virtual machine that runs this blog!</p>
<p>My goal with these posts is to inform more people about what we&rsquo;re doing in the OpenStack-Ansible community and bring on more contributors to the project.</p>
<p>There are plenty of updates since the <a href="https://major.io/2016/07/22/whats-happening-in-openstack-ansible-whoa-july-2016/">last post from mid-July</a>. We&rsquo;ve had our Mid-cycle meeting and there are plenty of new improvements in flight.</p>
<h2 id="new-releases">New releases</h2>
<p>The OpenStack-Ansible releases are announced on the OpenStack development mailing list. Here are the things you need to know:</p>
<h3 id="liberty">Liberty</h3>
<p>The latest Liberty release, 12.2.1, contains lots of updates and fixes. There are plenty of <a href="https://gist.github.com/anonymous/6fafe3a92ba4f8d20b17d1688de29c03#neutron">neutron bug fixes</a> included in the release along with <a href="https://github.com/openstack/openstack-ansible/commit/5cb4b90b5c00f3c6db06249aea7db1d259723022">upgrade improvements</a>. Deployers also have the option to <a href="https://github.com/openstack/openstack-ansible/commit/bc0046b751a0c554903263ad6781485013c2b51d">block all container restarts</a> until they are ready to reboot containers during a maintenance window.</p>
<ul>
<li><a href="https://review.openstack.org/#/c/355513/">Version 12.2.1</a></li>
<li><a href="http://docs.openstack.org/releasenotes/openstack-ansible/liberty.html">Release notes</a></li>
<li><a href="https://gist.github.com/anonymous/6fafe3a92ba4f8d20b17d1688de29c03">Detailed changelog</a></li>
</ul>
<h3 id="mitaka">Mitaka</h3>
<p>Mitaka is the latest stable release available and the latest version is 13.3.1. This release also <a href="https://gist.github.com/anonymous/02e9e4ce95cb54e903876d6ddf13c1ad#neutron">brings in a bunch of neutron fixes</a> and several <a href="https://gist.github.com/anonymous/02e9e4ce95cb54e903876d6ddf13c1ad#openstack-ansible">&ldquo;behind the scenes&rdquo;</a> fixes for OpenStack-Ansible.</p>
<ul>
<li><a href="https://review.openstack.org/355509">Version 13.3.1</a></li>
<li><a href="http://docs.openstack.org/releasenotes/openstack-ansible/mitaka.html">Release notes</a></li>
<li><a href="https://gist.github.com/anonymous/02e9e4ce95cb54e903876d6ddf13c1ad">Detailed changelog</a></li>
</ul>
<h2 id="notable-discussions">Notable discussions</h2>
<p>This section covers discussions from the OpenStack-Ansible weekly meetings, IRC channels, mailing lists, or in-person events.</p>
<h3 id="mid-cycle-meeting">Mid-cycle meeting</h3>
<p>We had a great mid-cycle meeting at the Rackspace headquarters in San Antonio, Texas:</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>The meeting drew community members from various companies from all over the United States and the United Kingdom. We talked about the improvements we need to make in the remainder of the Newton cycle, including upgrades, documentation improvements, and new roles.</p>
<p>Here is a run-down of the biggest topics:</p>
<h4 id="install-guide-overhaul">Install guide overhaul</h4>
<p>The new install guide is quickly coming together and it&rsquo;s much easier to follow for newcomers. There is a big need now for detailed technical reviews to ensure that the new content is clear and accurate!</p>
<h4 id="ansible-21">Ansible 2.1</h4>
<p>The decision was made to bump each of the role repositories to Ansible 2.1 to match the integrated repository. It was noted that Ansible 2.2 will bring some performance improvements once it is released.</p>
<h4 id="ubuntu-1604-xenial-support">Ubuntu 16.04 Xenial Support</h4>
<p>This is a high priority for the remainder of the Newton cycle. The Xenial gate jobs will be switched to voting and Xenial failures will need to be dealt with before any additional patches will be merged.</p>
<h4 id="ubuntu-1404-trusty-support">Ubuntu 14.04 Trusty Support</h4>
<p>Many of the upstream OpenStack projects are removing 14.04 support soon and OpenStack-Ansible will drop 14.04 support in the Ocata release.</p>
<h4 id="power-cpu-support">Power CPU support</h4>
<p>There&rsquo;s already support for Power systems as hypervisors within OpenStack-Ansible now and IBM is testing mixed x86 and PPC environments now. However, we still need some way to test these mixed environments in the OpenStack gate tests. Two IBMers from the OpenStack-Ansible community are working with the infra team to find out how this can be</p>
<p>done.</p>
<h4 id="inventory-improvements">Inventory improvements</h4>
<p>The inventory generation process for OpenStack-Ansible is getting more tests and better documentation. Generating inventory is a difficult process to understand, but it is critical for the project&rsquo;s success.</p>
<h4 id="gnocchi--telemetry-improvements">Gnocchi / Telemetry improvements</h4>
<p>We got an update on gnocchi/ceilometer and set some plans on how to go forward with the OpenStack services and the data storage challenges that go along with each.</p>
<h3 id="mailing-list">Mailing list</h3>
<p>The OpenStack-Ansible tag was fairly quiet on the OpenStack Development mailing list during the time frame of this report, but there were a few threads:</p>
<ul>
<li><a href="http://lists.openstack.org/pipermail/openstack-dev/2016-August/100883.html">Adding RHEL 7 STIG to openstack-ansible-security</a></li>
<li><a href="http://lists.openstack.org/pipermail/openstack-dev/2016-August/101400.html">git repo in infra_repo container not working</a></li>
</ul>
<h3 id="blogs">Blogs</h3>
<p>Michael Gugino wrote about <a href="https://medium.com/@michaelgugino/deploying-nova-lxd-hypervisors-with-openstack-ansible-39525157879d#.ot5yb7rwt">deploying nova-lxd with OpenStack-Ansible</a>. This is one of the newest features in OpenStack-Ansible.</p>
<p>I <a href="https://major.io/2016/08/03/openstack-instances-come-online-with-multiple-network-ports-attached/">wrote one about an issue</a> that was really difficult to track down. I had instances coming online with multiple network ports attached when I only asked for one port. It turned out to be a glance issue that caused a problem in nova.</p>
<h2 id="notable-developments">Notable developments</h2>
<p>This section covers some of the improvements coming to Newton, the upcoming OpenStack release.</p>
<h3 id="bug-fixes">Bug fixes</h3>
<p>Services were logging to stderr and this caused some log messages to be logged multiple times on the same host. This ate up additional disk space and disk I/O performance. <a href="https://review.openstack.org/#/q/topic:bug/1588051">Topic</a></p>
<p>The xtrabackup utility causes crashes during certain situations when the <code>compact</code> option is used. <a href="https://review.openstack.org/#/q/topic:bug/1590166">Reviews</a></p>
<h3 id="dashboard">Dashboard</h3>
<p>LBaaS v2 panels were added to Newton and Mitaka. Ironic and Magnum panels were added to Newton.</p>
<h3 id="documentation">Documentation</h3>
<p>Plenty of work was merged towards improving the installation guide to make it more concise and easy to follow. <a href="https://review.openstack.org/#/q/project:%255Eopenstack/openstack-ansible-.*+branch:master+topic:bp/osa-install-guide-overhaul">Topic</a></p>
<h3 id="multi-architecture-support">Multi-Architecture support</h3>
<p>Repo servers can now build Python wheels for multiple architectures. This allows for mixed control/data plane environments, such as x86 (Intel) control planes with PPC (Power8) hypervisors running PowerKVM or PowerVM. <a href="https://review.openstack.org/#/c/346863/">Review</a></p>
<h3 id="performance-improvements">Performance improvements</h3>
<p>The repo servers now act as an apt repository cache, which helps improve the speed of deployments. This also helps with deployers who don&rsquo;t have an active internet connection in their cloud.</p>
<p>The repo servers now only build the wheels and virtual environments necessary for the services which are actually being deployed. This reduces the wait required while the wheels and virtual environments are built, but it also has an added benefit of reducing the disk space consumed. <a href="https://review.openstack.org/#/q/topic:repo-build-optimise">Topic</a></p>
<h3 id="upgrades">Upgrades</h3>
<p>The MariaDB restarts during upgrades are now handled more carefully to avoid disruptions from container restarts. <a href="https://review.openstack.org/#/c/354642/">Review</a></p>
<p>Deployers now have the option to choose if they want packages updated during a deployment or during an upgrade. There are per-role switches as well as a global switch that can be toggled. <a href="https://review.openstack.org/#/q/topic:package-install-state-option">Topic</a></p>
<h2 id="feedback">Feedback?</h2>
<p>The goal of this newsletter is three fold:</p>
<ul>
<li>
<p>Keep OpenStack-Ansible developers updated with new changes</p>
</li>
<li>
<p>Inform operators about new features, fixes, and long-term goals</p>
</li>
<li>
<p>Bring more people into the OpenStack-Ansible community to share their use</p>
<p>cases, bugs, and code</p>
</li>
</ul>
<p>Please let me know if you spot any errors, areas for improvement, or items that I missed altogether. I&rsquo;m <code>mhayden</code> on Freenode IRC and you can find me on <a href="https://twitter.com/majorhayden">Twitter</a> anytime.</p>
]]></content></item><item><title>Preventing critical services from deploying on the same OpenStack host</title><link>https://major.io/2016/08/09/preventing-critical-services-from-deploying-on-the-same-openstack-host/</link><pubDate>Tue, 09 Aug 2016 17:07:35 +0000</pubDate><guid>https://major.io/2016/08/09/preventing-critical-services-from-deploying-on-the-same-openstack-host/</guid><description>OpenStack&amp;rsquo;s compute service, nova, manages all of the virtual machines within a OpenStack cloud. When you ask nova to build an instance, or a group of instances, nova&amp;rsquo;s scheduler system determines which hypervisors should run each instance. The scheduler uses filters to figure out where each instance belongs.
However, there are situations where the scheduler might put more than one of your instances on the same host, especially when resources are constrained.</description><content type="html"><![CDATA[<p><img src="/wp-content/uploads/2016/08/6312423035_2c53fe78e7_b-e1470762211193.jpg" alt="1"></p>
<p>OpenStack&rsquo;s compute service, nova, manages all of the virtual machines within a OpenStack cloud. When you ask nova to build an instance, or a group of instances, nova&rsquo;s scheduler system determines which hypervisors should run each instance. The scheduler uses <a href="http://docs.openstack.org/mitaka/config-reference/compute/scheduler.html#filters">filters</a> to figure out where each instance belongs.</p>
<p>However, there are situations where the scheduler might put more than one of your instances on the same host, especially when resources are constrained. This can be a problem when you deploy certain highly available applications, like MariaDB and Galera. If more than one of your database instances landed on the same physical host, a failure of that physical host could take down more than one database instance.</p>
<h2 id="filters-to-the-rescue">Filters to the rescue</h2>
<p>The scheduler offers the <a href="http://docs.openstack.org/mitaka/config-reference/compute/scheduler.html#servergroupantiaffinityfilter">ServerGroupAntiAffinityFilter</a> filter for these deployments. This allows a user to create a server group, apply a policy to the group, and then begin adding servers to that group.</p>
<p>If the scheduler filter can&rsquo;t find a way to fulfill the anti-affinity request (which often happens if the hosts are low on resources), it will fail the entire build transaction with an error. In other words, unless the entire request can be fulfilled, it won&rsquo;t be deployed.</p>
<p>Let&rsquo;s see how this works in action on an OpenStack Mitaka cloud deployed with <a href="http://docs.openstack.org/developer/openstack-ansible/">OpenStack-Ansible</a>.</p>
<h2 id="creating-a-server-group">Creating a server group</h2>
<p>We can use the <a href="http://docs.openstack.org/user-guide/common/cli-install-openstack-command-line-clients.html">openstackclient</a> tool to create our server group:</p>
<pre><code>$ openstack server group create --policy anti-affinity db-production
+----------+--------------------------------------+
| Field    | Value                                |
+----------+--------------------------------------+
| id       | cd234914-980a-42f2-b77c-602a7cc0080f |
| members  |                                      |
| name     | db-production                        |
| policies | anti-affinity                        |
+----------+--------------------------------------+
</code></pre><p>We&rsquo;ve told nova that we want all of the instances in the <code>db-production</code> group to land on different OpenStack hosts. I&rsquo;ll copy the <code>id</code> to my clipboard since I&rsquo;ll need that UUID for the next step.</p>
<h2 id="adding-hosts-to-the-group">Adding hosts to the group</h2>
<p>My small OpenStack cloud has four hypervisors, so I can add four instances to this server group:</p>
<pre><code>$ openstack server create \
  --flavor m1.small \
  --image &quot;Fedora 24&quot; \
  --nic net-id=bc8895ab-98f7-478f-a54a-36b121f7bb3f \
  --key-name personal_servers \
  --hint &quot;group=cd234914-980a-42f2-b77c-602a7cc0080f&quot; \
  --max 4
  prod-db
</code></pre><p>This <code>server create</code> command looks fairly standard, but I&rsquo;ve added the <code>--hint</code> parameter to specify that we want these servers scheduled as part of the group we just created. Also, I&rsquo;ve requested for four servers to be built at the same time. After a few moments, we should have four servers active:</p>
<pre><code>$ openstack server list --name prod-db -c ID -c Name -c Status
+--------------------------------------+-----------+--------+
| ID                                   | Name      | Status |
+--------------------------------------+-----------+--------+
| 7e7a81f3-eb02-4751-93c1-a0de999b8423 | prod-db-4 | ACTIVE |
| b742fb58-8ea4-4e26-bfbf-645a698fbb26 | prod-db-3 | ACTIVE |
| 78c7a43c-4deb-40da-a419-e62db37ab41a | prod-db-2 | ACTIVE |
| 7b8af038-6441-40c0-87c8-0a1acced17a6 | prod-db-1 | ACTIVE |
+--------------------------------------+-----------+--------+
</code></pre><p>If we check the instances, they should be on different hosts:</p>
<pre><code>$ for i in {1..4}; do openstack server show prod-db-${i} -c hostId -f shell; done
hostid=&quot;5fea4e5862f82f051e26caf926fe34bd3a9f1439b08a464f817b4c61&quot;
hostid=&quot;65d87faf6d9baa110afa5f2e0308781dde4629142170b2c9af1f090b&quot;
hostid=&quot;243f833055303efe838b3233f7ba6e1993fb28895ae11c724f10cc73&quot;
hostid=&quot;54df76a1e66bd8585cc3c1f8f38e8f4937456394f2409daf2a8b4c2e&quot;
</code></pre><p>Success!</p>
<p>If we try to build one more instance, it should fail since the scheduler cannot fulfill the anti-affinity policy applied to server group:</p>
<pre><code>$ openstack server create \
  --flavor m1.small \
  --image &quot;Fedora 24&quot; \
  --nic net-id=bc8895ab-98f7-478f-a54a-36b121f7bb3f \
  --key-name personal_servers \
  --hint &quot;group=cd234914-980a-42f2-b77c-602a7cc0080f&quot; \
  --wait \
  prod-db-5
Error creating server: prod-db-5
Error creating server
$ openstack server show prod-db-5 -c fault -f shell
fault=&quot;{u'message': u'No valid host was found. There are not enough hosts available.', u'code': 500...
</code></pre><p>The scheduler couldn&rsquo;t find a valid host for a fifth server in the anti-affinity group.</p>
<p><em>Photo credit: <a href="https://flic.kr/p/aBNPrV">&ldquo;crowded houses&rdquo; from jesuscm on Flickr</a></em></p>
]]></content></item><item><title>OpenStack instances come online with multiple network ports attached</title><link>https://major.io/2016/08/03/openstack-instances-come-online-with-multiple-network-ports-attached/</link><pubDate>Wed, 03 Aug 2016 14:40:16 +0000</pubDate><guid>https://major.io/2016/08/03/openstack-instances-come-online-with-multiple-network-ports-attached/</guid><description>I ran into an interesting problem recently in my production OpenStack deployment that runs the Mitaka release. On various occasions, instances were coming online with multiple network ports attached, even though I only asked for one network port.
The problem If I issued a build request for ten instances, I&amp;rsquo;d usually end up with this:
6 instances with one network port attached 2-3 instances with two network ports attached (not what I want) 1-2 instances with three or four network ports attached (definitely not what I want) When I examined the instances with multiple network ports attached, I found that one of the network ports would be marked as up while the others would be marked as down.</description><content type="html"><![CDATA[<p><img src="/wp-content/uploads/2016/08/308357541_222d1b2e2a_b-e1470234736818.jpg" alt="1"></p>
<p>I ran into an interesting problem recently in my production OpenStack deployment that runs the Mitaka release. On various occasions, instances were coming online with multiple network ports attached, even though I only asked for one network port.</p>
<h2 id="the-problem">The problem</h2>
<p>If I issued a build request for ten instances, I&rsquo;d usually end up with this:</p>
<ul>
<li>6 instances with one network port attached</li>
<li>2-3 instances with two network ports attached <em>(not what I want)</em></li>
<li>1-2 instances with three or four network ports attached <em>(<strong>definitely</strong> not what I want)</em></li>
</ul>
<p>When I examined the instances with multiple network ports attached, I found that one of the network ports would be marked as <em>up</em> while the others would be marked as <em>down</em>. However, the IP addresses associated with those extra ports would still be associated with the instance in horizon and via the nova API. All of the network ports seemed to be fully configured on the neutron side.</p>
<h2 id="digging-into-neutron">Digging into neutron</h2>
<p>The neutron API logs are fairly chatty, especially while instances are building, but I found two interesting log lines for one of my instances:</p>
<pre><code>172.29.236.41,172.29.236.21 - - [02/Aug/2016 14:03:11] &quot;GET /v2.0/ports.json?tenant_id=a7b0519330ed481884431102a72dd04c&amp;device_id=05eef1bb-5356-43d9-86c9-4d9854d4d46b HTTP/1.1&quot; 200 2137 0.025282
172.29.236.11,172.29.236.21 - - [02/Aug/2016 14:03:15] &quot;GET /v2.0/ports.json?tenant_id=a7b0519330ed481884431102a72dd04c&amp;device_id=05eef1bb-5356-43d9-86c9-4d9854d4d46b HTTP/1.1&quot; 200 3098 0.027803
</code></pre><p>There are two requests to create network ports for this instance and neutron is allocating ports to both requests. This would normally be just fine, but I only asked for one network port on this instance.</p>
<p>The IP addresses making the requests are unusual, though. <code>172.29.236.11</code> and <code>172.29.236.41</code> are two of the hypervisors within my cloud. Why are both of them asking neutron for network ports? Only one of those hypervisors should be building my instance, not both. After checking both hypervisors, I verified that the instance was only provisioned on one of the hosts and not both.</p>
<h2 id="looking-at-nova-compute">Looking at nova-compute</h2>
<p>The instance ended up on the <code>172.29.236.11</code> hypervisor once it finished building and the logs on that hypervisor looked fine:</p>
<pre><code>nova.virt.libvirt.driver [-] [instance: 05eef1bb-5356-43d9-86c9-4d9854d4d46b] Instance spawned successfully.
</code></pre><p>I logged into the <code>172.29.236.41</code> hypervisor since it was the one that asked neutron for a port but it never built the instance. The logs there had a much different story:</p>
<pre><code>[instance: 05eef1bb-5356-43d9-86c9-4d9854d4d46b] Instance failed to spawn
Traceback (most recent call last):
  File &quot;/openstack/venvs/nova-13.3.0/lib/python2.7/site-packages/nova/compute/manager.py&quot;, line 2218, in _build_resources
    yield resources
  File &quot;/openstack/venvs/nova-13.3.0/lib/python2.7/site-packages/nova/compute/manager.py&quot;, line 2064, in _build_and_run_instance
    block_device_info=block_device_info)
  File &quot;/openstack/venvs/nova-13.3.0/lib/python2.7/site-packages/nova/virt/libvirt/driver.py&quot;, line 2773, in spawn
    admin_pass=admin_password)
  File &quot;/openstack/venvs/nova-13.3.0/lib/python2.7/site-packages/nova/virt/libvirt/driver.py&quot;, line 3191, in _create_image
    instance, size, fallback_from_host)
  File &quot;/openstack/venvs/nova-13.3.0/lib/python2.7/site-packages/nova/virt/libvirt/driver.py&quot;, line 6765, in _try_fetch_image_cache
    size=size)
  File &quot;/openstack/venvs/nova-13.3.0/lib/python2.7/site-packages/nova/virt/libvirt/imagebackend.py&quot;, line 251, in cache
    *args, **kwargs)
  File &quot;/openstack/venvs/nova-13.3.0/lib/python2.7/site-packages/nova/virt/libvirt/imagebackend.py&quot;, line 591, in create_image
    prepare_template(target=base, max_size=size, *args, **kwargs)
  File &quot;/openstack/venvs/nova-13.3.0/lib/python2.7/site-packages/oslo_concurrency/lockutils.py&quot;, line 271, in inner
    return f(*args, **kwargs)
  File &quot;/openstack/venvs/nova-13.3.0/lib/python2.7/site-packages/nova/virt/libvirt/imagebackend.py&quot;, line 241, in fetch_func_sync
    fetch_func(target=target, *args, **kwargs)
  File &quot;/openstack/venvs/nova-13.3.0/lib/python2.7/site-packages/nova/virt/libvirt/utils.py&quot;, line 429, in fetch_image
    max_size=max_size)
  File &quot;/openstack/venvs/nova-13.3.0/lib/python2.7/site-packages/nova/virt/images.py&quot;, line 120, in fetch_to_raw
    max_size=max_size)
  File &quot;/openstack/venvs/nova-13.3.0/lib/python2.7/site-packages/nova/virt/images.py&quot;, line 110, in fetch
    IMAGE_API.download(context, image_href, dest_path=path)
  File &quot;/openstack/venvs/nova-13.3.0/lib/python2.7/site-packages/nova/image/api.py&quot;, line 182, in download
    dst_path=dest_path)
  File &quot;/openstack/venvs/nova-13.3.0/lib/python2.7/site-packages/nova/image/glance.py&quot;, line 383, in download
    _reraise_translated_image_exception(image_id)
  File &quot;/openstack/venvs/nova-13.3.0/lib/python2.7/site-packages/nova/image/glance.py&quot;, line 682, in _reraise_translated_image_exception
    six.reraise(new_exc, None, exc_trace)
  File &quot;/openstack/venvs/nova-13.3.0/lib/python2.7/site-packages/nova/image/glance.py&quot;, line 381, in download
    image_chunks = self._client.call(context, 1, 'data', image_id)
  File &quot;/openstack/venvs/nova-13.3.0/lib/python2.7/site-packages/nova/image/glance.py&quot;, line 250, in call
    result = getattr(client.images, method)(*args, **kwargs)
  File &quot;/openstack/venvs/nova-13.3.0/lib/python2.7/site-packages/glanceclient/v1/images.py&quot;, line 148, in data
    % urlparse.quote(str(image_id)))
  File &quot;/openstack/venvs/nova-13.3.0/lib/python2.7/site-packages/glanceclient/common/http.py&quot;, line 275, in get
    return self._request('GET', url, **kwargs)
  File &quot;/openstack/venvs/nova-13.3.0/lib/python2.7/site-packages/glanceclient/common/http.py&quot;, line 267, in _request
    resp, body_iter = self._handle_response(resp)
  File &quot;/openstack/venvs/nova-13.3.0/lib/python2.7/site-packages/glanceclient/common/http.py&quot;, line 83, in _handle_response
    raise exc.from_response(resp, resp.content)
ImageNotFound: Image 8feacda9-91fd-48ce-b983-54f7b6de6650 could not be found.
</code></pre><p>This is one of those occasions where I was glad to find an exception in the log. The image that couldn&rsquo;t be found is an image I&rsquo;ve used regularly in the environment before, and I know it exists.</p>
<h2 id="gandering-at-glance">Gandering at glance</h2>
<p>First off, I asked glance what it knew about the image:</p>
<pre><code>$ openstack image show 8feacda9-91fd-48ce-b983-54f7b6de6650
+------------------+------------------------------------------------------+
| Field            | Value                                                |
+------------------+------------------------------------------------------+
| checksum         | 8de08e3fe24ee788e50a6a508235aa64                     |
| container_format | bare                                                 |
| created_at       | 2016-08-03T01:25:34Z                                 |
| disk_format      | qcow2                                                |
| file             | /v2/images/8feacda9-91fd-48ce-b983-54f7b6de6650/file |
| id               | 8feacda9-91fd-48ce-b983-54f7b6de6650                 |
| min_disk         | 0                                                    |
| min_ram          | 0                                                    |
| name             | Fedora 24                                            |
| owner            | a7b0519330ed481884431102a72dd04c                     |
| properties       | description=''                                       |
| protected        | False                                                |
| schema           | /v2/schemas/image                                    |
| size             | 204590080                                            |
| status           | active                                               |
| tags             |                                                      |
| updated_at       | 2016-08-03T01:25:39Z                                 |
| virtual_size     | None                                                 |
| visibility       | public                                               |
+------------------+------------------------------------------------------+
</code></pre><p>If glance knows about the image, why can&rsquo;t that hypervisor build an instance with that image? While I was scratching my head, <a href="https://twitter.com/cloudnull">Kevin Carter</a> walked by my desk and joined in the debugging.</p>
<p>He asked about how I had deployed glance and what storage backend I was using. I was using the regular file storage backend since I don&rsquo;t have swift deployed in the environment. He asked me how many glance nodes I had (I had two) and if I was doing anything to sync the images between the glance nodes.</p>
<p>Then it hit me.</p>
<p><a href="/wp-content/uploads/2016/08/stitch_frustrated.gif"><!-- raw HTML omitted --></a></p>
<p>Although both glance nodes knew about the image (since that data is in the database), <strong>only one of the glance nodes had the actual image content (the actual qcow2 file) stored</strong>. That means that if a hypervisor requests the image from a glance node that knows about the image but doesn&rsquo;t have it stored, the hypervisor won&rsquo;t be able to retrieve the image.</p>
<p>Unfortunately, the checks go in this order on the nova-compute side:</p>
<ol>
<li>Ask glance if this image exists and if this tenant can use it</li>
<li>Configure the network</li>
<li>Retrieve the image</li>
</ol>
<p>If a hypervisor rolls through steps one and two without issues, but then fails on step 3, the network port will be provisioned but won&rsquo;t come up on the instance. There&rsquo;s nothing that cleans up that port in the Mitaka release, so it requires manual intervention.</p>
<h2 id="the-fix">The fix</h2>
<p>As a temporary workaround, I took one of the glance nodes offline so that only one glance node is being used. After hundreds of builds, all of the instances came up with only one network port attached!</p>
<p>There are a few options for long-term fixes.</p>
<p>I could deploy swift and put glance images into swift. That would allow me to use multiple glance nodes with the same swift backend. Another option would be to use an existing swift deployment, such as Rackspace&rsquo;s Cloud Files product.</p>
<p>Since I&rsquo;m not eager to deploy swift in my environment for now, I decided to remove the second glance node and reconfigure nova to use only one glance node. That means I&rsquo;m running with only one glance node and a failure there could be highly annoying. However, that trade-off is fine with me until I can get around to deploying swift.</p>
<p><strong>UPDATE:</strong> I&rsquo;ve opened <a href="https://bugs.launchpad.net/nova/+bug/1609526">a bug</a> for nova so that the network ports are cleaned up if the instance fails to build.</p>
<p><em>Photo credit: <a href="https://flic.kr/p/tfpXk">Flickr: pascalcharest</a></em></p>
]]></content></item><item><title>Setting up a telnet handler for OpenStack Zuul CI jobs in GNOME 3</title><link>https://major.io/2016/07/22/setting-up-a-telnet-handler-in-gnome-3/</link><pubDate>Fri, 22 Jul 2016 19:44:07 +0000</pubDate><guid>https://major.io/2016/07/22/setting-up-a-telnet-handler-in-gnome-3/</guid><description>The OpenStack Zuul system has gone through some big changes recently, and one of those changes is around how you monitor a running CI job. I work on OpenStack-Ansible quite often, and the gate jobs can take almost an hour to complete at times. It can be helpful to watch the output of a Zuul job to catch a problem or follow a breakpoint.
New Zuul In the previous version of Zuul, you could access the Jenkins server that was running the CI job and monitor its progress right in your browser.</description><content type="html"><![CDATA[<p>The OpenStack Zuul system has gone through some big changes recently, and one of those changes is around how you monitor a running CI job. I work on OpenStack-Ansible quite often, and the gate jobs can take almost an hour to complete at times. It can be helpful to watch the output of a Zuul job to catch a problem or follow a breakpoint.</p>
<h2 id="new-zuul">New Zuul</h2>
<p>In the previous version of Zuul, you could access the Jenkins server that was running the CI job and monitor its progress right in your browser. Today, you can <a href="http://status.openstack.org/zuul/">monitor the progress of a job via telnet</a>. It&rsquo;s much easier to use and it&rsquo;s a lighter-weight way to review a bunch of text.</p>
<p>Some of you might be saying: <strong>&ldquo;It&rsquo;s 2016. Telnet? Unencrypted? Seriously?&quot;</strong></p>
<p>Before you get out the pitchforks, all of the data is read-only in the telnet session, and nothing sensitive is transmitted. Anything that comes through the telnet session is content that exists in an open source repository within OpenStack. If someone steals the output of the job, they&rsquo;re not getting anything valuable.</p>
<p>I was having a lot of trouble figuring out how to set up a handler for <code>telnet://</code> URL&rsquo;s that I clicked in Chrome or Firefox. If I clicked a link in Chrome, it would be passed off to <code>xdg-open</code>. I&rsquo;d press OK on the window and then nothing happened.</p>
<h2 id="creating-a-script">Creating a script</h2>
<p>First off, I needed a script that would take the URL coming from an application and actually do something with it. The script will receive a URL as an argument that looks like <code>telnet://SERVER_ADDRESS:PORT</code> and that must be handed off to the <code>telnet</code> executable. Here&rsquo;s my basic script:</p>
<pre><code>#!/bin/bash

# Remove the telnet:// and change the colon before the port
# number to a space.
TELNET_STRING=$(echo $1 | sed -e 's/telnet:\/\///' -e 's/:/ /')

# Telnet to the remote session
/usr/bin/telnet $TELNET_STRING

# Don't close out the terminal unless we are done
read -p &quot;Press a key to exit&quot;
</code></pre><p>I saved that in <code>~/bin/telnet.sh</code>. A quick test with localhost should verify that the script works:</p>
<pre><code>$ chmod +x ~/bin/telnet.sh
$ ~/bin/telnet.sh telnet://127.0.0.1:12345
Trying 127.0.0.1...
telnet: connect to address 127.0.0.1: Connection refused
Press a key to exit
</code></pre><h2 id="linking-up-with-gnome">Linking up with GNOME</h2>
<p>We need a <code>.desktop</code> file so that GNOME knows how to run our script. Save a file like this to <code>~/.local/share/applications/telnet.desktop</code>:</p>
<pre><code>[Desktop Entry]
Version=1.0
Name=Telnet
GenericName=Telnet
Comment=Telnet Client
Exec=/home/major/bin/telnet.sh %U
Terminal=true
Type=Application
Categories=TerminalEmulator;Network;Telnet;Internet;BBS;
MimeType=x-scheme/telnet
X-KDE-Protocols=telnet
Keywords=Terminal;Emulator;Network;Internet;BBS;Telnet;Client;
</code></pre><p>Change the path in <code>Exec</code> to match where you placed your script.</p>
<p>We need to tell GNOME how to handle the <code>x-scheme-handler/telnet</code> mime type. We do that with <code>xdg</code> utilities:</p>
<pre><code>$ xdg-mime default telnet.desktop x-scheme-handler/telnet
$ xdg-mime query default x-scheme-handler/telnet
telnet.desktop
</code></pre><p>Awesome! When you click a link in Chrome, the following should happen:</p>
<ul>
<li>Chrome will realize it has no built-in handler and will hand off to <code>xdg-open</code></li>
<li><code>xdg-open</code> will check its list of mime types for a telnet handler</li>
<li><code>xdg-open</code> will parse <code>telnet.desktop</code> and run the command in the <code>Exec</code> line within a terminal</li>
<li>Our <code>telnet.sh</code> script runs with the <code>telnet://</code> URI provided as an argument</li>
<li>The remote telnet session is connected</li>
</ul>
]]></content></item><item><title>What’s Happening in OpenStack-Ansible (WHOA) – July 2016</title><link>https://major.io/2016/07/22/whats-happening-in-openstack-ansible-whoa-july-2016/</link><pubDate>Fri, 22 Jul 2016 15:48:18 +0000</pubDate><guid>https://major.io/2016/07/22/whats-happening-in-openstack-ansible-whoa-july-2016/</guid><description>This post is the second installment in the series of What&amp;rsquo;s Happening in OpenStack-Ansible (WHOA) posts that I&amp;rsquo;m assembling each month. My goal is to inform more people about what we&amp;rsquo;re doing in the OpenStack-Ansible community and bring on more contributors to the project.
July brought lots of changes for the OpenStack-Ansible project and the remaining work for the Newton release is coming together well. Many of the changes made in the Newton branch have made deployments faster, more reliable and more repeatable.</description><content type="html"><![CDATA[<p><a href="/wp-content/uploads/2011/11/openstack-justheo.png"><!-- raw HTML omitted --></a>This post is the second installment in the series of <a href="/tags/whoa/">What&rsquo;s Happening in OpenStack-Ansible (WHOA)</a> posts that I&rsquo;m assembling each month. My goal is to inform more people about what we&rsquo;re doing in the OpenStack-Ansible community and bring on more contributors to the project.</p>
<p>July brought lots of changes for the OpenStack-Ansible project and the remaining work for the <a href="http://releases.openstack.org/newton/schedule.html">Newton release</a> is coming together well. Many of the changes made in the Newton branch have made deployments faster, more reliable and more repeatable.</p>
<p>Let&rsquo;s get to the report!</p>
<h2 id="new-releases">New releases</h2>
<p>You can always find out about the newest releases for most OpenStack projects on the OpenStack Development mailing list, but I&rsquo;ll give you the shortest summary possible here.</p>
<h3 id="kilo">Kilo</h3>
<p>The final Kilo release, <a href="https://review.openstack.org/#/c/336505/">11.2.17</a>, is in the books! If you&rsquo;re on Kilo, it&rsquo;s definitely time to move forward.</p>
<h3 id="liberty">Liberty</h3>
<p>The latest Liberty release is now <a href="https://review.openstack.org/#/c/342310/">12.1.0</a>. For more information on what&rsquo;s included, review the <a href="http://docs.openstack.org/releasenotes/openstack-ansible/liberty.html">release notes</a> or view a <a href="https://gist.github.com/major/c965c67ae56d3a34a30b32d8212c258f">detailed changelog</a>.</p>
<h3 id="mitaka">Mitaka</h3>
<p>Mitaka is the latest stable branch and it&rsquo;s currently at version <a href="https://review.openstack.org/#/c/342307/">13.2.0</a>. It contains lots of bug fixes and a few small backported features. The latest details are always in the <a href="http://docs.openstack.org/releasenotes/openstack-ansible/mitaka.html">release notes</a> and the <a href="https://gist.github.com/major/def75c576cd22c09b5e62fcc2d138202">detailed changelog</a>.</p>
<h2 id="notable-discussions">Notable discussions</h2>
<p>The OpenStack-Ansible mid-cycle is quickly approaching! It runs from August 10th through the 12th at Rackspace&rsquo;s headquarters in San Antonio, Texas. All of the signup information is on the <a href="https://etherpad.openstack.org/p/osa-midcycle-newton">etherpad</a> along with the proposed agenda. If you&rsquo;re interested in OpenStack deployment automation with Ansible, please feel free to join us!</p>
<p>Support for Open vSwitch is now in OpenStack-Ansible, along with Distributed Virtual Routing (DVR). Travis Truman wrote a <a href="http://lists.openstack.org/pipermail/openstack-dev/2016-July/098858.html">blog post</a> about using the new Open vSwitch support. The support for DVR <a href="https://review.openstack.org/#/c/338546/">was added</a> very recently.</p>
<p>We had a good discussion around standardizing how OpenStack&rsquo;s python services are deployed. Some projects are now recommending the use of uwsgi with their API services. During this week&rsquo;s IRC meeting, we agreed as a group that the best option would be to standardize on uwsgi if possible during the Newton release. If that&rsquo;s not possible, it should be done early in the Ocata release.</p>
<p>Jean-Philippe Evrard <a href="https://openstack.nimeyo.com/90118/openstack-openstack-nominating-philippe-openstack-openstack">was nominated to be a core developer</a> on OpenStack-Ansible and the thread received many positive comments over the week. Congratulations, JP!</p>
<h2 id="notable-developments">Notable developments</h2>
<p>Lots of work is underway in the Newton release to add support for new features, squash bugs, and reduce the time it takes to deploy a cloud.</p>
<h3 id="documentation">Documentation</h3>
<p>Documentation seems to go one of two ways with most projects:</p>
<ul>
<li>Sufficient documentation that is organized poorly (OpenStack-Ansible&rsquo;s current state)</li>
<li>Insufficient documentation that is organized well</li>
</ul>
<p>One of the complaints I heard at the summit was <strong><em>&ldquo;What the heck are we thinking with chapter four?&quot;</em></strong></p>
<p>To be fair, <a href="http://docs.openstack.org/developer/openstack-ansible/mitaka/install-guide/configure.html">that chapter</a> is gigantic. While it contains a myriad of useful information, advice, and configuration options, it&rsquo;s overwhelming for beginners and even seasoned deployers.</p>
<p>Work is underway to overhaul the installation guide and provide a simple, easy-to-follow, opinionated method for deploying an OpenStack cloud. This would allow beginners to start on solid ground and have a straightforward deployment guide. The additional information and configuration options would still be available in the documentation, but the documentation will provide strong recommendations for the best possible options.</p>
<h3 id="gnocchi-deployments">Gnocchi deployments</h3>
<p>OpenStack-Ansible can now deploy <a href="http://docs.openstack.org/developer/gnocchi/basic.html">Gnocchi</a>. Gnocchi provides a time series database as a service and it&rsquo;s handy for use with ceilometer, which stores a lot of time-based information.</p>
<h3 id="multiple-rabbitmq-clusters">Multiple RabbitMQ clusters</h3>
<p>Some OpenStack services communicate very frequently with RabbitMQ and that can cause issues for some other services. OpenStack-Ansible now supports independent RabbitMQ clusters for certain services. This allows a deployer to use a different RabbitMQ cluster for handling telemetry traffic than they use for handling nova&rsquo;s messages.</p>
<h3 id="powervm-support">PowerVM support</h3>
<p>Lots of changes were added to allow for multiple architecture support, which is required for full PowerVM support. Some additional fixes for higher I/O performance and OVS on Power support arrived as well.</p>
<h3 id="repo-server-improvements">Repo server improvements</h3>
<p>Building the repo server takes quite a bit of time as repositories are cloned, wheels are built, and virtual environments are assembled. A series of patches merged into the project that aim to reduce the time to build a repo server.</p>
<p>Previously, the repo server built every possible virtual environment that could be needed for an OpenStack-Ansible deployment. Today, the repo server only builds virtual environments for those services that will be deployed. This saves time during the build process and a fair amount of disk space as well.</p>
<p>Source code is also kept on the repo server so that it won&rsquo;t need to be downloaded again for multiple architecture builds.</p>
<p>Additional changes are on the way to only clone the necessary git repositories to the repo server.</p>
<h3 id="ubuntu-1604-xenial-support">Ubuntu 16.04 (Xenial) support</h3>
<p>Almost all of the OpenStack-Ansible roles in Newton have Ubuntu 16.04 support and the integrated gate job is turning green a lot more often this week. We still need some testers who can do some real world multi-server deployments and shake out any bugs that don&rsquo;t appear in an all-in-one (AIO) build.</p>
<h2 id="feedback">Feedback?</h2>
<p>The goal of this newsletter is three fold:</p>
<ul>
<li>
<p>Keep OpenStack-Ansible developers updated with new changes</p>
</li>
<li>
<p>Inform operators about new features, fixes, and long-term goals</p>
</li>
<li>
<p>Bring more people into the OpenStack-Ansible community to share their use</p>
<p>cases, bugs, and code</p>
</li>
</ul>
<p>Please let me know if you spot any errors, areas for improvement, or items that I missed altogether. I&rsquo;m <code>mhayden</code> on Freenode IRC and you can find me on <a href="https://twitter.com/majorhayden">Twitter</a> anytime.</p>
]]></content></item><item><title>Join me on Thursday to talk about OpenStack LBaaS and security hardening</title><link>https://major.io/2016/07/19/join-me-on-thursday-to-talk-about-openstack-lbaas-and-security/</link><pubDate>Tue, 19 Jul 2016 14:09:40 +0000</pubDate><guid>https://major.io/2016/07/19/join-me-on-thursday-to-talk-about-openstack-lbaas-and-security/</guid><description>If you want to learn more about load balancers and security hardening in OpenStack clouds, join me on Thursday for the Rackspace Office Hours podcast! Walter Bentley, Kenneth Hui and I will be talking about some of the new features available in the 12.2 release of Rackspace Private Cloud powered by OpenStack.
The release has a tech preview of OpenStack&amp;rsquo;s Load Balancer as a Service project. The new LBaaSv2 API is stable and makes it easy to create load balancers, add pools, and add members.</description><content type="html"><![CDATA[<p><img src="/wp-content/uploads/2016/07/podcast-header-openstack.png" alt="1"></p>
<p>If you want to learn more about load balancers and security hardening in <a href="http://openstack.org/">OpenStack</a> clouds, join me on Thursday for the <a href="http://ohpodcast.com/live/">Rackspace Office Hours podcast</a>! <a href="https://twitter.com/djstayflypro">Walter Bentley</a>, <a href="https://twitter.com/kenhuiny">Kenneth Hui</a> and I will be talking about some of the new features available in the <a href="http://blog.rackspace.com/rackspace-private-cloud-v12-2-hardening-openstack/">12.2 release</a> of <a href="https://www.rackspace.com/cloud/private/openstacksolutions/openstack">Rackspace Private Cloud powered by OpenStack</a>.</p>
<p>The release has a tech preview of OpenStack&rsquo;s <a href="https://wiki.openstack.org/wiki/Neutron/LBaaS">Load Balancer as a Service project</a>. The new LBaaSv2 API is stable and makes it easy to create load balancers, add pools, and add members. Health monitors can watch over servers and remove those servers from the load balancers if they don&rsquo;t respond properly.</p>
<p><img src="/wp-content/uploads/2016/04/OpenStack-Summit-Austin-2016-Automated-Security-Hardening-with-OpenStack-Ansible-Major-Hayden-1.png" alt="9"></p>
<p>I <a href="/2016/04/26/talk-recap-automated-security-hardening-openstack-ansible/">talked about the security hardening feature</a> extensively at this year&rsquo;s OpenStack Summit in Austin and it is now available in the 12.2 release of RPC.</p>
<p>The new Ansible role and its tasks apply over 200 security hardening configurations to OpenStack hosts (control plane and hypervisors) and it comes with extensive auditor-friendly documentation. The documentation also allows deployers to fine-tune many of the configurations and disable the ones they don&rsquo;t want. Deployers also have the option to tighten some configurations depending on their industry requirements.</p>
<p>Join us this Thursday, July 21st, at 1:00 PM CDT (<a href="http://everytimezone.com/#2016-7-21,360,cn3">check your time zone</a>) to talk more about these new features and OpenStack in general.</p>
]]></content></item><item><title>Bring back two and three finger taps in Fedora 24</title><link>https://major.io/2016/07/05/bring-back-two-three-finger-taps-fedora-24/</link><pubDate>Wed, 06 Jul 2016 04:54:13 +0000</pubDate><guid>https://major.io/2016/07/05/bring-back-two-three-finger-taps-fedora-24/</guid><description>Most of the recent Fedora upgrades have been quite smooth. There were definitely some rough spots back in Fedora 15 and Fedora 17 with the /bin migration and the switch to systemd. The upgrade from Fedora 23 to Fedora 24 has been really easy except for one minor quirk: my two and three finger taps don&amp;rsquo;t seem to work on the touchpad.
I use a Lenovo ThinkPad X1 Carbon (3rd gen) and it has a clickpad along with physical buttons across the top.</description><content type="html"><![CDATA[<p><a href="/wp-content/uploads/2012/01/fedorainfinity.png"><!-- raw HTML omitted --></a>Most of the recent Fedora upgrades have been quite smooth. There were definitely some rough spots back in Fedora 15 and Fedora 17 with the <code>/bin</code> migration and the switch to systemd. The upgrade from Fedora 23 to Fedora 24 has been really easy except for one minor quirk: my two and three finger taps don&rsquo;t seem to work on the touchpad.</p>
<p>I use a <a href="/2015/03/30/review-lenovo-x1-carbon-3rd-generation-and-linux/">Lenovo ThinkPad X1 Carbon (3rd gen)</a> and it has a clickpad along with physical buttons across the top. I use the two finger taps (to do a secondary click) frequently. After the Fedora 24 upgrade, I can still do <em>clicks</em> with one, two or three fingers, but the <em>taps</em> don&rsquo;t work.</p>
<p>After a little digging in <code>xinput</code>, I began to narrow down the problem:</p>
<pre><code>[major@arsenic ~]$ xinput list
⎡ Virtual core pointer                      id=2    [master pointer  (3)]
⎜   ↳ Virtual core XTEST pointer                id=4    [slave  pointer  (2)]
⎜   ↳ SynPS/2 Synaptics TouchPad                id=11   [slave  pointer  (2)]
⎜   ↳ TPPS/2 IBM TrackPoint                     id=12   [slave  pointer  (2)]
⎣ Virtual core keyboard                     id=3    [master keyboard (2)]
    ↳ Virtual core XTEST keyboard               id=5    [slave  keyboard (3)]
    ↳ Power Button                              id=6    [slave  keyboard (3)]
    ↳ Video Bus                                 id=7    [slave  keyboard (3)]
    ↳ Sleep Button                              id=8    [slave  keyboard (3)]
    ↳ Integrated Camera                         id=9    [slave  keyboard (3)]
    ↳ AT Translated Set 2 keyboard              id=10   [slave  keyboard (3)]
    ↳ ThinkPad Extra Buttons                    id=13   [slave  keyboard (3)]
[major@arsenic ~]$ xinput list-props 11 | grep Tap
    Synaptics Tap Time (271):   180
    Synaptics Tap Move (272):   252
    Synaptics Tap Durations (273):  180, 100, 100
    Synaptics Tap Action (285): 0, 0, 0, 0, 1, 0, 0
</code></pre><h2 id="hunting-for-a-fix">Hunting for a fix</h2>
<p>It seems like this <code>Synaptics Tap Action (285)</code> is what I need to adjust. What do those numbers mean, anyway?</p>
<p>After some searching, I found the answer in some <a href="ftp://www.x.org/pub/X11R7.5/doc/man/man4/synaptics.4.html">Synaptics documentation</a>:</p>
<pre><code>Synaptics Tap Action
8 bit, up to MAX_TAP values (see synaptics.h), 0 disables an element. order: RT, RB, LT, LB, F1, F2, F3.
</code></pre><p>This seems like what I want, but what do those abbreviations mean at the end? I scrolled up on the page and found something useful:</p>
<pre><code>Option &quot;RTCornerButton&quot; &quot;integer&quot;
Which mouse button is reported on a right top corner tap. Set to 0 to disable. Property: &quot;Synaptics Tap Action&quot;
Option &quot;RBCornerButton&quot; &quot;integer&quot;
Which mouse button is reported on a right bottom corner tap. Set to 0 to disable. Property: &quot;Synaptics Tap Action&quot;
Option &quot;LTCornerButton&quot; &quot;integer&quot;
Which mouse button is reported on a left top corner tap. Set to 0 to disable. Property: &quot;Synaptics Tap Action&quot;
Option &quot;LBCornerButton&quot; &quot;integer&quot;
Which mouse button is reported on a left bottom corner tap. Set to 0 to disable. Property: &quot;Synaptics Tap Action&quot;
Option &quot;TapButton1&quot; &quot;integer&quot;
Which mouse button is reported on a non-corner one-finger tap. Set to 0 to disable. Property: &quot;Synaptics Tap Action&quot;
Option &quot;TapButton2&quot; &quot;integer&quot;
Which mouse button is reported on a non-corner two-finger tap. Set to 0 to disable. Property: &quot;Synaptics Tap Action&quot;
Option &quot;TapButton3&quot; &quot;integer&quot;
Which mouse button is reported on a non-corner three-finger tap. Set to 0 to disable. Property: &quot;Synaptics Tap Action&quot;
</code></pre><p>The last three are the ones I care about. Then the abbreviations made sense:</p>
<ul>
<li>F1: TapButton1</li>
<li>F2: TapButton2</li>
<li>F3: TapButton3</li>
</ul>
<p>The <code>TapButton1</code> setting was already set to <em>1</em>, which means a primary tap. I need <code>TapButton2</code> set to <em>3</em> (two fingers for a secondary button tap) and <code>TapButton3</code> set to <em>2</em> (three fingers for a middle button tap). Let&rsquo;s try with <code>xinput</code> directly first:</p>
<pre><code>xinput set-prop 11 &quot;Synaptics Tap Action&quot; 0 0 0 0 1 3 2
</code></pre><p><strong>SUCCESS!</strong> The secondary and middle taps have returned!</p>
<h2 id="making-it-stick">Making it stick</h2>
<p>Let&rsquo;s make the setting permanent. You could add this to a <code>~/.xprofile</code> or some other file that the display manager runs, but this isn&rsquo;t helpful if you have a touchpad that could be removed or re-added (like a USB touchpad). For this, we need an extra X configuration file.</p>
<p>I created a file called <code>/etc/X11/xorg.conf.d/99-xinput-fix-multi-finger-taps.conf</code> and added some configuration:</p>
<pre><code>Section &quot;InputClass&quot;
       Identifier &quot;tap-by-default&quot;
       MatchIsTouchpad &quot;on&quot;
       Option &quot;TapButton1&quot; &quot;1&quot;
       Option &quot;TapButton2&quot; &quot;3&quot;
       Option &quot;TapButton3&quot; &quot;2&quot;
EndSection
</code></pre><p>The configuration file specifies what we want to occur when one, two or three fingers tap on the pad. We&rsquo;re also being careful here to match only on touchpads to avoid tinkering with a mouse or other pointer device.</p>
<p>Log out of your X session and log in again. Your two and three finger taps should still be working!</p>
]]></content></item><item><title>Talk recap: The friendship of OpenStack and Ansible</title><link>https://major.io/2016/06/28/talk-recap/</link><pubDate>Wed, 29 Jun 2016 03:43:21 +0000</pubDate><guid>https://major.io/2016/06/28/talk-recap/</guid><description>The 2016 Red Hat Summit is underway in San Francisco this week and I delivered a talk with Robyn Bergeron earlier today. Our talk, When flexibility met simplicity: The friendship of OpenStack and Ansible, explained how Ansible can reduce the complexity of OpenStack environments without sacrificing the flexibility that private clouds offer.
The talk started at the same time as lunch began and the Partner Pavilion first opened, so we had some stiff competition for attendees' attention.</description><content type="html"><![CDATA[<p><img src="/wp-content/uploads/2016/06/When-flexibility-met-simplicity-The-friendship-of-OpenStack-and-Ansible-Red-Hat-Summit-2016.png" alt="1"></p>
<p>The 2016 Red Hat Summit is underway in San Francisco this week and I delivered a talk with <a href="https://twitter.com/robynbergeron">Robyn Bergeron</a> earlier today. Our talk, <em><a href="https://rh2016.smarteventscloud.com/connect/sessionDetail.ww?SESSION_ID=75675">When flexibility met simplicity: The friendship of OpenStack and Ansible</a></em>, explained how Ansible can reduce the complexity of OpenStack environments without sacrificing the flexibility that private clouds offer.</p>
<p>The talk started at the same time as lunch began and the Partner Pavilion first opened, so we had some stiff competition for attendees' attention. However, the live demo worked without issues and we had some good questions when the talk was finished.</p>
<p>This post will cover some of the main points from the talk and I&rsquo;ll share some links for the talk itself and some of the playbooks we ran during the live demo.</p>
<h2 id="it-is-complex-and-difficult">IT is complex and difficult</h2>
<p>Getting resources for projects at many companies is challenging. OpenStack makes this a little easier by delivering compute, network, and storage resources on demand. However, OpenStack&rsquo;s flexibility is a double-edged sword. It makes it very easy to obtain virtual machines, but it can be challenging to install and configure.</p>
<p>Ansible reduces some of that complexity without sacrificing flexibility. Ansible comes with plenty of pre-written modules that manage an OpenStack cloud at multiple levels for multiple types of users. Consumers, operators, and deployers can save time and reduce errors by using these modules and providing the parameters that fit their environment.</p>
<h2 id="ansible-and-openstack">Ansible and OpenStack</h2>
<p>Ansible and OpenStack are both open source projects that are heavily based on Python. Many of the same dependencies needed for Ansible are needed for OpenStack, so there is very little additional software required. Ansible tasks are written in YAML and the user only needs to pass some simple parameters to an existing module to get something done.</p>
<p>Operators are in a unique position since they can use Ansible to perform typical IT tasks, like creating projects and users. They can also assign fine-grained permissions to users with roles via reusable and extensible playbooks. Deployers can use projects like <a href="https://github.com/openstack/openstack-ansible">OpenStack-Ansible</a> to deploy a production-ready OpenStack cloud.</p>
<h2 id="lets-build-something">Let&rsquo;s build something</h2>
<p>In the talk, we went through a scenario for a live demo. In the scenario, the marketing team needed a new website for a new campaign. The IT department needed to create a project and user for them, and then the marketing team needed to build a server. This required some additional tasks, such as adding ssh keys, creating a security group (with rules) and adding a new private network.</p>
<p>The files from the live demo are up on GitHub:</p>
<ul>
<li><a href="https://github.com/major/ansible-openstack-summit-demo">major/ansible-openstack-summit-demo</a></li>
</ul>
<p>In the <code>operator-prep.yml</code>, we created a project and added a user to the project. That user was given the admin role so that the marketing team could have full access to their own project.</p>
<p>From there, we went through the tasks as if we were a member of the marketing team. The <code>marketing.yml</code> playbook went through all of the tasks to prepare for building an instance, actually building the instance, and then adding that instance to the dynamic inventory in Ansible. That playbook also verified the instance was up and performed additional configuration of the virtual machine itself - all in the same playbook.</p>
<h2 id="whats-next">What&rsquo;s next?</h2>
<p>Robyn shared lots of ways to <a href="https://www.ansible.com/community">get involved in the Ansible community</a>. <a href="https://www.ansible.com/ansiblefest">AnsibleFest 2016</a> is rapidly approaching and the <a href="https://www.openstack.org/summit/barcelona-2016/">OpenStack Summit in Barcelona</a> is happening this October.</p>
<h2 id="downloads">Downloads</h2>
<p>The presentation is available in a few formats:</p>
<ul>
<li><a href="http://majorhayden.com/presentations/When%20flexibility%20met%20simplicity-%20The%20friendship%20of%20OpenStack%20and%20Ansible%20-%20Red%20Hat%20Summit%202016.pdf">PDF</a></li>
<li><a href="http://www.slideshare.net/MajorHayden/when-flexibility-met-simplicity-the-friendship-of-openstack-and-ansible">Slideshare</a></li>
</ul>
]]></content></item><item><title>My list of must-see sessions at Red Hat Summit 2016</title><link>https://major.io/2016/06/26/almost-time-red-hat-summit-2016/</link><pubDate>Sun, 26 Jun 2016 10:52:38 +0000</pubDate><guid>https://major.io/2016/06/26/almost-time-red-hat-summit-2016/</guid><description>The Red Hat Summit starts this week in San Francisco, and a few folks asked me about the sessions that they shouldn&amp;rsquo;t miss. The schedule can be overwhelming for first timers and it can be difficult at times to discern the technical sessions from the ones that are more sales-oriented.
If you&amp;rsquo;re in San Francisco, and you want to learn a bit more about using Ansible to manage OpenStack environments, come to the session that I am co-presenting with Robyn Bergeron: When flexibility met simplicity: The friendship of OpenStack and Ansible.</description><content type="html"><![CDATA[<p><a href="/wp-content/uploads/2016/06/Screen-Shot-2016-01-05-at-10.52.09-AM.png"><!-- raw HTML omitted --></a>The <a href="https://www.redhat.com/en/summit">Red Hat Summit</a> starts this week in San Francisco, and a few folks asked me about the sessions that they shouldn&rsquo;t miss. The schedule can be overwhelming for first timers and it can be difficult at times to discern the technical sessions from the ones that are more sales-oriented.</p>
<p>If you&rsquo;re in San Francisco, and you want to learn a bit more about using Ansible to manage OpenStack environments, come to the session that I am co-presenting with Robyn Bergeron: <a href="https://rh2016.smarteventscloud.com/connect/sessionDetail.ww?SESSION_ID=75675">When flexibility met simplicity: The friendship of OpenStack and Ansible</a>.</p>
<h2 id="confirmed-good-sessions">Confirmed good sessions</h2>
<p>Here&rsquo;s a list of sessions that I&rsquo;ve seen in the past and I still highly recommend:</p>
<ul>
<li><a href="https://rh2016.smarteventscloud.com/connect/sessionDetail.ww?SESSION_ID=45621&amp;tclass=popup">Security-enhanced Linux for mere mortals</a> from Thomas Cameron</li>
<li><a href="https://rh2016.smarteventscloud.com/connect/sessionDetail.ww?SESSION_ID=41745&amp;tclass=popup">Container security</a> from Dan Walsh</li>
</ul>
<p>Dan is also doing a lab for container security called &ldquo;<a href="https://rh2016.smarteventscloud.com/connect/sessionDetail.ww?SESSION_ID=45181&amp;tclass=popup">A practical introduction to container security</a>&rdquo; that should be really interesting if you enjoy his regular sessions.</p>
<h2 id="sessions-i-want-to-see-this-year">Sessions I want to see this year</h2>
<ul>
<li><a href="https://rh2016.smarteventscloud.com/connect/sessionDetail.ww?SESSION_ID=44482&amp;tclass=popup">Are you listening to what SELinux is telling you</a> from Dan Walsh and Roland Wolters</li>
<li><a href="https://rh2016.smarteventscloud.com/connect/sessionDetail.ww?SESSION_ID=44515&amp;tclass=popup">Optimizing code for modern processors</a> from William Cohen</li>
<li><a href="https://rh2016.smarteventscloud.com/connect/sessionDetail.ww?SESSION_ID=44920">Ansible BoF</a> from Greg DeKoenigsberg and James Cammarata</li>
<li><a href="https://rh2016.smarteventscloud.com/connect/sessionDetail.ww?SESSION_ID=41743&amp;tclass=popup#.V2-yluz3SEY.twitter">Docker versis Systemd - Can&rsquo;t we just get along?</a> from Dan Walsh</li>
<li><a href="https://rh2016.smarteventscloud.com/connect/sessionDetail.ww?SESSION_ID=42478&amp;tclass=popup">Tracking huge files with Git LFS</a> from Tim Pettersen</li>
<li><a href="https://rh2016.smarteventscloud.com/connect/sessionDetail.ww?SESSION_ID=43820&amp;tclass=popup">Developer meet designer</a> from Andres Galante and Brian Leathern</li>
</ul>
<p>If you want to talk OpenStack, Ansible, or Rackspace at the Summit, send me something <a href="https://twitter.com/majorhayden/">on Twitter</a>. Have a great time!</p>
]]></content></item><item><title>New SELinux shirts are available</title><link>https://major.io/2016/06/16/new-selinux-shirts-available/</link><pubDate>Thu, 16 Jun 2016 21:22:52 +0000</pubDate><guid>https://major.io/2016/06/16/new-selinux-shirts-available/</guid><description>With the upcoming Red Hat Summit 2016 in San Francisco almost upon us, I decided to update the old SELinux shirts with two new designs:
You can buy these now over at Spreadshirt! There are styles there for men and women and I&amp;rsquo;ve priced them as low as the store will allow.
Spreadshirt is also running a sale for 15% off T-shirts until June 21st with the code TSHIRT16. Let&amp;rsquo;s make SELinux enforcing again!</description><content type="html"><![CDATA[<p>With the upcoming Red Hat Summit 2016 in San Francisco almost upon us, I decided to update the old SELinux shirts with two new designs:</p>
<!-- raw HTML omitted -->
<p>You can buy these now <a href="https://shop.spreadshirt.com/462205/">over at Spreadshirt</a>! There are styles there for men and women and I&rsquo;ve priced them as low as the store will allow.</p>
<p>Spreadshirt is also running a sale for 15% off T-shirts until June 21st with the code <code>TSHIRT16</code>. Let&rsquo;s make SELinux enforcing again!</p>
]]></content></item><item><title>What’s Happening in OpenStack-Ansible (WHOA) – June 2016</title><link>https://major.io/2016/06/15/whats-happening-openstack-ansible-whoa-june-2016/</link><pubDate>Wed, 15 Jun 2016 19:58:52 +0000</pubDate><guid>https://major.io/2016/06/15/whats-happening-openstack-ansible-whoa-june-2016/</guid><description>The world of OpenStack moves quickly. Each day brings new features, new bug fixes, and new ways of thinking. The OpenStack-Ansible community strives to understand these changes and make them easier for operators to implement.
The OpenStack-Ansible project is a collection of playbooks and roles written by operators for operators. These playbooks make it easier to deploy, maintain, and upgrade an OpenStack cloud.
Keeping up with the changes in the OpenStack-Ansible project is challenging.</description><content type="html"><![CDATA[<p><a href="/wp-content/uploads/2011/11/openstack-justheo.png"><!-- raw HTML omitted --></a>The world of OpenStack moves quickly. Each day brings new features, new bug fixes, and new ways of thinking. The <a href="https://wiki.openstack.org/wiki/OpenStackAnsible">OpenStack-Ansible community</a> strives to understand these changes and make them easier for operators to implement.</p>
<p>The <a href="http://docs.openstack.org/developer/openstack-ansible/">OpenStack-Ansible project</a> is a collection of playbooks and roles written by operators for operators. These playbooks make it easier to deploy, maintain, and upgrade an OpenStack cloud.</p>
<p>Keeping up with the changes in the OpenStack-Ansible project is challenging. After reading Hugh Blemings' <a href="http://hugh.blemings.id.au/openstack/lwood/">Last Week in OpenStack Dev</a> reports, I thought it would be useful to have a more focused newsletter on where OpenStack-Ansible has been</p>
<p>recently and where it will go. My goal is to share this on a monthly cadence, but that may change over time.</p>
<p>Without further ado, here is the inaugural WHOA report for June 2016!</p>
<h2 id="new-releases">New releases</h2>
<p>The OpenStack-Ansible project has four active branches under various stages of development.</p>
<h3 id="newton">Newton</h3>
<p>The Newton (master) branch is still under heavy development and will be released later in the year along with other OpenStack projects. For more details on the Newton development efforts, take a look at the <strong><em>Notable developments</em></strong> and <strong><em>On the horizon</em></strong> sections below.</p>
<h3 id="mitaka">Mitaka</h3>
<p>The latest release in the Mitaka branch is 13.1.2 and it was <a href="https://review.openstack.org/#/c/324759/">released</a> on June 2nd, 2016.</p>
<p>This release contained several new backported features and fixes, such as:</p>
<ul>
<li>Horizon: LBaaS v2 panels and IPv6 management support can be enabled</li>
<li>Swift: better handling for full disks</li>
<li>Security: fixes for audit logs filling disks and ssh configurations with <code>Match</code> stanzas</li>
</ul>
<p>More details are available in the full <a href="http://docs.openstack.org/releasenotes/openstack-ansible/mitaka.html">release notes for 13.1.2</a>.</p>
<p>The release also <a href="https://gist.github.com/major/dc2f1f9c99adaa180f69e176fde2fff9">contains many updates</a> for OpenStack services and related dependencies.</p>
<h3 id="liberty">Liberty</h3>
<p>The latest release in the Liberty branch is 12.0.14 and it was <a href="https://review.openstack.org/#/c/324760/">released</a> on June 2nd 2016.</p>
<p>This release contains many of the same fixes that appeared in the Mitaka release (see above).</p>
<p>More details are available in the full <a href="http://docs.openstack.org/releasenotes/openstack-ansible/liberty.html">release notes for 12.0.14</a>.</p>
<p>The release also <a href="https://gist.github.com/major/02d4ee0c2dac62ed279d8f052029c100">contains many updates</a> for OpenStack services and related dependencies.</p>
<h3 id="kilo">Kilo</h3>
<p>Although no releases have appeared in the Kilo branch in the last 30 days, <strong>work is being done for the final kilo release</strong>. This release will be tagged as 11.2.17 and should be available once the upstream OpenStack projects have completed their <em>kilo-eol</em> tags.</p>
<p>There is a <a href="https://openstack.nimeyo.com/86382/openstack-dev-stable-all-tagging-kilo-eol-for-the-world">mailing list thread</a> about the <em>kilo-eol</em> Kilo tagging efforts. It has status updates that are specific to each OpenStack project.</p>
<h2 id="notable-discussions">Notable discussions</h2>
<p>Want to discuss OpenStack-Ansible with the community? We want to hear from you!</p>
<p>Feel free to join us in <code>#openstack-ansible</code> on Freenode or send email to <a href="mailto:openstack-dev@lists.openstack.org">openstack- dev@lists.openstack.org</a> with <code>[openstack-ansible]</code> in the subject line. Our meeting times and logs from previous meetings are in the <a href="https://wiki.openstack.org/wiki/Meetings/openstack-ansible">OpenStack-Ansible wiki</a>.</p>
<h3 id="newton-mid-cycle">Newton mid-cycle</h3>
<p>The planning for the Newton mid-cycle is underway. It will be held at Rackspace&rsquo;s headquarters in San Antonio, Texas from August 10th through the 12th. You can find lots of details about the venues and hotel arrangements in the <a href="https://etherpad.openstack.org/p/osa-midcycle-newton">etherpad</a>.</p>
<p>It&rsquo;s possible that remote participation will work in the room, but that isn&rsquo;t guaranteed at this time. Previous attempts at videoconferencing didn&rsquo;t work terribly well, but we will give it our best try!</p>
<h2 id="notable-developments">Notable developments</h2>
<p>Many blueprints are in flight for the Newton release. I will touch on some of the most important and the most impactful ones here.</p>
<h3 id="ubuntu-1604-xenial-support">Ubuntu 16.04 (Xenial) support</h3>
<p>Many of the OpenStack-Ansible roles are compatible with Ubuntu 16.04 or are on the way to becoming compatible. All of the roles have non-voting gate jobs enabled for Ubuntu 16.04 to make it easier to for developers to see how their patches work on multiple versions of Ubuntu.</p>
<p>For the latest updates on which roles are compatible with Ubuntu 16.04, refer to the <a href="https://etherpad.openstack.org/p/openstack-ansible-newton-ubuntu16-04">etherpad</a>. The blueprint is <a href="https://blueprints.launchpad.net/openstack-ansible/+spec/multi-platform-host">on Launchpad</a> and you can follow along with the latest patches by filtering the <a href="https://review.openstack.org/#/q/topic:bp/support-ubuntu-1604,n,z">bp/support-ubuntu-1604</a> topic in Gerrit.</p>
<h3 id="installation-documentation-overhaul">Installation documentation overhaul</h3>
<p>At the Summit in Austin, many people mentioned that the OpenStack-Ansible installation documentation contains lots of great information, but it is very difficult to navigate. It can become even more challenging for newcomers.</p>
<p>A <a href="http://specs.openstack.org/openstack/openstack-ansible-specs/specs/newton/osa-install-guide-overhaul.html">new spec</a> lays out the plans for the improvements in the Newton release and work is already underway.</p>
<p>Here are some more helpful links if you want to follow the development or make improvements of your own:</p>
<ul>
<li><a href="https://blueprints.launchpad.net/openstack-ansible/+spec/osa-install-guide-overhaul">Blueprint</a></li>
<li><a href="https://review.openstack.org/#/q/topic:bp/osa-install-guide-overhaul,n,z">bp/osa-install-guide-overhaul</a> topic in Gerrit</li>
<li><a href="https://etherpad.openstack.org/p/openstack-ansible-newton-role-docs">Etherpad with notes from the OpenStack Summit</a></li>
</ul>
<h3 id="ansible-21-support">Ansible 2.1 support</h3>
<p>Ansible 2.1 <a href="https://review.openstack.org/#/c/321042/">is now the default Ansible version</a> used in OpenStack-Ansible for the Newton release. Individual role cleanups to address deprecation warnings and bare variables are <a href="https://review.openstack.org/#/q/topic:bp/ansible-2-1-support">in progress</a>.</p>
<h2 id="on-the-horizon">On the horizon</h2>
<p>The OpenStack-Ansible community is always looking for ways to improve OpenStack-Ansible. Some of these improvements allow the project to do something completely new, such as deploying a new OpenStack service. Others make the project easier to use or reduce the time required for deployment.</p>
<p>The following work items are in various stages of development in the Newton release.</p>
<h3 id="astara">Astara</h3>
<p>Work is underway to deploy Astara with OpenStack-Ansible. Astara provides a new way to handle layer 3 networking services, such as routers, load balancers, and VPN endpoints. Neutron would still handle the layer 2 networking (such as VLAN tagging), but Astara would run layer 3 services within service virtual machines that exist under an admin tenant.</p>
<p>For more details, see the <a href="https://blueprints.launchpad.net/openstack-ansible/+spec/add-support-for-astara">blueprint</a> or the <a href="https://etherpad.openstack.org/p/astara-openstack-ansible">etherpad</a>. Phil Hopkins is leading this effort and hopes to have a spec proposed soon.</p>
<h3 id="magnum">Magnum</h3>
<p>The <a href="https://wiki.openstack.org/wiki/Magnum">Magnum</a> project allows users to deploy and manage Docker and Kubernetes deployments via calls to the Magnum API.</p>
<p>Many of the OpenStack-Ansible and Magnum developers have <a href="https://review.openstack.org/#/q/project:openstack/openstack-ansible-os_magnum">submitted patches</a> to make the <a href="https://github.com/openstack/openstack-ansible-os_magnum">os_magnum role</a> more mature and feature-rich.</p>
<h3 id="multiple-architectures-and-powervm">Multiple architectures and PowerVM</h3>
<p>We welcomed some new community members from IBM who are working to implement PowerVM support in OpenStack-Ansible. Since most of our use cases were on x86-based systems, this requires some rework in various places within the project. Some of the biggest changes will be made within the repo server since some python modules contain code that must be compiled on the Power architecture.</p>
<p>Adam Reznechek from IBM is <a href="https://review.openstack.org/#/c/329637/">drafting a spec</a> that provides details on the required changes. These changes should allow deployers to mix and match hosts within an OpenStack environment. This could allow for some interesting use cases:</p>
<ul>
<li>Mixed deployments of x86 and Power hypervisors</li>
<li>x86 control plane with Power hypervisors</li>
<li>Power control plane with x86 hypervisors</li>
</ul>
<p>This flexible framework could allow for ARM development in later releases.</p>
<h3 id="operator-scripts">Operator scripts</h3>
<p>A new repository called <a href="https://github.com/openstack/openstack-ansible-ops">openstack-ansible-ops</a> will contain resources for operators of OpenStack-Ansible clouds.</p>
<p>One of the first proposed scripts is <code>osa-differ.py</code>. <a href="https://review.openstack.org/#/c/328469/">This script</a> allows operators to understand the OpenStack changes that exist between two OpenStack-Ansible releases. If you&rsquo;re waiting on a fix for a bug in an OpenStack service, you could use this script to see if the fix made it into a particular version of OpenStack-Ansible.</p>
<h3 id="sahara">Sahara</h3>
<p>The <a href="https://wiki.openstack.org/wiki/Sahara">Sahara</a> project provisions services like Hadoop or Spark to process big chunks of data. OpenStack-Ansible can deploy Sahara now with the new <a href="https://github.com/openstack/openstack-ansible-os_sahara">os_sahara role</a>.</p>
<h3 id="xen">Xen</h3>
<p>Many large clouds use the Xen hypervisor and Antony Messerli is working to implement Xen hypervisor support with libvirt in OpenStack-Ansible. A <a href="http://specs.openstack.org/openstack/openstack-ansible-specs/specs/newton/xen-virt-driver.html">new spec</a> recently merged that explains the benefits behind the work as well as the changes that are required to make it work.</p>
<h2 id="feedback">Feedback?</h2>
<p>The goal of this newsletter is three fold:</p>
<ul>
<li>
<p>Keep OpenStack-Ansible developers updated with new changes</p>
</li>
<li>
<p>Inform operators about new features, fixes, and long-term goals</p>
</li>
<li>
<p>Bring more people into the OpenStack-Ansible community to share their use</p>
<p>cases, bugs, and code</p>
</li>
</ul>
<p>Please let me know if you spot any errors, areas for improvement, or items that I missed altogether. I&rsquo;m <code>mhayden</code> on Freenode IRC and you can find me on <a href="https://twitter.com/majorhayden">Twitter</a> anytime.</p>
]]></content></item><item><title>Automated security hardening with Ansible: May updates</title><link>https://major.io/2016/05/26/automated-security-hardening-with-ansible-may-updates/</link><pubDate>Fri, 27 May 2016 02:40:33 +0000</pubDate><guid>https://major.io/2016/05/26/automated-security-hardening-with-ansible-may-updates/</guid><description>Lots of work has gone into the openstack-ansible-security Ansible role since I delivered a talk about it last month at the OpenStack Summit in Austin. Attendees asked for quite a few new features and I&amp;rsquo;ve seen quite a few bug reports (and that&amp;rsquo;s a good thing).
Here&amp;rsquo;s a list of the newest additions since the Summit:
New features Ubuntu 16.04 LTS (Xenial) support The role now works with Ubuntu 16.04 and its newest features, including systemd.</description><content type="html"><![CDATA[<p><img src="/wp-content/uploads/2016/05/15843531002_f92f4e6c50_o-e1464316716989.jpg" alt="1"></p>
<p>Lots of work has gone into the <a href="https://github.com/openstack/openstack-ansible-security">openstack-ansible-security</a> Ansible role since I delivered <a href="/2016/04/26/talk-recap-automated-security-hardening-openstack-ansible/">a talk about it last month</a> at the OpenStack Summit in Austin. Attendees asked for quite a few new features and I&rsquo;ve seen quite a few bug reports (and that&rsquo;s a good thing).</p>
<p>Here&rsquo;s a list of the newest additions since the Summit:</p>
<h2 id="new-features">New features</h2>
<h3 id="ubuntu-1604-lts-xenial-support">Ubuntu 16.04 LTS (Xenial) support</h3>
<p>The role now works with Ubuntu 16.04 and its newest features, including systemd. You can use the same variables as you used with Ubuntu 14.04 and it should take the same actions. Documentation updates are mostly merged with a few straggling reviews in the queue.</p>
<h3 id="centos-7-support">CentOS 7 support</h3>
<p>With all of the work going into the role to support Ubuntu 16.04 and systemd, CentOS 7 wasn&rsquo;t a huge stretch. Many of the package names and file locations were a little different, but those are now moved out into variables files to reduce the repetition of tasks. Some of the Linux Security Module tasks needed adjustments since SELinux is a different beast than AppArmor.</p>
<h3 id="following-the-stig-more-closely">Following the STIG more closely</h3>
<p>One of the common questions I had at the summit was: &ldquo;Can I use this thing on my non-OpenStack environments?&rdquo; You definitely can, but many of the configurations were tweaked to avoid causing problems with OpenStack environments. Some users asked if the configurations could be made more generic so that they followed the STIG more closely. This would reduce some compliance headaches and allow more people to use the role.</p>
<p>So far, I&rsquo;ve been making some of these adjustments to fix more things rather than simply checking them. That should make it easier to get closer to the STIG&rsquo;s requirements.</p>
<p>Another proposed idea is to create vars files that meet different criteria. For example, one vars file might be the ultra-secure, follow-the-STIG-to-the-letter configuration. This would be good for users that already know they want to apply the STIG&rsquo;s requirements fully. There could be another vars file that would apply most of the STIG&rsquo;s requirements, but it would steer clear of changing anything that could disrupt a production OpenStack environment.</p>
<h2 id="the-future">The future</h2>
<p>Here are a subset of the future plans and ideas:</p>
<ul>
<li>Better reporting for users who need to feed data into vulnerability management applications or SIEMs for compliance checks</li>
<li>Better testing, possibly with customized OpenSCAP XCCDF files</li>
<li>Cross-referenced controls to other hardening guides, such as CIS Benchmarks</li>
</ul>
<p>If you have any other ideas, feel free to stop by <code>#openstack-ansible</code> or <code>#openstack-security</code> on Freenode. You can find me there as <em>mhayden</em> and I would really enjoy hearing about your use cases!</p>
<p><em>Photo credit: <a href="https://www.flickr.com/photos/89165847@N00/15843531002/">Mikecogh</a></em></p>
]]></content></item><item><title>Test Fedora 24 Beta in an OpenStack cloud</title><link>https://major.io/2016/05/24/test-fedora-24-beta-openstack-cloud/</link><pubDate>Wed, 25 May 2016 03:17:35 +0000</pubDate><guid>https://major.io/2016/05/24/test-fedora-24-beta-openstack-cloud/</guid><description>Although there are a few weeks remaining before Fedora 24 is released, you can test out the Fedora 24 Beta release today! This is a great way to get a sneak peek at new features and help find bugs that still need a fix.
The Fedora Cloud image is available for download from your favorite local mirror or directly from Fedora&amp;rsquo;s servers. In this post, I&amp;rsquo;ll show you how to import this image into an OpenStack environment and begin testing Fedora 24 Beta.</description><content type="html"><![CDATA[<p>Although there are a few weeks remaining before <a href="https://fedoraproject.org/wiki/Releases/24/Schedule">Fedora 24</a> is released, you can test out the Fedora 24 Beta release today! This is a great way to get a <a href="https://fedoraproject.org/wiki/Releases/24/ChangeSet">sneak peek at new features</a> and help find bugs that still need a fix.</p>
<p><a href="/wp-content/uploads/2012/01/fedorainfinity.png"><!-- raw HTML omitted --></a></p>
<p>The <a href="https://getfedora.org/en/cloud/">Fedora Cloud</a> image is available for download from your favorite <a href="https://admin.fedoraproject.org/mirrormanager/mirrors/Fedora/24/x86_64">local mirror</a> or directly from <a href="https://getfedora.org/en/cloud/download/">Fedora&rsquo;s servers</a>. In this post, I&rsquo;ll show you how to import this image into an OpenStack environment and begin testing Fedora 24 Beta.</p>
<p>One last thing: this is beta software. It has been reliable for me so far, but your experience may vary. I would recommend waiting for the final release before deploying any mission critical applications on it.</p>
<h2 id="importing-the-image">Importing the image</h2>
<p>The older glance client (version 1) allows you to import an image from a URL that is reachable from your OpenStack environment. This is helpful since my OpenStack cloud has a much faster connection to the internet (1 Gbps) than my home does (~ 20 mbps upload speed). However, the functionality to import from a URL was <a href="https://wiki.openstack.org/wiki/Glance-v2-v1-client-compatability">removed in version 2 of the glance client</a>. The <a href="http://docs.openstack.org/developer/python-openstackclient/">OpenStackClient</a> doesn&rsquo;t offer the feature either.</p>
<p>There are two options here:</p>
<ul>
<li>Install an older version of the glance client</li>
<li>Use Horizon (the web dashboard)</li>
</ul>
<p>Getting an older version of glance client installed is challenging. The OpenStack requirements file for the liberty release leaves the version of glance client without a maximum version cap and it&rsquo;s difficult to get all of the dependencies in order to make the older glance client work.</p>
<p>Let&rsquo;s use Horizon instead so we can get back to the reason for the post.</p>
<h2 id="adding-an-image-in-horizon">Adding an image in Horizon</h2>
<p>Log into the Horizon panel and click <strong>Compute &gt; Images</strong>. Click <strong>+ Create Image</strong> at the top right of the page and a new window should appear. Add this information in the window:</p>
<ul>
<li><strong>Name:</strong> Fedora 24 Cloud Beta</li>
<li><strong>Image Source:</strong> Image Location</li>
<li><strong>Image Location:</strong> See footnote<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup></li>
<li><strong>Format:</strong> QCOW2 - QEMU Emulator</li>
<li><strong>Copy Data:</strong> <em>ensure the box is checked</em></li>
</ul>
<p>When you&rsquo;re finished, the window should look like this:</p>
<p><a href="/wp-content/uploads/2016/05/horizon_image.png"><!-- raw HTML omitted --></a></p>
<p>Click <strong>Create Image</strong> and the images listing should show <strong>Saving</strong> for a short period of time. Once it switches to <strong>Active</strong>, you&rsquo;re ready to build an instance.</p>
<h2 id="building-the-instance">Building the instance</h2>
<p>Since we&rsquo;re already in Horizon, we can finish out the build process there.</p>
<p>On the image listing page, find the row with the image we just uploaded and click <strong>Launch Instance</strong> on the right side. A new window will appear. The <strong>Image Name</strong> drop down should already have the Fedora 24 Beta image selected. From here, just choose an instance name, select a security group and keypair (on the <em>Access &amp; Security</em> tab), and a network (on the <em>Networking</em> tab). Be sure to choose a flavor that has some available storage as well (<em>m1.tiny</em> is not enough).</p>
<p>Click <strong>Launch</strong> and wait for the instance to boot.</p>
<p>Once the instance build has finished, you can connect to the instance over ssh as the <em>fedora</em> user. If your <a href="/2016/05/16/troubleshooting-openstack-network-connectivity/">security group allows the connection</a> and your keypair was configured correctly, you should be inside your new Fedora 24 Beta instance!</p>
<p>Not sure what to do next? Here are some suggestions:</p>
<ul>
<li>Update all packages and reboot <em>(to ensure that you are testing the latest updates)</em></li>
<li>Install some familiar applications and verify that they work properly</li>
<li>Test out your existing automation or configuration management tools</li>
<li>Open bug tickets!</li>
</ul>
<section class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1" role="doc-endnote">
<p>Fedora 24 no longer exists on download mirrors. <a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</section>
]]></content></item><item><title>Troubleshooting OpenStack network connectivity</title><link>https://major.io/2016/05/16/troubleshooting-openstack-network-connectivity/</link><pubDate>Tue, 17 May 2016 02:43:41 +0000</pubDate><guid>https://major.io/2016/05/16/troubleshooting-openstack-network-connectivity/</guid><description>NOTE: This post is a work in progress. If you find something that I missed, feel free to leave a comment. I&amp;rsquo;ve made plenty of silly mistakes, but I&amp;rsquo;m sure I&amp;rsquo;ll make a few more. :)
Completing a deployment of an OpenStack cloud is an amazing feeling. There is so much automation and power at your fingertips as soon as you&amp;rsquo;re finished. However, the mood quickly turns sour when you create that first instance and it never responds to pings.</description><content type="html"><![CDATA[<p><em>NOTE: This post is a work in progress. If you find something that I missed, feel free to leave a comment. I&rsquo;ve made plenty of silly mistakes, but I&rsquo;m sure I&rsquo;ll make a few more. :)</em></p>
<hr>
<p>Completing a deployment of an OpenStack cloud is an <strong>amazing feeling</strong>. There is so much automation and power at your fingertips as soon as you&rsquo;re finished. However, the mood quickly turns sour when you create that first instance and it never responds to pings.</p>
<p>It&rsquo;s the same feeling I get when I hang Christmas lights every year only to find that a whole section didn&rsquo;t light up. If you&rsquo;ve ever seen <a href="https://en.wikipedia.org/wiki/National_Lampoon%27s_Christmas_Vacation">National Lampoon&rsquo;s Christmas Vacation</a>, you know what I&rsquo;m talking about:</p>
<p><a href="/wp-content/uploads/2016/05/Chevy-Chase-in-National-Lampoons-Christmas-Vacation.jpg"><!-- raw HTML omitted --></a></p>
<p>I&rsquo;ve stumbled into plenty of problems (and solutions) along the way and I&rsquo;ll detail them here in the hopes that it can help someone avoid throwing a keyboard across the room.</p>
<h2 id="security-groups">Security groups</h2>
<p>Security groups get their own section because I forget about them constantly. <a href="http://docs.openstack.org/openstack-ops/content/security_groups.html">Security groups</a> are a great feature that lets you limit inbound and outbound access to a particular network port.</p>
<p>However, OpenStack&rsquo;s default settings are fairly locked down. That&rsquo;s great from a security perspective, but it can derail your first instance build if you&rsquo;re not thinking about it.</p>
<p>You have two options to allow traffic:</p>
<ul>
<li>Add more permissive rules to the <code>default</code> security group</li>
<li>Create a new security group and add appropriate rules into it</li>
</ul>
<p>I usually ensure that ICMP traffic is allowed into any port with the <code>default</code> security group applied, and then I create a another security group specific to the class of server I&rsquo;m building (like <code>webservers</code>). Changing a security group rule or adding a new security group to a port takes effect in a few seconds.</p>
<h2 id="something-is-broken-in-the-instance">Something is broken in the instance</h2>
<p>Try to <a href="http://docs.openstack.org/user-guide/cli_access_instance_through_a_console.html">get console access</a> to the instance through Horizon or via the command line tools. I generally find an issue in one of these areas:</p>
<ul>
<li>The IP address, netmask, or default gateway are incorrect</li>
<li>Additional routes should have been applied, but were not applied</li>
<li>Cloud-init didn&rsquo;t run, or it had a problem when it ran</li>
<li>The default iptables policy in the instance is overly restrictive</li>
<li>The instance isn&rsquo;t configured to bring up an instance by default</li>
<li>Something is preventing the instance from getting a DHCP address</li>
</ul>
<p>If the network configuration looks incorrect, cloud-init may have had a problem during startup. Look in <code>/var/log/</code> or in <code>journald</code> for any explanation of why cloud-init failed.</p>
<p>There&rsquo;s also the chance that the network configuration is correct, but the instance can&rsquo;t get a DHCP address. Verify that there are no iptables rules in place on the instance that might block DHCP requests and replies.</p>
<p>Some Linux distributions don&rsquo;t send <a href="https://en.wikipedia.org/wiki/Address_Resolution_Protocol#ARP_announcements">gratuitous ARP packets</a> when they bring an interface online. Tools like <a href="https://en.wikipedia.org/wiki/Arping">arping</a> can help with these problems.</p>
<p>If you find that you can connect to almost anything from within the instance, but you can&rsquo;t connect to the instance from the outside, verify your security groups (see the previous section). In my experience, a lopsided ingress/egress filter almost always points to a security group problem.</p>
<h2 id="something-is-broken-in-openstacks-networking-layer">Something is broken in OpenStack&rsquo;s networking layer</h2>
<p>Within the OpenStack control plane, the nova service talks to neutron to create network ports and manage addresses on those ports. One of the requests or responses may have been lost along the way or you may have stumbled into a bug.</p>
<p>If your instance couldn&rsquo;t get an IP address via DHCP, make sure the DHCP agent is running on the server that has your neutron agents. Restarting the agent should bring the DHCP server back online if it isn&rsquo;t running.</p>
<p>You can also hop into the network namespace that the neutron agent uses for your network. Start by running:</p>
<pre><code># ip netns list
</code></pre><p>Look for a namespace that starts with <code>qdhcp-</code> and ends with your network&rsquo;s UUID. You can run commands inside that namespace to verify that networking is functioning:</p>
<pre><code># ip netns exec qdhcp-NETWORK_UUID ip addr
# ip netns exec qdhcp-NETWORK_UUID ping INSTANCE_IP_ADDRESS
</code></pre><p>If your agent can ping the instance&rsquo;s address, but you can&rsquo;t ping the instance&rsquo;s address, there could be a problem on the underlying network - either within the virtual networking layer (bridges and virtual switches) or on the hardware layer (between the server and upstream network devices).</p>
<p>Try to use <code>tcpdump</code> to dump traffic on the neutron agent and on the instance&rsquo;s network port. Do you see any traffic at all? You may find a problem with incorrect VLAN ID&rsquo;s here or you may see activity that gives you more clue (like one half of an ARP or DHCP exchange).</p>
<h2 id="something-is-broken-outside-of-openstack">Something is broken outside of OpenStack</h2>
<p>Diagnosing these problems can become a bit challenging since it involves logging into other systems.</p>
<p>If you are using VLAN networks, be sure that the proper VLAN ID is being set for the network. Run <code>openstack network show</code> and look for <code>provider:segmentation_id</code>. If that&rsquo;s correct, be sure that all of your servers can transmit packets with that VLAN tag applied. I often remember to allow tagged traffic on all of the hypervisors and then I forget to do the same in the control plane.</p>
<p>Be sure that your router has the VLAN configured and has the correct IP address configuration applied. It&rsquo;s possible that you&rsquo;ve configured all of the VLAN tags correctly in all places, but then fat-fingered an IP address in OpenStack or on the router.</p>
<p>While you&rsquo;re in the router, test some pings to your instance. If you can ping from the router to the instance, but not from your desk to the instance, your router might not be configured correctly.</p>
<p>For instances on private networks, ensure that you created a router on the network. This is something I tend to forget. Also, be sure that you have the right routes configured between you and your OpenStack environment so that you can route traffic to your private networks through the router. If this isn&rsquo;t feasible for you, another option could be OpenStack&rsquo;s <a href="https://github.com/openstack/neutron-vpnaas">VPN-as-a-service</a> feature.</p>
<p>Another issue could be the cabling between servers and the nearest switch. If a cable is crossed, it could mean that a valid VLAN is being blocked at the switch because it&rsquo;s coming in on the wrong port.</p>
<h2 id="when-its-something-else">When it&rsquo;s something else</h2>
<p>There are some situations that aren&rsquo;t covered here. If you think of any, please leave a comment below.</p>
<p>As with any other troubleshooting, I go back to this quote from <a href="https://en.wikipedia.org/wiki/Zebra_(medicine)">Dr. Theodore Woodward</a> about diagnosing illness in the medical field:</p>
<blockquote>
<p>When you hear hoofbeats, think of horses not zebras.</p>
</blockquote>
<p>Look for the simplest solutions and work from the smallest domain (the instance) to the widest (the wider network). Make small changes and go back to the instance each time to verify that something changed. Once you find the solution, document it! Someone will surely appreciate it later.</p>
]]></content></item><item><title>Getting started with gertty</title><link>https://major.io/2016/05/11/getting-started-gertty/</link><pubDate>Wed, 11 May 2016 13:45:53 +0000</pubDate><guid>https://major.io/2016/05/11/getting-started-gertty/</guid><description>When you&amp;rsquo;re ready to commit code in an OpenStack project, your patch will eventually land in a Gerrit queue for review. The web interface works well for most users, but it can be challenging to use when you have a large amount of projects to monitor. I recently became a core developer on the OpenStack-Ansible project and I searched for a better solution to handle lots of active reviews.
This is where gertty can help.</description><content type="html"><![CDATA[<p><img src="/wp-content/uploads/2016/05/2191026054_2780871e26_b-e1462974197375.jpg" alt="1"></p>
<p>When you&rsquo;re ready to commit code in an OpenStack project, your patch will eventually land in a <a href="https://www.gerritcodereview.com/">Gerrit</a> queue for review. The web interface works well for most users, but it can be challenging to use when you have a large amount of projects to monitor. I recently became a core developer on the OpenStack-Ansible project and I searched for a better solution to handle lots of active reviews.</p>
<p>This is where <a href="https://github.com/openstack/gertty">gertty</a> can help. It&rsquo;s a console-based application that helps you navigate reviews efficiently. I&rsquo;ll walk you through the installation and configuration process in the remainder of this post.</p>
<h2 id="installing-gertty">Installing gertty</h2>
<p>The gertty package is available via <a href="https://pypi.python.org/pypi/gertty">pip</a>, <a href="https://github.com/openstack/gertty">GitHub</a>, and various package managers for certain Linux distributions. If you&rsquo;re on Fedora, just install <code>python-gertty</code> via <code>dnf</code>.</p>
<p>In this example, we will use pip:</p>
<pre><code>pip install gertty
</code></pre><h2 id="configuration">Configuration</h2>
<p>You will need a <code>.gertty.yaml</code> file in your home directory for gertty to run. I have an <a href="https://gist.github.com/major/6449c2eb3b17a446c3a42e34b976f6df">example on GitHub</a> that gives you a good start:</p>
<p>Be sure to change the <code>username</code> and <code>password</code> parts to match your Gerrit username and password. For OpenStack&rsquo;s gerrit server, you can get these credentials in the <a href="https://review.openstack.org/#/settings/http-password">user settings area</a>.</p>
<h2 id="getting-synchronized">Getting synchronized</h2>
<p>Now that gertty is configured, start it up on the console:</p>
<pre><code>$ gertty
</code></pre><p>Type a capital L (SHIFT + L) and wait for the list of projects to appear on the screen. You can choose projects to subscribe to (note that these are different than Gerrit&rsquo;s watched projects) by pressing your &rsquo;s' key.</p>
<p>However, if you need to follow quite a few projects that match a certain pattern, there&rsquo;s an easier way. Quit gertty (CTRL - q) and adjust the sqlite database that gertty uses:</p>
<pre><code>$ sqlite3 .gertty.db
SQLite version 3.8.6 2014-08-15 11:46:33
Enter &quot;.help&quot; for usage hints.
sqlite&gt; SELECT count(*) FROM project WHERE name LIKE '%openstack-ansible%';
39
sqlite&gt; UPDATE project SET subscribed=1 WHERE name LIKE '%openstack-ansible%';
sqlite&gt;
</code></pre><p>In this example, I&rsquo;ve subscribed to all projects that contain the string <code>openstack-ansible</code>.</p>
<p>I can start gertty once more and wait for it to sync my new projects down to my local workstation. Keep an eye on the <code>Sync:</code> status at the top right of the screen. It will count up as it enumerates reviews to retrieve and then count down as those reviews are downloaded.</p>
<p>You can also create custom dashboards for gertty based on custom queries. In my example configuration file above, I have a special dashboard that contains all OpenStack-Ansible reviews. That dashboard appears whenever I press F5. You can customize these dashboards to include any custom queries that you need for your projects.</p>
<p><em>Photo credit: <a href="https://www.flickr.com/photos/dirtyf/2191026054">Frank Taillandier</a></em></p>
]]></content></item><item><title>Preventing Ubuntu 16.04 from starting daemons when a package is installed</title><link>https://major.io/2016/05/05/preventing-ubuntu-16-04-starting-daemons-package-installed/</link><pubDate>Thu, 05 May 2016 15:54:27 +0000</pubDate><guid>https://major.io/2016/05/05/preventing-ubuntu-16-04-starting-daemons-package-installed/</guid><description>I&amp;rsquo;ve gone on some mini-rants in other posts about starting daemons immediately after they&amp;rsquo;re installed in Ubuntu and Debian. Things are a little different in Ubuntu 16.04 and I thought it might be helpful to share some tips for that release.
Before we do that, let&amp;rsquo;s go over something. I still don&amp;rsquo;t understand why this is a common practice within Ubuntu and Debian.
Take a look at the postinst-systemd-start script within the init-systems-helpers package (source link):</description><content type="html"><![CDATA[<p>I&rsquo;ve gone on some mini-rants in <a href="https://major.io/2015/10/14/what-i-learned-while-securing-ubuntu/">other</a> <a href="https://major.io/2014/06/26/install-debian-packages-without-starting-daemons/">posts</a> about starting daemons immediately after they&rsquo;re installed in Ubuntu and Debian. Things are a little different in Ubuntu 16.04 and I thought it might be helpful to share some tips for that release.</p>
<p>Before we do that, let&rsquo;s go over something. I still don&rsquo;t understand why this is a common practice within Ubuntu and Debian.</p>
<p>Take a look at the <code>postinst-systemd-start</code> script within the <code>init-systems-helpers</code> package (<a href="https://github.com/Debian/debhelper/blob/master/autoscripts/postinst-systemd-start">source link</a>):</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#719e07">if</span> <span style="color:#719e07">[</span> -d /run/systemd/system <span style="color:#719e07">]</span>; <span style="color:#719e07">then</span>
    systemctl --system daemon-reload &gt;/dev/null <span style="color:#719e07">||</span> <span style="color:#b58900">true</span>
    deb-systemd-invoke start <span style="color:#586e75">#UNITFILES# &gt;/dev/null || true</span>
<span style="color:#719e07">fi</span>
</code></pre></div><p>The <code>daemon-reload</code> is totally reasonable. We must tell systemd that we just deployed a new unit file or it won&rsquo;t know we did it. However, the next line makes no sense. Why would you immediately force the daemon to start (or restart)? The <code>deb-systemd-invoke</code> script does check to see if the unit is disabled before taking action on it, which is definitely a good thing. However, this automatic management of running daemons shouldn&rsquo;t be handled by a package manager.</p>
<p>If you don&rsquo;t want your package manager handling your daemons, you have a few options:</p>
<h2 id="the-policy-rcd-method">The policy-rc.d method</h2>
<p>This method involves creating a script called <code>/usr/sbin/policy-rc.d</code> with a special exit code:</p>
<pre><code># echo -e '#!/bin/bash\nexit 101' &gt; /usr/sbin/policy-rc.d
# chmod +x /usr/sbin/policy-rc.d
# /usr/sbin/policy-rc.d
# echo $?
101
</code></pre><p>This script is checked by the <code>deb-systemd-invoke</code> script in the <code>init-systems-helpers package</code> (<a href="https://anonscm.debian.org/git/collab-maint/init-system-helpers.git/tree/script/deb-systemd-invoke#n70">source link</a>). As long as this script is in place, dpkg triggers won&rsquo;t cause daemons to start, stop, or restart.</p>
<p>You can start your daemon at any time with <code>systemctl start service_name</code> whenever you&rsquo;re ready.</p>
<h2 id="the-systemd-mask-method">The systemd mask method</h2>
<p>If you need to prevent a single package from starting after installation, you can use systemd&rsquo;s <a href="http://0pointer.de/blog/projects/three-levels-of-off">mask feature</a> for that. When you run <code>systemctl mask nginx</code>, it will symlink <code>/etc/systemd/system/nginx.service</code> to <code>/dev/null</code>. When systemd sees that, it won&rsquo;t start the daemon.</p>
<p>However, since the package isn&rsquo;t installed yet, we can just mask it with a symlink:</p>
<pre><code># ln -s /dev/null /etc/systemd/system/nginx.service
</code></pre><p>You can install nginx now, configure it to meet your requirements, and start the service. Just run:</p>
<pre><code># systemctl enable nginx
# systemctl start nginx
</code></pre>]]></content></item><item><title>802.1x with NetworkManager using nmcli</title><link>https://major.io/2016/05/03/802-1x-networkmanager-using-nmcli/</link><pubDate>Tue, 03 May 2016 19:23:24 +0000</pubDate><guid>https://major.io/2016/05/03/802-1x-networkmanager-using-nmcli/</guid><description>Authenticating to a wired or wireless network using 802.1x is simple using NetworkManager&amp;rsquo;s GUI client. However, this gets challenging on headless servers without a graphical interface. The nmcli command isn&amp;rsquo;t able to store credentials in a keyring and this causes problems when you try to configure an interfaces with 802.1x authentication.
If you aren&amp;rsquo;t familiar with 802.1x, there is some light reading and heavier reading available on the topic.
Start by setting some basic configurations on the interface using the nmcli editor shell:</description><content type="html"><![CDATA[<p>Authenticating to a wired or wireless network using 802.1x is simple using NetworkManager&rsquo;s GUI client. However, this gets challenging on headless servers without a graphical interface. The <code>nmcli</code> command isn&rsquo;t able to store credentials in a keyring and this causes problems when you try to configure an interfaces with 802.1x authentication.</p>
<p>If you aren&rsquo;t familiar with 802.1x, there is some <a href="https://en.wikipedia.org/wiki/IEEE_802.1X">light reading</a> and <a href="https://www.sans.org/reading-room/whitepapers/authentication/implementing-ieee-8021x-wired-networks-34520">heavier reading</a> available on the topic.</p>
<p>Start by setting some basic configurations on the interface using the <code>nmcli</code> editor shell:</p>
<pre><code># nmcli con edit CONNECTION_NAME
nmcli&gt; set ipv4.method auto
nmcli&gt; set 802-1x.eap peap
nmcli&gt; set 802-1x.identity USERNAME
nmcli&gt; set 802-1x.phase2-auth mschapv2
nmcli&gt; save
nmcli&gt; quit
</code></pre><p>Be sure to set the <code>802-1x.eap</code> and <code>802-1x.phase2-auth</code> to the appropriate values for your network. You might have noticed that the password isn&rsquo;t specified here. That&rsquo;s because NetworkManager has no access to a keyring where it can store the password. That comes next.</p>
<p>Create a new file called <code>/etc/NetworkManager/system-connections/CONNECTION_NAME</code> to hold your password. If your connection name has spaces in it, be sure to maintain those spaces in the filename. Add the following to that file:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ini" data-lang="ini"><span style="color:#719e07">[connection]</span>
id<span style="color:#719e07">=</span><span style="color:#2aa198">CONNECTION_NAME</span>

<span style="color:#719e07">[802-1x]</span>
password<span style="color:#719e07">=</span><span style="color:#2aa198">YOUR_8021X_PASSWORD</span>
</code></pre></div><p>Save the file and close it. Restart NetworkManager to pick up the changes:</p>
<pre><code>systemctl restart NetworkManager
</code></pre><p>You may need to bring the interface down and up to test the new changes:</p>
<pre><code>nmcli con down CONNECTION_NAME
nmcli con up CONNECTION_NAME
</code></pre><p>Once the network settles down, the authentication should complete within a few seconds in most cases. Be sure to check your system journal or other NetworkManager logs for more details if the interface doesn&rsquo;t work properly.</p>
]]></content></item><item><title>Talk Recap: Automated security hardening with OpenStack-Ansible</title><link>https://major.io/2016/04/26/talk-recap-automated-security-hardening-openstack-ansible/</link><pubDate>Tue, 26 Apr 2016 21:19:02 +0000</pubDate><guid>https://major.io/2016/04/26/talk-recap-automated-security-hardening-openstack-ansible/</guid><description>Today is the second day of the OpenStack Summit in Austin and I offered up a talk on host security hardening in OpenStack clouds. You can download the slides or watch the video here:
Here&amp;rsquo;s a quick recap of the talk and the conversations afterward:
Security tug-of-war Information security is a challenging task, mainly because it is more than just a technical problem. Technology is a big part of it, but communication, culture, and compromise are also critical.</description><content type="html"><![CDATA[<p>Today is the second day of the <a href="https://www.openstack.org/summit/austin-2016/">OpenStack Summit in Austin</a> and I offered up a talk on host security hardening in OpenStack clouds. You can <a href="http://www.slideshare.net/MajorHayden/automated-security-hardening-with-openstackansible">download the slides</a> or watch the video here:</p>
<p><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --></p>
<p>Here&rsquo;s a quick recap of the talk and the conversations afterward:</p>
<h2 id="security-tug-of-war">Security tug-of-war</h2>
<p>Information security is a challenging task, mainly because it is more than just a technical problem. Technology is a big part of it, but communication, culture, and compromise are also critical. I flashed up this statement on the slides:</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>In the end, the information security teams, the developers <em>and</em> the auditors must be happy. This can be a challenging tightrope to walk, but automating some security allows everyone to get what they want in a scalable and repeatable way.</p>
<h2 id="meeting-halfway">Meeting halfway</h2>
<p>The <a href="http://docs.openstack.org/developer/openstack-ansible-security">openstack-ansible-security role</a> allows information security teams to meet developers or OpenStack deployers halfway. It can easily bolt onto existing Ansible playbooks and manage host security hardening for Ubuntu 14.04 systems. The role also works in non-OpenStack environments just as well. All of the documentation, configuration, and Ansible tasks are all included with the role.</p>
<p>The role itself applies security configurations to each host in an environment. Those configurations are based on the Security Technical Implementation Guide (STIG) from the Defense Information Systems Agency (DISA), which is part of the United States Department of Defense. The role takes the configurations from the STIG and makes small tweaks to fit an OpenStack environment. All of the tasks are carefully translated from the STIG for Red Hat Enterprise Linux 6 (there is no STIG for Ubuntu currently).</p>
<p>The role is available now as part of OpenStack-Ansible in the Liberty, Mitaka, and Newton releases. Simply adjust <code>apply_security_hardening</code> from <code>false</code> to <code>true</code> and deploy. For other users, the role can easily be used in any Ansible playbook. <em>(Be sure to review the configuration to ensure its defaults meet your requirements.)</em></p>
<h2 id="getting-involved">Getting involved</h2>
<p><a href="/wp-content/uploads/2011/11/openstack-justheo.png"><!-- raw HTML omitted --></a>We need your help! Upcoming plans include Ubuntu 16.04 and CentOS support, a rebase onto the RHEL 7 STIG (which will be finalized soon), and better reporting.</p>
<p>Join us later this week for the OpenStack-Ansible design summit sessions or anytime on Freenode in #openstack-ansible. We&rsquo;re on the OpenStack development mailing list as well (be sure to use the <code>[openstack-ansible][security]</code> tags.</p>
<h2 id="hallway-conversations">Hallway conversations</h2>
<p>Lots of people came by to chat afterwards and offered to join in the development. A few people were hoping it would have been the security &ldquo;silver bullet&rdquo;, and I reset some expectations.</p>
<p>Some attendees has good ideas around making the role more generic and adding an &ldquo;OpenStack switch&rdquo; that would configure many variables to fit an OpenStack environment. That would allow people to use it easily with non-OpenStack environments.</p>
<p>Other comments were around hardening inside of Linux containers. These users had &ldquo;heavy&rdquo; containers where the entire OS is virtualized and multiple processes might be running at the same time. Some of the configuration changes (especially the kernel tunables) don&rsquo;t make sense inside a container like that, but many of the others could be useful. For more information on securing Linux containers, watch the video from <a href="https://www.openstack.org/summit/austin-2016/summit-schedule/events/8615">Thomas Cameron&rsquo;s talk</a> here at the summit.</p>
<h2 id="thank-you">Thank you</h2>
<p>I&rsquo;d like to thank everyone for coming to the talk today and sharing their feedback. It&rsquo;s immensely useful and I pile all of that feedback into future talks. Also, I&rsquo;d like to thank all of the people at Rackspace who helped me review the slides and improve them.</p>
<!-- raw HTML omitted -->
]]></content></item><item><title>Lessons learned: Five years of colocation</title><link>https://major.io/2016/04/22/lessons-learned-four-years-colocation-hosting/</link><pubDate>Fri, 22 Apr 2016 13:30:52 +0000</pubDate><guid>https://major.io/2016/04/22/lessons-learned-four-years-colocation-hosting/</guid><description>Back in 2011, I decided to try out a new method for hosting my websites and other applications: colocation. Before that, I used shared hosting, VPS providers (&amp;ldquo;cloud&amp;rdquo; wasn&amp;rsquo;t a popular thing back then), and dedicated servers. Each had their drawbacks in different areas. Some didn&amp;rsquo;t perform well, some couldn&amp;rsquo;t recover from failure well, and some were terribly time consuming to maintain.
This post will explain why I decided to try colocation and will hopefully help you avoid some of my mistakes.</description><content type="html"><![CDATA[<p><img src="/wp-content/uploads/2016/04/15401776380_f5c6e357a2_k-e1461296519292.jpg" alt="1"></p>
<p>Back in 2011, I decided to try out a new method for hosting my websites and other applications: <a href="https://en.wikipedia.org/wiki/Colocation_centre">colocation</a>. Before that, I used shared hosting, VPS providers (&ldquo;cloud&rdquo; wasn&rsquo;t a popular thing back then), and dedicated servers. Each had their drawbacks in different areas. Some didn&rsquo;t perform well, some couldn&rsquo;t recover from failure well, and some were terribly time consuming to maintain.</p>
<p>This post will explain why I decided to try colocation and will hopefully help you avoid some of my mistakes.</p>
<h2 id="why-choose-colocation">Why choose colocation?</h2>
<p>For the majority of us, hosting an application involves renting something from another company. This includes tangible things, such as disk space, servers, and networking, as well as the intangible items, like customer support. We all choose how much we rent (and hope our provider does well) and how much we do ourselves.</p>
<p>Colocation usually involves renting space in a rack, electricity, and network bandwidth. In these environments, the customer is expected to maintain the server hardware, perimeter network devices (switches, routers, and firewalls), and all of the other equipment downstream from the provider&rsquo;s equipment. Providers generally offer good price points for these services since they only need to ensure your rack is secure, powered, and networked.</p>
<p>As an example, a quarter rack in Dallas at a low to mid-range colocation provider can cost between $200-400 per month. That normally comes with a power allotment (mine is 8 amps) and network bandwidth (more on that later). In my quarter rack, I have five 1U servers plus a managed switch. One of those servers acts as my firewall.</p>
<p>Let&rsquo;s consider a similar scenario at a dedicated hosting provider. I checked some clearance prices in Dallas while writing this post and found pricing for servers which have similar CPUs as my servers, but with much less storage. The pricing for five servers runs about $550/month and we haven&rsquo;t even considered a switch and firewall yet.</p>
<p>Cost is one factor, but I have some other preferences which push me strongly towards colocation:</p>
<ul>
<li><strong>Customized server configuration:</strong> I can choose the quantity and type of components I need</li>
<li><strong>Customized networking:</strong> My servers run an OpenStack private cloud and I need complex networking</li>
<li><strong>Physical access</strong>: Perhaps I&rsquo;m getting old, but I enjoy getting my hands dirty from time to time</li>
</ul>
<p>If you have similar preferences, the rest of this post should be a good guide for getting started with a colocation environment.</p>
<h2 id="step-1-buy-quality-parts">Step 1: Buy quality parts</h2>
<p>Seriously - <strong>buy quality parts</strong>. That doesn&rsquo;t mean you need to go buy the newest server available from a well-known manufacturer, but you do need to find reliable parts in a new or used server.</p>
<p>I&rsquo;ve built my environment entirely with <a href="https://www.supermicro.com/">Supermicro</a> servers. The <a href="http://www.supermicro.com/products/motherboard/Xeon/C202_C204/X9SCi-LN4F.cfm">X9SCi-LN4F</a> servers have been extremely reliable. My first two came brand new from <a href="http://www.siliconmechanics.com/">Silicon Mechanics</a> and I&rsquo;m a big fan of their products. Look at their Rackform R.133 for something very similar to what I have.</p>
<p>My last three have all come from <a href="http://stores.ebay.com/MrRackables">Mr. Rackables</a> (an eBay seller). They&rsquo;re used X9SCi-LN4F servers, but they&rsquo;re all in good condition. I couldn&rsquo;t beat the price for those servers, either.</p>
<p>Before you buy, do some reading on brand reliability and compatibility. If you plan to run Linux, do a Google search for the model of the server and the version of Linux you plan to run. Also, take a moment to poke around some forums to see what other people think of the server. Go to <a href="http://www.webhostingtalk.com/">WebHostingTalk</a> and search for hardware there, too.</p>
<p>Buying quality parts gives you a little extra piece of mind, especially if your colo is a long drive from your home or work.</p>
<h2 id="step-2-ask-questions">Step 2: Ask questions</h2>
<p>Your first conversation with most providers will be through a salesperson. That&rsquo;s not necessarily a bad thing since they will give you a feature and pricing overview fairly quickly. However, I like to ask additional questions of the salesperson to learn more about the technical staff.</p>
<p>Here are some good conversation starters for making it past a salesperson to the technical staff:</p>
<ul>
<li><em>Your website says I get 8 amps of power. How many plugs on the PDU can I use?</em></li>
<li><em>My applications need IPv6 connectivity. Is it possible to get something larger than a /64?</em></li>
<li><em>For IPv6 connectivity, will I use DHCP-PD or will you route a larger IPv6 block to me via SLAAC?</em></li>
<li><em>I&rsquo;d like to delegate reverse DNS to my own nameservers. Do you support that?</em></li>
<li><em>If I have a problem with a component, can I ship you a part so you can replace it?</em></li>
</ul>
<p>When you do get a response, look for a few things:</p>
<ul>
<li>Did the response come back in a reasonable timeframe? <em>(Keep in mind that you aren&rsquo;t a paying customer yet.)</em></li>
<li>Did the technical person take the time to fully explain their response?</li>
<li>Does the response leave a lot of room for creative interpretation? <em>(If so, ask for clarification.)</em></li>
<li>Does the technical invite additional questions?</li>
</ul>
<p>It&rsquo;s much better to find problems now than after you&rsquo;ve signed a contract. I&rsquo;ll talk more about contracts later.</p>
<h2 id="step-3-get-specific-on-networking">Step 3: Get specific on networking</h2>
<p>Look for cities with very diverse network options. Big cities, or cities with lots of datacenters, will usually have good network provider choices and better prices. In Texas, your best bet is Dallas (which is where I host).</p>
<p>I&rsquo;ve learned that bandwidth measurement is one of the biggest areas of confusion and &ldquo;creative interpretation&rdquo; in colocation hosting. Every datacenter I&rsquo;ve hosted with has handled things differently.</p>
<p>There are four main ways that most colocation providers handle networking:</p>
<h3 id="95th-percentile">95th percentile</h3>
<p>This is sometimes called <a href="https://en.wikipedia.org/wiki/Burstable_billing">burstable billing</a> since it allows you to have traffic bursts without getting charged a lot for it. In the old days, you had to commit to a rate and stick with it (more on that method next). 95th percentile billing allows you to have some spikes during the measurement period without requiring negotiations for a higher transfer rate.</p>
<p>Long story short, this billing method measures your bandwidth on regular intervals and throws out the top 5% of your intervals. This means that some unusual spikes (so long as it&rsquo;s less than 36 hours in a month) won&rsquo;t cause you to need a higher committed rate. For most people, this measurement method is beneficial since your spikes are thrown out. However, if you have sustained spikes, <a href="https://en.wikipedia.org/wiki/Burstable_billing#95th_percentile">this billing method can be painful</a>.</p>
<h3 id="committed-rate">Committed rate</h3>
<p>If you see a datacenter say something like &ldquo;$15 per megabit&rdquo;, they&rsquo;re probably measuring with <a href="https://en.wikipedia.org/wiki/Committed_information_rate">committed rates</a>. In this model, you choose a rate, like 10 megabits/second, and pay based on that rate. At $15 per megabit, your bandwidth charge would be $150 per month. The actual space in the rack and the power often costs extra.</p>
<p>Some people have good things to say about this billing method, but it seems awful to me. If you end up using 1 megabit per second, you still get billed for 10. If you have some spikes that creep over 10 megabit, even if you stay well under that rate as an average, the datacenter folks will ask you to do a higher commit. That means more money for something you may or may not use.</p>
<p>In my experience, this is is a <strong>serious red flag</strong>. This suggests that the datacenter network probably doesn&rsquo;t have much extra capacity available within the datacenter, with its providers, or both. You sometimes see this in cities where bandwidth is more expensive. If you find a datacenter that&rsquo;s totally perfect, but they want to do a committed rate, ask if there&rsquo;s an option for 95th percentile. If not, I strongly suggest looking elsewhere.</p>
<h3 id="total-bandwidth">Total bandwidth</h3>
<p>Total bandwidth billing is what you normally see from most dedicated server or VPS providers. They will promise a certain transfer allotment and a network port speed. You will often see something like &ldquo;10TB on a 100mbps port&rdquo; and that means you can transfer 10TB in a month at at 100 megabits per second. They often don&rsquo;t care how you consume the bandwidth, but if you pass 10TB, you will likely pay overages per gigabyte. (Be sure to ask about the overage price.)</p>
<p>This method is nice because you can watch it like a water meter. You can take your daily transfer, multiply by 30, and see if the transfer allotment is enough. Most datacenters will allow you to upgrade to a gigabit port for a fee and this will allow you to handle spikes a little easier.</p>
<p>For personal colocation, this bandwidth billing method is great. It could be a red flag for larger customers because it gives a hint that the datacenter might be oversubscribed on networking. If every customer wanted to burst to 100 megabit at the same time, there probably isn&rsquo;t enough network connectivity to allow everyone to get that guaranteed rate.</p>
<h3 id="bring-your-own">Bring your own</h3>
<p>Finally, you could always negotiate with bandwidth providers and bring your own bandwidth. This comes with its own challenges and it&rsquo;s probably not worth it for a personal colocation environment. Larger business like this method because they often have rates negotiated with providers for their office connectivity and they can often get a good rate within the colocation datacenter.</p>
<p>There are plenty of up-front costs with bringing your own bandwidth provider. Some of those costs may come from the datacenter itself, especially if new cabling is required.</p>
<h2 id="step-4-plan-for-failure">Step 4: Plan for failure</h2>
<p>Ensure that spare parts are available at a moment&rsquo;s notice if something goes wrong. In my case, I keep two extra hard drives in the rack with my servers as well as two sticks of RAM. My colocation datacenter is about four hours away by car, and I certainly don&rsquo;t want to make an emergency trip there to replace a broken hard drive.</p>
<p>Buying servers with out of band management can be a huge help during an emergency. Getting console access remotely can shave plenty of time off of an outage. The majority of Supermicro servers come with an out of band management controller by default. You can use simple IPMI commands to reboot the server or use the iKVM interface to interact with the console. Many of these management controllers allow you to mount USB drives remotely and re-image a server at any time.</p>
<p>Ask the datacenter if they have a network KVM that you could use or rent during an emergency. Be sure to ask about pricing and the time expectations when you request one to be connected to your servers.</p>
<h2 id="step-5-contracts-and-payment">Step 5: Contracts and payment</h2>
<p>Be sure to read the contract carefully. Pay special attention to how the datacenter handles outages and how you can request SLA credits. Take time to review any sections on what rights you have when they don&rsquo;t hold up their end of the deal.</p>
<p>As with any contract, find out what happens when the contract ends. Does it auto-renew? Do you keep the same rates? Can you go month to month? Reviewing these sections before signing could save a lot of money later.</p>
<h2 id="wrapping-up">Wrapping up</h2>
<p>Although cloud hosting has certainly made it easier to serve applications, there are still some people out there that prefer to have more customization than cloud hosting allows. For some applications, cloud hosting can be prohibitively expensive. Some applications don&rsquo;t tolerate a shared platform and the noisy neighbor issues that come with it.</p>
<p>Colocation can be a challenging, but rewarding, experience. As with anything, you must do your homework. I certainly hope this post helps make that homework a little easier.</p>
<hr>
<p><em>Since I know someone will ask me:</em> I host with <a href="http://corespace.com/">Corespace</a> and I&rsquo;ve been with them for a little over a year. They have been great so far and their staff has been friendly in person, via tickets, and via telephone.</p>
<p><em>Photo credit: <a href="https://www.flickr.com/photos/44176115@N07/15401776380/">Kev (Flickr)</a></em></p>
]]></content></item><item><title>OpenStack Summit in Austin is almost here!</title><link>https://major.io/2016/04/21/brace-openstack-summit-austin-coming/</link><pubDate>Thu, 21 Apr 2016 13:16:02 +0000</pubDate><guid>https://major.io/2016/04/21/brace-openstack-summit-austin-coming/</guid><description>OpenStack comes home to Austin on Monday for the OpenStack Summit! I will be there with plenty of other Rackers to learn, collaborate, and share our story.
If you&amp;rsquo;re interested in applying automated security hardening to an OpenStack cloud, be sure to drop by my talk on Tuesday from 11:15 to 11:55 AM. I&amp;rsquo;ll explain how Rackspace uses the openstack-ansible-security Ansible role to automatically apply hardening standards to servers in an OpenStack private cloud.</description><content type="html"><![CDATA[<p>OpenStack comes home to Austin on Monday for the <a href="https://www.openstack.org/summit/austin-2016/">OpenStack Summit</a>! I will be there with <a href="http://blog.rackspace.com/openstack-summit-austin-rackspace-experts/">plenty of other Rackers</a> to learn, collaborate, and share our story.</p>
<p><img src="/wp-content/uploads/2016/04/OpenStack-Summit-Austin-2016-Automated-Security-Hardening-with-OpenStack-Ansible-Major-Hayden-1.png" alt="3"></p>
<p>If you&rsquo;re interested in applying automated security hardening to an OpenStack cloud, be sure to <a href="https://www.openstack.org/summit/austin-2016/summit-schedule/events/7043">drop by my talk</a> on Tuesday from 11:15 to 11:55 AM. I&rsquo;ll explain how Rackspace uses the <a href="http://docs.openstack.org/developer/openstack-ansible-security/">openstack-ansible-security</a> Ansible role to automatically apply hardening standards to servers in an OpenStack private cloud. If you aren&rsquo;t able to attend, don&rsquo;t worry. I will post my slides and a summary here on the blog.</p>
<p>Here are the sessions on the top of my list:</p>
<ul>
<li><a href="https://www.openstack.org/summit/austin-2016/summit-schedule/events/9515">Ops: Large Deployments Team</a></li>
<li><a href="https://www.openstack.org/summit/austin-2016/summit-schedule/events/8644">DEFCON 3: OpenStack Meets the Information Security Department</a></li>
<li><a href="https://www.openstack.org/summit/austin-2016/summit-schedule/events/7796">Nova Cells V2: What&rsquo;s Going On?</a></li>
<li><a href="https://www.openstack.org/summit/austin-2016/summit-schedule/events/8735">Continuous Compliance &amp; Enforcement of Location-Based Policies in an OpenStack Cloud</a></li>
<li><a href="https://www.openstack.org/summit/austin-2016/summit-schedule/events/8545">Using Open Source Security Architecture to Defend against Targeted Attacks</a></li>
<li><a href="https://www.openstack.org/summit/austin-2016/summit-schedule/events/6893">Load Balancing as a Service, Mitaka and Beyond</a></li>
</ul>
<p>If you&rsquo;re at the Summit and you want to meet, feel free to email me at <a href="mailto:major@mhtx.net">major@mhtx.net</a> or <a href="https://twitter.com/majorhayden">connect with me on Twitter</a>. I hope it&rsquo;s a great week!</p>
]]></content></item><item><title>Thunderbird opens multiple windows</title><link>https://major.io/2016/04/20/thunderbird-opens-multiple-windows/</link><pubDate>Wed, 20 Apr 2016 13:31:56 +0000</pubDate><guid>https://major.io/2016/04/20/thunderbird-opens-multiple-windows/</guid><description>When I started Thunderbird today, it opened three windows. Each window was identical. I closed two of them and then quit Thunderbird.
As soon as I started Thunderbird, I had three windows again.
I found a Mozilla bug report from 2015 that had some tips for getting the additional windows closed.
Choose one of the open Thunderbird windows and select Close from the File menu. Do not use ALT-F4 or CTRL-W to close the window.</description><content type="html"><![CDATA[<p>When I started <a href="https://www.mozilla.org/en-US/thunderbird/">Thunderbird</a> today, it opened three windows. Each window was identical. I closed two of them and then quit Thunderbird.</p>
<p>As soon as I started Thunderbird, I had <strong>three windows again</strong>.</p>
<p><a href="/wp-content/uploads/2016/04/ugh.gif"><!-- raw HTML omitted --></a></p>
<p>I found a <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=531588#c12">Mozilla bug report from 2015</a> that had some tips for getting the additional windows closed.</p>
<p>Choose one of the open Thunderbird windows and select <strong>Close</strong> from the <strong>File</strong> menu. <strong>Do not</strong> use ALT-F4 or CTRL-W to close the window. Keep doing that until all of the windows are closed except for one. Then choose <strong>Quit</strong> from the hamburger menu drop down.</p>
<p>At that point, start Thunderbird again and you should have only one open window.</p>
<p><em>Note: You may find that one window does not respond to clicking <strong>Close</strong> - that&rsquo;s your root Thunderbird window and it cannot be closed. Be sure to close all of the others.</em></p>
]]></content></item><item><title>Enable IPv6 privacy in NetworkManager</title><link>https://major.io/2016/04/17/enable-ipv6-privacy-networkmanager/</link><pubDate>Sun, 17 Apr 2016 16:35:57 +0000</pubDate><guid>https://major.io/2016/04/17/enable-ipv6-privacy-networkmanager/</guid><description>On most IPv6-enabled networks, network addresses are distributed via stateless address autoconfiguration (SLAAC). That is a fancy way to say that hosts on an IPv6 network will configure their own IP addresses.
The process usually works like this:
The host sends out a router solicitation request: Hey, who is the router around here? The router replies with a prefix: I am the router and your IPv6 address should start with this prefix.</description><content type="html"><![CDATA[<p><img src="/wp-content/uploads/2016/04/25587307903_3ec82a6dde_b-e1460910591232.jpg" alt="1"></p>
<p>On most IPv6-enabled networks, network addresses are distributed via <a href="https://en.wikipedia.org/wiki/IPv6_address#Stateless_address_autoconfiguration">stateless address autoconfiguration (SLAAC)</a>. That is a fancy way to say that hosts on an IPv6 network will configure their own IP addresses.</p>
<p>The process usually works like this:</p>
<ol>
<li>The host sends out a router solicitation request: <em>Hey, who is the router around here?</em></li>
<li>The router replies with a prefix: <em>I am the router and your IPv6 address should start with this prefix.</em></li>
<li>The host uses its MAC address to generate the remaining bits of the IP address.</li>
</ol>
<p>The format of the IPv6 address generated by the host is called <a href="https://en.wikipedia.org/wiki/IPv6_address#Modified_EUI-64">EUI-64</a>. The host takes its MAC address, wedges <code>FF:FE</code> in the middle, and adds the prefix from the router on the front. For much more detail on this process, review the <a href="https://standards.ieee.org/develop/regauth/tut/eui64.pdf">IEEE&rsquo;s guidelines for EUI-64</a>. The <a href="https://wiki.archlinux.org/index.php/IPv6">Arch Linux wiki page on IPv6</a> has plenty of detail as well.</p>
<h2 id="time-to-talk-security">Time to talk security</h2>
<p>While SLAAC works really well on most networks and provides a highly efficient method for dealing with IP addresses, it can disclose more information about your computer or mobile device than you want to disclose. Websites will see the IPv6 address and they can determine the client&rsquo;s MAC address on networks that are using SLAAC. This could be used for tracking purposes - <strong>both legitimate and illegitimate</strong>.</p>
<p>Also, bear in mind that the first several bits of a MAC address will often identify the hardware vendor that manufactured your ethernet card or wireless chip. Depending on the vendor, this may expose what type of device you are using (computer or mobile device) and in some cases, which type of computer you are using (Mac vs PC).</p>
<p>In the worst cases, this information could be used to deliver targeted malware to your device. It could also be used to locate or identify a user of a device in a particular location.</p>
<h2 id="using-temporary-addresses">Using temporary addresses</h2>
<p>Most systems allow for <a href="https://en.wikipedia.org/wiki/IPv6_address#Temporary_addresses">temporary addressing</a>, and some even enable it by default. However, many Linux distributions do not enable temporary addresses by default.</p>
<p>There is a kernel tunable that controls temporary addressing on Linux systems:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ini" data-lang="ini"><span style="color:#586e75"># Do not use a temporary address</span>
net.ipv6.conf.all.use_tempaddr <span style="color:#719e07">=</span> <span style="color:#2aa198">0</span>
<span style="color:#586e75"># Set a temporary address, but do not make it the default</span>
net.ipv6.conf.all.use_tempaddr <span style="color:#719e07">=</span> <span style="color:#2aa198">1</span>
<span style="color:#586e75"># Set a temporary address and make it the default</span>
net.ipv6.conf.all.use_tempaddr <span style="color:#719e07">=</span> <span style="color:#2aa198">2</span>
</code></pre></div><p>NetworkManager can handle this setting as well. Just set the <code>ipv6.ip6-privacy</code> variable to 0, 1, or 2. For example, to enable temporary adrr</p>
<pre><code>nmcli connection modify eth0 ipv6.ip6-privacy 2
</code></pre><p>NetworkManager will activate this setting immediately and begin using the temporary address as the default.</p>
<h2 id="caveats">Caveats</h2>
<p>Temporary addresses are built based on the MAC address and a random time string, so they will change from time to time. Avoid using temporary addressing on devices that you regularly access via their IPv6 address, such as servers or other non-mobile systems.</p>
<p>Photo Credit: <a href="https://www.flickr.com/photos/110548908@N06/25587307903/">UnknownNet Photography</a></p>
]]></content></item><item><title>Automated Let’s Encrypt DNS challenges with Rackspace Cloud DNS</title><link>https://major.io/2016/03/31/automated-lets-encrypt-dns-challenges-with-rackspace-cloud-dns/</link><pubDate>Thu, 31 Mar 2016 19:39:50 +0000</pubDate><guid>https://major.io/2016/03/31/automated-lets-encrypt-dns-challenges-with-rackspace-cloud-dns/</guid><description>Let&amp;rsquo;s Encrypt has taken the world by storm by providing free SSL certificates that can be renewed via automated methods. They have issued over 1.4 million certificates since launch in the fall of 2015.
If you are not familiar with how Let&amp;rsquo;s Encrypt operates, here is an extremely simple explanation:
Create a private key Make a request for a new certificate Complete the challenge process You have a certificate! That is highly simplified, but there is plenty of detail available on how the whole system works.</description><content type="html"><![CDATA[<p><img src="/wp-content/uploads/2016/03/25476786513_393afd0d2f_b-e1459452983901.jpg" alt="gears_photo"></p>
<p><a href="https://letsencrypt.org/">Let&rsquo;s Encrypt</a> has taken the world by storm by providing free SSL certificates that can be renewed via automated methods. They have issued <a href="https://letsencrypt.org/stats/">over 1.4 million certificates</a> since launch in the fall of 2015.</p>
<p>If you are not familiar with how Let&rsquo;s Encrypt operates, here is an <em>extremely</em> simple explanation:</p>
<ol>
<li>Create a private key</li>
<li>Make a request for a new certificate</li>
<li>Complete the challenge process</li>
<li>You have a certificate!</li>
</ol>
<p>That is highly simplified, but there is <a href="https://letsencrypt.org/how-it-works/">plenty of detail available</a> on how the whole system works.</p>
<p>One of the most popular challenge methods is HTTP. That involves getting a challenge string from Let&rsquo;s Encrypt, placing the string at a known URL on your domain, and then waiting for verification of the challenge. The process is quick and Let&rsquo;s Encrypt <a href="https://letsencrypt.org/getting-started/">provides tools</a> that automate much of the process for you.</p>
<h2 id="a-challenger-appears">A challenger appears</h2>
<p>A DNS challenge is available in addition to the HTTP challenge. As you might imagine, this involves creating a DNS record with a string provided by Let&rsquo;s Encrypt. Once the DNS record is in place, it is verified and certificates are issued. The process goes something like this:</p>
<ol>
<li>Request a new certificate</li>
<li>Get a challenge string</li>
<li>Add a DNS TXT record on your domain with the challenge string as the data</li>
<li>Wait for DNS records to appear on your DNS server</li>
<li>Let&rsquo;s Encrypt checks for the DNS record</li>
<li>Clean up the DNS record</li>
<li>Get a certificate</li>
</ol>
<p>Wrapping automation around this method is often easier than using the HTTP method since it does not require any changes on web servers. If someone has 500 web servers but they change their DNS records through a single API with a DNS provider, it quickly becomes apparent that adding a single DNS record is much easier.</p>
<p>In addition, the HTTP challenge method creates problems for websites which are not entirely publicly accessible yet. A stealth startup or a pre-release site could acquire a certificate without needing to allow any access into the webserver. This is also helpful for sites which will never be public facing, such as those on intranets.</p>
<h2 id="automating-the-process">Automating the process</h2>
<p>After some research, I stumbled upon a project in GitHub called <a href="https://github.com/lukas2511/letsencrypt.sh">letsencrypt.sh</a>. The project consists of a bash script that makes all the necessary requests to Let&rsquo;s Encrypt&rsquo;s API for requesting and obtaining SSL certificates. However, DNS records are tricky since they are usually managed via an API or other non-trivial methods.</p>
<p>The project provides a hook feature which allows anyone to write a script that receives data and does the necessary DNS adjustments to complete the challenge process. I wrote a hook that interfaces with <a href="https://www.rackspace.com/en-us/cloud/dns">Rackspace&rsquo;s Cloud DNS API</a> and handles the creation of DNS records:</p>
<ul>
<li><a href="https://github.com/major/letsencrypt-rackspace-hook">GitHub: letsencrypt-rackspace-hook</a></li>
</ul>
<p>All of the installation and configuration instructions are in the <a href="https://github.com/major/letsencrypt-rackspace-hook/blob/master/README.rst">main README file</a> within the repository. You can begin issuing certificates with DNS challenges in a few minutes.</p>
<p>The hook works like this:</p>
<ol>
<li>letsencrypt.sh hands off the domain name and a challenge string to the hook</li>
<li>The hook adds a DNS record to Rackspace&rsquo;s DNS servers via the API</li>
<li>The hook keeps checking to see if the DNS record is publicly accessible</li>
<li>Once the DNS record appears, control is handed back to letsencrypt.sh</li>
<li>letsencrypt.sh tells Let&rsquo;s Encrypt to verify the challenge</li>
<li>Let&rsquo;s Encrypt verifies the challenge</li>
<li>The hook cleans up the DNS record and displays the paths to the new certificates and keys.</li>
</ol>
<p>From there, you can configure your configuration management software to push out the new certificate and keys to your production servers. Let&rsquo;s Encrypt certificates are currently limited to a 90-day duration, so be sure to configure this automation via a cron job. At the very least, set a calendar reminder for yourself a week or two in advance of the expiration.</p>
<p>Keep in mind that Let&rsquo;s Encrypt and Rackspace&rsquo;s DNS service are completely free. Free is a good thing.</p>
<p>Let me know what you think of the script! Feel free to make pull requests or issues if you find bugs. I am still working on some automated testing for the script and I hope to have that available in the next week or two.</p>
<p><em>Photo Credit: <a href="https://www.flickr.com/photos/137399762@N06/25476786513/">Aphernai</a> via <a href="http://compfight.com">Compfight</a> <a href="https://creativecommons.org/licenses/by-nc-sa/2.0/">cc</a></em></p>
]]></content></item><item><title>Mouse cursor disappears in GNOME 3</title><link>https://major.io/2016/03/11/mouse-cursor-disappears-gnome-3/</link><pubDate>Fri, 11 Mar 2016 15:36:25 +0000</pubDate><guid>https://major.io/2016/03/11/mouse-cursor-disappears-gnome-3/</guid><description>UPDATE: The fixed version of mutter is now in the Fedora updates repository. You should be able to update the package with dnf:
dnf -y upgrade mutter GNOME 3 has been rock solid for the last few months but something cropped up this week that derailed me for a short while. Whenever I moved my mouse cursor to the top bar (where the clock and status icons reside), the mouse cursor disappeared.</description><content type="html"><![CDATA[<p><img src="/wp-content/uploads/2016/03/7575830232_19678e9d5c_b-e1457710296562.jpg" alt="mouse_photo"></p>
<p><strong>UPDATE:</strong> The fixed version of mutter is now in the Fedora updates repository. You should be able to update the package with <code>dnf</code>:</p>
<pre><code>dnf -y upgrade mutter
</code></pre><hr>
<p>GNOME 3 has been rock solid for the last few months but something cropped up this week that derailed me for a short while. Whenever I moved my mouse cursor to the top bar (where the clock and status icons reside), the mouse cursor disappeared. The same thing happened if I pressed the Mod/Windows key to hop into the Activities display.</p>
<p>If I wiggled the mouse a bit, I could see the highlight move around to different windows and icons. The mouse cursor never appeared.</p>
<p>Lots of Google results led to dead ends. I stumbled onto a <a href="https://bugzilla.gnome.org/show_bug.cgi?id=746594">GNOME bug for gnome-shell</a> from early 2015 that seemed to cover the same problem. After adding in my comments, I created a <a href="https://bugzilla.redhat.com/show_bug.cgi?id=1316957">Fedora bug</a> to track the problem.</p>
<p>Around that time, Florian Müllner replied in the GNOME bug about trying <code>mutter-3.18.3-2</code>. My laptop was running <code>mutter-3.18.3-1</code> at the time. The <a href="https://bodhi.fedoraproject.org/updates/FEDORA-2016-30ef48bcb6">new version of mutter</a> was still in the <em>pending</em> state in Fedora&rsquo;s packaging infrastructure, so I pulled it down with <code>koji</code>:</p>
<pre><code>koji download-build --arch x86_64 mutter-3.18.3-2.fc23
sudo dnf install mutter-3.18.3-2.fc23.x86_64.rpm
</code></pre><p>After a reboot, everything was back to normal! The cursor appears reliably in the top bar, Activities screen, and other overlays. In addition, some of the transient cursor weirdness I had with some applications seems to be gone.</p>
<p><strong>UPDATE:</strong> <a href="https://twitter.com/Sesivany">Jiří Eischmann</a> tweeted yesterday about this problem:</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>In my particular case, my &ldquo;left&rdquo; monitor is my laptop screen and my &ldquo;right&rdquo; monitor is my external display. I configure the external monitor to be <em>above</em> my laptop monitor physically and logically, which is why the problem appears for me. Thanks for the clarification, Jiří!</p>
<p><em>Photo Credit: <a href="https://www.flickr.com/photos/77395664@N00/7575830232/">Perfectance</a> via <a href="http://compfight.com">Compfight</a> <a href="https://creativecommons.org/licenses/by-nc-sa/2.0/">cc</a></em></p>
]]></content></item><item><title>Recovering deleted Chrome bookmarks on Linux</title><link>https://major.io/2016/02/26/recovering-deleted-chrome-bookmarks-on-linux/</link><pubDate>Fri, 26 Feb 2016 15:31:15 +0000</pubDate><guid>https://major.io/2016/02/26/recovering-deleted-chrome-bookmarks-on-linux/</guid><description>After getting a bit overzealous with cleaning up bookmarks in Chrome, I discovered that I deleted a helpful Gerrit filter for OpenStack reviews. I worked hard to create the filter and I definitely needed it back.
Chrome keeps a file called Bookmarks.bak inside its configuration directory. You can find this file here:
/home/[username]/.config/google-chrome/Default/Bookmarks.bak # If using Chrome stable /home/[username]/.config/google-chrome-beta/Default/Bookmarks.bak # If using Chrome beta The file is stored in JSON format.</description><content type="html"><![CDATA[<p>After getting a bit overzealous with cleaning up bookmarks in Chrome, I discovered that I deleted a helpful Gerrit filter for OpenStack reviews. I worked hard to create the filter and I definitely needed it back.</p>
<p>Chrome keeps a file called <code>Bookmarks.bak</code> inside its configuration directory. You can find this file here:</p>
<pre><code>/home/[username]/.config/google-chrome/Default/Bookmarks.bak        # If using Chrome stable
/home/[username]/.config/google-chrome-beta/Default/Bookmarks.bak   # If using Chrome beta
</code></pre><p>The file is stored in JSON format. Open it up in your favorite text editor and search for your deleted bookmark.</p>
]]></content></item><item><title>Fight cynicism with curiosity</title><link>https://major.io/2016/02/17/fight-cynicism-curiosity/</link><pubDate>Wed, 17 Feb 2016 15:30:14 +0000</pubDate><guid>https://major.io/2016/02/17/fight-cynicism-curiosity/</guid><description>I&amp;rsquo;m always interested to talk to college students about technology and business in general. They have amazing ideas and they don&amp;rsquo;t place any limits on themselves. In particular, their curiosity is limitless.
A great question I joined several other local employers at the University of Texas at San Antonio last week for mock interviews with computer science students. We went through plenty of sample questions and gave feedback to the students on their content and delivery during the mock interviews.</description><content type="html"><![CDATA[<p><img src="/wp-content/uploads/2016/02/15447911129_b53e184b14_b-e1455722957400.jpg" alt="cover"></p>
<p>I&rsquo;m <a href="/2015/11/10/talking-to-college-students-about-information-security/">always interested to talk to college students</a> about technology and business in general. They have amazing ideas and they don&rsquo;t place any limits on themselves. In particular, their curiosity is limitless.</p>
<h2 id="a-great-question">A great question</h2>
<p>I joined several other local employers at the <a href="http://utsa.edu">University of Texas at San Antonio</a> last week for mock interviews with computer science students. We went through plenty of sample questions and gave feedback to the students on their content and delivery during the mock interviews. At the end, we opened it up for questions.</p>
<p>One student asked a question that really made me pause:</p>
<blockquote>
<p>What&rsquo;s the one thing that you learned while working that you didn&rsquo;t learn in college? What should we know that we won&rsquo;t learn in the classroom?</p>
</blockquote>
<h2 id="thinking-it-through">Thinking it through</h2>
<p>There are plenty of obvious things that came to mind when I thought about it:</p>
<ul>
<li>Be a team player</li>
<li>Remember the customer</li>
<li>Think globally, act locally</li>
<li>Don&rsquo;t &ldquo;call the baby ugly&rdquo;</li>
<li>Learn something new every day</li>
</ul>
<p>However, many of these are cliche or difficult to teach. It takes some real world experience with real people on real projects to really understand them. There must be some correlation between these things I&rsquo;ve learned since I entered the business world.</p>
<p>Then it hit me. The biggest key to my own success rests upon a single word: <strong>curiosity</strong>.</p>
<h2 id="being-curious">Being curious</h2>
<p>I&rsquo;ve always been curious about things for as long as I can remember. I&rsquo;ve questioned everything at one time or another and demanded to know the real story behind events in my work and personal life.</p>
<p>Being curious allows me to approach new things with more wonder and less fear. It has stopped me in my tracks when I&rsquo;ve tried to pass judgement on a person or a situation without asking the right questions first. It has brought me closer to more people at work, in open source communities, and at home.</p>
<p>It&rsquo;s not always easy. Being curious is exhausting. There are many times where I&rsquo;ve wanted less change and I&rsquo;ve rejected new ways of doing things. When those situations arise, I take a break and think about something else. I come back to it with more energy and a myriad of questions.</p>
<p>Being curious also leads you down those paths less traveled. Robert Frost&rsquo;s poem, <a href="http://www.poetryfoundation.org/poem/173536">The Road Not Taken</a>, ends with a paragraph that has special meaning to me as a curious person:</p>
<blockquote>
<p>I shall be telling this with a sigh</p>
</blockquote>
<blockquote>
<p>Somewhere ages and ages hence:</p>
</blockquote>
<blockquote>
<p>Two roads diverged in a wood, and I-</p>
</blockquote>
<blockquote>
<p>I took the one less traveled by,</p>
</blockquote>
<blockquote>
<p>And that has made all the difference.</p>
</blockquote>
<p>Being curious has taken me down this path less traveled many times. Almost every trip has been worth the trouble.</p>
<h2 id="curiosity-crushes-cynicism">Curiosity crushes cynicism</h2>
<p>Anyone who works in technology, especially software development or system administration, has found themselves looking over a block of code or a server deployment that someone else has prepared. Many of us have had one of these moments:</p>
<p><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --></p>
<p>That&rsquo;s totally natural. <em>It&rsquo;s human nature.</em></p>
<p>Luckily, that&rsquo;s a habit we can break and curiosity can be the tool that breaks it. Instead of immediately passing judgement, start making a list of questions in your head:</p>
<ul>
<li>Why was this designed in this way?</li>
<li>Is this something I can change?</li>
<li>What was the original use case?</li>
<li>Is there a better way to do this that already exists?</li>
<li>Who worked on it originally and what was their charter or goal?</li>
</ul>
<p>This situation appears <strong>very frequently</strong> in open source software. Quickly passing judgement about a particular piece of software or user community can often to the dangerous cycle of <a href="https://en.wikipedia.org/wiki/Not_invented_here">Not Invented Here (NIH)</a>. This leads to competing standards, projects, and communities.</p>
<p>A healthier approach is to look over the software and the community with a curious approach. Start asking questions and sharing your unique use cases with the community. You might find others in the community with a similar need and this can often convince the herd to change direction. Instead of building something on your own, you will belong to a community and stand on the shoulders of the work that is already done.</p>
<h2 id="my-advice-for-students">My advice for students</h2>
<p>My advice here is the same as what I told the UTSA student last week: <strong>always be curious</strong>.</p>
<p>Let curiosity drive your decisions and your growth.</p>
<p>Let curiosity push you through the challenging or difficult times.</p>
<p>Let curiosity guide your interactions with other people and encourage them to be curious as well.</p>
<p>As you refocus your energy from cynicism to curiosity, the momentum will build. Being curious will become one of the most beneficial habits you&rsquo;ll ever make.</p>
<p><em>Photo Credit: <a href="https://www.flickr.com/photos/32135758@N02/15447911129/">jinterwas</a> via <a href="http://compfight.com">Compfight</a> <a href="https://creativecommons.org/licenses/by-nc/2.0/">cc</a></em></p>
]]></content></item><item><title>Segmentation faults with sphinx and pyenv</title><link>https://major.io/2016/02/09/segmentation-faults-with-sphinx-and-pyenv/</link><pubDate>Tue, 09 Feb 2016 14:09:44 +0000</pubDate><guid>https://major.io/2016/02/09/segmentation-faults-with-sphinx-and-pyenv/</guid><description>I&amp;rsquo;m a big fan of the pyenv project because it makes installing multiple python versions a simple process. However, I kept stumbling into a segmentation fault whenever I tried to build documentation with sphinx in Python 2.7.11:
writing output... [100%] unreleased [app] emitting event: 'doctree-resolved'(&amp;lt;document: &amp;lt;section &amp;quot;current series release notes&amp;quot;...&amp;gt;&amp;gt;, u'unreleased') [app] emitting event: 'html-page-context'(u'unreleased', 'page.html', {'file_suffix': '.html', 'has_source': True, 'show_sphinx': True, 'last generating indices... genindex[app] emitting event: 'html-page-context'('genindex', 'genindex.html', {'pathto': &amp;lt;function pathto at 0x7f4279d51230&amp;gt;, 'file_suffix': '.</description><content type="html"><![CDATA[<p>I&rsquo;m a big fan of the <a href="https://github.com/yyuu/pyenv">pyenv</a> project because it makes installing multiple python versions a simple process. However, I kept stumbling into a segmentation fault whenever I tried to build documentation with sphinx in Python 2.7.11:</p>
<pre><code>writing output... [100%] unreleased
[app] emitting event: 'doctree-resolved'(&lt;document: &lt;section &quot;current series release notes&quot;...&gt;&gt;, u'unreleased')
[app] emitting event: 'html-page-context'(u'unreleased', 'page.html', {'file_suffix': '.html', 'has_source': True, 'show_sphinx': True, 'last

generating indices... genindex[app] emitting event: 'html-page-context'('genindex', 'genindex.html', {'pathto': &lt;function pathto at 0x7f4279d51230&gt;, 'file_suffix': '.html'
Segmentation fault (core dumped)
</code></pre><p>I tried a few different versions of sphinx, but the segmentation fault persisted. I did a quick reinstallation of Python 2.7.11 in the hopes that a system update of gcc/glibc was causing the problem:</p>
<pre><code>pyenv install 2.7.11
</code></pre><p>The same segmentation fault showed up again. After a ton of Google searching, I found that the <code>--enable-shared</code> option allows pyenv to use shared Python libraries at compile time:</p>
<pre><code>env PYTHON_CONFIGURE_OPTS=&quot;--enable-shared CC=clang&quot; pyenv install -vk 2.7.11
</code></pre><p>That worked! I&rsquo;m now able to run sphinx without segmentation faults.</p>
]]></content></item><item><title>Enabling kwallet after accidentally disabling it</title><link>https://major.io/2016/01/28/enabling-kwallet-after-accidentally-disabling-it/</link><pubDate>Thu, 28 Jan 2016 16:27:44 +0000</pubDate><guid>https://major.io/2016/01/28/enabling-kwallet-after-accidentally-disabling-it/</guid><description>Although I use GNOME 3 as my desktop environment, I prefer KDE&amp;rsquo;s kwallet service to gnome-keyring for some functions. The user interface is a little easier to use and it&amp;rsquo;s easier to link up to the keyring module in Python.
Accidentally disabling kwallet A few errant mouse clicks caused me to accidentally disable the kwalletd service earlier today and I was struggling to get it running again. The daemon is usually started by dbus and I wasn&amp;rsquo;t entirely sure how to start it properly.</description><content type="html"><![CDATA[<p><img src="/wp-content/uploads/2016/01/14283880173_bc12e718fe_b-e1453998408758.jpg" alt="cover"></p>
<p>Although I use GNOME 3 as my desktop environment, I prefer KDE&rsquo;s <a href="https://www.kde.org/applications/system/kwalletmanager/">kwallet</a> service to gnome-keyring for some functions. The user interface is a little easier to use and it&rsquo;s easier to link up to the <a href="https://pypi.python.org/pypi/keyring">keyring module</a> in Python.</p>
<h2 id="accidentally-disabling-kwallet">Accidentally disabling kwallet</h2>
<p>A few errant mouse clicks caused me to accidentally disable the kwalletd service earlier today and I was struggling to get it running again. The daemon is usually started by dbus and I wasn&rsquo;t entirely sure how to start it properly.</p>
<p>If I start kwalletmanager, I see the kwallet icon in the top bar. However, it&rsquo;s unresponsive to clicks. Starting kwalletmanager on the command line leads to lots of errors in the console:</p>
<pre><code>kwalletmanager(20406)/kdeui (Wallet): The kwalletd service has been disabled
kwalletmanager(20406)/kdeui (Wallet): The kwalletd service has been disabled
kwalletmanager(20406)/kdeui (Wallet): The kwalletd service has been disabled
</code></pre><p>Manually running kwalletd in the console wasn&rsquo;t successful either.</p>
<h2 id="using-kcmshell">Using kcmshell</h2>
<p>KDE provides a utility called kcmshell that allows you to start a configuration panel without running the entire KDE environment. If you disable kwallet accidentally like I did, this will bring up the configuration panel and allow you to re-enable it:</p>
<pre><code>kcmshell4 kwalletconfig
</code></pre><p>You should see kwallet&rsquo;s configuration panel appear:</p>
<p><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --></p>
<p>Click on <strong>Enable the KDE wallet subsystem</strong> and then click OK. Once the window closes, start kwalletmanager and you should be able to access your secrets in kwallet again.</p>
<p><em>Photo Credit: <a href="https://www.flickr.com/photos/73589829@N00/14283880173/">Wei</a> via <a href="http://compfight.com">Compfight</a> <a href="https://creativecommons.org/licenses/by-nc-nd/2.0/">cc</a></em></p>
]]></content></item><item><title>Tinkering with systemd’s predictable network names</title><link>https://major.io/2016/01/20/tinkering-with-systemds-predictable-network-names/</link><pubDate>Wed, 20 Jan 2016 19:46:52 +0000</pubDate><guid>https://major.io/2016/01/20/tinkering-with-systemds-predictable-network-names/</guid><description>I&amp;rsquo;ve talked about predictable network names (and seemingly unpredictable ones) on the blog before, but some readers asked me how they could alter the network naming to fit a particular situation. Oddly enough, my Supermicro 5028D-T4NT has a problem with predictable names and it&amp;rsquo;s a great example to use here.
The problem There&amp;rsquo;s plenty of detail in my post about the Supermicro 5028D-T4NT, but the basic gist is that something within the firmware is causing the all of the network cards in the server to show up as onboard.</description><content type="html"><![CDATA[<p><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --></p>
<p>I&rsquo;ve talked about <a href="/2015/08/21/understanding-systemds-predictable-network-device-names/">predictable network names</a> (and <a href="https://major.io/2014/08/06/unexpected-predictable-network-naming-systemd/">seemingly unpredictable ones</a>) on the blog before, but some readers asked me how they could alter the network naming to fit a particular situation. Oddly enough, my <a href="/2015/09/28/first-thoughts-linux-on-the-supermicro-5028d-t4nt/">Supermicro 5028D-T4NT</a> has a problem with predictable names and it&rsquo;s a great example to use here.</p>
<h2 id="the-problem">The problem</h2>
<p>There&rsquo;s plenty of detail in my post about <a href="/2015/09/28/first-thoughts-linux-on-the-supermicro-5028d-t4nt/">the Supermicro 5028D-T4NT</a>, but the basic gist is that something within the firmware is causing the all of the network cards in the server to show up as onboard. The server has two 1Gb network interfaces which show up as <code>eno1</code> and <code>eno2</code>, which makes sense. It also has two 10Gb network interfaces that systemd tries to name <code>eno1</code> and <code>eno2</code> as well. That&rsquo;s obviously not going to work, so they get renamed to <code>eth0</code> and <code>eth1</code>.</p>
<p>You can see what udev thinks in this output:</p>
<pre><code>P: /devices/pci0000:00/0000:00:02.2/0000:03:00.0/net/eth0
E: DEVPATH=/devices/pci0000:00/0000:00:02.2/0000:03:00.0/net/eth0
E: ID_BUS=pci
E: ID_MODEL_FROM_DATABASE=Ethernet Connection X552/X557-AT 10GBASE-T
E: ID_MODEL_ID=0x15ad
E: ID_NET_DRIVER=ixgbe
E: ID_NET_LINK_FILE=/usr/lib/systemd/network/99-default.link
E: ID_NET_NAME=eno1
E: ID_NET_NAME_MAC=enx0cc47a7591c8
E: ID_NET_NAME_ONBOARD=eno1
E: ID_NET_NAME_PATH=enp3s0f0
E: ID_OUI_FROM_DATABASE=Super Micro Computer, Inc.
E: ID_PATH=pci-0000:03:00.0
E: ID_PATH_TAG=pci-0000_03_00_0
E: ID_PCI_CLASS_FROM_DATABASE=Network controller
E: ID_PCI_SUBCLASS_FROM_DATABASE=Ethernet controller
E: ID_VENDOR_FROM_DATABASE=Intel Corporation
E: ID_VENDOR_ID=0x8086
E: IFINDEX=4
E: INTERFACE=eth0
E: SUBSYSTEM=net
E: SYSTEMD_ALIAS=/sys/subsystem/net/devices/eno1
E: TAGS=:systemd:
E: USEC_INITIALIZED=7449982
</code></pre><p>The <code>ID_NET_NAME_ONBOARD</code> takes precedence, but the <code>eno1</code> name is already in use at this point since udev has chosen names for the onboard 1Gb network interfaces already. Instead of falling back to <code>ID_NET_NAME_PATH</code>, it falls back to plain old <code>eth0</code>. This is confusing and less than ideal.</p>
<p>After a discussion in a <a href="https://github.com/systemd/systemd/issues/1390">Github issue</a>, it seems that the firmware is to blame. Don&rsquo;t worry - we still have some tricks we can do with systemd-networkd.</p>
<h2 id="workaround">Workaround</h2>
<p>Another handy systemd-networkd feature is a <a href="http://www.freedesktop.org/software/systemd/man/systemd.link.html">link</a> file. These files allow you to apply some network configurations to various interfaces. You can manage multiple interfaces with a single file with wildcards in the <code>[Match]</code> section.</p>
<p>In my case, I want to find any network interfaces that use the <code>ixgbe</code> driver (my 10Gb network interfaces) and apply a configuration change only to those interfaces. My goal is to get the system to name the interfaces using <code>ID_NET_NAME_PATH</code>, which would cause them to appear as <code>enp3s0f0</code> and <code>enp3s0f1</code>.</p>
<p>Let&rsquo;s create a link file to handle our quirky hardware:</p>
<pre><code># /etc/systemd/network/10gb-quirks.link
[Match]
Driver=ixgbe

[Link]
NamePolicy=path
</code></pre><p>This file tells systemd to find any devices using the <code>ixgbe</code> driver and force them to use their PCI device path for the naming. After a reboot, the interfaces look like this:</p>
<pre><code># networkctl  |grep ether
  2 eno1             ether              degraded    configured
  4 eno2             ether              off         unmanaged
  9 enp3s0f0         ether              off         unmanaged
 10 enp3s0f1         ether              off         unmanaged
</code></pre><p>Awesome! They&rsquo;re now named based on their PCI path and that should remain true even through future upgrades. There are plenty of other tricks that you can do with <a href="http://www.freedesktop.org/software/systemd/man/systemd.link.html">link</a> files, including completely custom naming for any interface.</p>
<h2 id="caveats">Caveats</h2>
<p>As Sylvain noted in the comments below, systemd-networkd provides a default <code>99-default.link</code> file that specifies how links should be handled. If you make a link file that sorts after that file, such as <code>ixgbe-quirks.link</code>, it won&rsquo;t take effect. Be sure that your link file comes first by starting it off with a number less than 99. This is why my <code>10gb-quirks.link</code> file works in my example above.</p>
<p><em>Photo Credit: <a href="https://www.flickr.com/photos/7819308@N05/461574071/">realblades</a> via <a href="http://compfight.com">Compfight</a> <a href="https://creativecommons.org/licenses/by-sa/2.0/">cc</a></em></p>
]]></content></item><item><title>Updating Dell PowerEdge BIOS from Linux</title><link>https://major.io/2016/01/18/updating-dell-poweredge-bios-from-linux/</link><pubDate>Mon, 18 Jan 2016 20:53:38 +0000</pubDate><guid>https://major.io/2016/01/18/updating-dell-poweredge-bios-from-linux/</guid><description>Updating Dell PowerEdge firmware from Linux is quite easy, but it isn&amp;rsquo;t documented very well. I ended up with a set of PowerEdge R710&amp;rsquo;s at work for a lab environment and the BIOS versions were different on each server.
Downloading the latest firmware Start by heading over to Dell&amp;rsquo;s support site and enter your system&amp;rsquo;s service tag. You can use lshw to find your service tag:
# lshw | head lab05 description: Rack Mount Chassis product: PowerEdge R710 () vendor: Dell Inc.</description><content type="html"><![CDATA[<p><img src="/wp-content/uploads/2016/01/2466171910_7d18907322_o-e1453148385165.jpg" alt="cover"></p>
<p>Updating Dell PowerEdge firmware from Linux is quite easy, but it isn&rsquo;t documented very well. I ended up with a set of PowerEdge R710&rsquo;s at work for a lab environment and the BIOS versions were different on each server.</p>
<h2 id="downloading-the-latest-firmware">Downloading the latest firmware</h2>
<p>Start by heading over to <a href="http://support.dell.com/">Dell&rsquo;s support site</a> and enter your system&rsquo;s service tag. You can use <code>lshw</code> to find your service tag:</p>
<pre><code># lshw | head
lab05
    description: Rack Mount Chassis
    product: PowerEdge R710 ()
    vendor: Dell Inc.
    serial: [service tag should be here]
    width: 64 bits
    capabilities: smbios-2.6 dmi-2.6 vsyscall32
    configuration: boot=normal chassis=rackmount uuid=44454C4C-3700-104A-8052-B2C04F564831
  *-core
       description: Motherboard
</code></pre><p>After entering the service tag, follow these steps:</p>
<ul>
<li>Click <strong>Drivers &amp; downloads</strong> on the left</li>
<li>Click <strong>Change OS</strong> at the top right and choose <strong>Red Hat Enterprise Linux 7</strong></li>
<li>Click the <strong>BIOS</strong> dropdown in the list</li>
<li>Click <strong>Other file formats available</strong></li>
<li>Look for the file ending in <strong>BIN</strong> and click <strong>Download file</strong> underneath it</li>
</ul>
<p>Copy that file to your server that needs a BIOS update.</p>
<h2 id="installing-firmware-update-tools">Installing firmware update tools</h2>
<p>Start by getting the right packages installed. I&rsquo;ll cover the CentOS/RHEL and Ubuntu methods here. At the moment, Fedora doesn&rsquo;t build kernels with the <code>dell_rbu</code> module enabled, but there&rsquo;s a <a href="https://lists.fedoraproject.org/archives/list/kernel@lists.fedoraproject.org/thread/L623WBK7HAAQWD5FG2MFBD7SIGNGXXVJ/">discussion about getting that fixed</a>.</p>
<p>For CentOS, you&rsquo;ll need to get the Dell Linux repository configured first:</p>
<pre><code>wget http://linux.dell.com/repo/hardware/latest/bootstrap.cgi
sh bootstrap.cgi
yum -y install firmware-addon-dell
</code></pre><p>For Ubuntu, the package is in the upstream repositories already:</p>
<pre><code>apt-get -y install firmware-addon-dell
</code></pre><h2 id="extract-and-flash-the-bios-header">Extract and flash the BIOS header</h2>
<p>Dell packages up a BIOS header (the actual firmware blob that needs to be flashed) within the BIN file you downloaded earlier. The latest version of the BIOS for my R710 is 6.4.0, so my file is called <code>R710_BIOS_4HKX2_LN_6.4.0.BIN</code>. Let&rsquo;s start by extracting the header file:</p>
<pre><code>bash R710_BIOS_4HKX2_LN_6.4.0.BIN --extract bios
</code></pre><p>You should now have a directory in your current directory called <code>bios</code>. The header file is within <code>bios/payload/</code> and you&rsquo;ll use that to flash the BIOS:</p>
<pre><code># modprobe dell_rbu
# dellBiosUpdate-compat --hdr bios/payload/R710-060400.hdr --update
Supported RBU type for this system: (MONOLITHIC, PACKET)
Using RBU v2 driver. Initializing Driver.
Setting RBU type in v2 driver to: PACKET
writing (4096) to file: /sys/devices/platform/dell_rbu/packet_size
Writing RBU data (4096bytes/dot): ...........................
Done writing packet data.
Activate CMOS bit to notify BIOS that update is ready on next boot.
Update staged sucessfully. BIOS update will occur on next reboot.
</code></pre><p>It&rsquo;s now time to reboot! If you watch the console via iDRAC, you&rsquo;ll see a 3-4 minute delay on the next reboot while the staged BIOS image is flashed. When the server boots, use <code>lshw</code> to verify that the BIOS version has been updated.</p>
<p><em>Photo Credit: <a href="https://www.flickr.com/photos/21881956@N05/2466171910/">vaxomatic</a> via <a href="http://compfight.com">Compfight</a> <a href="https://creativecommons.org/licenses/by/2.0/">cc</a></em></p>
]]></content></item><item><title>Nobody is using your software project. Now what?</title><link>https://major.io/2016/01/15/nobody-using-software-project-now/</link><pubDate>Fri, 15 Jan 2016 17:35:48 +0000</pubDate><guid>https://major.io/2016/01/15/nobody-using-software-project-now/</guid><description>Working with open source software is an amazing experience. The collaborative process around creation, refinement, and even maintenance, drives more developers to work on open source software more often. However, every developer finds themselves writing code that very few people actually use.
For some developers, this can be really bothersome. You offer your code up to the world only to find that the world is much less interested than you expected.</description><content type="html"><![CDATA[<p><img src="/wp-content/uploads/2016/01/8627526293_6d3f0edd17_b-e1452879213307.jpg" alt="cover"></p>
<p>Working with open source software is an amazing experience. The collaborative process around creation, refinement, and even maintenance, drives more developers to work on open source software more often. However, every developer finds themselves writing code that very few people <em>actually use</em>.</p>
<p>For some developers, this can be really bothersome. You offer your code up to the world only to find that the world is much less interested than you expected. We see projects that fit the <a href="https://en.wikipedia.org/wiki/Field_of_Dreams">&ldquo;build it, and they will come&rdquo;</a> methodology all the time, but it can hurt when our projects don&rsquo;t have the same impact.</p>
<p>Start by asking yourself a question:</p>
<h2 id="does-it-matter">Does it matter?</h2>
<p>Many of us write software that has a very limited audience. Perhaps we wrote something that worked around a temporary problem or solved an issue that very few poeple would see. Sometimes we write software to work with a project that doesn&rsquo;t have a large user base.</p>
<p>In these situations, it often doesn&rsquo;t matter if other contributors don&rsquo;t show up to collaborate.</p>
<p>However, if you&rsquo;re eager to build a community around an open source project, here are some tips that have worked well for me.</p>
<h2 id="make-it-approachable">Make it approachable</h2>
<p>Sites like <a href="http://stackoverflow.com/">StackOverflow</a> became immensely popular over time because they provide simple, approachable solutions that normally come with a small amount of explanation. Not all of the code snippets are examples of high quality software development, but that&rsquo;s not the point here. People can search, review something, and get on their way.</p>
<p>Making software more approachable is completely based on your audience. Complicated software, like the <a href="https://cryptography.io/en/latest/">cryptography</a> Python library, has an approach towards experienced software developers who want a robust method for handling cryptographic operations. Compare that to the <a href="http://docs.python-requests.org/en/latest/">requests</a> Python library. The developers on that project have an audience of Python developers of all skill levels and they lead off with a simple example and very approachable documentation.</p>
<p>Both of those approaches are very different but extremely effective to their respective audiences.</p>
<p>Once you know your audience, make these changes to make your software more approachable to them:</p>
<ul>
<li><strong>Describe your project&rsquo;s sweet spot.</strong> what does it do better than every other project?</li>
<li><strong>What does your project not do well?</strong> This could clue developers into better projects for their needs or entice them to submit patches for improvements.</li>
<li><strong>How do developers get started?</strong> This should include simple ways to install the software, test it after installation, and examples of ways to quickly begin using it.</li>
<li><strong>How do you want to receive improvements?</strong> If someone finds a bug or area for improvement, how should they submit it and what should their expectations be?</li>
</ul>
<p>If you haven&rsquo;t figured it out already, <strong>documentation is required</strong>. Projects without documentation are quickly skipped over by most developers for a good reason: if you haven&rsquo;t taken the time to help people understand how to use your project, why should they take the time to understand it themselves. Projects without documentation are often assumed to be less mature and not production-ready.</p>
<p>Once you make it this far, it&rsquo;s time to <a href="http://www.psychotactics.com/the-main-difference-between-extroverts-and-introverts/">charge your extrovert battery</a> and promote it.</p>
<h2 id="promoting-the-project">Promoting the project</h2>
<p>Some of the best-written software projects with the best documentation often find themselves limited by the fact that nobody knows they exist. This can become a challenging problem to solve because it involves actively reaching out to the audience you&rsquo;ve identified in the previous steps.</p>
<p>The first step is to do some writing about your software project and what problems it tries to solve. The type of writing and the medium for sharing it is completely up to you.</p>
<p>Some people prefer <a href="http://blog.rackspace.com/why-technical-people-should-blog-but-dont/">writing blog posts</a> on their own blog. You may be able to get additional readers by publishing it on external sites, such as <a href="http://medium.com">Medium</a>, or as a guest author on another site. For example, <a href="http://opensource.com">opensource.com</a> invites guest authors to write about various software projects or solutions provided by open source software. If your project is closely affiliated with another large software project, you may be able to publish a post as a guest author on their project site.</p>
<p>Social media can be helpful if it&rsquo;s used wisely with the right audience. Your followers must be able to get some value from whatever you link them to in your social media posts. Steer clear of clickbait-type posts and be genuine. If you want to build a community, your integrity is your most important asset.</p>
<h2 id="technical-talks">Technical talks</h2>
<p>The most effective method for sharing a project is to do it in person. Yes, this means giving a technical talk to an audience. That means standing in front of people. It&rsquo;s the kind of thing that make an introvert pause. However, if you care about your project, you can <a href="http://www.slideshare.net/MajorHayden/taming-the-technical-talk">tame that technical talk</a> and make a great connection with your audience.</p>
<p>The return on investment in technical talks often takes the form of a <a href="https://en.wikipedia.org/wiki/Dynamics_(music)#Gradual_changes">decrescendo</a>. Feedback flows in quickly as soon as the talk is over and gradually decreases over time. The rate of decrease largely depends on the impact you make on your audience. A high-impact, emotionally appealing presentation will yield a long tail of feedback that decreases very slowly. Your project might appear in presentations made by other people and you&rsquo;ll often get additional feedback and involvement from those talks as well.</p>
<p><em>Photo Credit: <a href="https://www.flickr.com/photos/56944665@N00/8627526293/">Wanaku</a> via <a href="http://compfight.com">Compfight</a> <a href="https://creativecommons.org/licenses/by-nc-nd/2.0/">cc</a></em></p>
]]></content></item><item><title>supernova 2.2.0 is available</title><link>https://major.io/2015/12/04/supernova-2-2-0-is-available/</link><pubDate>Fri, 04 Dec 2015 15:04:39 +0000</pubDate><guid>https://major.io/2015/12/04/supernova-2-2-0-is-available/</guid><description>Thanks to all of the contributors that helped make a new release of supernova possible! Version 2.2.0 is available on GitHub or PyPi.
Changes There&amp;rsquo;s now a fix for some Pygi keyring errors that appeared on the console for some systems that use GnomeKeyring for credential storage. Thanks to dbolackrs for the fix and to gtmanfred for updating the tests.
Justin added some functionality to provide shorter listings of environment variables when you ask supernova to print all of the configurations from your .</description><content type="html"><![CDATA[<p><a href="/wp-content/uploads/2011/08/OpenStackLogo_270x279.jpg"><!-- raw HTML omitted --></a>Thanks to all of the contributors that helped make a new release of <a href="https://github.com/major/supernova">supernova</a> possible! Version 2.2.0 is available on <a href="https://github.com/major/supernova/releases/tag/v2.2.0">GitHub</a> or <a href="https://pypi.python.org/pypi/supernova/2.2.0">PyPi</a>.</p>
<h2 id="changes">Changes</h2>
<p>There&rsquo;s now a <a href="https://github.com/major/supernova/commit/bbe747bef8f226e6bc2397babb6bef079b33e153">fix</a> for some Pygi keyring errors that appeared on the console for some systems that use GnomeKeyring for credential storage. Thanks to <a href="https://github.com/dbolackrs">dbolackrs</a> for the fix and to <a href="https://github.com/gtmanfred">gtmanfred</a> for updating the tests.</p>
<p>Justin <a href="https://github.com/major/supernova/commit/53cdf26feacb2f01aaf3988f370931a0e7ac758a">added some functionality</a> to provide shorter listings of environment variables when you ask supernova to print all of the configurations from your <code>.supernova</code> file. This a big help for users with lots of environments configured on their system.</p>
<p>Finally, Daniel was able to get the <code>[DEFAULT]</code> configuration section <a href="https://github.com/major/supernova/commit/3b2398c1423d5a1b7d6965a78b4bb5dde5329cda">working again</a>. Users with lots of environments that share common configuration items can simplify their configuration files with this change.</p>
<h2 id="updating">Updating</h2>
<p>If you&rsquo;ve already installed supernova with <code>pip</code>, you can get your update now:</p>
<pre><code>pip install -U supernova
</code></pre><p>Builds for Fedora will be up soon.</p>
]]></content></item><item><title>Custom keyboard shortcuts for Evolution in GNOME</title><link>https://major.io/2015/11/27/custom-keyboard-shortcuts-for-evolution-in-gnome/</link><pubDate>Sat, 28 Nov 2015 05:33:29 +0000</pubDate><guid>https://major.io/2015/11/27/custom-keyboard-shortcuts-for-evolution-in-gnome/</guid><description>I&amp;rsquo;ve been a big fan of Thunderbird for years, but it lacks features in some critical areas. For example, I need Microsoft Exchange and Google Apps connectivity for my mail and contacts, but Thunderbird needs some extensions to make that connectivity easier. There are some great extensions available, but they lack polish since they&amp;rsquo;re not part of the core product.
My muscle memory for keyboard shortcuts in Thunderbird left me fumbling in Evolution.</description><content type="html"><![CDATA[<p>I&rsquo;ve been a big fan of Thunderbird for years, but it lacks features in some critical areas. For example, I need Microsoft Exchange and Google Apps connectivity for my mail and contacts, but Thunderbird needs some extensions to make that connectivity easier. There are some great extensions available, but they lack polish since they&rsquo;re not part of the core product.</p>
<p>My muscle memory for keyboard shortcuts in Thunderbird left me fumbling in Evolution. Some of the basics that I used regularly, such as writing a new email or collapsing/expanding threads, were wildly different. For example, there&rsquo;s no keyboard shortcut for expanding threads in Evolution by default.</p>
<h2 id="the-search">The search</h2>
<p>In my quest to adjust some of the default keyboard shortcuts for Evolution, I found lots of documentation about previous versions of GNOME in documentation and countless forum posts. None of the old tricks, like editable menus and easily adjusted dconf settings, work any longer.</p>
<p>I stumbled onto an <a href="https://mail.gnome.org/archives/evolution-list/2015-August/msg00068.html">email thread</a> from August 2015 on this very topic and I was eager to find out if GNOME 3.18&rsquo;s Evolution would look at the same <code>.config/evolution/accels</code> file as the one mentioned in the thread.</p>
<p>First, I started Evolution with strace so I could review the system calls made during its startup:</p>
<pre><code>strace -q -o evolution-trace.out -s 1500 evolution
</code></pre><p>Sure enough, Evolution was looking for the accels file:</p>
<pre><code>$ grep accels evolution-trace.out
open(&quot;/home/user/.config/evolution/accels&quot;, O_RDONLY) = 10
open(&quot;/home/user/.config/evolution/accels&quot;, O_WRONLY|O_CREAT|O_TRUNC, 0644) = 34
</code></pre><h2 id="adding-custom-keyboard-shortcuts">Adding custom keyboard shortcuts</h2>
<p>Editing the accels file is easy for most changes, but <strong>be sure Evolution is stopped prior to editing the file</strong>. The file should look something like this:</p>
<pre><code>; evolution GtkAccelMap rc-file         -*- scheme -*-
; this file is an automated accelerator map dump
;
; (gtk_accel_path &quot;&lt;Actions&gt;/new-source/memo-list-new&quot; &quot;&quot;)
; (gtk_accel_path &quot;&lt;Actions&gt;/switcher/switch-to-tasks&quot; &quot;&lt;Primary&gt;4&quot;)
; (gtk_accel_path &quot;&lt;Actions&gt;/mailto/add-to-address-book&quot; &quot;&quot;)
; (gtk_accel_path &quot;&lt;Actions&gt;/mail/mail-next-thread&quot; &quot;&quot;)
</code></pre><p>Editing an existing shortcut is easy. For example, the default shortcut for creating a new email is CTRL-SHIFT-M:</p>
<pre><code>m&quot;)
</code></pre><p>I prefer Thunderbird&rsquo;s default of CTRL-N for new emails:</p>
<pre><code>n&quot;)
</code></pre><p>Those edits are quite easy, but things get interesting with other characters. For example, Thunderbird uses the asterisk (<code>*</code>) for expanding threads and backslash (<code>\</code>) for collapsing them. Those characters are special in the context of the <code>accels</code> file and they can&rsquo;t be used. Here&rsquo;s an example of how to set keyboard shortcuts with those:</p>
<pre><code>/mail/mail-threads-expand-all&quot; &quot;asterisk&quot;)
(gtk_accel_path &quot;&lt;Actions&gt;/mail/mail-threads-collapse-all&quot; &quot;backslash&quot;)
</code></pre><p>To determine the names of those special characters, use <code>xmodmap</code>:</p>
<pre><code>$ xmodmap -pk | grep backslash
     51     0x005c (backslash)  0x007c (bar)    0x005c (backslash)  0x007c (bar)
</code></pre><h2 id="checking-your-work">Checking your work</h2>
<p>Once you make your adjustments, Evolution should display those new keyboard shortcuts in its menus. For example, here&rsquo;s my new shortcut for writing new emails:</p>
<p><a href="/wp-content/uploads/2015/11/evolution_shortcut.png"><!-- raw HTML omitted --></a></p>
<p>Go back and adjust as many of the shortcuts as necessary. However, <strong>remember to quit Evolution before editing the file</strong>.</p>
]]></content></item><item><title>Talking to college students about information security</title><link>https://major.io/2015/11/10/talking-to-college-students-about-information-security/</link><pubDate>Tue, 10 Nov 2015 14:50:52 +0000</pubDate><guid>https://major.io/2015/11/10/talking-to-college-students-about-information-security/</guid><description>I was recently asked to talk to Computer Information Systems students at the University of the Incarnate Word here in San Antonio about information security in the business world. The students are learning plenty of the technical parts of information security and the complexity that comes from dealing with complicated computer networks. As we all know, it&amp;rsquo;s the non-technical things that are often the most important in those tough situations.</description><content type="html"><![CDATA[<p><img src="/wp-content/uploads/2015/11/UIW_CIMG7805-e1447167032674.jpg" alt="1"></p>
<p>I was recently asked to talk to <a href="http://www.uiw.edu/cis/">Computer Information Systems</a> students at the <a href="http://www.uiw.edu/">University of the Incarnate Word</a> here in San Antonio about information security in the business world. The students are learning plenty of the technical parts of information security and the complexity that comes from dealing with complicated computer networks. As we all know, it&rsquo;s the non-technical things that are often the most important in those tough situations.</p>
<p>My talk, &ldquo;Five lessons I learned about information security&rdquo;, lasted for about 30 minutes and then I took plenty of technical and non-technical questions from the students. I&rsquo;ve embedded the slides below and I&rsquo;ll go through the lessons within the post here.</p>
<h2 id="lesson-1-information-security-requires-lots-of-communication-and-relationships">Lesson 1: Information security requires lots of communication and relationships</h2>
<p>Most of what information security professionals do involves talking about security. There are exceptions to this, however. For example, if your role is highly technical in nature and you&rsquo;re expected to monitor a network or disassemble malware, then you might be spending the majority of your time in front of a screen doing highly technical work.</p>
<p>For the rest of us, we spend a fair amount of time talking about what needs to be secured, why it needs to be secured, and the best way to do it. Information security professionals shouldn&rsquo;t be alone in this work, though. They must find ways to get the business bought in and involved.</p>
<p>I talked about three general buckets of mindsets that the students might find in an average organization:</p>
<blockquote>
<p>&ldquo;Security is mission critical for us and it&rsquo;s how we maintain our customers' trust.&rdquo;</p>
</blockquote>
<p>These are your <strong>allies</strong> in the business and they must be &ldquo;read into&rdquo; what&rsquo;s happening in the business. Share intelligence with them regularly and highlight their accomplishments to your leadership <strong>as well as theirs</strong>.</p>
<blockquote>
<p>&ldquo;Security is really important, but we have lots of features to release. We will get to it.&rdquo;</p>
</blockquote>
<p>These people often see security as a bolt-on, value added product feature. Share methods of building in security from the start and <strong>make it easier</strong> for them to build secure products. Also, this is a great opportunity to create <strong>technical standards</strong> as opposed to policies (more on that later).</p>
<blockquote>
<p>&ldquo;I opened this weird file from someone I didn&rsquo;t know and now my computer is acting funny.&rdquo;</p>
</blockquote>
<p>There&rsquo;s no way to sugar-coat this one: this group is your <strong>biggest risk</strong>. Take steps to prevent them from making mistakes in the first place and regularly send them high-level security communications. Your goal here is to send them information that is easy to read and keeps security <strong>front of mind</strong> for them without inundating them with details.</p>
<h2 id="lesson-2-spend-the-majority-of-your-time-and-money-on-detection-and-response-capabilities">Lesson 2: Spend the majority of your time and money on detection and response capabilities</h2>
<p>This is something that my coworker Aaron and I talk about in our presentation called <a href="http://blog.rackspace.com/rackspacesolve-atlanta-session-recap-the-new-normal/">&ldquo;The New Normal&rdquo;</a>. Make it easier to detect an intruder and respond to the intrusion. Don&rsquo;t allow them to quietly sneak through your network undetected. Force them to be a bull in a china shop. If they cross a network segment, make sure there&rsquo;s an alert for that. Ensure that they have to make a bunch of noise if they wander around in your network.</p>
<p>When something is detected, you need to do two things well: respond to the incident and communicate with the rest of the organization about it.</p>
<p>The response portion requires you to have plenty of information available when it&rsquo;s time to assess a situation. Ensure that logs and alerts are funneled into centralized systems that can aggregate and report on the events in real time (or close to real time). Take that information and understand where the intruders are, what data is at risk, and how severe the situation really is.</p>
<p>From there, find a way to alert the rest of the organization. The United States Department of Defense uses <a href="https://en.wikipedia.org/wiki/DEFCON">DEFCON</a> for this. They can communicate the severity of a situation very quickly to thousands of people by using a simple number designation. That number tells everyone what to do, no matter where they are. Everyone has an idea of the gravity of the situation without needing to know a ton of details.</p>
<p>This is also a good opportunity to share limited intelligence with your allies in the business. They may be able to go into battle with you and get additional information that will help the incident response process.</p>
<h2 id="lesson-3-people-process-and-technology-must-be-in-sync">Lesson 3: People, process, and technology must be in sync</h2>
<p>Everything revolves around this principle. If your processes and technology are great, but your people never follow the process and work around the technology, you have a problem. Great technology and smart people without process is also a dangerous mix. Just like a three-legged stool, all three legs must be strong to keep it stable. The same goes for any business.</p>
<p>When an incident happens, don&rsquo;t talk about people, what could have been done, or vendors. Why? Because no matter how delicate you are, you will eventually &ldquo;call the baby ugly&rdquo;. Calling the baby ugly means that you insult someone&rsquo;s work or character without intending to, and then that person withdraws from the process or approaches the situation defensively. That won&rsquo;t lead to a good outcome and will usually create plenty of animosity.</p>
<p>Assume the worst will happen again and make your processes and technologies better over time. This is an iterative process, so keep in mind that a thousand baby steps will always deliver more value than one giant step.</p>
<h2 id="lesson-4-set-standards-not-policies">Lesson 4: Set standards, not policies</h2>
<p>Policies are inevitable. We get them from our compliance programs, our governments, and other companies. They&rsquo;re required, but they&rsquo;re horribly annoying. Have you ever read through <a href="https://en.wikipedia.org/wiki/ISO/IEC_27002">ISO 27002</a> or <a href="https://en.wikipedia.org/wiki/NIST_Special_Publication_800-53">NIST 800-53</a>? If you have, you know what I mean. Don&rsquo;t get me started on <a href="https://www.pcisecuritystandards.org/security_standards/documents.php">PCI-DSS 3.1</a>, either.</p>
<p>What&rsquo;s my point? Policies are dry. They&rsquo;re long. They&rsquo;re often chock-full of requirements that are really difficult to translate into technical changes. There&rsquo;s no better way to clear out a room of technical people than to say &ldquo;Let&rsquo;s talk about PCI-DSS.&rdquo; (Seriously, try this at least once. It&rsquo;s amazing.)</p>
<p>You need to use the right kind of psychology to get the results you want. Threatening someone with policy is like getting someone to go in for a root canal. They know they need it, but they know how much it will hurt.</p>
<p>Instead, create technical standards that are actionable and valuable. If you know you need to meet PCI-DSS and ISO 27002 for your business, create a technical standard that allows someone in the business to design systems that meet both compliance programs. Make it actionable and then show them the results of their labor when they&rsquo;re done.</p>
<p>Also, give them a method for checking their systems against the standard in an automated way. Nobody wants <a href="https://en.wikipedia.org/wiki/The_Spanish_Inquisition_(Monty_Python)">The Spanish Inquisition</a> showing up at the end of a project to say &ldquo;Hey, you missed something!&rdquo;. They&rsquo;ll be able to check their progress along the way.</p>
<h2 id="lesson-5-dont-take-security-incidents-personally">Lesson 5: Don&rsquo;t take security incidents personally</h2>
<p>This one is still a challenge for me. Security incidents will happen. They certainly won&rsquo;t be fun. However, when the smoke clears, look at the positive aspects of the incident. These situations highlight two critical things:</p>
<ol>
<li>Room for improvement (and perhaps additional spending)</li>
<li>What attackers really want from your business</li>
</ol>
<p>Take the time to understand what type of attacker you just dealt with and what their target really was. If a casual script kiddie found a weakness, you obviously need to invest in more security basics, like network segmentation and hardening standards. If a nation state or some other type of determined attacker found a weakness, you need to understand what they were trying to get. This can be challenging and sometimes third parties can help give an unbiased view.</p>
<h2 id="required-reading">Required reading</h2>
<p>There are three really helpful books I mentioned in the presentation:</p>
<ul>
<li><a href="http://heathbrothers.com/books/switch/">Switch: How to Change Things When Change is Hard</a></li>
<li><a href="http://www.johnmaxwell.com/store/products/Winning-With-People-%5BPaperback%5D.html">Winning With People</a></li>
<li><a href="http://itrevolution.com/books/phoenix-project-devops-book/">The Phoenix Project</a></li>
</ul>
<p>These three books help you figure out how to make change, build relationships, and work around challenges in IT.</p>
<h2 id="final-thoughts">Final thoughts</h2>
<p>If you haven&rsquo;t been to your local university to meet the next generation of professionals, please take the time to do so. There&rsquo;s nothing more exciting than talking with people who have plenty of knowledge and are ready to embrace something new. In addition, they yearn to talk to people who have more experience in the real world.</p>
<p>Thanks to John Champion from UIW for asking me to do a talk! It was a fun experience and I can&rsquo;t wait to do the next one.</p>
<!-- raw HTML omitted -->
]]></content></item><item><title>systemd-networkd and macvlan interfaces</title><link>https://major.io/2015/10/26/systemd-networkd-and-macvlan-interfaces/</link><pubDate>Mon, 26 Oct 2015 13:50:36 +0000</pubDate><guid>https://major.io/2015/10/26/systemd-networkd-and-macvlan-interfaces/</guid><description>I spent some time working with macvlan interfaces on KVM hypervisors last weekend. They&amp;rsquo;re interesting because they&amp;rsquo;re not really a bridge. It allows you to assign multiple MAC addresses to a single interface and then allow the kernel to filter traffic into tap interfaces based on the MAC address in the packet. If you&amp;rsquo;re looking for a highly detailed explanation, head on over to waldner&amp;rsquo;s blog for a deep dive into the technology and the changes that come along with it.</description><content type="html"><![CDATA[<p>I spent some time working with <a href="http://virt.kernelnewbies.org/MacVTap">macvlan</a> interfaces on KVM hypervisors last weekend. They&rsquo;re interesting because they&rsquo;re not really a <em>bridge</em>. It allows you to assign multiple MAC addresses to a single interface and then allow the kernel to filter traffic into tap interfaces based on the MAC address in the packet. If you&rsquo;re looking for a highly detailed explanation, head on over to <a href="http://backreference.org/2014/03/20/some-notes-on-macvlanmacvtap/">waldner&rsquo;s blog</a> for a deep dive into the technology and the changes that come along with it.</p>
<h2 id="why-macvlan">Why macvlan?</h2>
<p>Bridging can become a pain to work with, especially when you&rsquo;re forced to add in creative filtering rules and keep them updated. The macvlan interfaces can help with that (read up on <a href="http://backreference.org/2014/03/20/some-notes-on-macvlanmacvtap/">VEPA</a> mode). There are some <a href="http://www.spinics.net/lists/netdev/msg103457.html">interesting email threads</a> showing that macvlan interfaces can improve network performance for various workloads. Low latency workloads can benefit from the simplicity and low overhead of macvlan interfaces.</p>
<h2 id="systemd-networkd-and-macvlan-interfaces">systemd-networkd and macvlan interfaces</h2>
<p>Fortunately for us, systemd-networkd makes configuring a macvlan interface <strong>really</strong> easy. I&rsquo;ve written about <a href="https://major.io/2015/03/26/creating-a-bridge-for-virtual-machines-using-systemd-networkd/">configuring bridges with systemd-networkd</a> and the process for macvlan interfaces is similar.</p>
<p>In my scenario, I have a 1U server with an ethernet interface called <code>enp4s0</code> (read up on <a href="https://major.io/2015/08/21/understanding-systemds-predictable-network-device-names/">interface naming with systemd-udevd</a>). I want to make a macvlan interface for virtual machines and I&rsquo;ll be attaching VM&rsquo;s to that interface via macvtap interfaces. It&rsquo;s similar to bridging where you make a bridge and then give everyone a port on the bridge.</p>
<p>Start by creating a network device for our macvlan interface:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ini" data-lang="ini"><span style="color:#586e75"># /etc/systemd/network/vmbridge.netdev</span>
<span style="color:#719e07">[NetDev]</span>
Name<span style="color:#719e07">=</span><span style="color:#2aa198">vmbridge</span>
Kind<span style="color:#719e07">=</span><span style="color:#2aa198">macvlan</span>

<span style="color:#719e07">[MACVLAN]</span>
Mode<span style="color:#719e07">=</span><span style="color:#2aa198">bridge</span>
</code></pre></div><p>I&rsquo;ve told systemd-networkd that I want a macvlan interface set up in bridge mode. This will allow hosts and virtual machines to talk to one another on the interface. You could choose <code>vepa</code> for the mode if you want additional security. However, this will force traffic out to your upstream switch/router and makes it challenging for hosts and guests to communicate with each other.</p>
<p>Now that we have a device configured, let&rsquo;s configure the IP address for the macvlan interface (similar to configuring a bridge):</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ini" data-lang="ini"><span style="color:#586e75"># /etc/systemd/network/vmbridge.network</span>
<span style="color:#719e07">[Match]</span>
Name<span style="color:#719e07">=</span><span style="color:#2aa198">vmbridge</span>

<span style="color:#719e07">[Network]</span>
IPForward<span style="color:#719e07">=</span><span style="color:#2aa198">yes</span>
Address<span style="color:#719e07">=</span><span style="color:#2aa198">192.168.250.33/24</span>
Gateway<span style="color:#719e07">=</span><span style="color:#2aa198">192.168.250.1</span>
DNS<span style="color:#719e07">=</span><span style="color:#2aa198">192.168.250.1</span>
</code></pre></div><p>Let&rsquo;s tell systemd-networkd that our physical network interface, <code>enp4s0</code>, is part of this interface:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ini" data-lang="ini"><span style="color:#586e75"># /etc/systemd/network/enp4s0.network</span>
<span style="color:#719e07">[Match]</span>
Name<span style="color:#719e07">=</span><span style="color:#2aa198">enp4s0</span>

<span style="color:#719e07">[Network]</span>
MACVLAN<span style="color:#719e07">=</span><span style="color:#2aa198">vmbridge</span>
</code></pre></div><p>This is very similar to a configuration for a standard Linux bridge. Once you&rsquo;ve reached this step, you&rsquo;ll most likely want to reboot to ensure all of your network devices come up properly.</p>
<h2 id="attaching-a-virtual-machine">Attaching a virtual machine</h2>
<p>Attaching a KVM virtual machine to the macvlan interface is quite easy. When you&rsquo;re creating a new VM using <code>virt-manager</code>, look for this setting in the wizard:</p>
<p><img src="/wp-content/uploads/2015/10/Selection_036.png" alt="6"></p>
<p>If you&rsquo;re installing via <code>virt-install</code> just use the following argument for your network configuration:</p>
<pre><code>--network type=direct,source=vmbridge,source_mode=bridge
</code></pre><p>You&rsquo;ll end up with interfaces like these after creating multiple virtual machines:</p>
<pre><code> mtu 1500 qdisc fq_codel state UNKNOWN mode DEFAULT group default qlen 500
    link/ether 52:54:00:83:53:f2 brd ff:ff:ff:ff:ff:ff promiscuity 0
    macvtap  mode bridge addrgenmode eui64
15: macvtap2@enp4s0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel state UNKNOWN mode DEFAULT group default qlen 500
    link/ether 52:54:00:f1:76:0b brd ff:ff:ff:ff:ff:ff promiscuity 0
    macvtap  mode bridge addrgenmode eui64
17: macvtap3@enp4s0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel state UNKNOWN mode DEFAULT group default qlen 500
    link/ether 52:54:00💿53:34 brd ff:ff:ff:ff:ff:ff promiscuity 0
    macvtap  mode bridge addrgenmode eui64
20: macvtap1@enp4s0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel state UNKNOWN mode DEFAULT group default qlen 500
    link/ether 52:54:00:18:79:d3 brd ff:ff:ff:ff:ff:ff promiscuity 0
    macvtap  mode bridge addrgenmode eui64
</code></pre>]]></content></item><item><title>GRE tunnels with systemd-networkd</title><link>https://major.io/2015/10/16/gre-tunnels-with-systemd-networkd/</link><pubDate>Fri, 16 Oct 2015 23:54:52 +0000</pubDate><guid>https://major.io/2015/10/16/gre-tunnels-with-systemd-networkd/</guid><description>Switching to systemd-networkd for managing your networking interfaces makes things quite a bit simpler over standard networking scripts or NetworkManager. Aside from being easier to configure, it uses fewer resources on your system, which can be handy for smaller virtual machines or containers.
Managing tunnels between interfaces is also easier with systemd-networkd. This post will show you how to set up a GRE tunnel between two hosts running systemd-networkd.
Getting started You&amp;rsquo;ll need two hosts running a recent version of systemd-networkd.</description><content type="html"><![CDATA[<p>Switching to <a href="http://www.freedesktop.org/software/systemd/man/systemd-networkd.service.html">systemd-networkd</a> for managing your networking interfaces makes things quite a bit simpler over standard networking scripts or NetworkManager. Aside from being easier to configure, it uses fewer resources on your system, which can be handy for smaller virtual machines or containers.</p>
<p>Managing tunnels between interfaces is also easier with systemd-networkd. This post will show you how to set up a <a href="https://en.wikipedia.org/wiki/Generic_Routing_Encapsulation">GRE tunnel</a> between two hosts running systemd-networkd.</p>
<h2 id="getting-started">Getting started</h2>
<p>You&rsquo;ll need two hosts running a recent version of systemd-networkd. I&rsquo;d recommend Fedora 22 since it provides very recent versions of systemd which include enhancements to systemd-networkd.</p>
<p>For this example, I&rsquo;ve built one Rackspace Cloud Server in the DFW datacenter and another in IAD. I&rsquo;ll connect them both together with a simple GRE tunnel.</p>
<h2 id="switch-to-systemd-networkd">Switch to systemd-networkd</h2>
<p>I&rsquo;ve <a href="/2015/08/27/build-a-network-router-and-firewall-with-fedora-22-and-systemd-networkd/">detailed out this process before</a> but I&rsquo;ll do it again here. First off, we will need a directory made on both servers to hold systemd-networkd configuration files:</p>
<pre><code>mkdir /etc/systemd/network
</code></pre><p>Let&rsquo;s add a very simple network configuration for our <code>eth0</code> interface on both hosts:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ini" data-lang="ini"><span style="color:#586e75"># cat /etc/systemd/network/eth0.network</span>
<span style="color:#719e07">[Match]</span>
Name<span style="color:#719e07">=</span><span style="color:#2aa198">eth0</span>

<span style="color:#719e07">[Network]</span>
Address<span style="color:#719e07">=</span><span style="color:#2aa198">x.x.x.x/24</span>
Gateway<span style="color:#719e07">=</span><span style="color:#2aa198">x.x.x.x</span>
DNS<span style="color:#719e07">=</span><span style="color:#2aa198">8.8.8.8</span>
DNS<span style="color:#719e07">=</span><span style="color:#2aa198">8.8.4.4</span>
</code></pre></div><p>Do this on <strong>both servers</strong> and be sure to fill in the <code>Address</code> and <code>Gateway</code> lines with the correct data for your servers. Also, feel free to use something other than Google&rsquo;s DNS servers if needed.</p>
<p>It&rsquo;s time to get our services in order so that systemd-networkd will handle our networking after a reboot:</p>
<pre><code>systemctl disable network
systemctl disable NetworkManager
systemctl enable systemd-networkd
systemctl enable systemd-resolved
systemctl start systemd-resolved
rm -f /etc/resolv.conf
ln -s /run/systemd/resolve/resolv.conf /etc/resolv.conf
</code></pre><p><strong>Don&rsquo;t start systemd-networkd yet.</strong> Having systemd-networkd and NetworkManager fight over your interfaces can lead to a bad day.</p>
<p>Reboot both hosts and wait for them to come back online.</p>
<h2 id="configure-the-gre-tunnel">Configure the GRE tunnel</h2>
<p>A GRE tunnel is a simple way to encapsulate packets between two hosts and send almost any protocol across the tunnel you build. However, it&rsquo;s not encrypted. (If you&rsquo;re planning to use this long-term, consider only using encrypted streams across the link or add IPSec on top of the GRE tunnel.)</p>
<p>If we want to route traffic over the GRE tunnel, we will need IP addresses on both sides. I&rsquo;ll use 192.168.254.0/24 for this example, but you&rsquo;re free to use any subnet of any size (except /32!) for this network.</p>
<p>We need to tell systemd-networkd about a new network device that it doesn&rsquo;t know about. We do this with <code>.netdev</code> files. Create this file on both hosts:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ini" data-lang="ini"><span style="color:#586e75"># cat /etc/systemd/network/gre-example.netdev</span>
<span style="color:#719e07">[NetDev]</span>
Name<span style="color:#719e07">=</span><span style="color:#2aa198">gre-example</span>
Kind<span style="color:#719e07">=</span><span style="color:#2aa198">gre</span>
MTUBytes<span style="color:#719e07">=</span><span style="color:#2aa198">1480</span>

<span style="color:#719e07">[Tunnel]</span>
Remote<span style="color:#719e07">=</span><span style="color:#2aa198">[public ip of remote server]</span>
</code></pre></div><p>We&rsquo;re making a new network device called <code>gre-example</code> here and we&rsquo;re telling systemd-networkd about the servers participating in the link. Add this configuration file to <strong>both hosts</strong> but be sure that your <code>Remote=</code> line is correct. If you&rsquo;re writing the configuration file for the <strong>first</strong> host, then the <code>Remote=</code> line should have the IP address of your <strong>second</strong> host. Do the same thing on the second host, but use the IP address of your first host there.</p>
<p>Now that we have a network device, we need to tell systemd-networkd how to configure the IP address on these new GRE tunnels. Let&rsquo;s make a <code>.network</code> file for our GRE tunnel.</p>
<p>On the first host:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ini" data-lang="ini"><span style="color:#586e75"># cat /etc/systemd/network/gre-example.network</span>
<span style="color:#719e07">[Match]</span>
Name<span style="color:#719e07">=</span><span style="color:#2aa198">gre-example</span>

<span style="color:#719e07">[Network]</span>
Address<span style="color:#719e07">=</span><span style="color:#2aa198">192.168.254.1/24</span>
</code></pre></div><p>On the second host:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ini" data-lang="ini"><span style="color:#586e75"># cat /etc/systemd/network/gre-example.network</span>
<span style="color:#719e07">[Match]</span>
Name<span style="color:#719e07">=</span><span style="color:#2aa198">gre-example</span>

<span style="color:#719e07">[Network]</span>
Address<span style="color:#719e07">=</span><span style="color:#2aa198">192.168.254.2/24</span>
</code></pre></div><h2 id="bringing-up-the-tunnel">Bringing up the tunnel</h2>
<p>Although systemd-networkd knows we have a tunnel configured now, it&rsquo;s not sure which interface should manage the tunnel. In our case, our public interface (<code>eth0</code>) is required to be up for this tunnel to function. Go back to your original <code>eth0.network</code> files and add one line under the <code>[Network]</code> section for our tunnel:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ini" data-lang="ini"><span style="color:#719e07">[Network]</span>
Tunnel<span style="color:#719e07">=</span><span style="color:#2aa198">gre-example</span>
</code></pre></div><p>Restart systemd-networkd on <strong>both hosts</strong> and check the network interfaces:</p>
<pre><code># systemctl restart systemd-networkd
# networkctl
IDX LINK             TYPE               OPERATIONAL SETUP
  1 lo               loopback           carrier     unmanaged
  2 eth0             ether              routable    configured
  3 eth1             ether              off         unmanaged
  4 gre0             ipgre              off         unmanaged
  5 gretap0          ether              off         unmanaged
  6 gre-example      ipgre              routable    configured

6 links listed.
</code></pre><p>Hooray! Our GRE tunnel is up! However, we have a firewall in the way.</p>
<h2 id="fixing-the-firewall">Fixing the firewall</h2>
<p>We need to tell the firewall two things: trust the GRE interface and trust the public IP of the other server. Trusting the GRE interface is easy with firewalld — just add this on both hosts:</p>
<pre><code>firewall-cmd --add-interface=gre-example --zone=trusted
</code></pre><p>Now, we need a rich rule to tell firewalld to trust the public IP of each host. I talked about this <a href="/2014/11/24/trust-ip-address-firewallds-rich-rules/">last year</a> on the blog. Run this command on both hosts:</p>
<pre><code>firewall-cmd --zone=public --add-rich-rule='rule family=&quot;ipv4&quot; source address=&quot;[IP ADDRESS]&quot; accept'
</code></pre><p>If you run this on your first host, use the public IP address of your second host in the <code>firewall-cmd</code> command. Use the first host&rsquo;s public IP address when you run the command on the second host.</p>
<p>Save your configuration permanently on <strong>both hosts</strong>:</p>
<pre><code>firewall-cmd --runtime-to-permanent
</code></pre><p>Try to ping between your servers using the IP addresses we configured on the GRE tunnel and you should get some replies!</p>
<h2 id="final-words">Final words</h2>
<p>Remember that GRE tunnels <strong>are not encrypted</strong>. You can add IPSec over the tunnel or you can ensure that you use encrypted streams across the tunnel at all times (SSL, ssh, etc).</p>
]]></content></item><item><title>What I learned while securing Ubuntu</title><link>https://major.io/2015/10/14/what-i-learned-while-securing-ubuntu/</link><pubDate>Wed, 14 Oct 2015 20:53:12 +0000</pubDate><guid>https://major.io/2015/10/14/what-i-learned-while-securing-ubuntu/</guid><description>The blog posts have slowed down a bit lately because I&amp;rsquo;ve been heads down on a security project at work. I&amp;rsquo;m working with people in the OpenStack community to create a new Ansible role called openstack-ansible-security. The role aims to improve host security by using hardening standards to improve the configuration of various parts of the operating system.
This means applying security hardening to Ubuntu 14.04 systems since that&amp;rsquo;s the only host operating system supported by openstack-ansible at the moment.</description><content type="html"><![CDATA[<p>The blog posts have slowed down a bit lately because I&rsquo;ve been heads down on a security project at work. I&rsquo;m working with people in the OpenStack community to create a new Ansible role called <a href="http://specs.openstack.org/openstack/openstack-ansible-specs/specs/mitaka/security-hardening.html">openstack-ansible-security</a>. The role aims to improve host security by using hardening standards to improve the configuration of various parts of the operating system.</p>
<p>This means applying security hardening to Ubuntu 14.04 systems since that&rsquo;s the only host operating system supported by <a href="https://github.com/openstack/openstack-ansible">openstack-ansible</a> at the moment. I have plenty of experience with securing Red Hat-based systems like Red Hat Enteprise Linux, CentOS and Fedora; but Ubuntu is new territory entirely. The rest of this post is full of lessons learned along the way.</p>
<h2 id="searching-for-hardening-standards">Searching for hardening standards</h2>
<p>Finding a complete hardening standard for Ubuntu 14.04 is challenging. The <a href="http://www.cisecurity.org/">Center for Internet Security</a> offers <a href="https://benchmarks.cisecurity.org/downloads/browse/?category=benchmarks.os.linux.ubuntu">Ubuntu security benchmarks</a> with two big caveats:</p>
<ul>
<li>There are very few controls to apply (relative to what&rsquo;s available for RHEL)</li>
<li>The terms of use are highly restrictive (no derivative works allowed)</li>
</ul>
<p>With that idea off the table, I examined the other options that meet Requirement 2.2 of PCI-DSS 3.1 <a href="https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-1.pdf">[PDF]</a>. Anther choice was <a href="https://en.wikipedia.org/wiki/ISO/IEC_27002">ISO 27002</a>, but it&rsquo;s not terribly specific or easy to automate with scripts. The same goes for <a href="https://en.wikipedia.org/wiki/NIST_Special_Publication_800-53">NIST 800-53</a>.</p>
<p>After plenty of searching, the decision was made to go forth with the <a href="http://iase.disa.mil/stigs/Pages/index.aspx">Security Technical Implementation Guide (STIG)</a> from the <a href="http://www.disa.mil/">Defense Information Systems Agency (DISA)</a> (part of the US Department of Defense). The STIGs aren&rsquo;t licensed and they&rsquo;re in the public domain. The only downside is that the closest STIG for use with Ubuntu 14.04 is the <a href="https://www.stigviewer.com/stig/red_hat_enterprise_linux_6/">RHEL 6 STIG</a>.</p>
<p>Using the RHEL 6 STIG meant that plenty of things will need to be translated for the different tools, configuration files, and package names that come with Ubuntu. It was frustrating to search all over for a hardening standard that applies well to Ubuntu and comes with decent auditing tools, but this was the best we could find.</p>
<h2 id="automatically-starting-daemons">Automatically starting daemons</h2>
<p>The standard Ubuntu and Debian practice of automatically starting daemons has <a href="/2014/06/26/install-debian-packages-without-starting-daemons/">perplexed me before</a> and it still continues to do so. Starting a daemon before I&rsquo;ve had a chance to configure it makes little sense. The main argument is that the daemons come up with a highly secure configuration, so starting it automatically shouldn&rsquo;t be a big deal. I&rsquo;d prefer to install a package, have a look at the configuration, alter the configuration, and then start the daemon. Also, it had better not start after a reboot unless I explicitly ask it to do so.</p>
<p>There are plenty of examples where automatically starting a daemon with its default configuration is a bad idea. Take the postfix package as an example. If you install the package in non-interactive mode (as Ansible does by default), postfix will come online wth the following configuration option set:</p>
<pre><code>inet_interfaces = all
</code></pre><p>Since Ubuntu doesn&rsquo;t come with a firewall enabled by default, your postfix server is listening on all interfaces for mail immediately. The <code>mynetworks</code> configuration should prevent relaying, but any potential vulnerabilities in your postfix daemon are exposed to the network without your consent. I would prefer to configure postfix first before I ever allow it to run on my server.</p>
<h2 id="verifying-packages">Verifying packages</h2>
<p>Say what you will about RPM packages and the <code>rpm</code> command, but the verification portions of the <code>rpm</code> command are quite helpful. Here&rsquo;s an example of verifying the aide RPM in Fedora:</p>
<pre><code># rpm -Vv aide
.........  c /etc/aide.conf
.........  c /etc/logrotate.d/aide
.........    /usr/sbin/aide
.........    /usr/share/doc/aide
.........  d /usr/share/doc/aide/AUTHORS
.........  d /usr/share/doc/aide/COPYING
.........  d /usr/share/doc/aide/ChangeLog
.........  d /usr/share/doc/aide/NEWS
.........  d /usr/share/doc/aide/README
.........  d /usr/share/doc/aide/README.quickstart
.........    /usr/share/doc/aide/contrib
.........  d /usr/share/doc/aide/contrib/aide-attributes.sh
.........  d /usr/share/doc/aide/contrib/bzip2.sh
.........  d /usr/share/doc/aide/contrib/gpg2_check.sh
.........  d /usr/share/doc/aide/contrib/gpg2_update.sh
.........  d /usr/share/doc/aide/contrib/gpg_check.sh
.........  d /usr/share/doc/aide/contrib/gpg_update.sh
.........  d /usr/share/doc/aide/contrib/sshaide.sh
.........  d /usr/share/doc/aide/manual.html
.........  d /usr/share/man/man1/aide.1.gz
.........  d /usr/share/man/man5/aide.conf.5.gz
.........    /var/lib/aide
.........    /var/log/aide
</code></pre><p>If the verification finds that nothing in the package has changed, it won&rsquo;t print anything. I&rsquo;ve added the <code>-v</code> here to ensure that everything is printed to the console. In the output, you can see that everything is checked. That includes configuration files, log directories, libraries, and documentation. If I change the content of the <code>aide.conf</code> by adding a comment, I see that change:</p>
<pre><code># echo &quot;# Comment&quot; &gt;&gt; /etc/aide.conf
# rpm -V aide
S.5....T.  c /etc/aide.conf
</code></pre><p>The <code>5</code> denotes that the MD5 checksum on the file has changed since the package was installed. What happens if I change the owner, group, and mode of the <code>aide.conf</code>?</p>
<pre><code># chown major:major /etc/aide.conf
# rpm -V aide
S.5..UGT.  c /etc/aide.conf
</code></pre><p>Now I have a <code>UG</code> there that denotes a user/group ownership change. Similar messages appear for changes to the permissions on files or directories. The <code>restorecon</code> command even lets you figure out when SELinux contexts have changed. If you set a file to have the wrong ownership or permission, one <code>rpm</code> command gets you back to normal:</p>
<pre><code># rpm --setperms --setugids aide
</code></pre><p>On the Ubuntu side, you can use the <code>debsums</code> package to help with some verification:</p>
<pre><code># debsums aide
/usr/bin/aide                                                                 OK
/usr/share/doc/aide/NEWS.Debian.gz                                            OK
/usr/share/doc/aide/changelog.Debian.gz                                       OK
...
# debums aide-common
/usr/bin/aide-attributes                                                      OK
/usr/bin/aide.wrapper                                                         OK
/usr/sbin/aideinit                                                            OK
...
</code></pre><p>But wait — where are the configuration files? Where are the log and library directories? If you type these commands on an Ubuntu system, you&rsquo;ll see that the configuration files and directories aren&rsquo;t checked. In addition, there&rsquo;s not a method for querying whether a particular file in a package has changed ownership or has had its mode changed. There&rsquo;s also no option to restore the right permissions and ownership after an errant <code>chown -R</code> or <code>chmod -R</code>.</p>
<h2 id="managing-aide">Managing AIDE</h2>
<p>The <a href="http://aide.sourceforge.net/">AIDE</a> package is critical for secure deployments since it helps administrators monitor for file integrity on a regular basis. However, Ubuntu ships with some interesting configuration files and wrappers for AIDE.</p>
<p>One of the unique configuration files is this one:</p>
<pre><code># cat /etc/aide/aide.conf.d/99_aide_root
/ Full
</code></pre><p>This causes AIDE to wander all over the system, indexing all types of files. It&rsquo;s best to limit AIDE to a small number of directories whenever possible so that the AIDE runs complete quickly and the database file remains relatively small. Plenty of disk I/O can be used during AIDE runs, so it&rsquo;s best to limit the scope.</p>
<p>Also, trying to initialize the database provides an unhelpful error:</p>
<pre><code># aide --init
Couldn't open file /var/lib/aide/please-dont-call-aide-without-parameters/aide.db.new for writing
</code></pre><p>That path doesn&rsquo;t exist, and I&rsquo;m confused because I did pass a parameter to <code>aide</code>. Long story short, you must use the <code>aideinit</code> command to initialize the aide database. That&rsquo;s actually a bash script which then calls on <code>aide.wrapper</code> (another bash script) to actually run the <code>aide</code> binary for you. Better yet, <code>aideinit</code> is in <code>/usr/sbin</code> while <code>aide.wrapper</code> is in <code>/usr/bin</code>. This leads to plenty of confusion.</p>
<h2 id="linux-security-modules">Linux Security Modules</h2>
<p>It&rsquo;s possible to run SELinux on Ubuntu, but the policies aren&rsquo;t as well maintained as they are on other distributions. AppArmor is the recommended LSM on Ubuntu, but it doesn&rsquo;t provide the granularity of SELinux. For example, SELinux confines almost every single process on a minimal Fedora system, but AppArmor confines almost nothing on a minimal Ubuntu-based system. AppArmor policies aren&rsquo;t terribly restrictive and it&rsquo;s possible to work around them due to their reliance on path names.</p>
<p>Fortunately, both LSM&rsquo;s provide decent coverage with virtual machines and containers (using libvirt&rsquo;s sVirt capability).</p>
<h2 id="summary">Summary</h2>
<p>The upside is that there is plenty of room for security improvements, especially around usability, in Ubuntu. Ubuntu-centric hardening standards are difficult to find and challenging to apply. Every distribution has its quirks and differences, but it seems like securing Ubuntu comes with more unusual hoops to jump through relative to Red Hat-based distributions, OpenSUSE, and even Arch.</p>
<p>I plan to open some bugs for some of these smaller issues in the coming days. However, some of the larger philosophical issues (like automatically starting daemons) will be tougher to tackle.</p>
]]></content></item><item><title>Customizing systemd’s network device names</title><link>https://major.io/2015/09/28/customizing-systemds-network-device-names/</link><pubDate>Tue, 29 Sep 2015 02:08:22 +0000</pubDate><guid>https://major.io/2015/09/28/customizing-systemds-network-device-names/</guid><description>Earlier today, I wrote a post about my first thoughts on the Supermicro 5028D-T4NT server. The 10Gb interfaces on the server came up with the names eth0 and eth1. That wasn&amp;rsquo;t what I expected. There&amp;rsquo;s tons of detail on the problem in the blog post as well as the Github issue.
Kay Sievers gave a hint about how to adjust the interfacing naming in a more granular way than simply disabling the predictable network names.</description><content type="html"><![CDATA[<p><img src="/wp-content/uploads/2015/09/Wikimedia_Foundation_Servers-8055_17-e1443492445994.jpg" alt="1"></p>
<p>Earlier today, I wrote a post about my <a href="/2015/09/28/first-thoughts-linux-on-the-supermicro-5028d-t4nt/">first thoughts on the Supermicro 5028D-T4NT server</a>. The 10Gb interfaces on the server came up with the names <code>eth0</code> and <code>eth1</code>. That wasn&rsquo;t what I expected. There&rsquo;s tons of detail on the problem in the blog post as well as the <a href="https://github.com/systemd/systemd/issues/1390">Github issue</a>.</p>
<p>Kay Sievers <a href="https://github.com/systemd/systemd/issues/1390#issuecomment-143860466">gave a hint</a> about how to adjust the interfacing naming in a more granular way than simply disabling the predictable network names. The <a href="http://www.freedesktop.org/software/systemd/man/systemd.link.html">documentation</a> on .link files is quite helpful. Skip to the <code>NamePolicy=</code> section under <code>[Link]</code> and look the options there.</p>
<p>Looking back to another post I wrote about <a href="/2015/08/21/understanding-systemds-predictable-network-device-names/">predictable device naming in systemd</a>, we can see how these names fit. In my case, I&rsquo;d like to have the network device names <code>enp3s0f0</code> and <code>enp3s0f1</code> instead of <code>eth0</code> and <code>eth1</code>.</p>
<p>Here&rsquo;s the file I created:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ini" data-lang="ini"><span style="color:#586e75"># cat /etc/systemd/network/10gb.link</span>
<span style="color:#719e07">[Match]</span>
Driver<span style="color:#719e07">=</span><span style="color:#2aa198">ixgbe</span>

<span style="color:#719e07">[Link]</span>
NamePolicy<span style="color:#719e07">=</span><span style="color:#2aa198">path</span>
</code></pre></div><p>The interfaces came up with the expected names after a reboot:</p>
<pre><code># ip link
6: enp3s0f0: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000
    link/ether 0c:c4:7a:75:91:c8 brd ff:ff:ff:ff:ff:ff
7: enp3s0f1: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000
    link/ether 0c:c4:7a:75:91:c9 brd ff:ff:ff:ff:ff:ff
</code></pre><p>That will be my workaround until something can be fixed in the server&rsquo;s firmware itself or in systemd.</p>
<p><em>Photo credit: Wikimedia Commons</em></p>
]]></content></item><item><title>First thoughts: Linux on the Supermicro 5028D-TN4T</title><link>https://major.io/2015/09/28/first-thoughts-linux-on-the-supermicro-5028d-t4nt/</link><pubDate>Mon, 28 Sep 2015 12:55:51 +0000</pubDate><guid>https://major.io/2015/09/28/first-thoughts-linux-on-the-supermicro-5028d-t4nt/</guid><description>I&amp;rsquo;ve recently moved over to Rackspace&amp;rsquo;s OpenStack Private Cloud team and the role is full of some great challenges. One of those challenges was figuring out a home lab for testing.
The search My first idea was to pick up some lower-power machines that would give me some infrastructure at a low price with a low power bill as well. I found some Dell Optiplex 3020&amp;rsquo;s on Newegg with Haswell i3&amp;rsquo;s that came in at a good price point.</description><content type="html"><![CDATA[<p>I&rsquo;ve recently moved over to Rackspace&rsquo;s OpenStack Private Cloud team and the role is full of some great challenges. One of those challenges was figuring out a home lab for testing.</p>
<h2 id="the-search">The search</h2>
<p>My first idea was to pick up some lower-power machines that would give me some infrastructure at a low price with a low power bill as well. I found some Dell Optiplex 3020&rsquo;s on Newegg with Haswell i3&rsquo;s that came in at a good price point. In addition, they delivered the virtualization extensions that I needed without a high TDP.</p>
<p>Once I started talking about my search on Twitter, someone piped in with a suggestion:</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p><img src="/wp-content/uploads/2015/09/SYS-5028D-TN4T_open.jpg" alt="1"></p>
<p>Supermicro, eh? I&rsquo;ve had great success with two Supermicro boxes from Silicon Mechanics (and I can&rsquo;t say enough good things about both companies) in my colocation environment. I decided to take a closer look at the <a href="http://www.supermicro.com/products/system/midtower/5028/sys-5028d-tn4t.cfm">Supermicro 5028D-TN4T</a>.</p>
<p>There&rsquo;s a great review on <a href="http://www.anandtech.com/show/9185/intel-xeon-d-review-performance-per-watt-server-soc-champion/3">AnandTech</a> about the Supermicro 5028D-TN4T. It gets plenty of praise for packing a lot of advanced features into a small, energy-efficient server. AnandTech found that the <a href="http://www.anandtech.com/show/9185/intel-xeon-d-review-performance-per-watt-server-soc-champion/15">idle power draw was around 30 watts</a> and as low as 27 watts in some cases. I haven&rsquo;t tested it with my <a href="http://www.p3international.com/products/p4400.html">Kill A Watt</a> yet, but I intend to do so later this week.</p>
<h2 id="initial-thoughts">Initial thoughts</h2>
<p>This chassis is <strong>small</strong>. I snapped a quick photo for some folks who were asking about it on Twitter:</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>I&rsquo;ll have better pictures soon in a more detailed review. If you&rsquo;re itching for more photos now, head on over to the AnandTech article I mentioned earlier.</p>
<p>Installing the RAM was a piece of cake, but I did need to hold a fan shroud out of the way as I installed some of them. There are three spots for installing SSD drives: one for an M.2 SATA drive and two 2.5″ drive spots. Routing the cables to the SSD drives is quite easy, but you will have to clip a zip tie or two (<em>carefully</em>).</p>
<p>The IPMI is fantastic, as expected. If you&rsquo;ve ever used other Supermicro servers with built-in IPMI, then you&rsquo;ll recognize the interface. You have full control over power, fans, and serial output. In addition, the standard iKVM interface is there so you can view the graphical console remotely, attach disks over the network, and power cycle the server. The IPMI was configured to use DHCP out of the box.</p>
<p>The fan noise is a bit higher than I&rsquo;d like during boot, but it&rsquo;s nothing like your average 1U/2U server. It&rsquo;s louder than my Optiplex 3020 (which is whisper silent) but much quieter than the ASA 5520. The system is very quiet once it finishes booting and it settles down.</p>
<h2 id="linux-fun">Linux fun</h2>
<p>As expected, everything worked fine in Linux - except the 10Gb interfaces. It has a X557 controller for the dual 10Gb interfaces:</p>
<pre><code># lspci | grep Eth
03:00.0 Ethernet controller: Intel Corporation Ethernet Connection X552/X557-AT 10GBASE-T
03:00.1 Ethernet controller: Intel Corporation Ethernet Connection X552/X557-AT 10GBASE-T
05:00.0 Ethernet controller: Intel Corporation I350 Gigabit Network Connection (rev 01)
05:00.1 Ethernet controller: Intel Corporation I350 Gigabit Network Connection (rev 01)
</code></pre><p>The downside here is that the X557 PHY ID wasn&rsquo;t <a href="https://git.kernel.org/cgit/linux/kernel/git/stable/linux-stable.git/commit/?id=c2c78d5c35e4f4a9226360bc432dc81b47f163e4">added until Linux 4.2</a>. However, I upgraded the server from Fedora 22 to 23 (and picked up Linux 4.2.1 along the way), and everything worked.</p>
<p>The onboard 1Gb interfaces showed up as <code>eno1</code> and <code>eno2</code>, as expected, but the 10Gb cards showed up as <code>eth0</code> and <code>eth1</code>. If you&rsquo;ve read my post on <a href="/2015/08/21/understanding-systemds-predictable-network-device-names/">systemd&rsquo;s predictable interface named</a>, you&rsquo;ll notice this is a little unpredictable. The 10Gb interfaces seem to come up as <code>eno1</code> and <code>eno2</code> in udev, but that won&rsquo;t work since the onboard I350 ethernet ports already use those names:</p>
<pre><code>P: /devices/pci0000:00/0000:00:02.2/0000:03:00.0/net/eth0
E: DEVPATH=/devices/pci0000:00/0000:00:02.2/0000:03:00.0/net/eth0
E: ID_BUS=pci
E: ID_MODEL_FROM_DATABASE=Ethernet Connection X552/X557-AT 10GBASE-T
E: ID_MODEL_ID=0x15ad
E: ID_NET_DRIVER=ixgbe
E: ID_NET_LINK_FILE=/usr/lib/systemd/network/99-default.link
E: ID_NET_NAME=eno1
E: ID_NET_NAME_MAC=enx0cc47a7591c8
E: ID_NET_NAME_ONBOARD=eno1
E: ID_NET_NAME_PATH=enp3s0f0
E: ID_OUI_FROM_DATABASE=Super Micro Computer, Inc.
E: ID_PATH=pci-0000:03:00.0
E: ID_PATH_TAG=pci-0000_03_00_0
E: ID_PCI_CLASS_FROM_DATABASE=Network controller
E: ID_PCI_SUBCLASS_FROM_DATABASE=Ethernet controller
E: ID_VENDOR_FROM_DATABASE=Intel Corporation
E: ID_VENDOR_ID=0x8086
E: IFINDEX=4
E: INTERFACE=eth0
E: SUBSYSTEM=net
E: SYSTEMD_ALIAS=/sys/subsystem/net/devices/eno1
E: TAGS=:systemd:
E: USEC_INITIALIZED=7449982
</code></pre><p>I <a href="https://github.com/systemd/systemd/issues/1390">opened up a Github issue for systemd</a> and it&rsquo;s getting some attention. We&rsquo;ll hopefully see it fixed soon.</p>
<h2 id="more-to-come">More to come</h2>
<p>Keep an eye out for a more detailed review once I start throwing some OpenStack workloads on the Supermicro server. I&rsquo;ll also take some more detailed photos and share the additional parts I added to my server.</p>
]]></content></item><item><title>systemd in Fedora 22: Failed to restart service: Access Denied</title><link>https://major.io/2015/09/18/systemd-in-fedora-22-failed-to-restart-service-access-denied/</link><pubDate>Fri, 18 Sep 2015 19:43:35 +0000</pubDate><guid>https://major.io/2015/09/18/systemd-in-fedora-22-failed-to-restart-service-access-denied/</guid><description>If you&amp;rsquo;re running Fedora 22 and you&amp;rsquo;ve recently updated to systemd-219-24.fc22, you might see errors like these:
# systemctl restart postfix Failed to restart postfix.service: Access denied Your audit logs will have entries like these:
type=USER_AVC msg=audit(1442602150.292:763): pid=1 uid=0 auid=4294967295 ses=4294967295 subj=system_u:system_r:init_t:s0 msg='avc: denied { start } for auid=n/a uid=0 gid=0 path=&amp;quot;/run/systemd/system/session-4.scope&amp;quot; cmdline=&amp;quot;/usr/lib/systemd/systemd-logind&amp;quot; scontext=system_u:system_r:systemd_logind_t:s0 tcontext=system_u:object_r:systemd_unit_file_t:s0 tclass=service exe=&amp;quot;/usr/lib/systemd/systemd&amp;quot; sauid=0 hostname=? addr=? terminal=?' type=USER_AVC msg=audit(1442602150.437:768): pid=1 uid=0 auid=4294967295 ses=4294967295 subj=system_u:system_r:init_t:s0 msg='avc: denied { start } for auid=n/a uid=0 gid=0 path=&amp;quot;/usr/lib/systemd/system/user@.</description><content type="html"><![CDATA[<p><a href="/wp-content/uploads/2012/01/fedorainfinity.png"><!-- raw HTML omitted --></a>If you&rsquo;re running Fedora 22 and you&rsquo;ve recently updated to <a href="https://bodhi.fedoraproject.org/updates/FEDORA-2015-15821">systemd-219-24.fc22</a>, you might see errors like these:</p>
<pre><code># systemctl restart postfix
Failed to restart postfix.service: Access denied
</code></pre><p>Your audit logs will have entries like these:</p>
<pre><code>type=USER_AVC msg=audit(1442602150.292:763): pid=1 uid=0 auid=4294967295 ses=4294967295 subj=system_u:system_r:init_t:s0 msg='avc:  denied  { start } for auid=n/a uid=0 gid=0 path=&quot;/run/systemd/system/session-4.scope&quot; cmdline=&quot;/usr/lib/systemd/systemd-logind&quot; scontext=system_u:system_r:systemd_logind_t:s0 tcontext=system_u:object_r:systemd_unit_file_t:s0 tclass=service  exe=&quot;/usr/lib/systemd/systemd&quot; sauid=0 hostname=? addr=? terminal=?'
type=USER_AVC msg=audit(1442602150.437:768): pid=1 uid=0 auid=4294967295 ses=4294967295 subj=system_u:system_r:init_t:s0 msg='avc:  denied  { start } for auid=n/a uid=0 gid=0 path=&quot;/usr/lib/systemd/system/user@.service&quot; cmdline=&quot;/usr/lib/systemd/systemd-logind&quot; scontext=system_u:system_r:systemd_logind_t:s0 tcontext=system_u:object_r:systemd_unit_file_t:s0 tclass=service  exe=&quot;/usr/lib/systemd/systemd&quot; sauid=0 hostname=? addr=? terminal=?'
type=USER_AVC msg=audit(1442602150.440:769): pid=1 uid=0 auid=4294967295 ses=4294967295 subj=system_u:system_r:init_t:s0 msg='avc:  denied  { start } for auid=n/a uid=0 gid=0 path=&quot;/run/systemd/system/session-4.scope&quot; cmdline=&quot;/usr/lib/systemd/systemd-logind&quot; scontext=system_u:system_r:systemd_logind_t:s0
</code></pre><p>There&rsquo;s a <a href="https://bugzilla.redhat.com/show_bug.cgi?id=1224211">very active bug</a> under review to get it fixed. As a workaround, you can re-execute systemd with the following command:</p>
<pre><code>systemctl daemon-reexec
</code></pre><p>That should allow you to stop, start, and restart services properly again. Also, you&rsquo;ll be able to switch runlevels for reboots and shutdowns.</p>
<p>Keep an eye on the bug for more details as they develop. Kudos to <a href="https://fedoraproject.org/wiki/User:Kevin">Kevin Fenzi</a> for the workaround!</p>
]]></content></item><item><title>Time Warner Road Runner, Linux, and large IPv6 subnets</title><link>https://major.io/2015/09/11/time-warner-road-runner-linux-and-large-ipv6-subnets/</link><pubDate>Fri, 11 Sep 2015 21:08:44 +0000</pubDate><guid>https://major.io/2015/09/11/time-warner-road-runner-linux-and-large-ipv6-subnets/</guid><description>Although Time Warner Cable is now Spectrum and wide-dhcpv6 is quite old, this post is still what I&amp;rsquo;m using today (in 2019)!
I&amp;rsquo;ve written about how to get larger IPv6 subnets from Time Warner Cable&amp;rsquo;s Road Runner service on a Mikrotik router before, but I&amp;rsquo;ve converted to using a Linux server as my router for my home. Getting the larger /56 IPv6 subnet is a little tricky and it&amp;rsquo;s not terribly well documented.</description><content type="html"><![CDATA[<blockquote>
<p>Although Time Warner Cable is now Spectrum and wide-dhcpv6 is quite
old, this post is still what I&rsquo;m using today (in 2019)!</p>
</blockquote>
<p>I&rsquo;ve written about how to <a href="/2014/09/11/howto-time-warner-cable-ipv6/">get larger IPv6 subnets from Time Warner Cable&rsquo;s Road Runner service</a> on a Mikrotik router before, but I&rsquo;ve converted to using a Linux server as my router for my home. Getting the larger /56 IPv6 subnet is a little tricky and it&rsquo;s not terribly well documented.</p>
<h2 id="my-network">My network</h2>
<p>My Linux router has two bridges, <em>br0</em> and <em>br1</em>, that handle WAN and LAN traffic respectively. This is a fairly simple configuration.</p>
<pre><code>                +-------------------+
                |                   |
+-----------+   |                   |     +----------+
|Cable modem+---+ br0          br1  +-----+LAN switch|
+-----------+   |                   |     +----------+
                |    Linux router   |
                +-------------------+
</code></pre><p>Ideally, I&rsquo;d like to have a single address assigned to <em>br0</em> so that my Linux router can reach IPv6 destinations. I&rsquo;d also like a /64 assigned to the <em>br1</em> interface so that I can distribute addresses from that subnet to devices on my LAN.</p>
<h2 id="getting-dhcpv6-working">Getting DHCPv6 working</h2>
<p>The <a href="http://sourceforge.net/projects/wide-dhcpv6/">wide-dhcpv6</a> package provides a DHCPv6 client and also takes care of assigning some addresses for you. Installing it is easy with dnf:</p>
<pre><code>dnf install wide-dhcpv6
</code></pre><p>We will create a new configuration file at <code>/etc/wide-dhcpv6/dhcp6c.conf</code>:</p>
<pre><code>interface br0 {
 send ia-pd 1;
 send ia-na 1;
};

id-assoc na 1 {
};

id-assoc pd 1 {
 prefix ::/56 infinity;
 prefix-interface br0 {
  sla-id 1;
  sla-len 8;
 };
 prefix-interface br1 {
  sla-id 2;
  sla-len 8;
 };
 prefix-interface vlan1 {
  sla-id 3;
  sla-len 8;
 };
};
</code></pre><p>If this configuration file makes sense to you without explanation, I&rsquo;m impressed. Let&rsquo;s break it up into pieces to understand it.</p>
<p>The first section with <em>interface br0</em> specifies that we want to do our DHCPv6 requests on the <em>br0</em> interface. The configuration lines inside the curly braces says we want to specify a <a href="https://en.wikipedia.org/wiki/Prefix_delegation">prefix delegation</a> (the <em>IA_PD</em> DHCPv6 option) and we also want a stateful (SLAAC) address assigned on <em>br0</em> (the <em>IA_NA</em> DHCPv6 option). These are just simple flags that tell the upstream DHCPv6 server that we want to specify a particular prefix size and that we also want a single address (via SLAAC) for our external interface.</p>
<p>The <em>id-assoc na 1</em> section specifies that we want to accept the default SLAAC address provided by the upstream network device.</p>
<p>The <em>id-assoc pd 1</em> section gives the upstream DHCPv6 server a hint that we really want a /56 block of IPv6 addresses. The next three sections give our DHCPv6 client an idea of how we want addresses configured on our internal network devices. The three interfaces in each <em>prefix-interface</em> section will receive a different block (noted by the <em>sla-id</em> increasing by one each time). Also, the block size we intend to assign is a /64 (<em>sla-len</em> is 8, which means we knock 8 bits off a /56 and end up with a /64). Don&rsquo;t change your <em>sla-id</em> after you set it. That will cause the DHCPv6 client to move your /64 address blocks around to a different interface.</p>
<p>Still with me? This stuff is really confusing and documentation is sparse.</p>
<p>Start the DHCPv6 client and ensure it comes up at boot:</p>
<pre><code>systemctl enable dhcp6c
systemctl start dhcp6c
</code></pre><p>Run <code>ip addr</code> and look for IPv6 blocks configured on each interface. In my case, <em>br0</em> got a single address, and the other interfaces received unique /64&rsquo;s.</p>
<h2 id="telling-the-lan-about-ipv6">Telling the LAN about IPv6</h2>
<p>The router is working now, but we need to tell our devices on the LAN that we have some IPv6 addresses available. You have different options for this, such as dnsmasq or radvd, but we will use radvd here:</p>
<pre><code>dnf -y install radvd
</code></pre><p>If you open <code>/etc/radvd.conf</code>, you&rsquo;ll notice a helpful comment block at the top with a great example configuration. I only want to announce IPv6 on my <em>br1</em> interface, so I&rsquo;ll add this configuration block:</p>
<pre><code>interface br1
{
  AdvSendAdvert on;
  MaxRtrAdvInterval 30;

  prefix ::/64
  {
    AdvOnLink on;
    AdvAutonomous on;
    AdvRouterAddr off;
  };
};
</code></pre><p>You don&rsquo;t actually need to specify the IPv6 prefix since radvd is smart enough to examine your interface and discover the IPv6 subnet assigned to it. This configuration says we will send router advertisements, let systems on the network choose their own addresses, and we will advertise those addresses as soon as the link comes up.</p>
<p>Let&rsquo;s start radvd and ensure it comes up at boot:</p>
<pre><code>systemctl enable radvd
systemctl start radvd
</code></pre><p>Connect a machine to your LAN and you should receive an IPv6 address shortly after the link comes up!</p>
<h2 id="troubleshooting">Troubleshooting</h2>
<p>If you&rsquo;re having trouble getting an IPv6 address, double-check your iptables rules. You will need to ensure you&rsquo;re allowing UDP 546 into your external interface. Here are some examples you can use:</p>
<pre><code># If you're using firewalld
firewall-cmd --add-port=546/udp
firewall-cmd --add-port=546/udp --permanent
# If you're using bare ip6tables
ip6tables -A INPUT -p udp -m udp --dport 546 -j ACCEPT
</code></pre>]]></content></item><item><title>supernova is coming to Fedora repositories</title><link>https://major.io/2015/09/11/supernova-is-coming-to-fedora-repositories/</link><pubDate>Fri, 11 Sep 2015 13:13:42 +0000</pubDate><guid>https://major.io/2015/09/11/supernova-is-coming-to-fedora-repositories/</guid><description>If you use Fedora, you will soon be able to install supernova via a Fedora package! The packages are currently in the testing repositories but they will soon be available in the stable repositories for Fedora 22, 23, and rawhide.
Want it right now? If you want to install supernova now, simply tell dnf to install it from the updates-testing repository:
dnf install --enablerepo=updates-testing supernova supernova in EPEL A few people have asked for supernova to be added to EPEL, but the version of the click module for python is too old.</description><content type="html"><![CDATA[<p><a href="/wp-content/uploads/2012/01/fedorainfinity.png"><!-- raw HTML omitted --></a>If you use Fedora, you will soon be able to install <a href="https://github.com/major/supernova">supernova</a> via a Fedora package! The <a href="https://bodhi.fedoraproject.org/updates/?packages=supernova">packages are currently in the testing repositories</a> but they will soon be available in the stable repositories for Fedora 22, 23, and rawhide.</p>
<h3 id="want-it-right-now">Want it right now?</h3>
<p>If you want to install supernova now, simply tell dnf to install it from the updates-testing repository:</p>
<pre><code>dnf install --enablerepo=updates-testing supernova
</code></pre><h3 id="supernova-in-epel">supernova in EPEL</h3>
<p>A few people have asked for supernova to be added to <a href="https://fedoraproject.org/wiki/EPEL">EPEL</a>, but the version of the <a href="http://click.pocoo.org/5/">click</a> module for python is too old. Getting supernova into EPEL isn&rsquo;t completely off the table, but it will require some additional work.</p>
<p>Many thanks to <a href="https://fedoraproject.org/wiki/User:Immanetize">Pete</a> and Carl for helping me with the package review and bug fixes.</p>
]]></content></item><item><title>Chronicles of SELinux: Dealing with web content in unusual directories</title><link>https://major.io/2015/09/10/chronicles-of-selinux-dealing-with-web-content-in-unusual-directories/</link><pubDate>Thu, 10 Sep 2015 13:40:35 +0000</pubDate><guid>https://major.io/2015/09/10/chronicles-of-selinux-dealing-with-web-content-in-unusual-directories/</guid><description>I&amp;rsquo;ve decided to start a series of posts called &amp;ldquo;Chronicles of SELinux&amp;rdquo; where I hope to educate more users on how to handle SELinux denials with finesse rather than simply disabling it entirely. To kick things off, I&amp;rsquo;ll be talking about dealing with web content in the first post.
First steps If you&amp;rsquo;d like to follow along, simply hop onto a system running Fedora 21 (or later), CentOS 7 or Red Hat Enterprise Linux 7.</description><content type="html"><![CDATA[<p><img src="/wp-content/uploads/2013/07/selinux-penguin-new_medium.png" alt="1"></p>
<p>I&rsquo;ve decided to start a series of posts called &ldquo;Chronicles of SELinux&rdquo; where I hope to educate more users on how to handle SELinux denials with finesse rather than simply <a href="http://stopdisablingselinux.com/">disabling it entirely</a>. To kick things off, I&rsquo;ll be talking about dealing with web content in the first post.</p>
<h2 id="first-steps">First steps</h2>
<p>If you&rsquo;d like to follow along, simply hop onto a system running Fedora 21 (or later), CentOS 7 or Red Hat Enterprise Linux 7. We need SELinux in enforcing mode on the host, so be sure to check the status with <code>getenforce</code>. Depending on what <code>getenforce</code> returns, you&rsquo;ll need to make adjustments:</p>
<ul>
<li><em>Enforcing</em>: No adjustments needed - you&rsquo;re all set!</li>
<li><em>Permissive</em>: Run <code>setenforce 1</code> and adjust SELinux configuration file (see below)</li>
<li><em>Disabled</em>: Adjust the SELinux configuration file and reboot (see below)</li>
</ul>
<p>To enable enforcing mode in the SELinux configuration file, edit <code>/etc/selinux/config</code> and ensure your <code>SELINUX</code> line has <code>enforcing</code>:</p>
<pre><code># This file controls the state of SELinux on the system.
# SELINUX= can take one of these three values:
#     enforcing - SELinux security policy is enforced.
#     permissive - SELinux prints warnings instead of enforcing.
#     disabled - No SELinux policy is loaded.
SELINUX=enforcing
</code></pre><p>If <code>getenforce</code> returned <em>Disabled</em> earlier, you will need to reboot to get SELinux working. Also be sure that the <code>selinux-policy-targeted</code> package is installed and run <code>fixfiles onboot -B</code> to relabel the system on reboot <em>(<a href="#comment-2246455243">thanks to immanetize for the comment</a>)</em>.</p>
<p>Let&rsquo;s install httpd and create a developer user:</p>
<pre><code># For Fedora
dnf -y install httpd
# For CentOS/RHEL
yum -y install httpd

useradd developer
systemctl enable httpd
systemctl start httpd
</code></pre><p>On to the guide!</p>
<h2 id="hosting-content-in-an-unique-directory">Hosting content in an unique directory</h2>
<p>On Red Hat-based systems, httpd expects to find its content in <code>/var/www/html</code>, but some system administrators prefer to have content stored elsewhere on the system. It could be on a SAN or other remote storage, but it could also just be in a different directory to make things easier for the business.</p>
<p>Let&rsquo;s consider a situation where the web content is hosted from <code>/web/</code>. We can create the directory:</p>
<pre><code>[root@fedora22 ~]# mkdir -v /web
mkdir: created directory '/web'
</code></pre><p>We can edit <code>/etc/httpd/conf/httpd.conf</code> and set our new <code>DocumentRoot</code>:</p>
<pre><code>#
# DocumentRoot: The directory out of which you will serve your
# documents. By default, all requests are taken from this directory, but
# symbolic links and aliases may be used to point to other locations.
#
DocumentRoot &quot;/web&quot;
</code></pre><p>Let&rsquo;s reload the httpd configuration:</p>
<pre><code>systemctl reload httpd
</code></pre><p>And now we can add some amazing web content:</p>
<pre><code> /web/index.html
</code></pre><p>It&rsquo;s time to test our web server:</p>
<pre><code># curl -i localhost/index.html
HTTP/1.1 403 Forbidden
Date: Thu, 10 Sep 2015 12:54:19 GMT
Server: Apache/2.4.16 (Fedora)
Content-Length: 219
Content-Type: text/html; charset=iso-8859-1
</code></pre><p>Oh, come on. What&rsquo;s with this 403 error?</p>
<h2 id="investigating-the-403">Investigating the 403</h2>
<p>The first step for any situation like this is to review some logs. Let&rsquo;s check the logs for httpd:</p>
<pre><code>[Thu Sep 10 12:55:04.541789 2015] [core:error] [pid 16597] (13)Permission denied: [client ::1:49860] AH00035: access to /index.html denied (filesystem path '/web/index.html') because search permissions are missing on a component of the path
</code></pre><p>Search permissions are missing? What? Let&rsquo;s check the permissions on our web directory:</p>
<pre><code># ls -al /web
total 12
drwxr-xr-x.  2 root root 4096 Sep 10 12:53 .
dr-xr-xr-x. 19 root root 4096 Sep 10 12:51 ..
-rw-r--r--.  1 root root   21 Sep 10 12:54 index.html
</code></pre><p>The httpd user has the ability to get into the directory (<code>o+x</code> is set on <code>/web/</code>) and the httpd user can read the file (<code>o+r</code> is set on <code>/web/index.html</code>). Let&rsquo;s check the system journal just in case:</p>
<pre><code># journalctl -n 1 | tail
-- Logs begin at Thu 2015-09-10 12:31:37 UTC, end at Thu 2015-09-10 12:55:04 UTC. --
Sep 10 12:55:04 fedora22 audit[16597]: &lt;audit-1400&gt; avc:  denied  { getattr } for  pid=16597 comm=&quot;httpd&quot; path=&quot;/web/index.html&quot; dev=&quot;xvda1&quot; ino=524290 scontext=system_u:system_r:httpd_t:s0 tcontext=unconfined_u:object_r:default_t:s0 tclass=file permissive=0
</code></pre><p>That&rsquo;s quite a long log line. Let&rsquo;s break it into pieces:</p>
<ul>
<li><code>avc: denied { getattr } for pid=16597 comm=&quot;httpd&quot;</code>: httpd tried to do something and was denied</li>
<li><code>path=&quot;/web/index.html&quot; dev=&quot;xvda1&quot; ino=524290</code>: path to the file (index.html) involved in the denial</li>
<li><code>scontext=system_u:system_r:httpd_t:s0</code>: the SELinux context of the httpd process</li>
<li><code>tcontext=unconfined_u:object_r:default_t:s0</code>: the SELinux contect that is actually applied to our index.html</li>
<li><code>tclass=file</code>: the denial came from accessing a file (index.html)</li>
<li><code>permissive=0</code>: we&rsquo;re in enforcing mode, not permissive mode</li>
</ul>
<p>Long story short, when httpd tried to access our <code>/web/index.html</code> file, the httpd process was labeled with <code>httpd_t</code>, but the kernel found that the HTML file was labeled with <code>default_t</code>. The httpd process (labeled with <code>httpd_t</code>) isn&rsquo;t allowed to read files that are labeled as <code>default_t</code>, so the access is denied.</p>
<h2 id="fixing-it-the-right-way">Fixing it the right way</h2>
<p>Since we know what SELinux expects for this file (from the log line in the journal), we can apply the right context and re-test. The <code>chcon</code> command has a handy argument that allows you to reference a file or directory, and apply the contexts from there. Since we know that <code>/var/www/html</code> has the right contexts already, we can use it as a reference:</p>
<pre><code># chcon -v -R --reference=/var/www/html /web
changing security context of '/web/index.html'
changing security context of '/web'
</code></pre><p>Now we see some different contexts on <code>/web</code>:</p>
<pre><code># ls -alZ /web/
total 12
drwxr-xr-x.  2 root root system_u:object_r:httpd_sys_content_t:s0 4096 Sep 10 13:19 .
dr-xr-xr-x. 19 root root system_u:object_r:root_t:s0              4096 Sep 10 13:19 ..
-rw-r--r--.  1 root root system_u:object_r:httpd_sys_content_t:s0   21 Sep 10 13:19 index.html
</code></pre><p>Let&rsquo;s test again:</p>
<pre><code># curl -I localhost/index.html
HTTP/1.1 403 Forbidden
Date: Thu, 10 Sep 2015 13:21:22 GMT
Server: Apache/2.4.16 (Fedora)
Content-Type: text/html; charset=iso-8859-1
</code></pre><p>Darn! What&rsquo;s in the httpd logs?</p>
<pre><code>[Thu Sep 10 13:21:22.267719 2015] [authz_core:error] [pid 16593] [client ::1:49861] AH01630: client denied by server configuration: /web/index.html
</code></pre><p>Ah, we cleared the SELinux problem but now httpd is upset. Just below the <code>DocumentRoot</code> line that we edited earlier, look for two <code>Directory</code> blocks. Change <code>/var/www/</code> and <code>/var/www/html</code> to <code>/web</code> in those blocks. Reload the httpd configuration and try once more:</p>
<pre><code># systemctl reload httpd
# curl -I localhost/index.html
HTTP/1.1 200 OK
Date: Thu, 10 Sep 2015 13:25:16 GMT
Server: Apache/2.4.16 (Fedora)
Last-Modified: Thu, 10 Sep 2015 13:19:47 GMT
ETag: &quot;15-51f6474064d50&quot;
Accept-Ranges: bytes
Content-Length: 21
Content-Type: text/html; charset=UTF-8
</code></pre><p>Success!</p>
<h2 id="long-term-fix">Long term fix</h2>
<p>The <code>chcon</code> method is good for fixing one-off issues and for testing, but we need a good long term fix. SELinux has some file contexts already configured for certain directories, but not for our custom web directory. You can examine the defaults here:</p>
<pre><code># semanage fcontext -l  | grep ^/var/www/html
/var/www/html(/.*)?/sites/default/files(/.*)?      all files          system_u:object_r:httpd_sys_rw_content_t:s0
/var/www/html(/.*)?/sites/default/settings\.php    regular file       system_u:object_r:httpd_sys_rw_content_t:s0
/var/www/html(/.*)?/uploads(/.*)?                  all files          system_u:object_r:httpd_sys_rw_content_t:s0
/var/www/html(/.*)?/wp-content(/.*)?               all files          system_u:object_r:httpd_sys_rw_content_t:s0
/var/www/html/[^/]*/cgi-bin(/.*)?                  all files          system_u:object_r:httpd_sys_script_exec_t:s0
/var/www/html/cgi/munin.*                          all files          system_u:object_r:munin_script_exec_t:s0
/var/www/html/configuration\.php                   all files          system_u:object_r:httpd_sys_rw_content_t:s0
/var/www/html/munin(/.*)?                          all files          system_u:object_r:munin_content_t:s0
/var/www/html/munin/cgi(/.*)?                      all files          system_u:object_r:munin_script_exec_t:s0
/var/www/html/owncloud/data(/.*)?                  all files          system_u:object_r:httpd_sys_rw_content_t:s0
</code></pre><p>SELinux&rsquo;s tools have a concept of <em>equivalency</em>. This allows you to say that one directory is <em>equivalent</em> to another one in the long term. We already used <code>chcon</code> to apply contexts with a reference to a directory with valid contexts, but this equivalency concept gives us a longer term fix. Here&rsquo;s the command to use:</p>
<pre><code>semanage fcontext --add --equal /var/www /web
</code></pre><p>If we break this down, we&rsquo;re saying we want to add a new file context where <code>/web</code> is equal to <code>/var/www</code>. This means we want the same SELinux contexts applied in the same places and want them treated equally. After running the <code>semanage</code> command, let&rsquo;s make an <code>index2.html</code> file to test:</p>
<pre><code> /web/index2.html
# curl -I localhost/index2.html
HTTP/1.1 200 OK
Date: Thu, 10 Sep 2015 13:35:24 GMT
Server: Apache/2.4.16 (Fedora)
Last-Modified: Thu, 10 Sep 2015 13:34:11 GMT
ETag: &quot;15-51f64a78266c8&quot;
Accept-Ranges: bytes
Content-Length: 21
Content-Type: text/html; charset=UTF-8
</code></pre><p>Great! We didn&rsquo;t have to use <code>chcon</code> this time around because we configured <code>/web</code> as an equivalent directory to <code>/var/www</code>. Let&rsquo;s double check the contexts:</p>
<pre><code># ls -alZ /web
total 16
drwxr-xr-x.  2 root root unconfined_u:object_r:httpd_sys_content_t:s0 4096 Sep 10 13:34 .
dr-xr-xr-x. 19 root root system_u:object_r:root_t:s0                  4096 Sep 10 13:33 ..
-rw-r--r--.  1 root root unconfined_u:object_r:httpd_sys_content_t:s0   21 Sep 10 13:34 index2.html
-rw-r--r--.  1 root root unconfined_u:object_r:httpd_sys_content_t:s0   21 Sep 10 13:33 index.html
</code></pre><p>Perfect! We now have all of the security benefits of SELinux in a completely custom web directory.</p>
]]></content></item><item><title>Impostor syndrome talk: FAQs and follow-ups</title><link>https://major.io/2015/09/02/impostor-syndrome-talk-faqs-and-follow-ups/</link><pubDate>Wed, 02 Sep 2015 15:34:34 +0000</pubDate><guid>https://major.io/2015/09/02/impostor-syndrome-talk-faqs-and-follow-ups/</guid><description>I&amp;rsquo;ve had a great time talking to people about my &amp;ldquo;Be an inspiration, not an impostor&amp;rdquo; talk that I delivered in August. I spoke to audiences at Fedora Flock 2015, Texas Linux Fest, and at Rackspace. The biggest lesson I learned is that delivering talks is exhausting!
Frequently Asked Questions Someone asked a good one at Fedora Flock:
How do you deal with situations where you are an impostor for a reason you can&amp;rsquo;t change?</description><content type="html"><![CDATA[<p>I&rsquo;ve had a great time talking to people about my <a href="/2015/08/14/fedora-flock-2015-keynote-slides/">&ldquo;Be an inspiration, not an impostor&rdquo;</a> talk that I delivered in August. I spoke to audiences at Fedora Flock 2015, Texas Linux Fest, and at Rackspace. The biggest lesson I learned is that delivering talks is exhausting!</p>
<h2 id="frequently-asked-questions">Frequently Asked Questions</h2>
<p>Someone asked a good one at Fedora Flock:</p>
<blockquote>
<p>How do you deal with situations where you are an impostor for a reason you can&rsquo;t change? For example, if you&rsquo;re the only woman in a male group or you&rsquo;re the youngest person in a mostly older group?</p>
</blockquote>
<p>I touched on this a bit in the presentation, but it&rsquo;s a great question. This is one of those times where you have to persevere and overcome the things you can&rsquo;t change by improving in all of the areas where you can change.</p>
<p>For example, if you&rsquo;re the youngest in the group, find ways to relate to the older group. Find out what they value and what they don&rsquo;t. If they prefer communication in person over electronic methods, change your communication style and medium. However, you shouldn&rsquo;t have to change your complete identity just for the rest of the group. Just make an adjustment so that you get the right response.</p>
<p>Also, impostor syndrome isn&rsquo;t restricted to a particular gender or age group. I&rsquo;ve seen it in both men and women in equal amounts, and I&rsquo;ve even seen it in people with 40 years of deep experience. It affects us all from time to time, and we need structured frameworks (like OODA) to fight it.</p>
<blockquote>
<p>How do I battle impostor syndrome without becoming cocky and overconfident?</p>
</blockquote>
<p>The opposite of impostor syndrome, often called the Dunning-Kruger Effect, is just as dangerous. Go back the observe and orient steps of the OODA loop (see the slides toward the end of the presentation) to be sure that you&rsquo;re getting good feedback from your peers and leaders. Back up your assertions with facts and solid reasoning to avoid cognitive bias. Bounce those ideas and assertions off the people you trust.</p>
<p>When I make an assertion or try to get someone else to change what they&rsquo;re doing, I&rsquo;ll often end with &ldquo;Am I off-base here?&rdquo; or &ldquo;Let me know if I&rsquo;m on the right track&rdquo; to give others an opportunity to provide criticism. The added benefit is that these phrases could drag someone with impostor syndrome out of the shadows and into the discussion.</p>
<p>That leads into another good question I received:</p>
<blockquote>
<p>How can we reduce impostor syndrome in open source communities as a whole?</p>
</blockquote>
<p>The key here is to find ways to get people involved, and then get them more involved over time. If someone is interested in participating but they aren&rsquo;t sure how to start, come up with ways they can get involved in less-formal ways. This could be through bug triaging, fixing simple bugs, writing documentation, or simply joining some IRC meetings. I&rsquo;ve seen several communities go through a process of tagging bugs with &ldquo;easy&rdquo; tags so that beginners can try to fix them.</p>
<p>Another more direct option is to call upon people to do certain things in the community and assign them a mentor to help them do it. If someone isn&rsquo;t talking during an IRC meeting or piping up on a mailing list, call them out - gently. It could be something as simple as: &ldquo;Hey, [name], we know you&rsquo;re knowledgeable in [topic]. Do you think this is a good idea?&rdquo; Do that a few times and you&rsquo;ll find their confidence to participate will rise quickly.</p>
<h2 id="follow-ups">Follow-ups</h2>
<h3 id="insides-vs-outsides">Insides vs. outsides</h3>
<p>Someone stopped me outside the talk room at Texas Linux Fest and said a leader at his church summarized impostor syndrome as &ldquo;comparing your insides to someone else&rsquo;s outsides&rdquo;. That led me to do some thinking.</p>
<p>Each and every one of us has strengths and weaknesses. I&rsquo;d wager that we all have at least once vice (I have plenty), and there are things about ourselves that we don&rsquo;t like. Everyone has insecurities about something in their life, whether it&rsquo;s personal or professional. These are things we can&rsquo;t see from looking at someone on the outside. We&rsquo;re taking our laundry list of issues and comparing it to something we think is close to perfection.</p>
<p>Don&rsquo;t do that. It&rsquo;s on my last slide in the presentation.</p>
<h3 id="you-know-at-least-one-thing-someone-else-wants-to-know">You know at least one thing someone else wants to know</h3>
<p>After doing the talk at Rackspace, I was pulled into quite a few hallway conversations and I received feedback about my presentation. In addition, many people talked about their desire to get up and do a talk, too. What I heard most often was: &ldquo;I want to do a talk, but I don&rsquo;t know what to talk about.&rdquo;</p>
<p>It reminds me of a <a href="/2012/03/30/why-technical-people-should-blog-but-dont/">post I wrote about writing technical blogs</a>. There is at least one thing you know that someone else wants to know. You might be surprised that the most hit post on my blog is an old one about <a href="https://major.io/2007/02/09/delete-single-iptables-rules/">deleting an iptables rule</a>. Deleting an iptables rule is an extremely basic step in system administration but it&rsquo;s tough to remember how to do it if you don&rsquo;t use the iptables syntax regularly.</p>
<p>Rackspace holds Tech Talk Tuesdays during lunch at our headquarters in San Antonio each week. It&rsquo;s open to Rackers and escorted guests only for now, but our topic list is wide open. Rackers have talked about highly technical topics and they&rsquo;ve also talked about how to brew beer. I&rsquo;ve encouraged my coworkers to think about something within their domain of expertise and deliver a talk on that topic.</p>
<h3 id="talk-about-your-qualifications-and-experience-without-bragging">Talk about your qualifications and experience without bragging</h3>
<p>You can be humble and talk about your strengths at the same time. They aren&rsquo;t mutually exclusive. It can be a challenge to bring these things up during social settings, especially job interviews. My strategy is to weave these aspects about myself into a story. Humans love stories.</p>
<p>As an example, if you&rsquo;re asked about your experience with Linux, tell a short story about a troubleshooting issue from your past and how you solved it. If you&rsquo;re asked about your python development experience, talk about a project you created or a hard problem you solved in someone else&rsquo;s project. Through the story, talk about your thought process when you were solving the problem. Try your best to keep it brief. These stories will keep the other people in the room interested and it won&rsquo;t come off as bragging.</p>
]]></content></item><item><title>Build a high performance KVM hypervisor on Rackspace’s OnMetal servers</title><link>https://major.io/2015/08/28/build-a-high-performance-kvm-hypervisor-on-rackspaces-onmetal-servers/</link><pubDate>Fri, 28 Aug 2015 14:00:16 +0000</pubDate><guid>https://major.io/2015/08/28/build-a-high-performance-kvm-hypervisor-on-rackspaces-onmetal-servers/</guid><description>I received some good feedback about my post on systemd-networkd and bonded interfaces on Rackspace&amp;rsquo;s OnMetal servers, and I decided to write about another use case. Recent product updates allow you to attach a Cloud Block Storage volume, and this opens up quite a few new possibilities for deployments.
So why not create a high-performance KVM hypervisor on an OnMetal server? Let&amp;rsquo;s do this.
Disclaimer WHOA THERE. These are amazing servers and because of that, they&amp;rsquo;re priced much differently than Cloud Servers are.</description><content type="html"><![CDATA[<p>I received some good feedback about <a href="/2015/08/21/using-systemd-networkd-with-bonding-on-rackspaces-onmetal-servers/">my post on systemd-networkd and bonded interfaces</a> on <a href="http://www.rackspace.com/en-us/cloud/servers/onmetal">Rackspace&rsquo;s OnMetal servers</a>, and I decided to write about another use case. Recent product updates allow you to <a href="http://www.rackspace.com/knowledge_center/article/attach-a-cloud-block-storage-volume-to-an-onmetal-server">attach a Cloud Block Storage volume</a>, and this opens up quite a few new possibilities for deployments.</p>
<p>So why not create a high-performance KVM hypervisor on an OnMetal server? Let&rsquo;s do this.</p>
<h2 id="disclaimer">Disclaimer</h2>
<p><strong><em>WHOA THERE.</em></strong> These are amazing servers and because of that, they&rsquo;re priced much differently than Cloud Servers are. Be sure to review the pricing for <a href="http://www.rackspace.com/en-us/cloud/servers/onmetal">OnMetal</a> and <a href="http://www.rackspace.com/cloud/block-storage">Cloud Block Storage</a> before going through this guide. <strong>Don&rsquo;t end up with an unexpected bill by building one of these servers and forgetting to destroy it.</strong></p>
<h2 id="building-the-server">Building the server</h2>
<p>We can build our server using command line tools. One of my tools, <a href="https://github.com/major/supernova">supernova</a>, makes this quite easy. My IAD environment is called <code>prodiad</code> and I can boot an OnMetal server like this:</p>
<pre><code>supernova prodiad boot \
  --flavor onmetal-memory1 \
  --image 4c361a4a-51b4-4e29-8a35-3b0e25e49ee1 \
  --key_name personal_servers \
  --poll \
  kvm-onmetal
</code></pre><p>In the command above, I&rsquo;ve built an OnMetal Memory server. I&rsquo;ll end up with some hardware like this:</p>
<ul>
<li>Dual Intel Xeon E5-2630 v2 2.6Ghz</li>
<li>12 cores total</li>
<li>512GB RAM</li>
<li>10Gbps connectivity</li>
<li>32GB disk</li>
</ul>
<p>Everything looks amazing except for the storage — but we&rsquo;ll fix that soon. I&rsquo;ve also built the server with Fedora 22 and provided my public ssh key.</p>
<p>Wait a few minutes after running the supernova command and you should be back to a prompt. Verify that your new OnMetal server is pinging, but keep in mind it may still be in the process of booting up or configuring itself.</p>
<h2 id="adding-storage">Adding storage</h2>
<p>Getting additional storage for an OnMetal server is done in two steps: provisioning the LUN and attaching it to the host. This is a bit easier in Cloud Servers since the actual attachment is done behind the scenes. You end up with a disk that attaches itself to the virtual machine at the hypervisor layer. OnMetal is a little different, but the process is still very straightforward.</p>
<p>Let&rsquo;s start by making four 100GB SSD volumes. We will eventually put these into a RAID 10 volume.</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#719e07">for</span> i in <span style="color:#586e75">`</span>seq <span style="color:#2aa198">1</span> 4<span style="color:#586e75">`</span>; <span style="color:#719e07">do</span>
    supernova prodiad volume-create <span style="color:#cb4b16">\
</span><span style="color:#cb4b16"></span>      --display-name onmetal-kvm-<span style="color:#2aa198">${</span><span style="color:#268bd2">i</span><span style="color:#2aa198">}</span> <span style="color:#cb4b16">\
</span><span style="color:#cb4b16"></span>      --volume-type SSD <span style="color:#cb4b16">\
</span><span style="color:#cb4b16"></span>      <span style="color:#2aa198">100</span>
<span style="color:#719e07">done</span>
</code></pre></div><p>We can list our new volumes:</p>
<pre><code>$ supernova prodiad volume-list
+--------------------------------------+-----------+---------------+------+-------------+-------------+
| ID                                   | Status    | Display Name  | Size | Volume Type | Attached to |
+--------------------------------------+-----------+---------------+------+-------------+-------------+
| 0beb1f81-eb04-4aca-9b14-c952f9eb81e2 | available | onmetal-kvm-4 | 100  | SSD         |             |
| 83b9d6d9-e7eb-4b53-9342-fa2fd3670bb4 | available | onmetal-kvm-3 | 100  | SSD         |             |
| a593cbbe-089f-4ede-81f4-003717b2309f | available | onmetal-kvm-2 | 100  | SSD         |             |
| 2c51e09f-d984-4de5-8852-c0f9c6176e00 | available | onmetal-kvm-1 | 100  | SSD         |             |
+--------------------------------------+-----------+---------------+------+-------------+-------------+
</code></pre><p>It&rsquo;s now time to attach our volumes to our OnMetal server. Let&rsquo;s get our OnMetal server&rsquo;s UUID:</p>
<pre><code>$ supernova prodiad list --name kvm-onmetal --minimal
[SUPERNOVA] Running nova against prodiad...
+--------------------------------------+-------------+
| ID                                   | Name        |
+--------------------------------------+-------------+
| 6a80d0b9-ce3e-4693-bedb-d843fea7cb0b | kvm-onmetal |
+--------------------------------------+-------------+
</code></pre><p>Now we&rsquo;re ready to attach the volumes:</p>
<pre><code>ONMETAL_UUID=6a80d0b9-ce3e-4693-bedb-d843fea7cb0b
supernova prodiad volume-attach $ONMETAL_UUID 2c51e09f-d984-4de5-8852-c0f9c6176e00
supernova prodiad volume-attach $ONMETAL_UUID a593cbbe-089f-4ede-81f4-003717b2309f
supernova prodiad volume-attach $ONMETAL_UUID 83b9d6d9-e7eb-4b53-9342-fa2fd3670bb4
supernova prodiad volume-attach $ONMETAL_UUID 0beb1f81-eb04-4aca-9b14-c952f9eb81e2
</code></pre><p>Let&rsquo;s log into the OnMetal server and get it ready. Install the <code>iscsi-initator-utils</code> package and set up the services:</p>
<pre><code>dnf -y install iscsi-initiator-utils
systemctl enable iscsid
systemctl start iscsid
</code></pre><p>Our iSCSI IQN data is in our OnMetal server&rsquo;s metadata. Grab your metadata JSON with this command:</p>
<pre><code>supernova prodiad show 6a80d0b9-ce3e-4693-bedb-d843fea7cb0b | grep metadata
</code></pre><p>If you copy/paste the JSON data into a file, you can use Python to make the JSON easier to read:</p>
<pre><code>cat iscsi_metadata.json | python -m json.tool
</code></pre><p>Start by putting your server&rsquo;s initiator name into a file. It should be called <code>initiator_name</code> in the JSON data.</p>
<pre><code>echo InitiatorName=iqn.2008-10.org.openstack:735f1804-bf47-4b28-b9fc-cbff3995635e &gt; /etc/iscsi/initiatorname.iscsi
</code></pre><p>Do the iSCSI logins for each `target_iqn` and `target_portal` in your JSON output. It should look something like this each time:</p>
<pre><code># iscsiadm -m discovery --type sendtargets --portal $TARGET_PORTAL
# iscsiadm -m node --targetname=$TARGET_IQN --portal $TARGET_PORTAL --login
</code></pre><p>When you&rsquo;re all done, you should have four new disks:</p>
<pre><code># ls /dev/disk/by-path/
ip-10.190.141.11:3260-iscsi-iqn.2010-11.com.rackspace:a593cbbe-089f-4ede-81f4-003717b2309f-lun-0
ip-10.190.141.44:3260-iscsi-iqn.2010-11.com.rackspace:0beb1f81-eb04-4aca-9b14-c952f9eb81e2-lun-0
ip-10.190.142.17:3260-iscsi-iqn.2010-11.com.rackspace:2c51e09f-d984-4de5-8852-c0f9c6176e00-lun-0
ip-10.190.143.103:3260-iscsi-iqn.2010-11.com.rackspace:83b9d6d9-e7eb-4b53-9342-fa2fd3670bb4-lun-0
</code></pre><h2 id="building-the-raid-volume">Building the RAID volume</h2>
<p>We can build the raid volume using the paths from above to prevent against device name changes later. Let&rsquo;s make a RAID 10 volume:</p>
<pre><code>dnf -y install mdadm
mdadm --create /dev/md0 --level=10 --raid-devices=4 /dev/disk/by-path/*
</code></pre><p>Check the status of the new RAID volume:</p>
<pre><code># cat /proc/mdstat
Personalities : [raid10]
md0 : active raid10 sdd[3] sdc[2] sdb[1] sde[0]
      209584128 blocks super 1.2 512K chunks 2 near-copies [4/4] [UUUU]
      [&gt;....................]  resync =  0.7% (1534400/209584128) finish=15.8min speed=219200K/sec
</code></pre><p>Come on, our storage volumes are faster than that. Let&rsquo;s speed it up a bit:</p>
<pre><code># sysctl -w dev.raid.speed_limit_max=99999999
# cat /proc/mdstat
Personalities : [raid10]
md0 : active raid10 sdd[3] sdc[2] sdb[1] sde[0]
      209584128 blocks super 1.2 512K chunks 2 near-copies [4/4] [UUUU]
      [====&gt;................]  resync = 21.1% (44229312/209584128) finish=2.9min speed=925564K/sec
</code></pre><p>That&rsquo;s more like it. Let&rsquo;s put a XFS filesystem on the volume and get it mounted:</p>
<pre><code>dnf -y install xfsprogs
mkfs.xfs /dev/md0
mkdir /mnt/raid
echo &quot;/dev/md0 /mnt/raid xfs defaults,noatime 0 1&quot; &gt;&gt; /etc/fstab
mount -a
</code></pre><h2 id="getting-kvm-going">Getting KVM going</h2>
<p>It&rsquo;s time to get packages updated and installed:</p>
<pre><code>dnf -y upgrade
dnf -y install libvirt libvirt-daemon* virt-install virt-manager xorg-x11-xauth gnome-icon-theme gnome-themes-standard dejavu*
systemctl start libvirtd
systemctl enable libvirtd
</code></pre><p>We can create a qcow volume and begin installing Fedora into a virtual machine:</p>
<pre><code>qemu-img create -f qcow2 /mnt/raid/fedora-kvm.qcow2 20G
virt-install --name=fedora-kvm --ram=16384 \
    --vcpus=4 --os-variant=fedora21 --accelerate \
    --hvm --network network=default \
    --disk /mnt/raid/fedora-kvm.qcow2 \
    --location http://iad.mirror.rackspace.com/fedora/releases/22/Server/x86_64/os/ \
    --noautoconsole --graphics vnc --autostart
</code></pre><p>Logout and then ssh to the server again, this time with <code>-Y</code> for X forwarding. Run <code>virt-manager</code> and verify that the VM is running.</p>
<p><img src="/wp-content/uploads/2015/08/virt-manager-listing.png" alt="6"></p>
<p>Double-click on the virtual machine listed there and the anaconda installer should be on the screen.</p>
<p><img src="/wp-content/uploads/2015/08/onmetal-kvm-vm.png" alt="installer_screenshot"></p>
<!-- raw HTML omitted -->
<p>Let the installation complete and you&rsquo;ll have a KVM virtual machine ready to roll!</p>
<h2 id="additional-thoughts">Additional thoughts</h2>
<p>Obviously, this is a very manual process. It could be automated with scripts, or an orchestration framework, like Ansible. In addition, deployment of virtual machines could be automated with OpenStack. However, my goal here was to demonstrate a new use case for OnMetal servers. I&rsquo;ll add the automation to my long list of to-do&rsquo;s.</p>
]]></content></item><item><title>Fedora 23 Alpha in boot.rackspace.com</title><link>https://major.io/2015/08/27/fedora-23-alpha-in-boot-rackspace-com/</link><pubDate>Thu, 27 Aug 2015 13:03:57 +0000</pubDate><guid>https://major.io/2015/08/27/fedora-23-alpha-in-boot-rackspace-com/</guid><description>Fedora 23&amp;rsquo;s Alpha release was announced earlier this month and work is underway for the beta release. The full list of dates for the Fedora 23 release is in the Fedora wiki.
If you&amp;rsquo;d like to try Fedora 23 Alpha a little sooner, check out boot.rackspace.com. I added support for Fedora 23 in the menus last night.
Quick start If you want to get underway quickly, simply download the boot.rackspace.com ISO and attach it to a virtual machine:</description><content type="html"><![CDATA[<p>Fedora 23&rsquo;s Alpha release <a href="https://lists.fedoraproject.org/pipermail/announce/2015-August/003284.html">was announced earlier this month</a> and work is underway for the beta release. The full list of dates for the Fedora 23 release is in the <a href="https://fedoraproject.org/wiki/Releases/23/Schedule">Fedora wiki</a>.</p>
<p>If you&rsquo;d like to try Fedora 23 Alpha a little sooner, check out <a href="http://bootrackspacecom.readthedocs.org/en/latest/">boot.rackspace.com</a>. I <a href="https://github.com/rackerlabs/boot.rackspace.com/pull/21">added support for Fedora 23</a> in the menus last night.</p>
<h2 id="quick-start">Quick start</h2>
<p>If you want to get underway quickly, simply download the boot.rackspace.com ISO and attach it to a virtual machine:</p>
<pre><code>wget http://boot.rackspace.com/ipxe/boot.rackspace.com-main.iso
</code></pre><p>When it boots, you&rsquo;ll be able to select Fedora 23&rsquo;s Alpha release from the menus. The Workstation, Atomic, and Server images are available.</p>
<p><a href="/wp-content/uploads/2015/08/f23_alpha_bootrackspace.png"><!-- raw HTML omitted --></a></p>
<p>Enjoy!</p>
]]></content></item><item><title>Build a network router and firewall with Fedora 22 and systemd-networkd</title><link>https://major.io/2015/08/27/build-a-network-router-and-firewall-with-fedora-22-and-systemd-networkd/</link><pubDate>Thu, 27 Aug 2015 12:38:43 +0000</pubDate><guid>https://major.io/2015/08/27/build-a-network-router-and-firewall-with-fedora-22-and-systemd-networkd/</guid><description>This post originally appeared on the Fedora Magazine blog.
One of my favorite features of Fedora 22 is systemd-networkd and all of the new features that came with it in recent systemd versions. The configuration files are easy to read, bridging is simple, and tunnels are resilient.
I&amp;rsquo;ve recently started using a small Linux server at home again as a network router and firewall. However, I used systemd-networkd this time and had some great results.</description><content type="html"><![CDATA[<p><em>This post <a href="http://fedoramagazine.org/build-network-router-firewall-fedora-22-systemd-networkd/">originally appeared</a> on the Fedora Magazine blog.</em></p>
<hr>
<p>One of my favorite features of Fedora 22 is <a href="http://www.freedesktop.org/software/systemd/man/systemd-networkd.html">systemd-networkd</a> and all of the new features that came with it in recent systemd versions. The configuration files are easy to read, bridging is simple, and tunnels are resilient.</p>
<p>I&rsquo;ve recently started using a small Linux server at home again as a network router and firewall. However, I used systemd-networkd this time and had some great results. Let&rsquo;s get started!</p>
<h2 id="overview">Overview</h2>
<p>Our example router in this example has two network interfaces:</p>
<ul>
<li>eth0: public internet connectivity</li>
<li>eth1: private LAN (192.168.3.1/24)</li>
</ul>
<p>We want machines on the private LAN to route their traffic through the router to the public internet via NAT. Also, we want clients on the LAN to get their IP addresses assigned automatically.</p>
<h2 id="network-configuration">Network configuration</h2>
<p>All of the systemd-networkd configuration files live within <code>/etc/systemd/network</code> and we need to create that directory:</p>
<pre><code>mkdir /etc/systemd/network
</code></pre><p>We need to write a network configuration file for our public interface that systemd-networkd can read. Open up <code>/etc/systemd/network/eth0.network</code> and write these lines:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ini" data-lang="ini"><span style="color:#719e07">[Match]</span>
Name<span style="color:#719e07">=</span><span style="color:#2aa198">eth0</span>

<span style="color:#719e07">[Network]</span>
Address<span style="color:#719e07">=</span><span style="color:#2aa198">PUBLIC_IP_ADDRESS/CIDR</span>
Gateway<span style="color:#719e07">=</span><span style="color:#2aa198">GATEWAY</span>
DNS<span style="color:#719e07">=</span><span style="color:#2aa198">8.8.8.8</span>
DNS<span style="color:#719e07">=</span><span style="color:#2aa198">8.8.4.4</span>
IPForward<span style="color:#719e07">=</span><span style="color:#2aa198">yes</span>
</code></pre></div><p>If we break this configuration file down, we&rsquo;re telling systemd-networkd to apply this configuration to any devices that are called <code>eth0</code>. Also, we&rsquo;re specifying a public IP address and CIDR mask (like /24 or /22) so that the interface can be configured. The gateway address will be added to the routing table. We&rsquo;ve also provided DNS servers to use with systemd-resolved (more on that later).</p>
<p>I added <code>IPForward=yes</code> so that systemd-networkd will automatically enable forwarding for the interface via sysctl. (That always seems to be the step I forget when I build a Linux router.)</p>
<p>Let&rsquo;s do the same for our LAN interface. Create this configuration file and store it as <code>/etc/systemd/network/eth1.network</code>:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ini" data-lang="ini"><span style="color:#719e07">[Match]</span>
Name<span style="color:#719e07">=</span><span style="color:#2aa198">eth1</span>

<span style="color:#719e07">[Network]</span>
Address<span style="color:#719e07">=</span><span style="color:#2aa198">192.168.3.1/24</span>
IPForward<span style="color:#719e07">=</span><span style="color:#2aa198">yes</span>
</code></pre></div><p>We don&rsquo;t need to specify a gateway address here because this interface will be the gateway for the LAN.</p>
<h2 id="prepare-the-services">Prepare the services</h2>
<p>If we&rsquo;re planning to use systemd-networkd, we need to ensure that it runs instead of traditional network scripts or NetworkManager:</p>
<pre><code>systemctl disable network
systemctl disable NetworkManager
systemctl enable systemd-networkd
</code></pre><p>Also, let&rsquo;s be sure to use systemd-resolved to handle our <code>/etc/resolv.conf</code>:</p>
<pre><code>systemctl enable systemd-resolved
systemctl start systemd-resolved
rm -f /etc/resolv.conf
ln -s /run/systemd/resolve/resolv.conf /etc/resolv.conf
</code></pre><h2 id="reboot">Reboot</h2>
<p>We&rsquo;re now set to reboot! It&rsquo;s possible to bring up systemd-networkd without rebooting but I&rsquo;d rather verify with a reboot now than get goosed with a broken network after a reboot later.</p>
<p>Once your router is back up, run <code>networkctl</code> and verify that you have <em>routable</em> in the output for both interfaces:</p>
<pre><code>[root@router ~]# networkctl
IDX LINK             TYPE               OPERATIONAL SETUP
  1 lo               loopback           carrier     unmanaged
  2 eth0             ether              routable    configured
  3 eth1             ether              routable    configured
</code></pre><h2 id="dhcp">DHCP</h2>
<p>Now that both network interfaces are online, we need something to tell our clients about the IP configuration they should be using. There are plenty of good options here, but I prefer <a href="http://www.thekelleys.org.uk/dnsmasq/doc.html">dnsmasq</a>. It has served me well over the years and it provides some handy features along with DHCP, such as DNS caching, TFTP and IPv6 router announcements.</p>
<p>Let&rsquo;s install dnsmasq and enable it at boot:</p>
<pre><code>dnf -y install dnsmasq
systemctl enable dnsmasq
</code></pre><p>Open <code>/etc/dnsmasq.conf</code> in your favorite text editor and edit a few lines:</p>
<ul>
<li>Uncomment <code>dhcp-authoritative</code></li>
<li>This tells dnsmasq that it&rsquo;s the exclusive DHCP server on the network and that it should answer all requests</li>
<li>Uncomment <code>interface=</code> and add <code>eth1</code> on the end (should look like <code>interface=eth1</code> when you&rsquo;re done)</li>
<li>Most ISP&rsquo;s filter DHCP replies on their public networks, but we don&rsquo;t want to take chances here. We need to restrict DHCP to our public interface only.</li>
<li>Look for the <code>dhcp-range</code> line and change it to <code>dhcp-range=192.168.3.50,192.168.3.150,12h</code></li>
<li>We&rsquo;re giving clients 12 hour leases on 192.168.3.0/24</li>
</ul>
<p>Save the file and start dnsmasq:</p>
<pre><code>systemctl start dnsmasq
</code></pre><h2 id="firewall">Firewall</h2>
<p>We&rsquo;re almost done! Now it&rsquo;s time to tell iptables to <a href="https://en.wikipedia.org/wiki/Network_address_translation">masquerade</a> any packets from our LAN to the internet. But wait, it&rsquo;s 2015 and we have tools like <code>firewall-cmd</code> to do that for us in Fedora.</p>
<p>Let&rsquo;s enable masquerading, allow DNS, and allow DHCP traffic. We can then make the state permanent:</p>
<pre><code>firewall-cmd --add-masquerade
firewall-cmd --add-service=dns --add-service=dhcp
firewall-cmd --runtime-to-permanent
</code></pre><h2 id="testing">Testing</h2>
<p>Put a client machine on your LAN network and you should be able to ping some public sites from the client:</p>
<pre><code>[root@client ~]# ping -c 4 icanhazip.com
PING icanhazip.com (104.238.141.75) 56(84) bytes of data.
64 bytes from lax.icanhazip.com (104.238.141.75): icmp_seq=1 ttl=52 time=69.8 ms
64 bytes from lax.icanhazip.com (104.238.141.75): icmp_seq=2 ttl=52 time=69.7 ms
64 bytes from lax.icanhazip.com (104.238.141.75): icmp_seq=3 ttl=52 time=69.6 ms
64 bytes from lax.icanhazip.com (104.238.141.75): icmp_seq=4 ttl=52 time=69.7 ms

--- icanhazip.com ping statistics ---
4 packets transmitted, 4 received, 0% packet loss, time 3005ms
rtt min/avg/max/mdev = 69.659/69.758/69.874/0.203 ms
</code></pre><h2 id="extras">Extras</h2>
<p>If you need to adjust your network configuration, just run <code>systemctl restart systemd-networkd</code> afterwards. I&rsquo;ve found that it&rsquo;s quite intelligent about the network devices and it won&rsquo;t reconfigure anything that hasn&rsquo;t changed.</p>
<p>The <code>networkctl</code> command is very powerful. Check out the <code>status</code> and <code>lldp</code> functions to get more information about your network devices and the networks they&rsquo;re connected to.</p>
<p>When something goes wrong, look in your systemd journal:</p>
<pre><code>[root@router ~]# journalctl -u systemd-networkd
-- Logs begin at Fri 2015-07-31 01:22:38 UTC, end at Fri 2015-07-31 02:11:24 UTC. --
Jul 31 01:46:14 router systemd[1]: Starting Network Service...
Jul 31 01:46:14 router systemd-networkd[286]: Enumeration completed
Jul 31 01:46:14 router systemd[1]: Started Network Service.
Jul 31 01:46:15 router systemd-networkd[286]: eth1            : link configured
Jul 31 01:46:15 router systemd-networkd[286]: eth0            : gained carrier
Jul 31 01:46:15 router systemd-networkd[286]: eth0            : link configured
Jul 31 01:46:16 router systemd-networkd[286]: eth1            : gained carrier
</code></pre>]]></content></item><item><title>Slides from my Texas Linux Fest 2015 talk</title><link>https://major.io/2015/08/22/slides-from-my-texas-linux-fest-2015-talk/</link><pubDate>Sat, 22 Aug 2015 19:42:52 +0000</pubDate><guid>https://major.io/2015/08/22/slides-from-my-texas-linux-fest-2015-talk/</guid><description>Thanks to all of the people who attended my &amp;ldquo;Be an inspiration, not an impostor&amp;rdquo; talk at Texas Linux Fest 2015. Some A/V issues caused my time slot to get squeezed and the audience had to put up with the &amp;ldquo;ludicrous speed&amp;rdquo; version of the presentation.
The slides are a little different from the slides at Fedora Flock, but they&amp;rsquo;re mainly the same:</description><content type="html"><![CDATA[<p>Thanks to all of the people who attended my &ldquo;Be an inspiration, not an impostor&rdquo; talk at Texas Linux Fest 2015. Some A/V issues caused my time slot to get squeezed and the audience had to put up with the &ldquo;ludicrous speed&rdquo; version of the presentation.</p>
<p>The slides are a little different from the slides at <a href="https://major.io/2015/08/14/fedora-flock-2015-keynote-slides/">Fedora Flock</a>, but they&rsquo;re mainly the same:</p>
<!-- raw HTML omitted -->
]]></content></item><item><title>Understanding systemd’s predictable network device names</title><link>https://major.io/2015/08/21/understanding-systemds-predictable-network-device-names/</link><pubDate>Fri, 21 Aug 2015 21:15:36 +0000</pubDate><guid>https://major.io/2015/08/21/understanding-systemds-predictable-network-device-names/</guid><description>I talked a bit about systemd&amp;rsquo;s network device name in my earlier post about systemd-networkd and bonding and I received some questions about how systemd rolls through the possible names of network devices to choose the final name. These predictable network device names threw me a curveball last summer when I couldn&amp;rsquo;t figure out how the names were constructed.
Let&amp;rsquo;s walk through this process.
What&amp;rsquo;s in a name? Back in the systemd-networkd bonding post, I dug into a dual port Intel network card that showed up in a hotplug slot:</description><content type="html"><![CDATA[<p><img src="/wp-content/uploads/2015/08/2229782090_838eaa8574_o-e1440191509854.jpg" alt="1"></p>
<p>I talked a bit about systemd&rsquo;s network device name in my earlier post about <a href="/2015/08/21/using-systemd-networkd-with-bonding-on-rackspaces-onmetal-servers/">systemd-networkd and bonding</a> and I received some questions about how systemd rolls through the possible names of network devices to choose the final name. These predictable network device names <a href="/2014/08/06/unexpected-predictable-network-naming-systemd/">threw me a curveball last summer</a> when I couldn&rsquo;t figure out how the names were constructed.</p>
<p>Let&rsquo;s walk through this process.</p>
<h2 id="whats-in-a-name">What&rsquo;s in a name?</h2>
<p>Back in the systemd-networkd bonding post, I dug into a dual port Intel network card that showed up in a hotplug slot:</p>
<pre><code># udevadm info -e | grep -A 9 ^P.*eth0
P: /devices/pci0000:00/0000:00:03.2/0000:08:00.0/net/eth0
E: DEVPATH=/devices/pci0000:00/0000:00:03.2/0000:08:00.0/net/eth0
E: ID_BUS=pci
E: ID_MODEL_FROM_DATABASE=82599ES 10-Gigabit SFI/SFP+ Network Connection (Ethernet OCP Server Adapter X520-2)
E: ID_MODEL_ID=0x10fb
E: ID_NET_DRIVER=ixgbe
E: ID_NET_LINK_FILE=/usr/lib/systemd/network/99-default.link
E: ID_NET_NAME_MAC=enxa0369f2cec90
E: ID_NET_NAME_PATH=enp8s0f0
E: ID_NET_NAME_SLOT=ens9f0
</code></pre><p>This udev database dump shows that it came up with a few different names for the network interface:</p>
<ul>
<li><code>ID_NET_NAME_MAC=enxa0369f2cec90</code></li>
<li><code>ID_NET_NAME_PATH=enp8s0f0</code></li>
<li><code>ID_NET_NAME_SLOT=ens9f0</code></li>
</ul>
<p>Where do these names come from? We can dig into systemd&rsquo;s source code to figure out the origin of the names and which one is selected as the final choice.</p>
<h2 id="down-the-udev-rabbit-hole">Down the udev rabbit hole</h2>
<p>Let&rsquo;s take a look at <a href="https://github.com/systemd/systemd/blob/master/src/udev/udev-builtin-net_id.c">src/udev/udev-builtin-net_id.c</a>:</p>
<pre><code>/*
 * Predictable network interface device names based on:
 *  - firmware/bios-provided index numbers for on-board devices
 *  - firmware-provided pci-express hotplug slot index number
 *  - physical/geographical location of the hardware
 *  - the interface's MAC address
 *
 * http://www.freedesktop.org/wiki/Software/systemd/PredictableNetworkInterfaceNames
 *
 * Two character prefixes based on the type of interface:
 *   en -- ethernet
 *   sl -- serial line IP (slip)
 *   wl -- wlan
 *   ww -- wwan
 *
 * Type of names:
 *   b&lt;number&gt;                             -- BCMA bus core number
 *   ccw&lt;name&gt;                             -- CCW bus group name
 *   o&lt;index&gt;[d&lt;dev_port&gt;]                 -- on-board device index number
 *   s&lt;slot&gt;[f&lt;function&gt;][d&lt;dev_port&gt;]     -- hotplug slot index number
 *   x&lt;MAC&gt;                                -- MAC address
 *   [P&lt;domain&gt;]p&lt;bus&gt;s&lt;slot&gt;[f&lt;function&gt;][d&lt;dev_port&gt;]
 *                                         -- PCI geographical location
 *   [P&lt;domain&gt;]p&lt;bus&gt;s&lt;slot&gt;[f&lt;function&gt;][u&lt;port&gt;][..][c&lt;config&gt;][i&lt;interface&gt;]
 *                                         -- USB port number chain
</code></pre><p>So here&rsquo;s where our names actually begin. Ethernet cards will always start with <em>en</em>, but they might be followed by a <em>p</em> (for PCI slots), a <em>s</em> (for hotplug PCI-E slots), and <em>o</em> (for onboard cards). Scroll down just a bit more for some examples starting at line 56.</p>
<h2 id="real-world-examples">Real-world examples</h2>
<p>We already looked at the hotplug slot naming from Rackspace&rsquo;s OnMetal servers. They show up as <em>ens9f0</em> and <em>ens9f1</em>. That means they&rsquo;re on a hotplug slot which happens to be slot 9. The function indexes are 0 and 1 (for both ports on the Intel 82599ES).</p>
<h3 id="linux-firewall-with-a-dual-port-pci-card">Linux firewall with a dual-port PCI card</h3>
<p>Here&rsquo;s an example of my Linux firewall at home. It&rsquo;s a Dell Optiplex 3020 with an Intel I350-T2 (dual port):</p>
<pre><code># udevadm info -e | grep -A 10 ^P.*enp1s0f1
P: /devices/pci0000:00/0000:00:01.0/0000:01:00.1/net/enp1s0f1
E: DEVPATH=/devices/pci0000:00/0000:00:01.0/0000:01:00.1/net/enp1s0f1
E: ID_BUS=pci
E: ID_MODEL_FROM_DATABASE=I350 Gigabit Network Connection (Ethernet Server Adapter I350-T2)
E: ID_MODEL_ID=0x1521
E: ID_NET_DRIVER=igb
E: ID_NET_LINK_FILE=/usr/lib/systemd/network/99-default.link
E: ID_NET_NAME=enp1s0f1
E: ID_NET_NAME_MAC=enxa0369f6e5227
E: ID_NET_NAME_PATH=enp1s0f1
E: ID_OUI_FROM_DATABASE=Intel Corporate
</code></pre><p>And the output from <code>lspci</code>:</p>
<pre><code># lspci -s 01:00
01:00.0 Ethernet controller: Intel Corporation I350 Gigabit Network Connection (rev 01)
01:00.1 Ethernet controller: Intel Corporation I350 Gigabit Network Connection (rev 01)
</code></pre><p>This card happens to sit on PCI bus 1 (enp1), slot 0 (s0). Since it&rsquo;s a dual-port card, it has two function indexes (f0 and f1). That leaves me with two predictable names: <em>enp1s0f1</em> and <em>enp1s0f0</em>.</p>
<h3 id="1u-server-with-four-ethernet-ports">1U server with four ethernet ports</h3>
<p>Let&rsquo;s grab another example. Here&rsquo;s a SuperMicro 1U X9SCA server with four onboard PCI ethernet cards:</p>
<pre><code># udevadm info -e | grep -A 10 ^P.*enp2s0
P: /devices/pci0000:00/0000:00:1c.4/0000:02:00.0/net/enp2s0
E: DEVPATH=/devices/pci0000:00/0000:00:1c.4/0000:02:00.0/net/enp2s0
E: ID_BUS=pci
E: ID_MODEL_FROM_DATABASE=82574L Gigabit Network Connection
E: ID_MODEL_ID=0x10d3
E: ID_NET_DRIVER=e1000e
E: ID_NET_LINK_FILE=/usr/lib/systemd/network/99-default.link
E: ID_NET_NAME=enp2s0
E: ID_NET_NAME_MAC=enx00259025963a
E: ID_NET_NAME_PATH=enp2s0
E: ID_OUI_FROM_DATABASE=Super Micro Computer, Inc.
</code></pre><p>And here&rsquo;s all four ports in <code>lspci</code>:</p>
<pre><code># for i in `seq 2 5`; do lspci -s 0${i}:; done
02:00.0 Ethernet controller: Intel Corporation 82574L Gigabit Network Connection
03:00.0 Ethernet controller: Intel Corporation 82574L Gigabit Network Connection
04:00.0 Ethernet controller: Intel Corporation 82574L Gigabit Network Connection
05:00.0 Ethernet controller: Intel Corporation 82574L Gigabit Network Connection
</code></pre><p>These are interesting because they&rsquo;re not all on the same PCI bus. They sit on buses 2-5 in slot 0. There are no function indexes here, so they&rsquo;re named <em>enp2s0</em> through <em>enp5s0</em>. These aren&rsquo;t true <em>onboard</em> cards, so they&rsquo;re named based on their locations.</p>
<h3 id="storage-server-with-onboard-ethernet">Storage server with onboard ethernet</h3>
<p>Here&rsquo;s an example of a server with a true inboard ethernet card:</p>
<pre><code>$ udevadm info -e | grep -A 11 ^P.*eno1
P: /devices/pci0000:00/0000:00:19.0/net/eno1
E: DEVPATH=/devices/pci0000:00/0000:00:19.0/net/eno1
E: ID_BUS=pci
E: ID_MODEL_FROM_DATABASE=Ethernet Connection I217-V
E: ID_MODEL_ID=0x153b
E: ID_NET_DRIVER=e1000e
E: ID_NET_LABEL_ONBOARD=en Onboard LAN
E: ID_NET_LINK_FILE=/usr/lib/systemd/network/99-default.link
E: ID_NET_NAME_MAC=enxe03f49b159c0
E: ID_NET_NAME_ONBOARD=eno1
E: ID_NET_NAME_PATH=enp0s25
E: ID_OUI_FROM_DATABASE=ASUSTek COMPUTER INC.
</code></pre><p>And the <code>lspci</code> output:</p>
<pre><code>$ lspci -s 00:19.0
00:19.0 Ethernet controller: Intel Corporation Ethernet Connection I217-V (rev 05)
</code></pre><p>This card has a new name showing up in udev: <code>ID_NET_NAME_ONBOARD</code>. The systemd udev code has some special handling for onboard cards because they usually sit on the main bus. The naming can get a bit ugly because that 19 would need to be converted into hex for the name.</p>
<p>If systemd didn&rsquo;t handle onboard cards differently, this card might be named something ugly like enp0s13 (since 19 in decimal becomes 13 in hex). That&rsquo;s really confusing.</p>
<h2 id="picking-the-final-name">Picking the final name</h2>
<p>As we&rsquo;ve seen above, udev makes a big list of names in the udev database. However, there can only be one name in the OS when you try to use the network card.</p>
<p>Let&rsquo;s wander back into the code. this time we&rsquo;re going to take a look in <a href="https://github.com/systemd/systemd/blob/master/src/udev/net/link-config.c#L403">src/udev/net/link-config.c</a> starting at around line 403:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-c" data-lang="c">name_policy) {
        NamePolicy <span style="color:#719e07">*</span>policy;

        <span style="color:#719e07">for</span> (policy <span style="color:#719e07">=</span> config<span style="color:#719e07">-&gt;</span>name_policy;
             <span style="color:#719e07">!</span>new_name <span style="color:#719e07">&amp;&amp;</span> <span style="color:#719e07">*</span>policy <span style="color:#719e07">!=</span> _NAMEPOLICY_INVALID; policy<span style="color:#719e07">++</span>) {
                <span style="color:#719e07">switch</span> (<span style="color:#719e07">*</span>policy) {
                        <span style="color:#719e07">case</span> NAMEPOLICY_KERNEL:
                                respect_predictable <span style="color:#719e07">=</span> <span style="color:#b58900">true</span>;
                                <span style="color:#719e07">break</span>;
                        <span style="color:#719e07">case</span> NAMEPOLICY_DATABASE:
                                new_name <span style="color:#719e07">=</span> udev_device_get_property_value(device, <span style="color:#2aa198">&#34;ID_NET_NAME_FROM_DATABASE&#34;</span>);
                                <span style="color:#719e07">break</span>;
                        <span style="color:#719e07">case</span> NAMEPOLICY_ONBOARD:
                                new_name <span style="color:#719e07">=</span> udev_device_get_property_value(device, <span style="color:#2aa198">&#34;ID_NET_NAME_ONBOARD&#34;</span>);
                                <span style="color:#719e07">break</span>;
                        <span style="color:#719e07">case</span> NAMEPOLICY_SLOT:
                                new_name <span style="color:#719e07">=</span> udev_device_get_property_value(device, <span style="color:#2aa198">&#34;ID_NET_NAME_SLOT&#34;</span>);
                                <span style="color:#719e07">break</span>;
                        <span style="color:#719e07">case</span> NAMEPOLICY_PATH:
                                new_name <span style="color:#719e07">=</span> udev_device_get_property_value(device, <span style="color:#2aa198">&#34;ID_NET_NAME_PATH&#34;</span>);
                                <span style="color:#719e07">break</span>;
                        <span style="color:#719e07">case</span> NAMEPOLICY_MAC:
                                new_name <span style="color:#719e07">=</span> udev_device_get_property_value(device, <span style="color:#2aa198">&#34;ID_NET_NAME_MAC&#34;</span>);
                                <span style="color:#719e07">break</span>;
                        <span style="color:#719e07">default</span><span style="color:#719e07">:</span>
                                <span style="color:#719e07">break</span>;
                }
        }
}
</code></pre></div><p>If we look at the overall case statement, you can see that the first match is the one that takes precedence. Working from top to bottom, udev takes the first match of:</p>
<ul>
<li><code>ID_NET_NAME_FROM_DATABASE</code></li>
<li><code>ID_NET_NAME_ONBOARD</code></li>
<li><code>ID_NET_NAME_SLOT</code></li>
<li><code>ID_NET_NAME_PATH</code></li>
<li><code>ID_NET_NAME_MAC</code></li>
</ul>
<p>If we go back to our OnMetal example way at the top of the post, we can follow the logic. The udev database contained the following:</p>
<pre><code>E: ID_NET_NAME_MAC=enxa0369f2cec90
E: ID_NET_NAME_PATH=enp8s0f0
E: ID_NET_NAME_SLOT=ens9f0
</code></pre><p>The udev daemon would start with <code>ID_NET_NAME_FROM_DATABASE</code>, but that doesn&rsquo;t exist for this card. Next, it would move to <code>ID_NET_NAME_ONBOARD</code>, but that&rsquo;s not present. Next comes <code>ID_NET_NAME_SLOT</code>, and we have a match! The <code>ID_NET_NAME_SLOT</code> entry has <em>ens9f0</em> and that&rsquo;s the final name for the network device.</p>
<p>This loop also handles some special cases. The first check is to see if someone requested for udev to not use predictable naming. We saw this in the <a href="/2015/08/21/using-systemd-networkd-with-bonding-on-rackspaces-onmetal-servers/">systemd-networkd bonding post</a> when the bootloader configuration contained <em>net.ifnames=0</em>. If that kernel command line parameter is present, predictable naming logic is skipped.</p>
<p>Another special case is <code>ID_NET_NAME_FROM_DATABASE</code>. Those ports come from udev&rsquo;s internal <a href="https://github.com/systemd/systemd/blob/master/hwdb/20-net-ifname.hwdb">hardware database</a>. That file only has one item at the moment and it&rsquo;s for a particular Dell iDRAC network interface.</p>
<h2 id="perplexed-by-hex">Perplexed by hex</h2>
<p>If the PCI slot numbers don&rsquo;t seem to line up, be sure to <a href="/2014/08/06/unexpected-predictable-network-naming-systemd/">read my post from last summer</a>. I ran into a peculiar Dell server with a dual port Intel card on PCI bus 42. The interface ended up with a name of <em>enp66s0f0</em> and I was stumped.</p>
<p>The name <em>enp66s0f0</em> seems to say that we have a card on PCI bus 66, in slot 0, with multiple function index numbers (for multiple ports). However, systemd does a conversion of PCI slot numbers into hex. That means that decimal 66 becomes 42 in hex.</p>
<p>Most servers won&rsquo;t be this complicated, but it&rsquo;s key to remember the hex conversion.</p>
<h2 id="feedback">Feedback</h2>
<p>Are these systemd-related posts interesting? Let me know. I&rsquo;m a huge fan of systemd and I enjoy writing about it.</p>
<p><em>Photo credit: <a href="https://www.flickr.com/photos/mlibrary/2229782090">University of Michigan Library</a></em></p>
]]></content></item><item><title>Using systemd-networkd with bonding on Rackspace’s OnMetal servers</title><link>https://major.io/2015/08/21/using-systemd-networkd-with-bonding-on-rackspaces-onmetal-servers/</link><pubDate>Fri, 21 Aug 2015 14:00:46 +0000</pubDate><guid>https://major.io/2015/08/21/using-systemd-networkd-with-bonding-on-rackspaces-onmetal-servers/</guid><description>I&amp;rsquo;ve written about systemd-networkd in the past and how easy it can be to set up new network devices and tunnels. However, the documentation on systemd-networkd with bonding is a bit lacking (but I have a pull request pending for that).
Rackspace&amp;rsquo;s OnMetal Servers are a good place to test since they have bonded networks configured by default. They&amp;rsquo;re also quite fast and always fun for experiments.
To get started, head on over to the Rackspace Cloud control panel and build a compute-1 OnMetal server and choose Fedora 22 as your operating system.</description><content type="html"><![CDATA[<p><img src="/wp-content/uploads/2015/08/OnMetal_Graphic.png" alt="1"></p>
<p>I&rsquo;ve written about <a href="/2015/03/26/creating-a-bridge-for-virtual-machines-using-systemd-networkd/">systemd-networkd</a> in the past and how easy it can be to set up new network devices and tunnels. However, the documentation on systemd-networkd with bonding is a bit lacking (but I have a <a href="https://github.com/systemd/systemd/pull/1001">pull request pending</a> for that).</p>
<p><a href="http://www.rackspace.com/en-us/cloud/servers/onmetal">Rackspace&rsquo;s OnMetal Servers</a> are a good place to test since they have bonded networks configured by default. They&rsquo;re also quite fast and always fun for experiments.</p>
<p>To get started, head on over to the <a href="https://mycloud.rackspace.com/">Rackspace Cloud control panel</a> and build a <em>compute-1</em> OnMetal server and choose Fedora 22 as your operating system. Once it starts pinging and you&rsquo;re able to log in, start following the guide below.</p>
<h2 id="network-device-naming">Network device naming</h2>
<p>By default, most images come with <a href="http://www.freedesktop.org/wiki/Software/systemd/PredictableNetworkInterfaceNames/">systemd&rsquo;s predictable network naming</a> disabled. You can see the kernel command line adjustments here:</p>
<pre><code># cat /boot/extlinux.conf
TIMEOUT 1
default linux

LABEL Fedora (4.1.5-200.fc22.x86_64) 22 (Twenty Two)
      KERNEL /boot/vmlinuz-4.1.5-200.fc22.x86_64
      APPEND root=/dev/sda1 console=ttyS4,115200n8 8250.nr_uarts=5 modprobe.blacklist=mei_me net.ifnames=0 biosdevname=0 LANG=en_US.UTF-8
      initrd /boot/initramfs-4.1.5-200.fc22.x86_64.img
</code></pre><p>This ensures that both network devices show up as <em>eth0</em> and <em>eth1</em>. Although it isn&rsquo;t my favorite way to configure a server, it does make it easier for most customers to get up an running quickly with some device names that they are familiar with from virtualized products.</p>
<p>We need to figure out what systemd plans to call these interfaces when we allow udev to name them predictably. The easiest method for figuring out what udev wants to call these devices is to dump the udev database and use <code>grep</code>:</p>
<pre><code># udevadm info -e | grep -A 9 ^P.*eth0
P: /devices/pci0000:00/0000:00:03.2/0000:08:00.0/net/eth0
E: DEVPATH=/devices/pci0000:00/0000:00:03.2/0000:08:00.0/net/eth0
E: ID_BUS=pci
E: ID_MODEL_FROM_DATABASE=82599ES 10-Gigabit SFI/SFP+ Network Connection (Ethernet OCP Server Adapter X520-2)
E: ID_MODEL_ID=0x10fb
E: ID_NET_DRIVER=ixgbe
E: ID_NET_LINK_FILE=/usr/lib/systemd/network/99-default.link
E: ID_NET_NAME_MAC=enxa0369f2cec90
E: ID_NET_NAME_PATH=enp8s0f0
E: ID_NET_NAME_SLOT=ens9f0
</code></pre><p>Look for those lines that contain <code>ID_NET_NAME_*</code>. Those tell us what udev prefers to call these network devices. The last name you see in the list is what the interface will be called. Here&rsquo;s what you need to look for in that output:</p>
<pre><code>E: ID_NET_NAME_MAC=enxa0369f2cec90
E: ID_NET_NAME_PATH=enp8s0f0
E: ID_NET_NAME_SLOT=ens9f0
</code></pre><p>We can see that this device is in slot 0 of PCI bus 8. However, since udev is able to dig in a bit further, it decides to name the device <em>ens9f0</em>, which means:</p>
<ul>
<li>Hotplug slot 9</li>
<li>Function index number 0</li>
</ul>
<p>Udev rolls through a list of possible network names and uses the very last one as the name of the network device. Gentoo&rsquo;s documentation has a <a href="https://wiki.gentoo.org/wiki/Udev/Upgrade_Guide#Example_interface_IDs">nice explanation</a>. In our case, <code>ID_NET_NAME_SLOT</code> took precedence over the others since this particular device sits in a hotplug PCI-Express slot.</p>
<p>We can find the slot number here:</p>
<pre><code># lspci -v -s 08:00.00 | head -n 3
08:00.0 Ethernet controller: Intel Corporation 82599ES 10-Gigabit SFI/SFP+ Network Connection (rev 01)
    Subsystem: Intel Corporation Ethernet OCP Server Adapter X520-2
    Physical Slot: 9
</code></pre><p>Although this is a bit confusing, it can be helpful in servers when parts are added, removed, or replaced. You&rsquo;ll always be assured that the same device in the same slot will never be renamed.</p>
<p>Our first ethernet device is called <em>ens9f0</em>, but what is the second device called?</p>
<pre><code># udevadm info -e | grep -A 9 ^P.*eth1
P: /devices/pci0000:00/0000:00:03.2/0000:08:00.1/net/eth1
E: DEVPATH=/devices/pci0000:00/0000:00:03.2/0000:08:00.1/net/eth1
E: ID_BUS=pci
E: ID_MODEL_FROM_DATABASE=82599ES 10-Gigabit SFI/SFP+ Network Connection (Ethernet OCP Server Adapter X520-2)
E: ID_MODEL_ID=0x10fb
E: ID_NET_DRIVER=ixgbe
E: ID_NET_LINK_FILE=/usr/lib/systemd/network/99-default.link
E: ID_NET_NAME_MAC=enxa0369f2cec91
E: ID_NET_NAME_PATH=enp8s0f1
E: ID_NET_NAME_SLOT=ens9f1
</code></pre><p>Now we know our ethernet devices are called <em>ens9f0</em> and <em>ens9f1</em>. It&rsquo;s time to configure systemd-networkd.</p>
<h2 id="bond-interface-creation">Bond interface creation</h2>
<p>Ensure that you have a <code>/etc/systemd/network/</code> directory on your server and create the network device file:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ini" data-lang="ini"><span style="color:#586e75"># /etc/systemd/network/bond1.netdev</span>
<span style="color:#719e07">[NetDev]</span>
Name<span style="color:#719e07">=</span><span style="color:#2aa198">bond1</span>
Kind<span style="color:#719e07">=</span><span style="color:#2aa198">bond</span>

<span style="color:#719e07">[Bond]</span>
Mode<span style="color:#719e07">=</span><span style="color:#2aa198">802.3ad</span>
TransmitHashPolicy<span style="color:#719e07">=</span><span style="color:#2aa198">layer3+4</span>
MIIMonitorSec<span style="color:#719e07">=</span><span style="color:#2aa198">1s</span>
LACPTransmitRate<span style="color:#719e07">=</span><span style="color:#2aa198">fast</span>
</code></pre></div><p>We&rsquo;re telling systemd-networkd that we want a new bond interface called <em>bond1</em> configured using 802.3ad mode. (Want to geek out on 802.3ad? Check out <a href="http://www.ieee802.org/3/hssg/public/apr07/frazier_01_0407.pdf">IEEE&rsquo;s PDF</a>.) In addition, we specify a transmit hash policy, a monitoring frequency, and a requested rate for LACP updates.</p>
<p>Now that we have a device defined, we need to provide some network configuration:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ini" data-lang="ini"><span style="color:#586e75"># /etc/systemd/network/bond1.network</span>
<span style="color:#719e07">[Match]</span>
Name<span style="color:#719e07">=</span><span style="color:#2aa198">bond1</span>

<span style="color:#719e07">[Network]</span>
VLAN<span style="color:#719e07">=</span><span style="color:#2aa198">public</span>
VLAN<span style="color:#719e07">=</span><span style="color:#2aa198">servicenet</span>
BindCarrier<span style="color:#719e07">=</span><span style="color:#2aa198">ens9f0 ens9f1</span>
</code></pre></div><p>This tells systemd-networkd that we have an interface called <em>bond1</em> and it has two VLANs configured on it (more on that later). Also, we specify the interfaces participating in the bond. This ensures that the bond comes up and down cleanly as interfaces change state.</p>
<p>As one last step, we need to configure the physical interfaces themselves:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ini" data-lang="ini"><span style="color:#586e75"># /etc/systemd/network/ens9f0.network</span>
<span style="color:#719e07">[Match]</span>
Name<span style="color:#719e07">=</span><span style="color:#2aa198">ens9f0</span>

<span style="color:#719e07">[Network]</span>
Bond<span style="color:#719e07">=</span><span style="color:#2aa198">bond1</span>
</code></pre></div><div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ini" data-lang="ini"><span style="color:#586e75"># /etc/systemd/network/ens9f1.network</span>
<span style="color:#719e07">[Match]</span>
Name<span style="color:#719e07">=</span><span style="color:#2aa198">ens9f1</span>

<span style="color:#719e07">[Network]</span>
Bond<span style="color:#719e07">=</span><span style="color:#2aa198">bond1</span>
</code></pre></div><p>These files help systemd-networkd understand which interfaces are participating in the bond. You can get fancy here with your <code>[Match]</code> sections and use only one interface file with <code>ens9f*</code>, but I prefer to be more explicit. Check the documentation for systemd-networkd for that.</p>
<h2 id="public-network-vlan">Public network VLAN</h2>
<p>The public network for your OnMetal server is delivered via a VLAN. Packets are tagged as VLAN 101 and you need to configure your network interface to handle that traffic. We already told systemd-networkd about our VLANs within the <em>bond1.network</em> file, but now we need to explain the configuration for the public network VLAN.</p>
<p>Start by creating a network device file:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ini" data-lang="ini"><span style="color:#586e75"># /etc/systemd/network/public.netdev</span>
<span style="color:#719e07">[NetDev]</span>
Name<span style="color:#719e07">=</span><span style="color:#2aa198">public</span>
Kind<span style="color:#719e07">=</span><span style="color:#2aa198">vlan</span>
MACAddress<span style="color:#719e07">=</span><span style="color:#2aa198">xx:xx:xx:xx:xx:xx</span>

<span style="color:#719e07">[VLAN]</span>
Id<span style="color:#719e07">=</span><span style="color:#2aa198">101</span>
</code></pre></div><p>You can get the correct MAC address from the information in your server&rsquo;s config drive:</p>
<pre><code>mkdir /mnt/configdrive
mount /dev/sda2 /mnt/configdrive/
python -m json.tool /mnt/configdrive/openstack/latest/vendor_data.json
</code></pre><p>Look inside the <code>network_info</code> section for <code>vlan0</code>. It will look something like this:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json">{
    <span style="color:#268bd2">&#34;ethernet_mac_address&#34;</span>: <span style="color:#2aa198">&#34;xx:xx:xx:xx:xx:xx&#34;</span>,
    <span style="color:#268bd2">&#34;id&#34;</span>: <span style="color:#2aa198">&#34;vlan0&#34;</span>,
    <span style="color:#268bd2">&#34;type&#34;</span>: <span style="color:#2aa198">&#34;vlan&#34;</span>,
    <span style="color:#268bd2">&#34;vlan_id&#34;</span>: <span style="color:#2aa198">101</span>,
    <span style="color:#268bd2">&#34;vlan_link&#34;</span>: <span style="color:#2aa198">&#34;bond0&#34;</span>
},
</code></pre></div><p>Take what you see in <em>ethernet_mac_address</em> and use that MAC address on the <code>MACAddress</code> line in your <em>public.netdev</em> file above. <strong>If you skip this part, your packets won&rsquo;t make it onto the network.</strong> For security reasons, the switch strictly checks to ensure that the right VLAN/IP/MAC combination is use when you communicate on the network.</p>
<p>Now that we have a network device, let&rsquo;s actually configure the network on it:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ini" data-lang="ini"><span style="color:#586e75"># /etc/systemd/network/public.network</span>
<span style="color:#719e07">[Match]</span>
Name<span style="color:#719e07">=</span><span style="color:#2aa198">public</span>

<span style="color:#719e07">[Network]</span>
DNS<span style="color:#719e07">=</span><span style="color:#2aa198">8.8.8.8</span>
DNS<span style="color:#719e07">=</span><span style="color:#2aa198">8.8.4.4</span>

<span style="color:#719e07">[Address]</span>
Address<span style="color:#719e07">=</span><span style="color:#2aa198">xxx.xxx.xxx.xxx/24</span>

<span style="color:#719e07">[Route]</span>
Destination<span style="color:#719e07">=</span><span style="color:#2aa198">0.0.0.0/0</span>
Gateway<span style="color:#719e07">=</span><span style="color:#2aa198">xxx.xxx.xxx.1</span>
</code></pre></div><p>To get your IP address and gateway, you can use <code>ip addr</code> and <code>ip route</code>. Or, you can look in your config drive within the <em>networks</em> section for the same data. Ensure that your IP address and gateway are configured correctly. I&rsquo;ve used Google&rsquo;s default DNS servers here but you can use your own if you prefer.</p>
<h2 id="servicenet-vlan">ServiceNet VLAN</h2>
<p>Rackspace&rsquo;s ServiceNet is the backend network that connects you to other servers as well as other Rackspace products, like Cloud Databases and Cloud Files. We will configure this network in the same fashion, starting with the network device file:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ini" data-lang="ini"><span style="color:#586e75"># /etc/systemd/network/servicenet.netdev</span>
<span style="color:#719e07">[NetDev]</span>
Name<span style="color:#719e07">=</span><span style="color:#2aa198">servicenet</span>
Kind<span style="color:#719e07">=</span><span style="color:#2aa198">vlan</span>
MACAddress<span style="color:#719e07">=</span><span style="color:#2aa198">xx:xx:xx:xx:xx:xx</span>

<span style="color:#719e07">[VLAN]</span>
Id<span style="color:#719e07">=</span><span style="color:#2aa198">401</span>
</code></pre></div><p>As we did before, go look in your config drive for the right MAC address to use. You&rsquo;ll look in the <em>network_info</em> section again but this time you&rsquo;ll look for <em>vlan1</em></p>
<p>Now we&rsquo;re ready to create the network file:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ini" data-lang="ini"><span style="color:#586e75"># /etc/systemd/network/servicenet.network</span>
<span style="color:#719e07">[Match]</span>
Name<span style="color:#719e07">=</span><span style="color:#2aa198">servicenet</span>

<span style="color:#719e07">[Network]</span>
Address<span style="color:#719e07">=</span><span style="color:#2aa198">xxx.xxx.xxx.xxx/20</span>

<span style="color:#719e07">[Route]</span>
Destination<span style="color:#719e07">=</span><span style="color:#2aa198">10.176.0.0/12</span>
Gateway<span style="color:#719e07">=</span><span style="color:#2aa198">10.184.0.1</span>

<span style="color:#719e07">[Route]</span>
Destination<span style="color:#719e07">=</span><span style="color:#2aa198">10.208.0.0/12</span>
Gateway<span style="color:#719e07">=</span><span style="color:#2aa198">10.184.0.1</span>
</code></pre></div><p>Review your config drive json for the correct IP address and routes. Your routes will likely be the same as mine, but that can change over time.</p>
<h2 id="enable-systemd-networkd">Enable systemd-networkd</h2>
<p>All of our configuration files are in place, but now we need to enable systemd-networkd at boot time:</p>
<pre><code>systemctl disable network
systemctl disable NetworkManager
systemctl enable systemd-networkd
systemctl enable systemd-resolved
</code></pre><p>We also need to let systemd-resolved handle our DNS resolution:</p>
<pre><code>systemctl start systemd-resolved
rm /etc/resolv.conf
ln -s /run/systemd/resolve/resolv.conf /etc/resolv.conf
</code></pre><p>Finally, there&rsquo;s one last gotcha that is only on OnMetal that needs to be removed. Comment out the second and third line in <code>/etc/rc.d/rc.local</code>:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#719e07">#!/usr/bin/sh
</span><span style="color:#719e07"></span><span style="color:#586e75">#sleep 20</span>
<span style="color:#586e75">#/etc/init.d/network restart</span>
<span style="color:#b58900">exit</span> <span style="color:#2aa198">0</span>
</code></pre></div><p>That&rsquo;s there as a workaround for some network issues that sometimes appear during first boot. We won&rsquo;t need it with systemd-networkd.</p>
<h2 id="reboot">Reboot</h2>
<p>We&rsquo;re ready to test our new configuration! First, let&rsquo;s disable the forced old interface names on the kernel command line. Open <code>/boot/extlinux.conf</code> and ensure that the following two items are not in the kernel command line:</p>
<ul>
<li><code>net.ifnames=0</code></li>
<li><code>biosdevname=0</code></li>
</ul>
<p>Remove them from any kernel command lines you see and save the file. Reboot and cross your fingers.</p>
<h2 id="checking-our-work">Checking our work</h2>
<p>If you get pings after a reboot, you did well! If you didn&rsquo;t. you can use OnMetal&rsquo;s rescue mode to hop into a temporary OS and mount your root volume. Be sure to look inside <code>/var/log/messages</code> for signs of typos or other errors.</p>
<p>We can use some simple tools to review our network status:</p>
<pre><code># networkctl
IDX LINK             TYPE               OPERATIONAL SETUP
  1 lo               loopback           carrier     unmanaged
  2 bond0            ether              off         unmanaged
  3 bond1            ether              degraded    configured
  4 public           ether              routable    configured
  5 servicenet       ether              routable    configured
  6 ens9f0           ether              carrier     configured
  7 ens9f1           ether              carrier     configured
</code></pre><p>Don&rsquo;t be afraid of the <em>degraded</em> status for <em>bond1</em>. That&rsquo;s there because systemd doesn&rsquo;t have networking configuration for the interface since we do that with our VLANs. Also, both physical network interfaces are listed as <em>carrier</em> because they don&rsquo;t have network configuration, either. They&rsquo;re just participating in the bond.</p>
<p>Feel free to ignore <em>bond0</em>, too. The bonding module in the Linux kernel automatically creates the interface when it&rsquo;s loaded.</p>
<h2 id="extra-credit-switch-to-grub2">Extra credit: Switch to grub2</h2>
<p>Sure, extlinux is fine for most use cases, but I prefer something a little more powerful. Luckily, switching to grub2 is quite painless:</p>
<pre><code>dnf -y remove syslinux-extlinux
rm -f /boot/extlinux.conf
dnf -y install grubby grub2
grub2-mkconfig -o /boot/grub2/grub.cfg
grub2-install /dev/sda
</code></pre><p>Simply reboot and you&rsquo;ll be booting with grub2!</p>
]]></content></item><item><title>Research Paper: Securing Linux Containers</title><link>https://major.io/2015/08/14/research-paper-securing-linux-containers/</link><pubDate>Fri, 14 Aug 2015 20:45:50 +0000</pubDate><guid>https://major.io/2015/08/14/research-paper-securing-linux-containers/</guid><description>It seems like there&amp;rsquo;s a new way to run containers every week. The advantages and drawbacks of each approach are argued about on mailing lists, in IRC channels, and in person, around the world. However, the largest amount of confusion seems to be around security.
Launching secure containers I&amp;rsquo;ve written about launching secure containers on this blog many times before:
Launch secure LXC containers on Fedora 20 using SELinux and sVirt Improving LXC template security Try out LXC with an Ansible playbook CoreOS vs.</description><content type="html"><![CDATA[<p>It seems like there&rsquo;s a new way to run containers every week. The advantages and drawbacks of each approach are argued about on mailing lists, in IRC channels, and in person, around the world. However, the largest amount of confusion seems to be around security.</p>
<h3 id="launching-secure-containers">Launching secure containers</h3>
<p>I&rsquo;ve written about launching secure containers on this blog many times before:</p>
<ul>
<li><a href="/2014/04/21/launch-secure-lxc-containers-on-fedora-20-using-selinux-and-svirt/">Launch secure LXC containers on Fedora 20 using SELinux and sVirt</a></li>
<li><a href="/2015/06/18/improving-lxc-template-security/">Improving LXC template security</a></li>
<li><a href="/2014/12/17/try-lxc-ansible-playbook/">Try out LXC with an Ansible playbook</a></li>
<li><a href="/2014/05/13/coreos-vs-project-atomic-a-review/">CoreOS vs. Project Atomic: A Review</a></li>
</ul>
<p><a href="/wp-content/uploads/2015/08/GCUX.Gold_.hi_.res_.png"><!-- raw HTML omitted --></a></p>
<p>However, my goal this time around was to do something more comprehensive and slightly more formal. After getting my <a href="http://www.giac.org/certified-professional/major-hayden/138471">GSEC and GCUX</a> certifications from <a href="http://www.giac.org/">SANS/GIAC</a>, there was an option to enhance the certification to a gold status by <a href="http://www.giac.org/certifications/gold">writing a peer-reviewed research paper</a> on a topic related to the exam. It was a great experience to go through the review process and get feedback on the technical material as well as the structure of the paper itself.</p>
<h3 id="the-paper">The paper</h3>
<p>Without further ado, here are links to the <em>Securing Linux Containers</em> paper:</p>
<ul>
<li><a href="/wp-content/uploads/2015/08/Securing-Linux-Containers-GCUX-Gold-Paper-Major-Hayden.pdf">PDF version without watermarks</a></li>
<li><a href="https://www.sans.org/reading-room/whitepapers/linux/securing-linux-containers-36142">PDF version from SANS</a> <em>(has some watermarks and SANS/GIAC extra pages)</em></li>
</ul>
<p>The paper is written for readers who have some level of familiarity with Linux and some virtualization technologies. It&rsquo;s a useful paper even for people who haven&rsquo;t worked with containers.</p>
<p>It starts with an overview of Linux containers and how they differ from other types of virtualization, such as KVM or Xen. From there, it covers how to secure the host system underneath the containers and how to provide security within the containers themselves. There&rsquo;s also a section on how to start a simple container on CentOS 7 and inspect the security controls inside and outside the container.</p>
<h3 id="licensing">Licensing</h3>
<p><a href="/wp-content/uploads/2015/08/by-sa.png"><!-- raw HTML omitted --></a>I&rsquo;m also very proud to announce that the paper is licensed under the <a href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License (CC-BY-SA)</a>. You are free to quote it as much as you like (even for commercial purposes), but I&rsquo;d ask that you maintain the same license and attribute me as the author.</p>
<h3 id="thank-you">Thank you</h3>
<p>This paper wouldn&rsquo;t have been possible without some serious help from these awesome people:</p>
<ul>
<li><strong>Richard Carbone</strong> was my advisor from SANS and he helped tremendously</li>
<li><strong>Dan Walsh</strong> reviewed the content and gave me several pointers on topics to add and adjust</li>
<li><strong>Paul Voccio, Antony Messerli, and Brad McConnell</strong> from Rackspace also provided feedback</li>
<li>My mother, <strong>Neta Greene</strong>, is the best educator I know and she fueled my interest in writing and sharing with others</li>
</ul>
<h3 id="feedback">Feedback</h3>
<p>Please let me know if you spot any errors or areas that need clarification. This is one of my favorite topics and I enjoy talking about it. Find me on Freenode IRC as <em>mhayden</em> and I&rsquo;ll be glad to talk more there.</p>
]]></content></item><item><title>Fedora Flock 2015: Keynote slides</title><link>https://major.io/2015/08/14/fedora-flock-2015-keynote-slides/</link><pubDate>Fri, 14 Aug 2015 15:10:08 +0000</pubDate><guid>https://major.io/2015/08/14/fedora-flock-2015-keynote-slides/</guid><description>Fedora Flock 2015 is still going here in Rochester, New York, and I kicked off our second day with a keynote talk about overcoming impostor syndrome.
If you&amp;rsquo;d like to review the slides, they&amp;rsquo;re on SlideShare:
Quite a few people came up after the talk and throughout the day to share some of their stories and challenges. It was extremely rewarding to have those conversations and share solutions.
I&amp;rsquo;ll be doing the talk once more at Texas Linux Fest in San Marcos on August 22.</description><content type="html"><![CDATA[<p><a href="http://www.flocktofedora.org/">Fedora Flock 2015</a> is still going here in Rochester, New York, and I kicked off our second day <a href="http://flock2015.sched.org/event/f0d4e309dd6363f56f9516ced394a42f">with a keynote talk</a> about overcoming <a href="https://en.wikipedia.org/wiki/Impostor_syndrome">impostor syndrome</a>.</p>
<p>If you&rsquo;d like to review the slides, <a href="http://www.slideshare.net/MajorHayden/be-an-inspiration-not-an-impostor-fedora-flock-2015">they&rsquo;re on SlideShare</a>:</p>
<!-- raw HTML omitted -->
<p>Quite a few people came up after the talk and throughout the day to share some of their stories and challenges. It was extremely rewarding to have those conversations and share solutions.</p>
<p>I&rsquo;ll be doing the talk once more at Texas Linux Fest in San Marcos on August 22.</p>
]]></content></item><item><title>Review: JayBird BlueBuds X Sport Bluetooth Headphones</title><link>https://major.io/2015/08/07/review-jaybird-bluebuds-x-sport-bluetooth-headphones/</link><pubDate>Fri, 07 Aug 2015 13:36:20 +0000</pubDate><guid>https://major.io/2015/08/07/review-jaybird-bluebuds-x-sport-bluetooth-headphones/</guid><description>As I try to motivate myself to exercise more often, I&amp;rsquo;ve struggled to find suitable headphones for running and other outdoor work. My criteria includes the following:
Needs to stay put on my ears, even when I get sweaty Wireless is preferable so I don&amp;rsquo;t get tangled For wireless, it should last three to four hours at a minimum It needs to sound decent The Bad I went through a slew of headphones.</description><content type="html"><![CDATA[<p>As I try to motivate myself to exercise more often, I&rsquo;ve struggled to find suitable headphones for running and other outdoor work. My criteria includes the following:</p>
<ul>
<li>Needs to stay put on my ears, even when I get sweaty</li>
<li>Wireless is preferable so I don&rsquo;t get tangled</li>
<li>For wireless, it should last three to four hours at a minimum</li>
<li>It needs to sound decent</li>
</ul>
<h3 id="the-bad">The Bad</h3>
<p>I went through a slew of headphones. Various earbuds and headphones from multiple brands would just fall off or they sounded tinny and horrible.</p>
<p>I moved on to the plain white headphones that come with iPhones as I had a few of those laying around the house. They wouldn&rsquo;t stay in if I started moving around, so a coworker suggested <a href="http://www.budfits.com/">Budfits</a>. It was an awesome fix for the price and the earbuds stayed in place fairly well. I&rsquo;d need to adjust them maybe once during a 30-minute run and that was manageable for me.</p>
<p>The cabling for those headphones bothered me if I was working in the yard or checking for cars when running across streets.</p>
<h3 id="the-search">The Search</h3>
<p><a href="/wp-content/uploads/2015/08/jaybird_bluebuds_x.jpeg"><!-- raw HTML omitted --></a>Ready to give up, I turned to Twitter and asked the masses for their recommendation. Quite a few people came back and recommended the <a href="http://www.jaybirdsport.com/bluebuds-x-bluetooth-headphones/">JayBird BlueBuds X</a>. They ticked quite a few boxes for me on paper. They&rsquo;re wireless, they last around eight hours on a charge, and they boast being sweat-proof. Although it was a little more money than I was ready to spend, I ordered them anyway.</p>
<p>Long story short: they&rsquo;ve been amazing. The battery life is more than enough for most of my tasks and they charge using a simple micro-USB cable (same that you use for Android phones). They rarely need adjustment after you get them in your ears even if you sweat a lot. I will usually spend two to three hours working out in the yard and I only need to adjust them once or twice during that time.</p>
<p>The sound quality is also very good. There is plenty of bass available and the mid/high range sound is very clear. They&rsquo;re in-ear earbuds but I can still hear some sounds around me. However, I do feel the need to take some extra looks while running across streets because they do block out more sound than your average on-ear earbuds. This should be part of your running routine already, though.</p>
<p>The BlueBuds can be worn under-ear or over ear depending on your preference. The manual has very detailed instructions on how to size the earbuds for your ears and how to route the cabling. <strong>Pay attention to the instructions.</strong> If you don&rsquo;t follow them closely, the buds will fall out of your ears or you&rsquo;ll have poor sound quality. I was getting grumpy about the lack of bass and then I re-read the instructions. I&rsquo;m not grumpy any longer.</p>
<p>You can also use the BlueBuds to make phone calls or hop on video conferences with your laptop. I&rsquo;ve paired them with a 2014 Moto X, my Lenovo X1 laptop, and a Nexus 7 tablet. Pairing is a very quick process and it&rsquo;s detailed well in the manual.</p>
<p>They&rsquo;re also very comfortable even when you aren&rsquo;t working out and getting sweaty. I&rsquo;ve used them on several plane flights so far and they do a great job of blocking out engine noise. Keep in mind that they&rsquo;re not noise-canceling - they just muffle the ambient sounds a bit.</p>
<p><em>Disclaimer: I haven&rsquo;t received any compensation for this review. I&rsquo;m just a slightly overweight guy who is trying to get some exercise while listening to my favorite music.</em></p>
]]></content></item><item><title>Less than a week until Fedora Flock 2015</title><link>https://major.io/2015/08/06/less-than-a-week-until-fedora-flock-2015/</link><pubDate>Thu, 06 Aug 2015 12:36:47 +0000</pubDate><guid>https://major.io/2015/08/06/less-than-a-week-until-fedora-flock-2015/</guid><description>There&amp;rsquo;s less than week until Fedora Flock 2015 in Rochester, New York! This will be my first ever trip to New York and I&amp;rsquo;m looking forward to it!
I&amp;rsquo;ll be speaking about impostor syndrome in my &amp;ldquo;Be an inspiration, not an impostor&amp;rdquo; talk on Thursday, August 13th. It&amp;rsquo;s a topic that hits close to home for me since I&amp;rsquo;ve wrestled with it quite a few times. Fortunately, it&amp;rsquo;s easy to overcome if we&amp;rsquo;re able to identify it in our colleagues and give them the right support.</description><content type="html"><![CDATA[<p><a href="/wp-content/uploads/2015/08/flock-2015-attendee.png"><!-- raw HTML omitted --></a>There&rsquo;s less than week until <a href="http://www.flocktofedora.org/">Fedora Flock 2015</a> in Rochester, New York! This will be my first ever trip to New York and I&rsquo;m looking forward to it!</p>
<p>I&rsquo;ll be speaking about <a href="https://en.wikipedia.org/wiki/Impostor_syndrome">impostor syndrome</a> in my &ldquo;Be an inspiration, not an impostor&rdquo; talk on Thursday, August 13th. It&rsquo;s a topic that hits close to home for me since I&rsquo;ve wrestled with it quite a few times. Fortunately, it&rsquo;s easy to overcome if we&rsquo;re able to identify it in our colleagues and give them the right support. It&rsquo;s amazing to see what happens when amazing people get over the hump and have an honest self-assessment of their capabilities.</p>
<p>Thanks to <a href="http://rackspace.com/">Rackspace</a> for being a gold sponsor and for sending me to join the Fedora community in Rochester. I&rsquo;ll be sure to share my slides along with a conference recap when I get back.</p>
]]></content></item><item><title>Automated testing for Ansible CIS playbook on RHEL/CentOS 6</title><link>https://major.io/2015/08/05/automated-testing-for-ansible-cis-playbook-on-rhelcentos-6/</link><pubDate>Wed, 05 Aug 2015 13:13:52 +0000</pubDate><guid>https://major.io/2015/08/05/automated-testing-for-ansible-cis-playbook-on-rhelcentos-6/</guid><description>I started working on the Ansible CIS playbook for CentOS and RHEL 6 back in 2014 and I&amp;rsquo;ve made a few changes to increase quality and make it easier to use.
First off, the role itself is no longer a submodule. You can now just clone the repository and get rolling. This should reduce the time it takes to get started.
Also, all pull requests to the repository now go through integration testing at Rackspace.</description><content type="html"><![CDATA[<p><a href="/wp-content/uploads/2014/08/image-ansible.png"><!-- raw HTML omitted --></a>I started working on the Ansible CIS playbook for CentOS and RHEL 6 <a href="/2014/08/19/audit-rhelcentos-6-security-benchmarks-ansible/">back in 2014</a> and I&rsquo;ve made a few changes to increase quality and make it easier to use.</p>
<p>First off, the role itself is no longer a submodule. You can now just clone the repository and get rolling. This should reduce the time it takes to get started.</p>
<p>Also, all pull requests to the repository now go through integration testing at Rackspace. Each pull request goes through the gauntlet:</p>
<ul>
<li>Syntax check on Travis-CI</li>
<li>Travis-CI builds a server at Rackspace</li>
<li>The entire Ansible playbook runs on the Rackspace Cloud Server</li>
<li>Results are sent back to GitHub</li>
</ul>
<p>The testing process usually takes under five minutes.</p>
<p><em>Stay tuned: Updates are coming for RHEL and CentOS 7. ;)</em></p>
]]></content></item><item><title>Live migration failures with KVM and libvirt</title><link>https://major.io/2015/08/03/live-migration-failures-with-kvm-and-libvirt/</link><pubDate>Mon, 03 Aug 2015 13:13:30 +0000</pubDate><guid>https://major.io/2015/08/03/live-migration-failures-with-kvm-and-libvirt/</guid><description>I decided to change some of my infrastructure back to KVM again, and the overall experience has been quite good in Fedora 22. Using libvirt with KVM is a breeze and the virt-manager tools make it even easier. However, I ran into some problems while trying to migrate virtual machines from one server to another.
The error # virsh migrate --live --copy-storage-all bastion qemu+ssh://root@192.168.250.33/system error: internal error: unable to execute QEMU command 'drive-mirror': Failed to connect socket: Connection timed out That error message wasn&amp;rsquo;t terribly helpful.</description><content type="html"><![CDATA[<p>I decided to change some of my infrastructure back to KVM again, and the overall experience has been quite good in Fedora 22. Using libvirt with KVM is a breeze and the virt-manager tools make it even easier. However, I ran into some problems while trying to migrate virtual machines from one server to another.</p>
<h3 id="the-error">The error</h3>
<pre><code># virsh migrate --live --copy-storage-all bastion qemu+ssh://root@192.168.250.33/system
error: internal error: unable to execute QEMU command 'drive-mirror': Failed to connect socket: Connection timed out
</code></pre><p>That error message wasn&rsquo;t terribly helpful. I started running through my usual list of checks:</p>
<ul>
<li><em>Can the hypervisors talk to each other?</em> Yes, iptables is disabled.</li>
<li><em>Are ssh keys configured?</em> Yes, verified.</li>
<li><em>What about ssh host keys being accepted on each side?</em> Both sides can ssh without interaction.</li>
<li><em>SELinux?</em> No AVC&rsquo;s logged.</li>
<li><em>Libvirt logs?</em> Nothing relevant in libvirt&rsquo;s qemu logs.</li>
<li><em>Filesystem permissions for libvirt&rsquo;s directories?</em> Identical on both sides.</li>
<li><em>Libvirt daemon running on both sides?</em> Yes.</li>
</ul>
<p>I was pretty confused at this point. A quick Google search didn&rsquo;t reveal too many relevant issues, but I did find a <a href="https://bugzilla.redhat.com/show_bug.cgi?id=1025699">Red Hat Bug from 2013</a> that affected RHEL 7. The issue in the bug was that libvirt wasn&rsquo;t using the right ports to talk between servers and those packets were being dropped by iptables. My iptables rules were empty.</p>
<h3 id="debug-time">Debug time</h3>
<p>I ran the same command with <code>LIBVIRT_DEBUG=1</code> at the front:</p>
<pre><code> debug.log
</code></pre><p>After scouring the pages and pages of output, I couldn&rsquo;t find anything useful.</p>
<h3 id="eureka">Eureka!</h3>
<p>I spotted an error message briefly in virt-manager or the debug logs that jogged my brain to think about a potential problem: hostnames. Both hosts had a fairly bare <code>/etc/hosts</code> file without IP/hostname pairs for each hypervisor. After editing both servers' <code>/etc/hosts</code> file to include the short and full hostnames for each hypervisor, I tested the live migration one more time.</p>
<p><strong>Success!</strong></p>
<p>The migration went off without a hitch in virt-manager and via the <code>virsh</code> client. I migrated several VM&rsquo;s, including the one running this site, with no noticeable interruption.</p>
]]></content></item><item><title>Very slow ssh logins on Fedora 22</title><link>https://major.io/2015/07/27/very-slow-ssh-logins-on-fedora-22/</link><pubDate>Mon, 27 Jul 2015 12:09:44 +0000</pubDate><guid>https://major.io/2015/07/27/very-slow-ssh-logins-on-fedora-22/</guid><description>I&amp;rsquo;ve recently set up a Fedora 22 firewall/router at home (more on that later) and I noticed that remote ssh logins were extremely slow. In addition, sudo commands seemed to stall out for the same amount of time (about 25-30 seconds).
I&amp;rsquo;ve done all the basic troubleshooting already:
Switch to UseDNS no in /etc/ssh/sshd_config Set GSSAPIAuthentication no in /etc/ssh/sshd_config Tested DNS resolution These lines kept cropping up in my system journal when I tried to access the server using ssh:</description><content type="html"><![CDATA[<p>I&rsquo;ve recently set up a Fedora 22 firewall/router at home (more on that later) and I noticed that remote ssh logins were extremely slow. In addition, sudo commands seemed to stall out for the same amount of time (about 25-30 seconds).</p>
<p>I&rsquo;ve done all the basic troubleshooting already:</p>
<ul>
<li>Switch to <code>UseDNS no</code> in <code>/etc/ssh/sshd_config</code></li>
<li>Set <code>GSSAPIAuthentication no</code> in <code>/etc/ssh/sshd_config</code></li>
<li>Tested DNS resolution</li>
</ul>
<p>These lines kept cropping up in my system journal when I tried to access the server using ssh:</p>
<pre><code>dbus[4865]: [system] Failed to activate service 'org.freedesktop.login1': timed out
sshd[7391]: pam_systemd(sshd:session): Failed to create session: Activation of org.freedesktop.login1 timed out
sshd[7388]: pam_systemd(sshd:session): Failed to create session: Activation of org.freedesktop.login1 timed out
</code></pre><p>The process list on the server looked fine. I could see <code>dbus-daemon</code> and <code>systemd-logind</code> processes and they were in good states. However, it looked like <code>dbus-daemon</code> had restarted at some point and <code>systemd-logind</code> had not been restarted since then. I crossed my fingers and bounced <code>systemd-logind</code>:</p>
<pre><code>systemctl restart systemd-logind
</code></pre><p>Success! Logins via ssh and escalations with sudo worked instantly.</p>
]]></content></item><item><title>Restoring wireless and Bluetooth state after reboot in Fedora 22</title><link>https://major.io/2015/07/19/restoring-wireless-and-bluetooth-state-after-reboot-in-fedora-22/</link><pubDate>Sun, 19 Jul 2015 22:14:30 +0000</pubDate><guid>https://major.io/2015/07/19/restoring-wireless-and-bluetooth-state-after-reboot-in-fedora-22/</guid><description>My upgrade to Fedora 22 on the ThinkPad X1 Carbon was fairly uneventful and the hiccups were minor. One of the more annoying items that I&amp;rsquo;ve been struggling with for quite some time is how to boot up with the wireless LAN and Bluetooth disabled by default. Restoring wireless and Bluetooth state between reboots is normally handled quite well in Fedora.
In Fedora 21, NetworkManager saved my settings between reboots. For example, if I shut down with wifi off and Bluetooth on, the laptop would boot up later with wifi off and Bluetooth on.</description><content type="html"><![CDATA[<p><img src="/wp-content/uploads/2015/03/ThinkPad-Carbon-X1.jpg" alt="1"></p>
<p>My upgrade to Fedora 22 on the ThinkPad X1 Carbon was fairly uneventful and the hiccups were minor. One of the more annoying items that I&rsquo;ve been struggling with for quite some time is how to boot up with the wireless LAN and Bluetooth disabled by default. Restoring wireless and Bluetooth state between reboots is normally handled quite well in Fedora.</p>
<p>In Fedora 21, NetworkManager saved my settings between reboots. For example, if I shut down with wifi off and Bluetooth on, the laptop would boot up later with wifi off and Bluetooth on. This wasn&rsquo;t working well in Fedora 22: both the wifi and Bluetooth were always enabled by default.</p>
<h3 id="digging-into-rfkill">Digging into rfkill</h3>
<p>I remembered <a href="https://wireless.wiki.kernel.org/en/users/documentation/rfkill">rfkill</a> and began testing out some commands. It detected that I had disabled both devices via NetworkManager (soft):</p>
<pre><code>$ rfkill list
0: tpacpi_bluetooth_sw: Bluetooth
    Soft blocked: yes
    Hard blocked: no
2: phy0: Wireless LAN
    Soft blocked: yes
    Hard blocked: no
</code></pre><p>It looked like systemd has some hooks already configured to manage rfkill via the <a href="http://www.freedesktop.org/software/systemd/man/systemd-rfkill@.service.html">systemd-rfkill</a> service. However, something strange happened when I tried to start the service:</p>
<pre><code># systemctl start systemd-rfkill@0
Failed to start systemd-rfkill@0.service: Unit systemd-rfkill@0.service is masked.
</code></pre><p>Well, that&rsquo;s certainly weird. While looking into why it&rsquo;s masked, I found an empty file in <code>/etc/systemd</code>:</p>
<pre><code># ls -al /etc/systemd/system/systemd-rfkill@.service
-rwxr-xr-x. 1 root root 0 May 11 16:36 /etc/systemd/system/systemd-rfkill@.service
</code></pre><p>I don&rsquo;t remember making that file. Did something else put it there?</p>
<pre><code># rpm -qf /etc/systemd/system/systemd-rfkill@.service
tlp-0.7-4.fc22.noarch
</code></pre><p>Ah, <a href="http://linrunner.de/en/tlp/tlp.html">tlp</a>!</p>
<h3 id="configuring-tlp">Configuring tlp</h3>
<p>I looked in tlp&rsquo;s configuration file in <code>/etc/default/tlp</code> and found a few helpful configuration items:</p>
<pre><code># Restore radio device state (Bluetooth, WiFi, WWAN) from previous shutdown
# on system startup: 0=disable, 1=enable.
# Hint: the parameters DEVICES_TO_DISABLE/ENABLE_ON_STARTUP/SHUTDOWN below
#   are ignored when this is enabled!
RESTORE_DEVICE_STATE_ON_STARTUP=0

# Radio devices to disable on startup: bluetooth, wifi, wwan.
# Separate multiple devices with spaces.
#DEVICES_TO_DISABLE_ON_STARTUP=&quot;bluetooth wifi wwan&quot;

# Radio devices to enable on startup: bluetooth, wifi, wwan.
# Separate multiple devices with spaces.
#DEVICES_TO_ENABLE_ON_STARTUP=&quot;wifi&quot;

# Radio devices to disable on shutdown: bluetooth, wifi, wwan
# (workaround for devices that are blocking shutdown).
#DEVICES_TO_DISABLE_ON_SHUTDOWN=&quot;bluetooth wifi wwan&quot;

# Radio devices to enable on shutdown: bluetooth, wifi, wwan
# (to prevent other operating systems from missing radios).
#DEVICES_TO_ENABLE_ON_SHUTDOWN=&quot;wwan&quot;

# Radio devices to enable on AC: bluetooth, wifi, wwan
#DEVICES_TO_ENABLE_ON_AC=&quot;bluetooth wifi wwan&quot;

# Radio devices to disable on battery: bluetooth, wifi, wwan
#DEVICES_TO_DISABLE_ON_BAT=&quot;bluetooth wifi wwan&quot;

# Radio devices to disable on battery when not in use (not connected):
# bluetooth, wifi, wwan
#DEVICES_TO_DISABLE_ON_BAT_NOT_IN_USE=&quot;bluetooth wifi wwan&quot;
</code></pre><p>So tlp&rsquo;s default configuration doesn&rsquo;t restore device state <strong>and</strong> it masked systemd&rsquo;s rfkill service. I adjusted one line in tlp&rsquo;s configuration and rebooted:</p>
<pre><code>DEVICES_TO_DISABLE_ON_STARTUP=&quot;bluetooth wifi wwan&quot;
</code></pre><p>After the reboot, both the wifi and Bluetooth functionality were shut off! That&rsquo;s exactly what I needed.</p>
<h3 id="extra-credit">Extra credit</h3>
<p>Thanks to a coworker, I was able to make a NetworkManager script to automatically shut off the wireless LAN whenever I connected to a network via ethernet. This is typically what I do when coming back from an in-person meeting to my desk (where I have ethernet connectivity).</p>
<p>If you want the same automation, just drop this script into <code>/etc/NetworkManager/dispatcher.d/70-wifi-wired-exclusive.sh</code> and make it executable:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#719e07">#!/bin/bash
</span><span style="color:#719e07"></span><span style="color:#b58900">export</span> <span style="color:#268bd2">LC_ALL</span><span style="color:#719e07">=</span>C

enable_disable_wifi <span style="color:#719e07">()</span>
<span style="color:#719e07">{</span>
        <span style="color:#268bd2">result</span><span style="color:#719e07">=</span><span style="color:#719e07">$(</span>nmcli dev | grep <span style="color:#2aa198">&#34;ethernet&#34;</span> | grep -w <span style="color:#2aa198">&#34;connected&#34;</span><span style="color:#719e07">)</span>
        <span style="color:#719e07">if</span> <span style="color:#719e07">[</span> -n <span style="color:#2aa198">&#34;</span><span style="color:#268bd2">$result</span><span style="color:#2aa198">&#34;</span> <span style="color:#719e07">]</span>; <span style="color:#719e07">then</span>
                nmcli radio wifi off
        <span style="color:#719e07">fi</span>
<span style="color:#719e07">}</span>

<span style="color:#719e07">if</span> <span style="color:#719e07">[</span> <span style="color:#2aa198">&#34;</span><span style="color:#268bd2">$2</span><span style="color:#2aa198">&#34;</span> <span style="color:#719e07">=</span> <span style="color:#2aa198">&#34;up&#34;</span> <span style="color:#719e07">]</span>; <span style="color:#719e07">then</span>
        enable_disable_wifi
<span style="color:#719e07">fi</span>
</code></pre></div><p>Unplug the ethernet connection, start wifi, and then plug the ethernet connection back in. Once NetworkManager fully connects (DHCP lease obtained, connectivity check passes), the wireless LAN should shut off automatically.</p>
]]></content></item><item><title>Making things more super with supernova 2.0</title><link>https://major.io/2015/07/18/making-things-more-super-with-supernova-2-0/</link><pubDate>Sat, 18 Jul 2015 17:42:29 +0000</pubDate><guid>https://major.io/2015/07/18/making-things-more-super-with-supernova-2-0/</guid><description>I started supernova a little over three years ago with the idea of making it easier to use novaclient. Three years and a few downloads later, it manages multiple different OpenStack clients, like nova, glance, and trove along with some handy features for users who manage a large number of environments.
What&amp;rsquo;s new? With some help from some friends who are much better at writing Python than I am (thanks Paul, Matt and Jason), I restructured supernova to make it more testable.</description><content type="html"><![CDATA[<p><a href="/wp-content/uploads/2011/08/OpenStackLogo_270x279.jpg"><!-- raw HTML omitted --></a>I started supernova a little over <a href="/2012/06/05/supernova-manage-multiple-openstack-nova-environments-with-ease/">three years ago</a> with the idea of making it easier to use novaclient. Three years and a few downloads later, it manages multiple different OpenStack clients, like nova, glance, and trove along with some handy features for users who manage a large number of environments.</p>
<h3 id="whats-new">What&rsquo;s new?</h3>
<p>With some help from some friends who are much better at writing Python than I am (thanks Paul, Matt and Jason), I restructured supernova to make it more testable. The big, awkward SuperNova class was dropped and there are fewer circular imports. In addition, I migrated the cli management components to use the <a href="http://click.pocoo.org/4/">click</a> module. It&rsquo;s now compatible with Python versions 2.6, 2.7, 3.3 and 3.4.</p>
<p>The overall functionality hasn&rsquo;t changed much, but there&rsquo;s a new option to specify a custom supernova configuration that sits in a non-standard location or with a filename other than <code>.supernova</code>. Simply use the <code>-c</code> flag:</p>
<pre><code>supernova -c ~/work/.supernova dfw list
supernova -c ~/personal/supernova-config-v1 staging list
</code></pre><p>The testing is done with <a href="https://travis-ci.org/major/supernova">Travis-CI</a> and code coverage is checked with <a href="https://codecov.io/github/major/supernova">Codecov</a>. Pull requests will automatically be checked with unit tests and I&rsquo;ll do my best to urge committers to keep test coverage at 100%.</p>
<h3 id="updating-supernova">Updating supernova</h3>
<p>Version 2.0.0 is already in PyPi, so an upgrade using pip is quite easy:</p>
<pre><code>pip install -U supernova
</code></pre>]]></content></item><item><title>Aruba access points, EAP, and wpa_supplicant 2.4 bugs</title><link>https://major.io/2015/07/17/aruba-access-points-eap-and-wpa_supplicant-2-4-bugs/</link><pubDate>Fri, 17 Jul 2015 12:29:29 +0000</pubDate><guid>https://major.io/2015/07/17/aruba-access-points-eap-and-wpa_supplicant-2-4-bugs/</guid><description>I stumbled upon a strange bug at work one day and found I couldn&amp;rsquo;t connect to our wireless access points any longer. After some investigation in the systemd journal, I found that my card associated with the access point but never went any further past that. It looked as if the authentication wasn&amp;rsquo;t ever taking place.
A quick dig through my recent dnf update history didn&amp;rsquo;t reveal much but then I found a tip from a coworker on an internal wiki that wpa_supplicant 2.</description><content type="html"><![CDATA[<p>I stumbled upon a strange bug at work one day and found I couldn&rsquo;t connect to our wireless access points any longer. After some investigation in the systemd journal, I found that my card associated with the access point but never went any further past that. It looked as if the authentication wasn&rsquo;t ever taking place.</p>
<p>A quick dig through my recent dnf update history didn&rsquo;t reveal much but then I found a tip from a coworker on an internal wiki that wpa_supplicant 2.4 has problems with certain Aruba wireless access points.</p>
<p>There&rsquo;s an <a href="https://bugzilla.redhat.com/show_bug.cgi?id=1241930">open ticket on the Red Hat Bugzilla</a> about the issues in wpa_supplicant 2.4. The <a href="http://w1.fi/cgit/hostap/plain/wpa_supplicant/ChangeLog">changelog</a> for 2.4 is lengthy and it has plenty of mentions of <a href="https://en.wikipedia.org/wiki/Extensible_Authentication_Protocol">EAP</a>; Aruba&rsquo;s preferred protocol on certain networks. One of those changes could be related. A <a href="http://community.arubanetworks.com/t5/Unified-Wired-Wireless-Access/Internal-radius-server-incompatibility-with-the-new-wpa/td-p/236602">formal support case</a> is open with Aruba as well.</p>
<p>If this bug affects you, you can return to wpa_supplicant-2.3-3.fc22.x86_64 easily by running:</p>
<pre><code>dnf downgrade wpa_supplicant
</code></pre><p>This isn&rsquo;t a good long-term solution, but it fixes the bug and gets you back online.</p>
]]></content></item><item><title>Allow new windows to steal focus in GNOME 3</title><link>https://major.io/2015/07/06/allow-new-windows-to-steal-focus-in-gnome-3/</link><pubDate>Mon, 06 Jul 2015 12:36:05 +0000</pubDate><guid>https://major.io/2015/07/06/allow-new-windows-to-steal-focus-in-gnome-3/</guid><description>GNOME 3 generally works well for me but it has some quirks. One of those quirks is that new windows don&amp;rsquo;t actually pop up on the screen with focus as they do in Windows and OS X. When opening a new window, you get a “[Windowname] is ready” notification:
My preference is for new windows to pop in front and steal focus. I can see why that&amp;rsquo;s not the default since it might cause you to type something in another window where you weren&amp;rsquo;t expecting to.</description><content type="html"><![CDATA[<p>GNOME 3 generally works well for me but it has some quirks. One of those quirks is that new windows don&rsquo;t actually pop up on the screen with focus as they do in Windows and OS X. When opening a new window, you get a “[Windowname] is ready” notification:</p>
<p><img src="https://major.io/wp-content/uploads/2015/07/hangouts_is_ready.png" alt="1"></p>
<p>My preference is for new windows to pop in front and steal focus. I can see why that&rsquo;s not the default since it might cause you to type something in another window where you weren&rsquo;t expecting to. Fortunately, you can enable what GNOME calls <em>strict</em> window focus with a quick trip to <code>dconf-editor</code>.</p>
<p>Installing <code>dconf-editor</code> is easy:</p>
<pre><code># RHEL/CentOS 7 and Fedora 21
yum -y install dconf-editor
# Fedora 22
dnf -y install dconf-editor
</code></pre><p>Open <code>dconf-editor</code> and navigate to <strong>org -&gt; gnome -&gt; desktop -&gt; wm -&gt; preferences</strong>.</p>
<p>Once you&rsquo;re there, look for <em>focus-new-windows</em>. The default setting is <em>smart</em> which will keep new windows in the background and alert you via a notification. If you click on <em>smart</em>, a drop down will appear and you can select <em>strict</em>. That will enable functionality similar to OS X and Windows where new windows will pop up in the front and steal your focus.</p>
<p>The new setting takes effect immediately and there&rsquo;s no need to logout or close and reopen windows.</p>
<p><strong>UPDATE:</strong> If you&rsquo;d like to avoid installing <code>dconf-editor</code>, use Alexander&rsquo;s suggestion below and simply run:</p>
<pre><code>gsettings set org.gnome.desktop.wm.preferences focus-new-windows 'strict'
</code></pre>]]></content></item><item><title>Stumbling into the world of 4K displays [UPDATED]</title><link>https://major.io/2015/06/30/stumbling-into-the-world-of-4k-displays/</link><pubDate>Wed, 01 Jul 2015 04:33:43 +0000</pubDate><guid>https://major.io/2015/06/30/stumbling-into-the-world-of-4k-displays/</guid><description>Woot suckered me into buying a 4K display at a fairly decent price and now I have a Samsung U28D590D sitting on my desk at home. I ordered a mini-DisplayPort to DisplayPort from Amazon and it arrived just before the monitor hit my doorstep. It&amp;rsquo;s time to enter the world of 4K displays.
The unboxing of the monitor was fairly uneventful and it powered up after small amount of assembly. I plugged my mini-DP to DP cable into the monitor and then into my X1 Carbon 3rd gen.</description><content type="html"><![CDATA[<p><img src="/wp-content/uploads/2015/06/U28D590D_display.jpg" alt="1"></p>
<p>Woot <a href="http://www.woot.com/offers/samsung-28-4k-led-backlit-monitor-22">suckered me into buying a 4K display</a> at a fairly decent price and now I have a <a href="http://www.samsung.com/us/computer/monitors/LU28D590DS/ZA">Samsung U28D590D</a> sitting on my desk at home. I ordered a mini-DisplayPort to DisplayPort from Amazon and it arrived just before the monitor hit my doorstep. It&rsquo;s time to enter the world of 4K displays.</p>
<p>The unboxing of the monitor was fairly uneventful and it powered up after small amount of assembly. I plugged my mini-DP to DP cable into the monitor and then into my X1 Carbon 3rd gen. After a bunch of flickering, the display sprang to life but the image looked fuzzy. After some hunting, I found that the resolution wasn&rsquo;t at the monitor&rsquo;s maximum:</p>
<pre><code>$ xrandr -q
DP1 connected 2560x1440+2560+0 (normal left inverted right x axis y axis) 607mm x 345mm
   2560x1440     59.95*+
   1920x1080     60.00    59.94
   1680x1050     59.95
   1600x900      59.98
</code></pre><p>I bought this thing because it does 3840×2160. How confusing. After searching through the monitor settings, I found an option for &ldquo;DisplayPort version&rdquo;. It was set to version 1.1 but version 1.2 was available. I selected version 1.2 (which appears to come with something called HBR2) and then the display flickered for 5-10 seconds. There was no image on the display.</p>
<p>I adjusted GNOME&rsquo;s Display settings back down to 2560×1440. The display sprang back to life, but it was fuzzy again. I pushed the settings back up to 3840×2160. The flickering came back and the monitor went to sleep.</p>
<p>My laptop has an HDMI port and I gave that a try. I had a 3840×2160 display up immediately! Hooray! But wait - that resolution runs at 30Hz over <a href="https://en.wikipedia.org/wiki/HDMI#Version_1.4">HDMI 1.4</a>. HDMI 2.0 promises faster refresh rates but neither my laptop or the display support it. After trying to use the display at max resolution with a 30Hz refresh rate, I realized that it wasn&rsquo;t going to work.</p>
<p>The adventure went on and I joined #intel-gfx on Freenode. This is apparently a common problem with many onboard graphics chips as many of them cannot support a 4K display at 60Hz. It turns out that the i5-5300U (that&rsquo;s a Broadwell) <a href="http://ark.intel.com/products/85213/Intel-Core-i5-5300U-Processor-3M-Cache-up-to-2_90-GHz">can do it</a>.</p>
<p>One of the knowledgeable folks in the channel <a href="https://gist.github.com/ValdikSS/175f0f89d40b8689c0eb">suggested a new modeline</a>. That had no effect. The monitor flickered and went back to sleep as it did before.</p>
<p>I picked up some education on the <a href="https://community.amd.com/community/gaming/blog/2015/05/12/celebrating-a-new-generation-of-ultrahd-displays">difference between SST and MST displays</a>. MST displays essentially have two chips handling half of the display within the monitor. Both of those do the work to drive the entire display. SST monitors (the newer variety, like the one I bought) take a single stream and one single chip in the monitor figures out how to display the content.</p>
<p>At this point, I&rsquo;m stuck with a non-working display at 4K resolution over DisplayPort. I can get lower resolutions working via DisplayPort, but that&rsquo;s not ideal. 4K works over HDMI, but only at 30Hz. Again, not ideal. I&rsquo;ll do my best to update this post as I come up with some other ideas.</p>
<p><strong>UPDATE 2015-07-01:</strong> Thanks to Sandro Mathys for spotting a potential fix:</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>I <a href="http://support.lenovo.com/us/en/products/laptops-and-netbooks/thinkpad-x-series-laptops/thinkpad-x1-carbon-20bs-20bt/downloads/DS101953">found BIOS 1.08 waiting for me</a> on Lenovo&rsquo;s site. One of the last items fixed in the release notes was:</p>
<blockquote>
<p>(New) Supported the 60Hz refresh rate of 4K (3840 x 2160) resolution monitor.</p>
</blockquote>
<p>After a quick flash of a USB stick and a reboot to update the BIOS, the monitor sprang to life after logging into GNOME. It looks amazing! The graphics performance is still not amazing (but hey, this is Broadwell graphics we&rsquo;re talking about) but it does 3840×2160 at 60Hz without a hiccup. I tried unplugging and replugging the DisplayPort cable several times and it never flickered.</p>
]]></content></item><item><title>Fedora 22 and rotating GNOME wallpaper with systemd timers</title><link>https://major.io/2015/06/23/fedora-22-and-rotating-gnome-wallpaper-with-systemd-timers/</link><pubDate>Tue, 23 Jun 2015 17:25:02 +0000</pubDate><guid>https://major.io/2015/06/23/fedora-22-and-rotating-gnome-wallpaper-with-systemd-timers/</guid><description>My older post about rotating GNOME&amp;rsquo;s wallpaper with systemd timers doesn&amp;rsquo;t seem to work in Fedora 22. The DISPLAY=:0 environment variable isn&amp;rsquo;t sufficient to allow systemd to use gsettings.
Instead, the script run by the systemd timer must know a little bit more about dbus. More specifically, the script needs to know the address of the dbus session so it can communicate on the bus. That&amp;rsquo;s normally kept within the DBUS_SESSION_BUS_ADDRESS environment variable.</description><content type="html"><![CDATA[<p>My <a href="/2015/02/11/rotate-gnome-3s-wallpaper-systemd-user-units-timers/">older post</a> about rotating GNOME&rsquo;s wallpaper with systemd timers doesn&rsquo;t seem to work in Fedora 22. The <code>DISPLAY=:0</code> environment variable isn&rsquo;t sufficient to allow systemd to use <code>gsettings</code>.</p>
<p>Instead, the script run by the systemd timer must know a little bit more about dbus. More specifically, the script needs to know the address of the dbus session so it can communicate on the bus. That&rsquo;s normally kept within the <code>DBUS_SESSION_BUS_ADDRESS</code> environment variable.</p>
<p>Open a shell and you can verify that yours is set:</p>
<pre><code>$ env | grep ^DBUS_SESSION
DBUS_SESSION_BUS_ADDRESS=unix:abstract=/tmp/dbus-xxxxxxxxxx,guid=fa6ff8ded93c1df77eba3fxxxxxxxxxx
</code></pre><p>That is actually set when <code>gnome-session</code> starts as your user on your machine. for the script to work, we need to add a few lines at the top:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#719e07">#!/bin/bash
</span><span style="color:#719e07"></span>
<span style="color:#586e75"># These three lines are new</span>
<span style="color:#268bd2">USER</span><span style="color:#719e07">=</span><span style="color:#719e07">$(</span>whoami<span style="color:#719e07">)</span>
<span style="color:#268bd2">PID</span><span style="color:#719e07">=</span><span style="color:#719e07">$(</span>pgrep -u <span style="color:#268bd2">$USER</span> gnome-session<span style="color:#719e07">)</span>
<span style="color:#b58900">export</span> <span style="color:#268bd2">DBUS_SESSION_BUS_ADDRESS</span><span style="color:#719e07">=</span><span style="color:#719e07">$(</span>grep -z DBUS_SESSION_BUS_ADDRESS /proc/<span style="color:#268bd2">$PID</span>/environ|cut -d<span style="color:#719e07">=</span> -f2-<span style="color:#719e07">)</span>

<span style="color:#586e75"># These three lines are unchanged from the original script</span>
<span style="color:#268bd2">walls_dir</span><span style="color:#719e07">=</span><span style="color:#268bd2">$HOME</span>/Pictures/Wallpapers
<span style="color:#268bd2">selection</span><span style="color:#719e07">=</span><span style="color:#719e07">$(</span>find <span style="color:#268bd2">$walls_dir</span> -type f -name <span style="color:#2aa198">&#34;*.jpg&#34;</span> -o -name <span style="color:#2aa198">&#34;*.png&#34;</span> | shuf -n1<span style="color:#719e07">)</span>
gsettings <span style="color:#b58900">set</span> org.gnome.desktop.background picture-uri <span style="color:#2aa198">&#34;file://</span><span style="color:#268bd2">$selection</span><span style="color:#2aa198">&#34;</span>
</code></pre></div><p>Let&rsquo;s look at what the script is doing:</p>
<ul>
<li>First, we get the username of the user running the script</li>
<li>We look for the gnome-session process that is running as that user</li>
<li>We pull out the dbus environment variable from gnome-session&rsquo;s environment variables when it was first started</li>
</ul>
<p>Go ahead and adjust your script. Once you&rsquo;re done, test it by simply running the script manually and then using systemd to run it:</p>
<pre><code>$ bash ~/bin/rotate_bg.sh
$ systemctl --user start gnome-background-change
</code></pre><p>Both of those commands should now rotate your GNOME wallpaper in Fedora 22.</p>
]]></content></item><item><title>Book Review: Linux Kernel Development</title><link>https://major.io/2015/06/21/book-review-linux-kernel-development/</link><pubDate>Sun, 21 Jun 2015 15:26:54 +0000</pubDate><guid>https://major.io/2015/06/21/book-review-linux-kernel-development/</guid><description>I picked up a copy of Robert Love&amp;rsquo;s book, Linux Kernel Development, earlier this year and I&amp;rsquo;ve worked my way through it over the past several weeks. A few people recommended the book to me on Twitter and I&amp;rsquo;m so glad they did. This book totally changed how I look at a system running Linux.
You must be this tall to ride this ride I&amp;rsquo;ve never had formal education in computer science or software development in the past.</description><content type="html"><![CDATA[<p><a href="/wp-content/uploads/2015/06/linux_kernel_development_cover.jpg"><!-- raw HTML omitted --></a>I picked up a copy of <a href="https://www.rlove.org/">Robert Love&rsquo;s</a> book, <a href="http://www.informit.com/store/linux-kernel-development-9780672329463">Linux Kernel Development</a>, earlier this year and I&rsquo;ve worked my way through it over the past several weeks. A few people recommended the book to me on Twitter and I&rsquo;m so glad they did. This book totally changed how I look at a system running Linux.</p>
<h2 id="you-must-be-this-tall-to-ride-this-ride">You must be this tall to ride this ride</h2>
<p>I&rsquo;ve never had formal education in computer science or software development in the past. After all, my degree was in Biology and I was on the path to becoming a phyisician when this other extremely rewarding career came into play. <em>(That&rsquo;s a whole separate blog post in itself.)</em></p>
<p>Just to level-set: I can read C and make small patches when I spot problems. However, I&rsquo;ve never set out and started a project in C on my own and I haven&rsquo;t really made any large contributions to projects written in C. However, I&rsquo;m well-versed in Perl, Ruby, and Python mainly from job experience and leaning on some much more skilled colleagues.</p>
<p>The book recommends that you have a basic grasp of C and some knowledge around memory management and process handling. I found that I was able to fully understand about 70% of the book immediately, another 20% or so required some additional research and practice, while about 10% was mind-blowing. Obviously, that leaves me with plenty of room to grow.</p>
<p>Honestly, if you understand how most kernel tunables work and you know at least one language that runs on your average Linux box, you should be able to understand the majority of the material. Some sections might require some re-reading and you might need to go back and read a section when a later chapter sheds more light on the subject.</p>
<h2 id="moving-through-the-content">Moving through the content</h2>
<p>I won&rsquo;t go into a lot of detail around the content itself other than to say it&rsquo;s extremely comprehensive. After all, you wouldn&rsquo;t be reading a book about something as complex as the Linux kernel if you weren&rsquo;t ready for an onslaught of information.</p>
<p>The information is organized in an effective way. Initial concepts are familiar to someone who has worked in user space for quite some time. If you&rsquo;ve dealt with oom-killer, loaded kernel modules, or written some horrible code that later needed to be optimized, you&rsquo;ll find the beginning of the book to be very useful. Robert draws plenty of distinctions around kernel space, user space, and how they interact. He take special care to cover SMP-safe code and how to take non-SMP-safe code and improve it.</p>
<p>I found a ton of value in the memory management, locking, and the I/O chapters. I didn&rsquo;t fully understand the blocks of C code within the text but there was a ton of value in the deep explanations of how data flows (and doesn&rsquo;t flow) from memory to disk and back again.</p>
<h2 id="the-best-part">The best part</h2>
<p>If I had to pick one thing to entice more people to read the book, it would be the way Robert explains every concept in the book. He has a good formula that helps you understand the how, the what, <strong>and</strong> the why. So many books forget the <em>why</em>.</p>
<p>He takes the time to explain what frustrated the kernel developers that made them write a feature in the first place and then goes into detail about how they fixed it. He also talks about differences between other operating systems (like Unix, Windows, and others) and other hardware types (like ARM and Alpha). So many books leave this part out but it&rsquo;s often critical for understanding difficult topics. I learned this the hard way in my biology classes when I tried to memorize concepts rather than trying to understand the evolutionary or chemical reasons for why it occurs.</p>
<p>Robert also rounds out the book with plenty of debugging tips that allow readers to trudge through bug hunts with better chances of success. He helps open the doors to the Linux kernel community and gives tips on how to get the best interactions from the community.</p>
<h2 id="wrap-up">Wrap-up</h2>
<p>This book is worth it for anyone who wants to learn more about how their Linux systems operate or who want to actually write code for the kernel. Much of the deep workings of the kernel was a mystery to me before and I really only knew how to interact with a few interfaces.</p>
<p>Reading this book was like watching a cover being taken off of a big machine and listening to an expert explain how it works. It&rsquo;s definitely worth reading.</p>
]]></content></item><item><title>Improving LXC template security</title><link>https://major.io/2015/06/18/improving-lxc-template-security/</link><pubDate>Thu, 18 Jun 2015 19:52:11 +0000</pubDate><guid>https://major.io/2015/06/18/improving-lxc-template-security/</guid><description>I&amp;rsquo;ve been getting involved with the Fedora Security Team lately and we&amp;rsquo;re working as a group to crush security bugs that affect Fedora, CentOS (via EPEL) and Red Hat Enterprise Linux (via EPEL). During some of this work, I stumbled upon a group of Red Hat Bugzilla tickets talking about LXC template security.
The gist of the problem is that there&amp;rsquo;s a wide variance in how users and user credentials are handled by the different LXC templates.</description><content type="html"><![CDATA[<p><a href="/wp-content/uploads/2015/06/containers.png"><!-- raw HTML omitted --></a>I&rsquo;ve been getting involved with the <a href="https://fedoraproject.org/wiki/Security_Team">Fedora Security Team</a> lately and we&rsquo;re working as a group to crush security bugs that affect Fedora, CentOS (via EPEL) and Red Hat Enterprise Linux (via EPEL). During some of this work, I stumbled upon a <a href="https://bugzilla.redhat.com/show_bug.cgi?id=1132001">group of Red Hat Bugzilla tickets</a> talking about LXC template security.</p>
<p>The gist of the problem is that there&rsquo;s a wide variance in how users and user credentials are handled by the different LXC templates. An <a href="https://fedoraproject.org/wiki/LXC_Template_Security_Improvements">inventory of the current situation</a> revealed some horrifying problems with many OS templates.</p>
<p>Many of the templates set an <strong>awful</strong> default root password, like rooter, toor, or root. Some of the others create a regular user with sudo privileges and give it a default, predictable password unless the user specifies otherwise.</p>
<p>There are some bright spots, though. Fedora and CentOS templates will accept a root password from the user during the build and set a randomized password for the root user if a password isn&rsquo;t specified. Ubuntu Cloud takes another approach by locking out the root user and requiring cloud-init configuration data to configure the root account.</p>
<p>I kicked off a <a href="https://lists.linuxcontainers.org/pipermail/lxc-devel/2015-June/011883.html">mailing list thread</a> and wrote a <a href="https://github.com/lxc/lxc/pull/574">terrible pull request</a> to get things underway. Stéphane Graber requested that all templates use a shared script to handle users and credentials via standardized environment variables and command line arguments. In addition, all passwords for users (regular or root) should be empty with password-less logins disabled. Those are some reasonable requests and I&rsquo;m working on a shell script that&rsquo;s easy to import into LXC templates.</p>
<p>There&rsquo;s also a push to remove sshd from all LXC templates by default, but I&rsquo;m hoping to keep that one tabled until the credentials issue is solved.</p>
<p>If you&rsquo;d like to help out with the effort, let me know! I&rsquo;ll probably get some code up onto Github soon and as for comments.</p>
]]></content></item><item><title>Time for a new GPG key</title><link>https://major.io/2015/06/11/time-for-a-new-gpg-key/</link><pubDate>Thu, 11 Jun 2015 19:14:03 +0000</pubDate><guid>https://major.io/2015/06/11/time-for-a-new-gpg-key/</guid><description>After an unfortunate death of my Yubikey NEO and a huge mistake on backups, I&amp;rsquo;ve come to realize that it&amp;rsquo;s time for a new GPG key. My new one is already up on Keybase and there&amp;rsquo;s a plain text copy on my resume site.
Action required If you&amp;rsquo;re using a key for me with a fingerprint of 6DC99178, that one is no longer valid. My new one is C1011FB1.
For the impatient, here&amp;rsquo;s the easiest way to retrieve my new key:</description><content type="html"><![CDATA[<p><img src="/wp-content/uploads/2015/06/YubiKey-NEO-finger.jpg" alt="1"></p>
<p>After an unfortunate death of my <a href="https://www.yubico.com/products/yubikey-hardware/yubikey-neo/">Yubikey NEO</a> and a huge mistake on backups, I&rsquo;ve come to realize that it&rsquo;s time for a new GPG key. My new one is already up on <a href="https://keybase.io/mhayden">Keybase</a> and there&rsquo;s a <a href="http://majorhayden.com/pgp.txt">plain text copy on my resume site</a>.</p>
<h3 id="action-required">Action required</h3>
<p>If you&rsquo;re using a key for me with a fingerprint of <code>6DC99178</code>, that one is no longer valid. My new one is <code>C1011FB1</code>.</p>
<p>For the impatient, here&rsquo;s the easiest way to retrieve my new key:</p>
<pre><code>gpg2 --keyserver pgp.mit.edu --recv-key C1011FB1
</code></pre><h3 id="lessons-learned">Lessons learned</h3>
<p>Always ensure that you have complete backups of <strong>all of your keys</strong>. I made a mistake and forgot to back up my original signing subkey before I moved that key to my Yubikey. When the NEO died, so did the last copy of the most important subkey. It goes without saying but I don&rsquo;t plan on making that mistake again.</p>
<p>Always make a full backup of all keys and make a revocation certificate that also gets backed up. There&rsquo;s a <a href="https://alexcabal.com/creating-the-perfect-gpg-keypair/">good guide on this topic</a> if you&rsquo;re new to the process.</p>
<h3 id="wait-a-yubikey-stopped-working">Wait. A Yubikey stopped working?</h3>
<p>This is the first Yubikey failure that I&rsquo;ve ever experienced. I&rsquo;ve had two regular Yubikeys that are still working but this is my first NEO.</p>
<p>I emailed Yubico support earlier today about the problem and received an email back within 10-15 minutes. They offered me a replacement NEO with free shipping. It&rsquo;s still a bummer about the failure but at least they worked quickly to get me a free replacement.</p>
]]></content></item><item><title>Chrome 43 stuck in HiDPI mode</title><link>https://major.io/2015/06/10/chrome-43-stuck-in-hidpi-mode/</link><pubDate>Wed, 10 Jun 2015 12:36:03 +0000</pubDate><guid>https://major.io/2015/06/10/chrome-43-stuck-in-hidpi-mode/</guid><description>I ran some package updates last night and ended up with a new version of Google Chrome from the stable branch. After restarting Chrome, everything in the interface was huge. The icons in the bookmark bar, the text, the padding - all of it looked enormous.
After a little searching, I found a helpful line in the ArchLinux HiDPI documentation:
Full HiDPI support in Chrome is now available in the main branch google-chromeAUR as of version 43.</description><content type="html"><![CDATA[<p><a href="/wp-content/uploads/2015/06/Google_Chrome_icon_2011.png"><!-- raw HTML omitted --></a>I ran some package updates last night and ended up with a new version of Google Chrome from the stable branch. After restarting Chrome, everything in the interface was <strong>huge</strong>. The icons in the bookmark bar, the text, the padding - all of it looked enormous.</p>
<p>After a little searching, I found a <a href="https://wiki.archlinux.org/index.php/HiDPI#Chromium_.2F_Google_Chrome">helpful line</a> in the ArchLinux HiDPI documentation:</p>
<blockquote>
<p>Full HiDPI support in Chrome is now available in the main branch google-chromeAUR as of version 43.0.2357.2-1 and works out of the box as tested with Gnome and Cinnamon.</p>
</blockquote>
<p>It looks like there was a <a href="https://plus.google.com/+CraigTumblison/posts/NtW36w6yxiq">flag available</a> for quite some time to test the feature but it disappeared sometime in March. I scoured my list of flags as well as my Chrome configuration directories and couldn&rsquo;t find any trace of it.</p>
<h3 id="temporary-workaround">Temporary Workaround</h3>
<p>While I search for a fix, my current workaround is to manually edit the <code>.desktop</code> file that comes with the Chrome RPM. On my Fedora system, that file is <code>/usr/share/applications/google-chrome.desktop</code>. If you open that file, look for a line that starts with <code>Exec</code>:</p>
<pre><code>Exec=/usr/bin/google-chrome-stable %U
</code></pre><p>Change that line so that it includes <code>--force-device-scale-factor=1</code> to disable HiDPI support:</p>
<pre><code>Exec=/usr/bin/google-chrome-stable --force-device-scale-factor=1 %U
</code></pre><p>Depending on your display manager, you might need to do something to refresh the <code>.desktop</code> files. If you&rsquo;re using GNOME 3, just press Alt-F2, type <code>r</code>, and press enter. Your screen will flicker a lot and GNOME will restart in place. Try starting Chrome once more and you should be back to normal.</p>
<h3 id="still-having-problems">Still having problems?</h3>
<p>If you don&rsquo;t see a change after doing all of that, ensure that you <em>fully</em> exited Chrome. Depending on your configuration, Chrome might still be running in your taskbar even if you close all of the browser windows. If that&rsquo;s the case, completely exit Chrome using the taskbar menu or <code>pkill -f google-chrome</code>. Start Chrome again and you should be all set.</p>
]]></content></item><item><title>cups.service start operation timed out in Fedora 22</title><link>https://major.io/2015/06/09/cups-service-start-operation-timed-out-in-fedora-22/</link><pubDate>Tue, 09 Jun 2015 14:35:48 +0000</pubDate><guid>https://major.io/2015/06/09/cups-service-start-operation-timed-out-in-fedora-22/</guid><description>Applications on my Fedora 22 system kept stalling when I attempted to print. My system journal was full of these log messages:
systemd[1]: cups.service start operation timed out. Terminating. systemd[1]: Failed to start CUPS Scheduler. systemd[1]: Unit cups.service entered failed state. systemd[1]: cups.service failed. audit[1]: &amp;lt;audit-1130&amp;gt; pid=1 uid=0 auid=4294967295 ses=4294967295 subj=system_u:system_r:init_t:s0 msg='unit=cups comm=&amp;quot;systemd&amp;quot; exe=&amp;quot;/usr/lib/systemd/systemd&amp;quot; hostname=? addr=? terminal=? res=failed' If I tried to run systemctl start cups, the command would hang for quite a while and then fail.</description><content type="html"><![CDATA[<p>Applications on my Fedora 22 system kept stalling when I attempted to print. My system journal was full of these log messages:</p>
<pre><code>systemd[1]: cups.service start operation timed out. Terminating.
systemd[1]: Failed to start CUPS Scheduler.
systemd[1]: Unit cups.service entered failed state.
systemd[1]: cups.service failed.
audit[1]: &lt;audit-1130&gt; pid=1 uid=0 auid=4294967295 ses=4294967295 subj=system_u:system_r:init_t:s0 msg='unit=cups comm=&quot;systemd&quot; exe=&quot;/usr/lib/systemd/systemd&quot; hostname=? addr=? terminal=? res=failed'
</code></pre><p>If I tried to run <code>systemctl start cups</code>, the command would hang for quite a while and then fail. I broke out <a href="http://linux.die.net/man/1/strace">strace</a> and tried to figure out what was going wrong.</p>
<p>The strace output showed that cups was talking to my local DNS servers and was asking constantly for the IP address of my laptop&rsquo;s hostname.</p>
<p><img src="/wp-content/uploads/2015/06/iWKad22.jpg" alt="2"></p>
<p>Oh, I felt pretty stupid at this point.</p>
<p>I added my laptop&rsquo;s hostname onto the line starting with <code>127.0.0.1</code> in my <code>/etc/hosts</code> and tried to start cups once more. It started up in less than a second and is now working well.</p>
]]></content></item><item><title>PulseAudio popping with multiple sounds in Fedora 22</title><link>https://major.io/2015/06/08/pulseaudio-popping-with-multiple-sounds-in-fedora-22/</link><pubDate>Mon, 08 Jun 2015 13:37:24 +0000</pubDate><guid>https://major.io/2015/06/08/pulseaudio-popping-with-multiple-sounds-in-fedora-22/</guid><description>My transition from Fedora 21 to 22 on the ThinkPad X1 Carbon was fairly uneventful even with over 2,400 packages involved in the upgrade. The only problem I dealt with on reboot was that my icons on the GNOME 3 desktop were way too large. That&amp;rsquo;s a pretty easy problem to fix.
However, something else cropped up after a while. I started listening to music in Chrome and a Pidgin notification sound came through.</description><content type="html"><![CDATA[<p><img src="/wp-content/uploads/2015/06/8346700794_a9c0475bd8_b-e1433770480154.jpg" alt="1"></p>
<p>My transition from Fedora 21 to 22 on the ThinkPad X1 Carbon was fairly uneventful even with over 2,400 packages involved in the upgrade. The only problem I dealt with on reboot was that my icons on the GNOME 3 desktop were way too large. That&rsquo;s a pretty easy problem to fix.</p>
<p>However, something else cropped up after a while. I started listening to music in Chrome and a Pidgin notification sound came through. There was a quiet pop before the Pidgin sound and a loud pop on the end. Thunderbird&rsquo;s notifications sounded the same. The pops at the end of the sound were sometimes <strong>very</strong> loud and hurt my ears.</p>
<p>I started running PulseAudio in debug mode within a terminal:</p>
<pre><code>pulseaudio -k
pulseaudio --start
</code></pre><p>There were some messages about buffer underruns and latency issues but they were all very minimal. I loaded up <a href="http://freedesktop.org/software/pulseaudio/pavucontrol/">pavucontrol</a> and couldn&rsquo;t find anything unusual when multiple sounds played. I gave <a href="http://0pointer.de/lennart/projects/pavumeter/">pavumeter</a> a try and found something very interesting.</p>
<p>When Chrome was playing audio, the meters in pavumeter were at 80-90%. That seems to make sense because I keep Chrome as one of the loudest applications on my laptop. My logic there is that I don&rsquo;t want to get blasted by a notification tone that is drastically louder than my music.</p>
<p>However, if I received a Pidgin or Thunderbird notification while Chrome was playing music, the pavumeter showed the volume levels dropping to 30% or less. As soon as the sound was over, the meters snapped back to 80-90% and there was a big popping sound. I lowered Chrome&rsquo;s volume so that it showed up at the 30% level in pavumeter and forced a new Pidgin notification sound - the pops were still there.</p>
<p>I started searching in Google and stumbled upon <a href="https://wiki.archlinux.org/index.php/PulseAudio#Configuration_files">ArchLinux&rsquo;s PulseAudio documentation</a>. (Their documentation is really awesome.) There&rsquo;s a mention of the <em>flat-volumes</em> PulseAudio configuration option. If it&rsquo;s set to <em>no</em>, you get the older ALSA functionality where volumes can be set independently per application. The default is <em>yes</em> and that default comes with a warning in the documentation:</p>
<blockquote>
<p>Warning: The default behavior can sometimes be confusing and some applications, unaware of this feature, can set their volume to 100% at startup, potentially blowing your speakers or your ears. To restore the classic (ALSA) behavior set this to no.</p>
</blockquote>
<p>As a test, I switched <em>flat-volumes</em> to <em>no</em> in <code>/etc/pulse/daemon.conf</code>. I restarted PulseAudio with the new setting:</p>
<pre><code>pulseaudio -k
pulseaudio --start
</code></pre><p>I started music in Chrome and sent myself an IM in Pidgin. No pops! An email came through and Thunderbird and a notification sound played. No pops there, either!</p>
<p>GNOME 3 was a bit unhappy at my PulseAudio tinkering and the volume control disappeared from the menu at the top right. I logged out of my GNOME session and logged back in to find the volume control working again.</p>
<p><em>Photo Credit: <a href="https://www.flickr.com/photos/75638411@N05/8346700794/">Our Thrift Apt.</a> via <a href="http://compfight.com">Compfight</a> <a href="https://www.flickr.com/help/general/#147">cc</a></em></p>
]]></content></item><item><title>Upatre and icanhazip</title><link>https://major.io/2015/06/04/upatre-and-icanhazip/</link><pubDate>Fri, 05 Jun 2015 02:01:19 +0000</pubDate><guid>https://major.io/2015/06/04/upatre-and-icanhazip/</guid><description>I recently updated the icanhazip FAQ about the resurgence of the Upatre malware and how it&amp;rsquo;s abusing icanhazip.com. The abuse reports keep coming into the ISP&amp;rsquo;s where I host the site and it&amp;rsquo;s becoming a challenge to defend each one.
From what I&amp;rsquo;ve read, Upatre is a piece of malware that has been around in one form or another since 2013. Somewhere along the way, it began making calls to icanhazip.</description><content type="html"><![CDATA[<p><a href="/wp-content/uploads/2015/06/cant_have_nice_things.jpg"><!-- raw HTML omitted --></a>I recently updated the <a href="/icanhazip-com-faq/">icanhazip FAQ</a> about the resurgence of the <a href="http://www.trendmicro.com/vinfo/us/threat-encyclopedia/malware/upatre">Upatre</a> malware and how it&rsquo;s abusing icanhazip.com. The abuse reports keep coming into the ISP&rsquo;s where I host the site and it&rsquo;s becoming a challenge to defend each one.</p>
<p>From what I&rsquo;ve read, Upatre is a piece of malware that has been around in one form or another since 2013. Somewhere along the way, it began making calls to icanhazip.com to determine the public-facing IP address of the machines that it infects. I&rsquo;m sure this was done by the malware authors to figure out which kinds of targets they hit. If they know the external IP address, they can easily figure out how valuable the target may be.</p>
<p>The information security community has been really helpful and I&rsquo;ve received emails from several people with ways to identify the malicious requests and deny them. The malware changes over time and the most recent updates mimic the requests made by very recent versions of Firefox on Windows. Separating those requests out from the legitimate ones is extremely difficult.</p>
<p>I&rsquo;d like to explore some ways to provide sanitized log data from icanhazip to certain security organizations so they can find trends and help more people stomp out this highly annoying piece of malware (among others).</p>
<p>If you have any feedback on how this might be done, let me know. Also, if you think it&rsquo;s a horrible idea, let me know as well.</p>
]]></content></item><item><title>Adventures with GRE and IPSec on Mikrotik routers</title><link>https://major.io/2015/05/27/adventures-with-gre-and-ipsec-on-mikrotik-routers/</link><pubDate>Wed, 27 May 2015 13:46:28 +0000</pubDate><guid>https://major.io/2015/05/27/adventures-with-gre-and-ipsec-on-mikrotik-routers/</guid><description>I recently picked up a RB850GX2 from my favorite Mikrotik retailer, r0c-n0c. It&amp;rsquo;s a dual-core PowerPC board with five ethernet ports and some decent performance for the price.
I still have the RB493G in a colocation and I usually connect my home and the colo via OpenVPN or IPSec. Networking is not one of my best skills and I&amp;rsquo;m always looking to learn more about it when I can. I decided to try out a GRE tunnel on top of IPSec this time around.</description><content type="html"><![CDATA[<p><img src="https://major.io/wp-content/uploads/2015/05/mikrotik-routerboard-rb8_6221.jpg" alt="1"></p>
<p>I recently picked up a <a href="http://routerboard.com/RB850Gx2">RB850GX2</a> from my favorite Mikrotik retailer, <a href="https://www.roc-noc.com/mikrotik/routerboard/RB850Gx2.html">r0c-n0c</a>. It&rsquo;s a dual-core PowerPC board with five ethernet ports and some decent performance for the price.</p>
<p>I still have the RB493G in a colocation and I usually connect my home and the colo via OpenVPN or IPSec. Networking is not one of my best skills and I&rsquo;m always looking to learn more about it when I can. I decided to try out a GRE tunnel on top of IPSec this time around. Combining GRE and IPSec allows you to simplify connectivity between two network segments through an encrypted tunnel.</p>
<h2 id="the-setup">The Setup</h2>
<p>The LAN in my colo and at home is fairly simple: a /24 of RFC1918 space behind a Mikrotik doing NAT. My goal was to get a tunnel up between both environments so that I could reach devices behind my colo firewall from home and vice versa. I do plenty of ssh back and forth along with backups from time to time.</p>
<p>In this example, here&rsquo;s the current network configuration:</p>
<ul>
<li>Home: 192.168.50.0/24 on the LAN, 1.1.1.1 as the public IP</li>
<li>Colo: 192.168.150.0/24 on the LAN, 2.2.2.2 as the public IP</li>
</ul>
<p>I want devices on 192.168.50.0/24 to talk to 192.168.150.0/24 and vice versa. Let&rsquo;s get the GRE tunnel up first.</p>
<h2 id="gre">GRE</h2>
<p>Plain GRE tunnels aren&rsquo;t encrypted, but I prefer to set them up first to test connectivity prior to adding IPSec into the mix. IPSec can be a challenge to configure the first time around.</p>
<p>I&rsquo;ll first create a GRE interface at home:</p>
<pre><code>/interface gre
add !keepalive local-address=1.1.1.1 name=home-to-colo remote-address=2.2.2.2
</code></pre><p>We&rsquo;ll do the same on the colo router:</p>
<pre><code>/interface gre
add !keepalive local-address=2.2.2.2 name=colo-to-home remote-address=1.1.1.1
</code></pre><p>You can check to see if the GRE tunnel is running from either router:</p>
<pre><code>/interface gre print
</code></pre><p>Look for the <strong>R</strong> in the flags column.</p>
<p>If you&rsquo;ve made it this far, you now have a GRE tunnel configured but we can&rsquo;t pass any traffic across it yet. We need to add some IP&rsquo;s to both sides and configure some routes.</p>
<h2 id="ips-and-routes">IP&rsquo;s and Routes</h2>
<p>You have some freedom here to choose the IP addresses for both ends of your tunnel but don&rsquo;t choose anything that interferes with your current LAN IP addresses. In my case, I&rsquo;ll choose 10.10.10.1/30 and 10.10.10.2/30 for both ends of the tunnel.</p>
<p>I&rsquo;ll give the 10.10.10.2 address to the home firewall:</p>
<pre><code>/ip address
add address=10.10.10.2/30 interface=home-to-colo network=10.10.10.0
</code></pre><p>And I&rsquo;ll give the 10.10.10.1 address to the colo firewall:</p>
<pre><code>/ip address
add address=10.10.10.1/30 interface=colo-to-home network=10.10.10.0
</code></pre><p>At this point, systems at home can ping 10.10.10.1 (the colo router&rsquo;s GRE tunnel endpoint) and systems at the colo can ping 10.10.10.2 (the home router&rsquo;s GRE tunnel endpoint). That&rsquo;s great because we will use these IP&rsquo;s to route our LAN traffic across the tunnel.</p>
<p>We need to tell the home router how to get traffic from its LAN over to the colo LAN and vice versa. We can do that with the tunnel endpoints we just configured.</p>
<p>Let&rsquo;s tell the home router to use the colo router&rsquo;s GRE tunnel endpoint to reach the colo LAN:</p>
<pre><code>/ip route
add distance=1 dst-address=192.168.150.0/24 gateway=home-to-colo
</code></pre><p>And tell the colo router to use the home router&rsquo;s GRE endpoint to reach the home LAN:</p>
<pre><code>/ip route
add distance=1 dst-address=192.168.50.0/24 gateway=colo-to-home
</code></pre><p>We don&rsquo;t have to tell the router about the tunnel&rsquo;s IP address since those routes are generated automatically when we added the IP addresses to each side of the GRE tunnel.</p>
<p>If you&rsquo;ve made it this far, systems in your home LAN should be able to ping the colo LAN and vice versa. If not, go back and double-check your IP addresses on both sides of the tunnel and your routes.</p>
<h2 id="adding-ipsec">Adding IPSec</h2>
<p><strong>BEFORE YOU GO ANY FURTHER, ensure you have some sort of out-of-band access to both routers.</strong> If you make a big mistake like I did (more on that later), you&rsquo;re going to be glad you set up another way to reach your devices!</p>
<p>We have an GRE tunnel without encryption already and that&rsquo;s allowing us to pass traffic. That&rsquo;s fine, but it&rsquo;s not terribly secure to send our packets in that tunnel across a hostile internet. IPSec will allow us to tell both routers that we want packets between the public IP addresses of both routers to be encrypted. The GRE tunnel will take care of actually delivering the packets, however. IPSec isn&rsquo;t an interface and it can&rsquo;t be a conduit for networking all by itself.</p>
<p><!-- raw HTML omitted -->Have you configured another way to access both routers yet? Seriously, stop now and do that. I mean it.<!-- raw HTML omitted --></p>
<p>If you have native IPv6 access (not a IPv6 over IPv4 tunnel!) into each device, that can be a viable backup plan. Another option might be serial cables or a dedicated console connection. You&rsquo;ll thank me later.</p>
<p>Configuring IPSec is done in three chunks:</p>
<ul>
<li>Make a proposal: both routers must agree on how to authenticate each other and encrypt traffic</li>
<li>Configure a peer list: both routers need to know how to reach each other and have some shared secrets</li>
<li>Set a policy: both routers need to agree on which packets must be encrypted</li>
</ul>
<p>We will start with the proposal. The defaults are good for both routers. Add this configuration <strong>on both devices</strong>:</p>
<pre><code>/ip ipsec proposal
set [ find default=yes ] auth-algorithms=md5 enc-algorithms=aes-128-cbc,twofish
</code></pre><p>Now our routers agree on what methods they&rsquo;ll use to encrypt traffic. Feel free to adjust these algorithms later if needed. Let&rsquo;s tell each router about its peer.</p>
<p>At home:</p>
<pre><code>/ip ipsec peer
add address=2.2.2.2/32 nat-traversal=no secret=letshavefunwithipsec
</code></pre><p>At the colo:</p>
<pre><code>/ip ipsec peer
add address=1.1.1.1/32 nat-traversal=no secret=letshavefunwithipsec
</code></pre><p>Both routers now know about each other and they both have the same shared secret (please use a better shared secret in production). All we have left is configuring a policy.</p>
<p>At this point, ensure you&rsquo;re accessing both routers via an out-of-band method (native IPv6, console, serial, etc). <strong>YOU ARE ABOUT TO LOSE CONNECTIVITY TO YOUR REMOTE DEVICE.</strong></p>
<p>At home, we set up a policy that says all traffic between the public addresses of both firewalls must be encrypted (GRE will carry the traffic for us). <strong>Ensure that the CIDR portion of the IP address for dst-address/src-address is present!</strong></p>
<pre><code>/ip ipsec policy
add dst-address=2.2.2.2/32 sa-dst-address=2.2.2.2 sa-src-address=1.1.1.1 src-address=1.1.1.1/32 tunnel=yes
</code></pre><p>We will do something similar on the colo side. <strong>Again, ensure that the CIDR portion of the IP address for dst-address/src-address is present!</strong></p>
<pre><code>/ip ipsec policy
add dst-address=1.1.1.1/32 sa-dst-address=1.1.1.1 sa-src-address=2.2.2.2 src-address=2.2.2.2/32 tunnel=yes
</code></pre><p>You should now be able to ping across your GRE tunnel but it&rsquo;s encrypted this time! If you find that one of your devices is inaccessible, don&rsquo;t panic. Disable the policy you just added (<code>set disabled=yes number=[number of your policy]</code>) and review your configuration.</p>
<p>In the policy step, we told both routers that if traffic moves between the <em>src-address</em> and <em>dst-address</em>, we want it encrypted. Also, the <em>sa-src-address</em> and <em>sa-dst-address</em> gives the router a hint to figure out the identity of the peer and what their shared secret is.</p>
<h2 id="checking-our-work">Checking our work</h2>
<p>You can check your work with something like this on the home router:</p>
<pre><code>[major@Home] &gt; /ip ipsec remote-peers print
 0 local-address=1.1.1.1 remote-address=2.2.2.2 state=established side=initiator established=7h17m10s
</code></pre><p>If you have a line like that, your IPSec peers can communicate properly. To test the encryption, you have two options. One option is to put a device outside your firewall and dump traffic via a tap or hub.</p>
<p>Another option (albeit less accurate) is to use the profile tool built into RouterOS. Run the following:</p>
<pre><code>/tool profile
</code></pre><p>You&rsquo;ll see some output showing where the majority of your CPU is consumed. Now, transfer some large files between systems behind both routers. You can use <a href="/2010/03/20/testing-network-throughput-with-iperf/">iperf</a> for this as well if you really want to stress out the network link. When you do that, you should see <strong>encrypting</strong> in the profile output as a very large consumer of the CPU. If you only see something like <strong>gre</strong> or <strong>ethernet</strong> as your top CPU consumers, you may have missed something on your IPSec policy and your traffic is likely not being encrypted. This isn&rsquo;t true for all routers — it depends on your normal workloads.</p>
<h2 id="how-i-made-a-huge-mistake">How I made a huge mistake</h2>
<p>When I was going through this process, I made it through the GRE portion without a hitch. Everything worked well. Once I added IPSec to the mix, I used the GRE tunnel endpoints (10.10.10.1 and 10.10.10.2) as my <em>src-address</em> and <em>dst-address</em> in my IPSec policy. Nothing was getting encrypted and I was getting really frustrated.</p>
<p>I kept reading tutorials on various sites and came to realize that I didn&rsquo;t need an encryption policy between the tunnel endpoints, I needed a policy between the actual public addresses of the routers. I wasn&rsquo;t aware that the GRE tunnel would happily keep working between the two public IP addresses even with the IPSec policy in place between the IP addresses.</p>
<p>First mistake: I didn&rsquo;t access my colo router via an out-of-band path. Second mistake: I applied my IPSec policy on the home router first and was shocked that I lost connectivity to the colo router. That was a quick fix — I just disabled the IPSec policy on the home router and I could access the colo router again.</p>
<p>Just after adjusting the IPSec policy on the colo router to use the public IP addresses, I noticed that connectivity dropped. At this point, I expected that — I set up a policy there but I hadn&rsquo;t done it on the home router yet. I enabled the policy on the home router and then started pinging. Nothing.</p>
<p>Then came the Pingdom and UptimeRobot alerts for my sites in the colo. <strong>Oh crap.</strong></p>
<p><img src="https://major.io/wp-content/uploads/2015/05/ive-made-a-huge-mistake.gif" alt="5"></p>
<p>Once I was able to reach the colo router via IPv6 through some other VM&rsquo;s, I realized what happened. I left the CIDR mask off the <em>src-address</em> and <em>dst-address</em> in the IPSec policy.</p>
<p>Guess what RouterOS chose as a CIDR mask for me? <strong>/0.</strong> Ouch.</p>
<p>I quickly adjusted those to be /32&rsquo;s. Within seconds, everything was up again and the GRE tunnel began working. As the Pingdom alerts cleared and my heart rate returned to normal, I figured the best thing I should do is share my story so that others don&rsquo;t make the same mistake. ;)</p>
]]></content></item><item><title>Xen 4.5 crashes during boot on Fedora 22</title><link>https://major.io/2015/05/27/xen-4-5-crashes-during-boot-on-fedora-22/</link><pubDate>Wed, 27 May 2015 12:33:21 +0000</pubDate><guid>https://major.io/2015/05/27/xen-4-5-crashes-during-boot-on-fedora-22/</guid><description>If you&amp;rsquo;re currently running a Xen hypervisor on a Fedora release before 22, stay put for now.
There&amp;rsquo;s a bug in Xen when you compile it with GCC 5 that will cause your system to get an error during bootup. In my case, I&amp;rsquo;m sometimes getting the crash shortly after the hypervisor to dom0 kernel handoff and sometimes it&amp;rsquo;s happening later in the boot process closer to when I&amp;rsquo;d expect a login screen to appear.</description><content type="html"><![CDATA[<p><a href="https://major.io/wp-content/uploads/2012/06/xen_logo_small.png"><!-- raw HTML omitted --></a>If you&rsquo;re currently running a Xen hypervisor on a Fedora release before 22, <strong>stay put for now</strong>.</p>
<p>There&rsquo;s a bug in Xen when you compile it with GCC 5 that will cause your system to get an error during bootup. In my case, I&rsquo;m sometimes getting the crash shortly after the hypervisor to dom0 kernel handoff and sometimes it&rsquo;s happening later in the boot process closer to when I&rsquo;d expect a login screen to appear.</p>
<p>Here are some helpful links to follow the progress of the fix:</p>
<ul>
<li><a href="https://gist.github.com/major/baa0e2eee7de51a2bcd1">Crash logs from the kernel panic</a></li>
<li><a href="https://bugzilla.redhat.com/show_bug.cgi?id=1219197">Bug 1219197 – Xen BUG at page_alloc.c:1738</a> [Red Hat Bugzilla]</li>
<li><a href="http://bugzilla.xensource.com/bugzilla/show_bug.cgi?id=1908">Bug 1908 – Xen BUG at page_alloc.c:1738</a></li>
<li><a href="http://lists.xen.org/archives/html/xen-devel/2015-05/msg02604.html">xen-devel mailing list thread</a></li>
</ul>
<p>Michael Young found that Xen 4.5.1-rc1 (which has code very similar to 4.5) will compile and boot <a href="http://lists.xen.org/archives/html/xen-devel/2015-05/msg02769.html">if compiled with GCC 4.x in Fedora 21</a>. It&rsquo;s a decent workaround but it&rsquo;s certainly not a long term fix.</p>
<p>I&rsquo;m still doing some additional testing and I&rsquo;ll update this post as soon as there&rsquo;s more information available.</p>
]]></content></item><item><title>You have a problem and icanhazip.com isn’t one of them</title><link>https://major.io/2015/05/20/you-have-a-problem-and-icanhazip-com-isnt-one-of-them/</link><pubDate>Wed, 20 May 2015 12:50:41 +0000</pubDate><guid>https://major.io/2015/05/20/you-have-a-problem-and-icanhazip-com-isnt-one-of-them/</guid><description>I really enjoy operating icanhazip.com and the other domains. It&amp;rsquo;s fun to run some really busy services and find ways to reduce resource consumption and the overall cost of hosting.
My brain has a knack for optimization and improving the site is quite fun for me. So much so that I&amp;rsquo;ve decided to host all of icanhazip.com out of my own pocket starting today.
However, something seriously needs to change.</description><content type="html"><![CDATA[<p><img src="/wp-content/uploads/2015/05/5662811240_d686e98683_b-e1432125864107.jpg" alt="1"></p>
<p>I really enjoy operating <a href="https://icanhazip.com/">icanhazip.com</a> and the <a href="/icanhazip-com-faq/">other domains</a>. It&rsquo;s fun to run some really busy services and find ways to reduce resource consumption and the overall cost of hosting.</p>
<p>My brain has a knack for optimization and improving the site is quite fun for me. So much so that I&rsquo;ve decided to host all of icanhazip.com out of my own pocket starting today.</p>
<p><strong>However, something seriously needs to change.</strong></p>
<p>A complaint came in yesterday from someone who noticed that their machines were making quite a few requests to icanhazip.com. It turns out there was a problem with malware and the complaint implicated my site as part of the problem. One of my nodes was taken down as a precaution while I furiously worked to refute the claims within the complaint. Although the site stayed up on other nodes, it was an annoyance for some and I received a few tweets and emails about it.</p>
<p>Long story short, if you&rsquo;re sending me or my ISP a complaint about icanhazip.com, there&rsquo;s one thing you need to know: <strong>the problem is on your end, not mine</strong>. Either you have users making legitimate requests to my site or you have malware actively operating on your network.</p>
<p>No, it&rsquo;s not time to panic.</p>
<p><strong>You can actually use icanhazip.com as a tool to identify problems on your network.</strong></p>
<p>For example, add rules to your intrusion detection systems (IDS) to detect requests to the site in environments where you don&rsquo;t expect those requests to take place. Members of your support team might use the site regularly to test things but your Active Directory server shouldn&rsquo;t start spontaneously talking to my site overnight. That&rsquo;s a red flag and you can detect it <strong>easily</strong>.</p>
<p>Also, don&rsquo;t report the site as malicious or hosting malware when it&rsquo;s not. I&rsquo;ve been accused of distributing malware and participating in attacks but then, after further investigation, it was discovered that I was only returning an IPv4 address to a valid request. That hardly warrants the blind accusations that I often receive.</p>
<p>I&rsquo;ve taken some steps to ensure that there&rsquo;s a way to contact me with any questions or concerns you might have. For example:</p>
<ul>
<li>You can email abuse, postmaster, and security at icanhazip.com anytime</li>
<li>There&rsquo;s a HTTP header with a link to the FAQ (which has been there for years)</li>
<li>I monitor any tweets or blog posts that are written about the site</li>
</ul>
<p>As always, if you have questions or concerns, please reach out to me and read the <a href="/icanhazip-com-faq/">FAQ</a>. Thanks to everyone for all the support!</p>
<p><em>Photo Credit: <a href="https://www.flickr.com/photos/9813317@N08/5662811240/">Amir Kamran</a> via <a href="http://compfight.com">Compfight</a> <a href="https://www.flickr.com/help/general/#147">cc</a></em></p>
]]></content></item><item><title>Keep old kernels with yum and dnf</title><link>https://major.io/2015/05/18/keep-old-kernels-with-yum-and-dnf/</link><pubDate>Mon, 18 May 2015 14:22:56 +0000</pubDate><guid>https://major.io/2015/05/18/keep-old-kernels-with-yum-and-dnf/</guid><description>When you upgrade packages on Red Hat, CentOS and Fedora systems, the newer package replaces the older package. That means that files managed by RPM from the old package are removed and replaced with files from the newer package.
There&amp;rsquo;s one exception here: kernel packages.
Upgrading a kernel package with yum and dnf leaves the older kernel package on the system just in case you need it again. This is handy if the new kernel introduces a bug on your system or if you need to work through a compile of a custom kernel module.</description><content type="html"><![CDATA[<p>When you upgrade packages on Red Hat, CentOS and Fedora systems, the newer package replaces the older package. That means that files managed by RPM from the old package are removed and replaced with files from the newer package.</p>
<p>There&rsquo;s one exception here: kernel packages.</p>
<p>Upgrading a kernel package with yum and dnf leaves the older kernel package on the system just in case you need it again. This is handy if the new kernel introduces a bug on your system or if you need to work through a compile of a custom kernel module.</p>
<p>However, yum and dnf will clean up older kernels once you have more than three. The oldest kernel will be removed from the system and the newest three will remain. In some situations, you may want more than three to stay on your system.</p>
<p>To change the setting, simply open up <code>/etc/yum.conf</code> or <code>/etc/dnf/dnf.conf</code> in your favorite text editor. Look for this line:</p>
<pre><code>installonly_limit=3
</code></pre><p>To keep five kernels, simply replace the 3 with a 5. If you&rsquo;d like to keep every old kernel on the system forever, just change the 3 to a 0. A zero means you never want &ldquo;installonly&rdquo; packages (like kernels) to ever be removed from your system.</p>
]]></content></item><item><title>Automatic package updates with dnf</title><link>https://major.io/2015/05/11/automatic-package-updates-with-dnf/</link><pubDate>Tue, 12 May 2015 01:22:10 +0000</pubDate><guid>https://major.io/2015/05/11/automatic-package-updates-with-dnf/</guid><description>With Fedora 22&amp;rsquo;s release date quickly approaching, it&amp;rsquo;s time to familiarize yourself with dnf. It&amp;rsquo;s especially important since clean installs of Fedora 22 won&amp;rsquo;t have yum.
Almost all of the command line arguments are the same but automated updates are a little different. If you&amp;rsquo;re used to yum-updatesd, then you&amp;rsquo;ll want to look into dnf-automatic.
Installation Getting the python code and systemd unit files for automated dnf updates is a quick process:</description><content type="html"><![CDATA[<p><img src="/wp-content/uploads/2015/05/12428002945_bc47ae3529_b-e1431393503428.jpg" alt="1"></p>
<p>With Fedora 22&rsquo;s release date <a href="https://fedoraproject.org/wiki/Releases/22/Schedule">quickly approaching</a>, it&rsquo;s time to familiarize yourself with dnf. It&rsquo;s especially important since clean installs of Fedora 22 won&rsquo;t have yum.</p>
<p>Almost all of the command line arguments are the same but automated updates are a little different. If you&rsquo;re used to yum-updatesd, then you&rsquo;ll want to look into <a href="http://dnf.readthedocs.org/en/latest/automatic.html">dnf-automatic</a>.</p>
<h2 id="installation">Installation</h2>
<p>Getting the python code and systemd unit files for automated dnf updates is a quick process:</p>
<pre><code>dnf -y install dnf-automatic
</code></pre><h2 id="configuration">Configuration</h2>
<p>There&rsquo;s only one configuration file to review and most of the defaults are quite sensible. Open up <code>/etc/dnf/automatic.conf</code> with your favorite text editor and review the available options. The only adjustment I made was to change the <strong>emit_via</strong> option to <em>email</em> as opposed to the <em>stdio</em>.</p>
<p>You may want to change the <strong>email_to</strong> option if you want to redirect email elsewhere. In my case, I already have an email forward for the root user.</p>
<h2 id="dnf-automation">dnf Automation</h2>
<p>If you look at the contents of the dnf-automatic package, you&rsquo;ll find some python code, configuration files, and two important systemd files:</p>
<p>For Fedora 25 and earlier:</p>
<pre><code># rpm -ql dnf-automatic | grep systemd
/usr/lib/systemd/system/dnf-automatic.service
/usr/lib/systemd/system/dnf-automatic.timer
</code></pre><p>For Fedora 26 and later:</p>
<pre><code># rpm -ql dnf-automatic | grep systemd
/usr/lib/systemd/system/dnf-automatic-download.service
/usr/lib/systemd/system/dnf-automatic-download.timer
/usr/lib/systemd/system/dnf-automatic-install.service
/usr/lib/systemd/system/dnf-automatic-install.timer
/usr/lib/systemd/system/dnf-automatic-notifyonly.service
/usr/lib/systemd/system/dnf-automatic-notifyonly.timer
</code></pre><p>These systemd files are what makes dnf-automatic run. The service file contains the instructions so that systemd knows what to run. The timer file contains the frequency of the update checks (defaults to one day). We need to enable the timer and then start it.</p>
<p>For Fedora 25 and earlier:</p>
<pre><code>systemctl enable dnf-automatic.timer
</code></pre><p>For Fedora 26 and later:</p>
<pre><code>systemctl enable dnf-automatic-install.timer
</code></pre><p>Check your work:</p>
<pre><code># systemctl list-timers *dnf*
NEXT                         LEFT     LAST                         PASSED    UNIT                ACTIVATES
Tue 2015-05-12 19:57:30 CDT  23h left Mon 2015-05-11 19:57:29 CDT  14min ago dnf-automatic.timer dnf-automatic.service
</code></pre><p>The output here shows that the dnf-automatic job last ran at 19:57 on May 11th and it&rsquo;s set to run at the same time tomorrow, May 12th. Be sure to disable and stop your yum-updatesd service if you still have it running on your system from a previous version of Fedora.</p>
<p><em>Photo Credit: <a href="https://www.flickr.com/photos/50899563@N07/12428002945/">Outer Rim Emperor</a> via <a href="http://compfight.com">Compfight</a> <a href="https://www.flickr.com/help/general/#147">cc</a></em></p>
]]></content></item><item><title>Tweetdeck’s Chrome notifications stopped working</title><link>https://major.io/2015/05/08/tweetdecks-chrome-notifications-stopped-working/</link><pubDate>Fri, 08 May 2015 13:55:55 +0000</pubDate><guid>https://major.io/2015/05/08/tweetdecks-chrome-notifications-stopped-working/</guid><description>With the last few weeks, I noticed that Tweetdeck&amp;rsquo;s notifications weren&amp;rsquo;t showing up in Chrome any longer. I double-checked all of the Tweetdeck settings and notifications were indeed enabled. However, I found that Tweetdeck wasn&amp;rsquo;t allowed to send notifications when I checked in my Chrome settings.
Check your settings To check these for yourself, hop into Chrome&amp;rsquo;s content settings. Scroll down to Notifications and click Manage Exceptions. In my case, https://tweetdeck.</description><content type="html"><![CDATA[<p><a href="/wp-content/uploads/2015/05/Tweetdeck-Logo.png"><!-- raw HTML omitted --></a>With the last few weeks, I noticed that Tweetdeck&rsquo;s notifications weren&rsquo;t showing up in Chrome any longer. I double-checked all of the Tweetdeck settings and notifications were indeed enabled. However, I found that Tweetdeck wasn&rsquo;t allowed to send notifications when I checked in my Chrome settings.</p>
<h3 id="check-your-settings">Check your settings</h3>
<p>To check these for yourself, hop into <a href="chrome://settings/content">Chrome&rsquo;s content settings</a>. Scroll down to <strong>Notifications</strong> and click <strong>Manage Exceptions</strong>. In my case, <em><a href="https://tweetdeck.twitter.com">https://tweetdeck.twitter.com</a></em> was missing from the list entirely.</p>
<p>From here, you have two options: enable notifications for all sites (not ideal) or add an exception.</p>
<h3 id="the-big-hammer-approach">The big hammer approach</h3>
<p>To enable notifications for all sites (good for testing, not ideal in the long term), click <strong>Allow all sites to show notifications</strong> in the <strong>Notifications</strong> session.</p>
<h3 id="the-right-way">The right way</h3>
<p>To enable notifications just for Tweetdeck, you may be able to add a new exception right there in the Chrome settings interface. Many users are reporting that newer versions of Chrome don&rsquo;t allow for that. In that case, your fix involves editing your Chrome configuration on the command line.</p>
<p>Chrome preferences are in different locations depending on your OS:</p>
<ul>
<li>Windows: C:\Users&lt;username&gt;\AppData\Local\Google\Chrome\User Data\</li>
<li>Mac: ~/Library/Application Support/Google/Chrome/</li>
<li>Linux: ~/.config/google-chrome/</li>
</ul>
<p><strong>BEFORE EDITING ANYTHING,</strong> be sure you&rsquo;ve quit Chrome and ensured that nothing Chrome-related is running in the background. Seriously. Don&rsquo;t skip this step.</p>
<p>I&rsquo;m on Linux, so I&rsquo;ll open up <code>.config/google-chrome/Default/Preferences</code> in vim and make some edits. You&rsquo;re looking for some lines that look like this:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json"><span style="color:#2aa198">&#34;https://tweetdeck.twitter.com:443,https://tweetdeck.twitter.com:443&#34;</span>: {
   <span style="color:#268bd2">&#34;last_used&#34;</span>: {
      <span style="color:#268bd2">&#34;notifications&#34;</span>: <span style="color:#2aa198">1431092689.014171</span>
   }
},
</code></pre></div><p>Replace those lines with this:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json"><span style="color:#2aa198">&#34;https://tweetdeck.twitter.com,*&#34;</span>: {
   <span style="color:#268bd2">&#34;last_used&#34;</span>: {
      <span style="color:#268bd2">&#34;notifications&#34;</span>: <span style="color:#2aa198">1414673538.301078</span>
   },
   <span style="color:#268bd2">&#34;notifications&#34;</span>: <span style="color:#2aa198">1</span>
},
<span style="color:#2aa198">&#34;https://tweetdeck.twitter.com:443,https://tweetdeck.twitter.com:443&#34;</span>: {
   <span style="color:#268bd2">&#34;last_used&#34;</span>: {
      <span style="color:#268bd2">&#34;notifications&#34;</span>: <span style="color:#2aa198">1431094902.014302</span>
   }
},
</code></pre></div><p>Save the file and start up Chrome once more. Head on over to <a href="https://tweetdeck.twitter.com/">Tweetdeck</a> and you should now see the familiar Chrome toast notifications for Twitter updates!</p>
]]></content></item><item><title>HOWTO: Mikrotik OpenVPN server</title><link>https://major.io/2015/05/01/howto-mikrotik-openvpn-server/</link><pubDate>Fri, 01 May 2015 15:33:35 +0000</pubDate><guid>https://major.io/2015/05/01/howto-mikrotik-openvpn-server/</guid><description>&lt;p>&lt;a href="../wp-content/uploads/2015/05/rb850_picture.jpg">&lt;!-- raw HTML omitted -->&lt;/a>Mikrotik firewalls have been good to me over the years and they work well for multiple purposes. Creating an OpenVPN server on the device can allow you to connect into your local network when you&amp;rsquo;re on the road or protect your traffic when you&amp;rsquo;re using untrusted networks.&lt;/p>
&lt;p>Although Miktrotik&amp;rsquo;s implementation isn&amp;rsquo;t terribly robust (TCP only, client cert auth is wonky), it works quite well for most users. I&amp;rsquo;ll walk you through the process from importing certificates through testing it out with a client.&lt;/p></description><content type="html"><![CDATA[<p><a href="/wp-content/uploads/2015/05/rb850_picture.jpg"><!-- raw HTML omitted --></a>Mikrotik firewalls have been good to me over the years and they work well for multiple purposes. Creating an OpenVPN server on the device can allow you to connect into your local network when you&rsquo;re on the road or protect your traffic when you&rsquo;re using untrusted networks.</p>
<p>Although Miktrotik&rsquo;s implementation isn&rsquo;t terribly robust (TCP only, client cert auth is wonky), it works quite well for most users. I&rsquo;ll walk you through the process from importing certificates through testing it out with a client.</p>
<h3 id="import-certificates">Import certificates</h3>
<p>Creating a CA and signing a certificate and key is outside the scope of this post and there are plenty of sites that cover the basics of creating a <a href="/2007/08/02/generate-self-signed-certificate-and-key-in-one-line/">self-signed certificate</a>. You could also create a certificate signing request (CSR) on the Mikrotik and have that signed by a trusted CA. In my case, I have a simple CA already and I signed a certificate for myself.</p>
<p>Upload your certificate, key, and CA certificate (if applicable) to the Mikrotik. After that, import those files into the Mikrotik&rsquo;s certificate storage:</p>
<pre><code> import file-name=firewall.example.com.crt
passphrase:
     certificates-imported: 1
     private-keys-imported: 0
            files-imported: 1
       decryption-failures: 0
  keys-with-no-certificate: 0

[major@home] /certificate&gt; import file-name=firewall.example.com.pem
passphrase:
     certificates-imported: 0
     private-keys-imported: 1
            files-imported: 1
       decryption-failures: 0
  keys-with-no-certificate: 0

[major@home] /certificate&gt; import file-name=My_Personal_CA.crt
passphrase:
     certificates-imported: 1
     private-keys-imported: 0
            files-imported: 1
       decryption-failures: 0
  keys-with-no-certificate: 0
</code></pre><p><strong>Always import the certificate first, then the key.</strong> You should be able to do a <code>/certificate print</code> and see the entries for the files you imported. In the print output, look at the flags column and verify that the line with your certificate has a <strong>T</strong> and a <strong>K</strong>. If the K is missing, import the key one more time. If that still doesn&rsquo;t work, ensure that your certificate and key match.</p>
<p>The default naming conventions used for certificates is a little confusing. You can rename a certificate by running <code>set name=firewall.example.com number=0</code> (run a <code>/certificate print</code> to get the right number).</p>
<h3 id="openvpn-server-configuration">OpenVPN server configuration</h3>
<p>We&rsquo;re now ready to do the first steps of the OpenVPN setup on the Mikrotik. You can do this configuration via the Winbox GUI or via the web interface, but I prefer to use the command line. Let&rsquo;s start:</p>
<pre><code>/interface ovpn-server server
set certificate=firewall.example.com cipher=blowfish128,aes128,aes192,aes256 default-profile=default-encryption enabled=yes
</code></pre><p>This tells the device that we want to use the certificate we imported earlier along with all of the available ciphers. We&rsquo;re also selecting the <strong>default-encryption</strong> profile that we will configure in more detail later. Feel free to adjust your cipher list later on but I recommend allowing all of them until you&rsquo;re sure that the VPN configuration works.</p>
<p>We&rsquo;re now ready to add an OpenVPN interface. In Mikrotik terms, you can have multiple OpenVPN server profiles running under the same server. They will all share the same certificate, but each may have different authentication methods or network configurations. Let&rsquo;s define our first profile:</p>
<pre><code>/interface ovpn-server
add name=openvpn-inbound user=openvpn
</code></pre><p>There&rsquo;s now a profile with a username of <strong>openvpn</strong>. That will be the username that we use to connect to this VPN server.</p>
<h3 id="secrets">Secrets</h3>
<p>The router needs a way to identify the user we just created. We can define a secret easily:</p>
<pre><code>/ppp secret
add name=openvpn password=vpnsarefun profile=default-encryption
</code></pre><p>We&rsquo;ve set a password secret and defined a connection profile that corresponds to the secret.</p>
<h3 id="profiles">Profiles</h3>
<p>We&rsquo;ve been referring to this <strong>default-encryption</strong> profile several times and now it&rsquo;s time to configure it. This is one of the things I prefer to configure using the Winbox GUI or the web interface since there are plenty of options to review.</p>
<p>The most important part is how you connect the VPN connection into your internal network. You have a few options here. You can configure an IP address that will always be assigned to this connection no matter what. There are upsides and downsides with that choice. You&rsquo;ll always get the same IP on the inside network but you won&rsquo;t be able to connect to the same profile with multiple clients.</p>
<p>I prefer to set the bridge option to my internal network bridge (which I call <strong>lanbridge</strong>). That allows me to use my existing bridge configuration and filtering rules on my OpenVPN tunnels. My configuration looks something like this:</p>
<pre><code>/ppp profile
set 1 bridge=lanbridge local-address=default-dhcp only-one=no remote-address=default-dhcp
</code></pre><p>I&rsquo;ve told the router that I want VPN connections to be hooked up to my main bridge and it should get local and remote IP addresses from my default DHCP server. In addition, I&rsquo;ve also allowed more than one simultaneous connection to this profile.</p>
<p>The other defaults are fairly decent to get started. You can go back and adjust them later if needed.</p>
<h3 id="openvpn-client">OpenVPN client</h3>
<p>Every client has things configured a bit differently but I&rsquo;ll be working with a basic OpenVPN configuration file here that should work on most systems (or at least show you what to click in your client GUI).</p>
<p>Here&rsquo;s my OpenVPN client configuration file:</p>
<pre><code>remote firewall.example.com 1194 tcp-client
persist-key
auth-user-pass /etc/openvpn/firewall-creds.txt
tls-client
pull
ca /home/major/.cert/ca.crt
redirect-gateway def1
dev tun
persist-tun
cert /home/major/.cert/cert.crt
nobind
key /home/major/.cert/key.key
</code></pre><p>In my configuration, I refer to a <strong>/etc/openvpn/firewall-creds.txt</strong> file to hold my credentials. You can store the file anywhere (or this might be configurable in a GUI) but it should look like this:</p>
<pre><code>username
password
</code></pre><p>That&rsquo;s it - just a two line file with the username, a line feed, and a password.</p>
<p>At this point, you should be able to test your client.</p>
<h3 id="troubleshooting">Troubleshooting</h3>
<p><strong>Firewall</strong> - Ensure that you have a firewall rule set to allow traffic into your OpenVPN port. This could be something as simple as:</p>
<pre><code>/ip firewall filter add chain=input dst-port=1194 protocol=tcp
</code></pre><p><strong>Certificates</strong> - Check that your certificate and key were imported properly and that your client is configured to trust the self-signed certificate or the CA you used.</p>
<p><strong>Compression</strong> - For some reason, I have lots of problems if compression is enabled on the client. They range from connection failures to being unable to pass traffic through the tunnel after getting connected. Be sure that anything that mentions compression or LZO is disabled.</p>
<h3 id="security">Security</h3>
<p>There are some security improvements that can be made after configuring everything:</p>
<ul>
<li>Limit access to your OpenVPN port in your firewall to certain source IP&rsquo;s</li>
<li>Configure better passwords for your OpenVPN secret</li>
<li>Consider making a separate bridge or network segment for VPN users when they connect and apply filters to it</li>
<li>Adjust the list of ciphers in the default-encryption profile so that only the strongest can be used (may cause some clients to be unable to connect)</li>
</ul>]]></content></item><item><title>Rackspace::Solve Atlanta Session Recap: “The New Normal”</title><link>https://major.io/2015/04/15/rackspacesolve-atlanta-session-recap-the-new-normal/</link><pubDate>Wed, 15 Apr 2015 14:00:56 +0000</pubDate><guid>https://major.io/2015/04/15/rackspacesolve-atlanta-session-recap-the-new-normal/</guid><description>&lt;p>&lt;em>This post originally appeared on the &lt;a href="http://www.rackspace.com/blog/rackspacesolve-atlanta-session-recap-the-new-normal/">Rackspace Blog&lt;/a> and I&amp;rsquo;ve posted it here for readers of this blog. Feel free to send over any comments you have!&lt;/em>&lt;/p>
&lt;hr>
&lt;p>&lt;a href="../wp-content/uploads/2015/04/solve-logo-1.png">&lt;!-- raw HTML omitted -->&lt;/a>Most IT professionals would agree that 2014 was a long year. Heartbleed, Shellshock, Sandworm and POODLE were just a subset of the vulnerabilities that caused many of us to stay up late and reach for more coffee. As these vulnerabilities became public, I found myself fielding questions from non-technical family members after they watched the CBS Evening News and wondered what was happening. Security is now part of the popular discussion.&lt;/p>
&lt;p>Aaron Hackney and I delivered a presentation at Rackspace::Solve Atlanta called &amp;ldquo;The New Normal&amp;rdquo; where we armed the audience with security strategies that channel spending to the most effective security improvements. Our approach at Rackspace is simple and balanced: use common sense prevention strategies, invest heavily in detection, and be sure you&amp;rsquo;re ready to respond when (not if) disaster strikes. We try to help companies prioritize by focusing on a few key areas. Know when there&amp;rsquo;s a breach. Know what they touched. Know who&amp;rsquo;s responsible. Below, I&amp;rsquo;ve included five ways to put this approach into practice.&lt;/p></description><content type="html"><![CDATA[<p><em>This post originally appeared on the <a href="http://www.rackspace.com/blog/rackspacesolve-atlanta-session-recap-the-new-normal/">Rackspace Blog</a> and I&rsquo;ve posted it here for readers of this blog. Feel free to send over any comments you have!</em></p>
<hr>
<p><a href="/wp-content/uploads/2015/04/solve-logo-1.png"><!-- raw HTML omitted --></a>Most IT professionals would agree that 2014 was a long year. Heartbleed, Shellshock, Sandworm and POODLE were just a subset of the vulnerabilities that caused many of us to stay up late and reach for more coffee. As these vulnerabilities became public, I found myself fielding questions from non-technical family members after they watched the CBS Evening News and wondered what was happening. Security is now part of the popular discussion.</p>
<p>Aaron Hackney and I delivered a presentation at Rackspace::Solve Atlanta called &ldquo;The New Normal&rdquo; where we armed the audience with security strategies that channel spending to the most effective security improvements. Our approach at Rackspace is simple and balanced: use common sense prevention strategies, invest heavily in detection, and be sure you&rsquo;re ready to respond when (not if) disaster strikes. We try to help companies prioritize by focusing on a few key areas. Know when there&rsquo;s a breach. Know what they touched. Know who&rsquo;s responsible. Below, I&rsquo;ve included five ways to put this approach into practice.</p>
<p>First, common sense prevention includes using industry best practices like system and network hardening standards. Almost every device provides some kind of logging but we rarely review the logs and we often don&rsquo;t know which types of events should trigger suspicion. Monitoring logs, securely configuring devices, and segmenting networks will lead to a great prevention strategy without significant costs (in time or money).</p>
<p>Second, many businesses will overspend on more focused prevention strategies before they know what they&rsquo;re up against. This is where detection becomes key. Intrusion detection systems, log management systems, and NetFlow analysis can give you an idea of where an intruder might be within your systems and what they may have accessed. Combining these systems allows you to thwart the more advanced attackers that might use encrypted tunnels or move data via unusual protocols (like exfiltration via DNS or ICMP).</p>
<p>Third, when an incident does happen, everyone needs to know their place: including employees, partners, and customers. Every business needs a way to communicate incident severity without talking about the incident in great detail. If you&rsquo;ve seen the movie WarGames, you probably remember them changing DEFCON levels at NORAD. Everyone knew their place and their duties whenever the DECFON level changed even if they didn&rsquo;t know the specific nature of the incident. Think about how you will communicate when you really can&rsquo;t - this is critical.</p>
<p>Fourth, the data gathered by the layers of detection combined with the root cause analysis (RCA) from the incident response will show you where to spend on additional prevention. RCA will also give you the metrics you need for conversation with executives around security changes.</p>
<p>One last tip - when you think about changes, opt for a larger number of smaller changes. The implementation will be less expensive and the probability of employee and customer backlash is greatly reduced.</p>
<p>For more tips on making changes within a company, I highly recommend reading <a href="http://heathbrothers.com/books/switch/">Switch: How to Change When Change Is Hard</a>.</p>
<p>We&rsquo;d like to thank all of the Solve attendees who joined us for our talk. The questions after the talk were great and they led to plenty of hallway conversations afterwards. We hope to see you at a future Solve event!</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->]]></content></item><item><title>Woot! Eight years of my blog</title><link>https://major.io/2015/04/14/woot-eight-years-of-my-blog/</link><pubDate>Tue, 14 Apr 2015 18:53:34 +0000</pubDate><guid>https://major.io/2015/04/14/woot-eight-years-of-my-blog/</guid><description>&lt;p>The spring of 2015 marks eight years of this blog! I&amp;rsquo;ve learned plenty of tough lessons along the way and I&amp;rsquo;ve made some changes recently that might be handy for other people. After watching &lt;a href="https://twitter.com/sashalaund">Sasha Laundy&amp;rsquo;s&lt;/a> video from her &lt;a href="http://blog.sashalaundy.com/talks/asking-helping/">awesome talk at Pycon 2015&lt;/a>, I&amp;rsquo;m even more energized to share what I&amp;rsquo;ve learned with other people. &lt;em>(Seriously: Go watch that video or review the slides whether you work in IT or not. It&amp;rsquo;s worth your time.)&lt;/em>&lt;/p>
&lt;p>Let&amp;rsquo;s start from the beginning.&lt;/p></description><content type="html"><![CDATA[<p>The spring of 2015 marks eight years of this blog! I&rsquo;ve learned plenty of tough lessons along the way and I&rsquo;ve made some changes recently that might be handy for other people. After watching <a href="https://twitter.com/sashalaund">Sasha Laundy&rsquo;s</a> video from her <a href="http://blog.sashalaundy.com/talks/asking-helping/">awesome talk at Pycon 2015</a>, I&rsquo;m even more energized to share what I&rsquo;ve learned with other people. <em>(Seriously: Go watch that video or review the slides whether you work in IT or not. It&rsquo;s worth your time.)</em></p>
<p>Let&rsquo;s start from the beginning.</p>
<h3 id="history-lesson">History Lesson</h3>
<p>When I started at <a href="http://www.rackspace.com/">Rackspace</a> in late 2006, I came from a fairly senior role at a very small company. I felt like I knew a lot and then discovered I knew almost nothing compared to my new coworkers at Rackspace. Sure, some of that was <a href="/2014/02/04/be-an-inspiration-not-an-impostor/">impostor syndrome kicking in</a>, but much of it was due to being in the right place at the right time. I took a <em>lot</em> of notes in various places: notebooks, <a href="https://wiki.gnome.org/Apps/Tomboy">Tomboy notes</a>, and plain text files. It wasn&rsquo;t manageable and I knew I needed something else.</p>
<p><a href="/wp-content/uploads/2015/04/429788108_f8a6308501_o.jpg"><!-- raw HTML omitted --></a>Many of my customers were struggling to configure various applications on LAMP stacks and a frequent flier on my screen of tickets was WordPress. I installed it on a shared hosting account and began tossing my notes into it instead of the various other places. It was a bit easier to manage the content and it came with another handy feature: I could share links with coworkers when I knew how to fix something that they didn&rsquo;t. In the long run, this was the best thing that came out of using WordPress.</p>
<p>Fast forward to today and the blog has more than 640 posts, 3,500 comments, and 100,000 sessions per month. I get plenty of compliments via email along with plenty of criticism. Winston Churchill <a href="http://books.google.co.in/books?id=mfvSQbviy50C&amp;pg=PA79">said it best</a>:</p>
<blockquote>
<p>Criticism may not be agreeable, but it is necessary. It fulfils the same function as pain in the human body. It calls attention to an unhealthy state of things.</p>
</blockquote>
<p>I love all the comments and emails I get - happy or unhappy. That&rsquo;s what keeps me going.</p>
<h3 id="now-required-tls-with-optional-perfect-forward-secrecy">Now Required: TLS (with optional Perfect Forward Secrecy)</h3>
<p>I&rsquo;ve offered encrypted connections on the blog for quite some time but it&rsquo;s now a hard requirement. TLS 1.0, 1.1 and 1.2 are supported and the ciphers supporting <a href="http://en.wikipedia.org/wiki/Forward_secrecy">Perfect Forward Secrecy (PFS)</a> are preferred over those that don&rsquo;t. For the super technical details, feel free to review a scan from <a href="https://www.ssllabs.com/ssltest/analyze.html?d=major.io">Qualys' SSL Labs</a>.</p>
<p>You might be asking: &ldquo;Why does a blog need encryption if I&rsquo;m just coming by to read posts?&rdquo; My response is <strong>&ldquo;Why not?&quot;</strong>. The cost for SSL certificates in today&rsquo;s market is extremely inexpensive. For example, you can get three years on a <a href="https://cheapsslsecurity.com/comodo/positivessl.html">COMODO certificate at CheapSSL</a> for $5 USD per year. <em>(I&rsquo;m a promoter of CheapSSL - they&rsquo;re great.)</em></p>
<p>Requiring encryption doesn&rsquo;t add much overhead or load time but it may prevent someone from reading your network traffic or slipping in malicious code along with the reply from my server. Google also <a href="http://googleonlinesecurity.blogspot.com/2014/08/https-as-ranking-signal_6.html">bumps up search engine rankings</a> for sites with encryption available.</p>
<h3 id="moved-to-nginx">Moved to nginx</h3>
<p><a href="http://httpd.apache.org/">Apache</a> has served up this blog exclusively since 2007. It&rsquo;s always been my go-to web server of choice but I&rsquo;ve taken some deep dives into <a href="http://nginx.org/">nginx</a> configuration lately. I&rsquo;ve moved the blog over to a Fedora 21 virtual machine (on a Fedora 21 KVM hypervisor) running nginx with PHP running under <a href="http://php-fpm.org/">php-fpm</a>. It&rsquo;s also using nginx&rsquo;s <a href="http://nginx.org/en/docs/http/ngx_http_fastcgi_module.html#fastcgi_cache">fastcgi_cache</a> which has really surprised me with its performance. Once a page is cached, I&rsquo;m able to drag out about 800-900 Mbit/sec using <a href="http://httpd.apache.org/docs/2.2/programs/ab.html">ab</a>.</p>
<p>Another added benefit from the change is that I&rsquo;m now able to dump my caching-related plugins from WordPress. That means I have less to maintain and less to diagnose when something goes wrong.</p>
<h3 id="thanks">Thanks!</h3>
<p>Thanks for all of the emails, comments, and criticism over the years. I love getting those emails that say &ldquo;Hey, you helped me fix something&rdquo; or &ldquo;Wow, I understand that now&rdquo;. That&rsquo;s what keeps me going. ;)</p>]]></content></item><item><title>Run virsh and access libvirt as a regular user</title><link>https://major.io/2015/04/11/run-virsh-and-access-libvirt-as-a-regular-user/</link><pubDate>Sat, 11 Apr 2015 15:30:54 +0000</pubDate><guid>https://major.io/2015/04/11/run-virsh-and-access-libvirt-as-a-regular-user/</guid><description>&lt;p>&lt;a href="../wp-content/uploads/2015/04/libvirtLogo.png">&lt;!-- raw HTML omitted -->&lt;/a>&lt;a href="http://libvirt.org/">Libvirt&lt;/a> is a handy way to manage containers and virtual machines on various systems. On most distributions, you can only access the libvirt daemon via the root user by default. I&amp;rsquo;d rather use a regular non-root user to access libvirt and limit that access via groups.&lt;/p></description><content type="html"><![CDATA[<p><a href="/wp-content/uploads/2015/04/libvirtLogo.png"><!-- raw HTML omitted --></a><a href="http://libvirt.org/">Libvirt</a> is a handy way to manage containers and virtual machines on various systems. On most distributions, you can only access the libvirt daemon via the root user by default. I&rsquo;d rather use a regular non-root user to access libvirt and limit that access via groups.</p>
<p>Modern Linux distributions use <a href="http://en.wikipedia.org/wiki/Polkit">Polkit</a> to limit access to the libvirt daemon. You can add an extra rule to the existing set of Polkit rules to allow regular users to access libvirtd. Here&rsquo;s an example rule (in Javascript) from the <a href="https://wiki.archlinux.org/index.php/Libvirt#Using_polkit">ArchWiki</a>:</p>
<pre><code>/* Allow users in kvm group to manage the libvirt
daemon without authentication */
polkit.addRule(function(action, subject) {
    if (action.id == &quot;org.libvirt.unix.manage&quot; &amp;&amp;
        subject.isInGroup(&quot;wheel&quot;)) {
            return polkit.Result.YES;
    }
});
</code></pre><p>As shown on the ArchWiki, I saved this file as <code>/etc/polkit-1/rules.d/49-org.libvirt.unix.manager.rules</code>. I&rsquo;m using the <em>wheel</em> group to govern access to the libvirt daemon but you could use any group you choose. Just update the <code>subject.isInGroup</code> line in the rules file. You shouldn&rsquo;t have to restart any daemons after adding the new rule file.</p>
<p>I&rsquo;m now able to run virsh as my regular user:</p>
<pre><code>[major@host ~]$ id
uid=1000(major) gid=1000(major) groups=1000(major),10(wheel) context=unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c1023
[major@host ~]$ virsh list --all
 Id    Name                           State
----------------------------------------------------

</code></pre>]]></content></item><item><title>Review: Lenovo X1 Carbon 3rd generation and Linux</title><link>https://major.io/2015/03/30/review-lenovo-x1-carbon-3rd-generation-and-linux/</link><pubDate>Mon, 30 Mar 2015 14:15:52 +0000</pubDate><guid>https://major.io/2015/03/30/review-lenovo-x1-carbon-3rd-generation-and-linux/</guid><description>&lt;p>&lt;img src="https://major.io/wp-content/uploads/2015/03/ThinkPad-Carbon-X1.jpg" alt="1">&lt;/p>
&lt;p>After a &lt;a href="../2015/02/03/linux-support-dell-xps-13-9343-2015-model/">boatload of challenges&lt;/a> with what I thought would be my favorite Linux laptop, the &lt;a href="http://www.dell.com/us/p/xps-13-9343-laptop/pd">Dell XPS 13 9343&lt;/a>, I decided to take the plunge on a new &lt;a href="http://shop.lenovo.com/us/en/laptops/thinkpad/x-series/x1-carbon/">Lenovo X1 Carbon (3rd gen)&lt;/a>. My late-2013 MacBook Pro Retina (MacbookPro11,1) had plenty of quirks when running Linux and I was eager to find a better platform.&lt;/p></description><content type="html"><![CDATA[<p><img src="https://major.io/wp-content/uploads/2015/03/ThinkPad-Carbon-X1.jpg" alt="1"></p>
<p>After a <a href="/2015/02/03/linux-support-dell-xps-13-9343-2015-model/">boatload of challenges</a> with what I thought would be my favorite Linux laptop, the <a href="http://www.dell.com/us/p/xps-13-9343-laptop/pd">Dell XPS 13 9343</a>, I decided to take the plunge on a new <a href="http://shop.lenovo.com/us/en/laptops/thinkpad/x-series/x1-carbon/">Lenovo X1 Carbon (3rd gen)</a>. My late-2013 MacBook Pro Retina (MacbookPro11,1) had plenty of quirks when running Linux and I was eager to find a better platform.</p>
<h3 id="display--screen">Display &amp; Screen</h3>
<p>I opted for the model with the i5-5300U, 8GB RAM, 256GB SSD, and the 2560×1440 display. The high resolution display comes in two flavors: touch (glossy) and non-touch (matte). I went with the matte and I&rsquo;ve been very pleased with it so far. It comes up a bit short on pixels when you compare it with the XPS 13&rsquo;s 3200×1800 display but it&rsquo;s still very good. I run GNOME 3 with HIDPI disabled and having a few less pixels makes it much easier to read while still being <strong>very detailed</strong>.</p>
<p>The display is plenty bright and also very readable when set to very low brightness. Reducing the brightness also extends the battery life by quite a bit (more on that later). Gaming performance isn&rsquo;t good but you wouldn&rsquo;t want this laptop as a gaming rig, anyway.</p>
<h3 id="storage">Storage</h3>
<p>You can get the PCI-e storage option with 512GB but the high price tag hurts. The 256GB m2 SATA drive in my X1 is plenty fast. The drive in my laptop is a Samsung and it&rsquo;s a big improvement over some of the Sandisk drives I&rsquo;ve had in other Lenovo laptops.</p>
<h3 id="network">Network</h3>
<p>The wireless card is an <a href="http://www.intel.com/content/www/us/en/wireless-products/dual-band-wireless-ac-7265.html">Intel 7265</a> and is supported out of the box with the <a href="http://www.intel.com/support/wireless/wlan/sb/CS-034398.htm">iwlwifi</a> module in the upstream kernel. It also provides Bluetooth and it works like a charm. I&rsquo;ve paired up with many devices easily and transferred data as I&rsquo;d expect.</p>
<p>There&rsquo;s no full size ethernet port on this laptop (obviously). However, you can use the Lenovo proprietary ethernet dongle provided with the laptop and use the built-in <a href="http://ark.intel.com/products/71307/Intel-Ethernet-Connection-I218-LM">Intel I218-LM</a> ethernet card. It uses the e1000 driver and works out of the box.</p>
<h3 id="input-devices">Input devices</h3>
<p>The island-style keyboard takes a little getting used to when you&rsquo;re coming from the chiclets on the MacBook Pro. It feels great and the key travel is quite nice when compared with other laptops. The Dell XPS 13&rsquo;s key travel is poor in comparison.</p>
<p>The touchpad at the front of the laptop works quite well and the little trough right in front of the bottom of the pad is handy for click and drag gestures. The synaptics driver for X works right out of the box and libinput works, too.</p>
<p>The trackpoint (also called “keyboard nipple”) is fine but I can&rsquo;t use it worth a darn. I&rsquo;m downright horrible at it. That&rsquo;s not Lenovo&rsquo;s fault — my brain is probably dysfunctional. The trackpoint buttons (below the space bar) are hooked up to the touchpad and this has caused some problems. There&rsquo;s a fix to get the left and middle buttons working in Linux 4.0 and you&rsquo;ll <a href="https://bugzilla.redhat.com/show_bug.cgi?id=1200778">find that patch backported</a> in some other distributions, like Arch and Fedora. I don&rsquo;t use those buttons much but I could see how some people might want to do some two-handed click and drag gestures with them.</p>
<p>All of the keys on the keyboard work as expected, but you&rsquo;ll need to load up the <strong>thinkpad_acpi</strong> module to get the brightness buttons working. In my case, I had to force the module to load since the module didn&rsquo;t recognize my embedded controller:</p>
<pre><code>modprobe thinkpad_acpi force_load=1
</code></pre><p>Another nice benefit of the module is that you can control some of the LED&rsquo;s on the laptop programmatically. For example, you could blink the power button to signify your own custom alerts. You could also disable it entirely.</p>
<p><strong>Battery life</strong></p>
<p>Broadwell was supposed to bring some good power benefits and it&rsquo;s obvious that the X1 Carbon benefits from that CPU. I&rsquo;ve been off battery for about two hours while writing this post, handling email and updating some packages. GNOME says there is 84% of my battery left and it&rsquo;s estimating about 7 hours and 45 minutes remaining. I&rsquo;ve yet to see this laptop actually empty out entirely. I&rsquo;ve gone for 10 hour stretches with it and it still has one or two hours left.</p>
<p>I&rsquo;m not using any <a href="http://en.wikipedia.org/wiki/PowerTOP">powertop</a> tweaks, but I did install <a href="https://wiki.archlinux.org/index.php/TLP">tlp</a> and I&rsquo;m using it on startup. Some folks have tweaked a few additional things from powertop and they&rsquo;ve messed with the i915 module&rsquo;s refresh rate. That might give you another 5-10% on the battery but I&rsquo;m already very pleased with my current battery life.</p>
<p><strong>Linux compatibility</strong></p>
<p>There are two main issues:</p>
<ul>
<li>Trackpoint left/middle buttons don&rsquo;t work (fixed in 4.0 and backported in many distros)</li>
<li>Brightness and display switch keys don&rsquo;t work (load the <strong>thinkpad_acpi</strong> module for that)</li>
</ul>
<p>Considering that the fix for the first issue is widely available in most distributions and the second one is only a modprobe away, I&rsquo;d say this laptop is pretty darned Linux compatible. I&rsquo;m currently running Fedora 21 without any problems.</p>
<h3 id="wrap-up">Wrap up</h3>
<p>Thanks for reading this far! Let me know if I&rsquo;ve missed anything and I&rsquo;ll be glad to update the post.</p>]]></content></item><item><title>Share a wireless connection via ethernet in GNOME 3.14</title><link>https://major.io/2015/03/29/share-a-wireless-connection-via-ethernet-in-gnome-3-14/</link><pubDate>Mon, 30 Mar 2015 02:31:19 +0000</pubDate><guid>https://major.io/2015/03/29/share-a-wireless-connection-via-ethernet-in-gnome-3-14/</guid><description>&lt;p>There are some situations where you want to do the opposite of creating a wireless hotspot and you want to share a wireless connection to an ethernet connection. For example, if you&amp;rsquo;re at a hotel that offers only WiFi internet access, you could share that connection to an ethernet switch and plug in more devices. Also, you could get online with your wireless connection and create a small NAT network to test a network device without mangling your home network.&lt;/p></description><content type="html"><![CDATA[<p>There are some situations where you want to do the opposite of creating a wireless hotspot and you want to share a wireless connection to an ethernet connection. For example, if you&rsquo;re at a hotel that offers only WiFi internet access, you could share that connection to an ethernet switch and plug in more devices. Also, you could get online with your wireless connection and create a small NAT network to test a network device without mangling your home network.</p>
<p>Doing this in older versions of GNOME and NetworkManager <a href="http://askubuntu.com/questions/3063/share-wireless-connection-with-wired-ethernet-port">was fairly easy</a>. Newer versions can be a bit more challenging. To get started, I generally like to name my ethernet connections with something I can remember. In this example, I have a USB ethernet adapter that I want to use for sharing a wireless connection. Opening the Network panel in GNOME 3 gives me this:</p>
<p><a href="/wp-content/uploads/2015/03/Network_010.png"><!-- raw HTML omitted --></a></p>
<p>Click the cog wheel at the bottom right and then choose the <strong>Identity</strong> tab on the next window. Use a name for the interface that is easy to remember. I chose <strong>Home USB Ethernet</strong> for mine:</p>
<p><a href="/wp-content/uploads/2015/03/Wired_011.png"><!-- raw HTML omitted --></a></p>
<p>Press <strong>Apply</strong> and then go to a terminal. Type <code>nm-connection-editor</code> and you should get a window like this:</p>
<p><a href="/wp-content/uploads/2015/03/Network-Connections_012.png"><!-- raw HTML omitted --></a></p>
<p>We can add a shared network connection by pressing the <strong>Add</strong> button. Do the following afterwards:</p>
<ul>
<li>Choose <strong>Ethernet</strong> from the list and press <strong>Create…</strong></li>
<li>click <strong>IPv4 Settings</strong></li>
<li>Choose <strong>Shared to other computers</strong> in the <strong>Method</strong> drop-down menu</li>
<li>Enter <strong>Share via ethernet</strong> as the <strong>Connection name</strong> at the top (or choose a name you prefer)</li>
</ul>
<p>When that&rsquo;s all done, you can close the <strong>Network Connections</strong> menu we opened via the terminal. Now open the Network control panel once more. It should have two profiles for your ethernet connection now (mine is a USB ethernet device):</p>
<p><a href="/wp-content/uploads/2015/03/Network_013.png"><!-- raw HTML omitted --></a></p>
<p>If it&rsquo;s not already selected, just click on the <strong>Share via ethernet</strong> text. NetworkManager will automatically configure NAT, DHCP and firewall rules for you. When you&rsquo;re ready to go back to normal ethernet operation and you want to stop sharing, simply click on the other profile (mine is called <strong>Home USB Ethernet</strong>). NetworkManager will put the ethernet device back into the original way you had it configured (default is DHCP with automatic IPv6 via SLAAC).</p>]]></content></item><item><title>Creating a bridge for virtual machines using systemd-networkd</title><link>https://major.io/2015/03/26/creating-a-bridge-for-virtual-machines-using-systemd-networkd/</link><pubDate>Thu, 26 Mar 2015 13:17:08 +0000</pubDate><guid>https://major.io/2015/03/26/creating-a-bridge-for-virtual-machines-using-systemd-networkd/</guid><description>There are plenty of guides out there for making ethernet bridges in Linux to support virtual machines using built-in network scripts or NetworkManager. I decided to try my hand with creating a bridge using only systemd-networkd and it was surprisingly easy.
First off, you&amp;rsquo;ll need a version of systemd with networkd support. Fedora 20 and 21 will work just fine. RHEL/CentOS 7 and Arch Linux should also work. Much of the networkd support has been in systemd for quite a while, but if you&amp;rsquo;re looking for fancier network settings, like bonding, you&amp;rsquo;ll want at least systemd 216.</description><content type="html"><![CDATA[<p>There are plenty of guides out there for making ethernet bridges in Linux to support virtual machines using built-in network scripts or NetworkManager. I decided to try my hand with creating a bridge using only systemd-networkd and it was surprisingly easy.</p>
<p>First off, you&rsquo;ll need a version of systemd with networkd support. Fedora 20 and 21 will work just fine. RHEL/CentOS 7 and Arch Linux should also work. Much of the networkd support has been in systemd for quite a while, but if you&rsquo;re looking for fancier network settings, like bonding, you&rsquo;ll want at least systemd 216.</p>
<h3 id="getting-our-daemons-in-order">Getting our daemons in order</h3>
<p>Before we get started, ensure that systemd-networkd will run on a reboot and NetworkManager is disabled. We also need to make a config file director for systemd-networkd if it doesn&rsquo;t exist already. In addition, let&rsquo;s enable the caching resolver and make a symlink to systemd&rsquo;s <code>resolv.conf</code>:</p>
<pre><code>systemctl enable systemd-networkd
systemctl disable NetworkManager
systemctl enable systemd-resolved
ln -sf /run/systemd/resolve/resolv.conf /etc/resolv.conf
mkdir /etc/systemd/network
</code></pre><h3 id="configure-the-physical-network-adapter">Configure the physical network adapter</h3>
<p>In my case, the network adapter connected to my external network is <em>enp4s0</em> but yours will vary. Run <code>ip addr</code> to get a list of your network cards. Let&rsquo;s create <code>/etc/systemd/network/uplink.network</code> and put the following in it:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ini" data-lang="ini"><span style="color:#719e07">[Match]</span>
Name<span style="color:#719e07">=</span><span style="color:#2aa198">enp4s0</span>

<span style="color:#719e07">[Network]</span>
Bridge<span style="color:#719e07">=</span><span style="color:#2aa198">br0</span>
</code></pre></div><p>I&rsquo;m telling systemd to look for a device called <em>enp4s0</em> and then add it to a bridge called <em>br0</em> that we haven&rsquo;t configured yet. Be sure to change <em>enp4s0</em> to match your ethernet card.</p>
<h3 id="make-the-bridge">Make the bridge</h3>
<p>We need to tell systemd about our new bridge network device and we also need to specify the IP configuration for it. We start by creating <code>/etc/systemd/network/br0.netdev</code> to specify the device:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ini" data-lang="ini"><span style="color:#719e07">[NetDev]</span>
Name<span style="color:#719e07">=</span><span style="color:#2aa198">br0</span>
Kind<span style="color:#719e07">=</span><span style="color:#2aa198">bridge</span>
</code></pre></div><p>This file is fairly self-explanatory. We&rsquo;re telling systemd that we want a device called <em>br0</em> that functions as an ethernet bridge. Now create <code>/etc/systemd/network/br0.network</code> to specify the IP configuration for the <em>br0</em> interface:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ini" data-lang="ini"><span style="color:#719e07">[Match]</span>
Name<span style="color:#719e07">=</span><span style="color:#2aa198">br0</span>

<span style="color:#719e07">[Network]</span>
DNS<span style="color:#719e07">=</span><span style="color:#2aa198">192.168.250.1</span>
Address<span style="color:#719e07">=</span><span style="color:#2aa198">192.168.250.33/24</span>
Gateway<span style="color:#719e07">=</span><span style="color:#2aa198">192.168.250.1</span>
</code></pre></div><p>This file tells systemd that we want to apply a simple static network configuration to <em>br0</em> with a single IPv4 address. If you want to add additional DNS servers or IPv4/IPv6 addresses, just add more <code>DNS=</code> and <code>Address</code> lines right below the ones you see above. Yes, it&rsquo;s just that easy.</p>
<h3 id="lets-do-this">Let&rsquo;s do this</h3>
<p>Some folks are brave enough to stop NetworkManager and start all of the systemd services here but I prefer to reboot so that everything comes up cleanly. That will also allow you to verify that future reboots will cause the server to come back online with the right configuration. After the reboot, run <code>networkctl</code> and you&rsquo;ll get something like this (with color):</p>
<p><img src="/wp-content/uploads/2015/03/networkctl_screenshot.png" alt="1"></p>
<p>Here&rsquo;s what&rsquo;s in the screenshot:</p>
<pre><code>IDX LINK             TYPE               OPERATIONAL SETUP
  1 lo               loopback           carrier     unmanaged
  2 enp2s0           ether              off         unmanaged
  3 enp3s0           ether              off         unmanaged
  4 enp4s0           ether              degraded    configured
  5 enp5s0           ether              off         unmanaged
  6 br0              ether              routable    configured
  7 virbr0           ether              no-carrier  unmanaged

7 links listed.
</code></pre><p>My ethernet card has four ports and only <em>enp4s0</em> is in use. It has a <em>degraded</em> status because there is no IP address assigned to <em>enp4s0</em>. You can ignore that for now but it would be nice to see this made more clear in a future systemd release.</p>
<p>Look at <em>br0</em> and you&rsquo;ll notice that it&rsquo;s <em>configured</em> and <em>routable</em>. That&rsquo;s the best status you can get for an interface. You&rsquo;ll also see that my other ethernet devices are in the <em>unmanaged</em> state. I could easily add more <code>.network</code> files to <code>/etc/systemd/network</code> to configure those interfaces later.</p>
<h3 id="further-reading">Further reading</h3>
<p>As usual, the <a href="https://wiki.archlinux.org/index.php/systemd-networkd">Arch Linux wiki page on systemd-networkd</a> is a phenomenal resource. There&rsquo;s a detailed overview of all of the available systemd-networkd configuration file options over at <a href="http://www.freedesktop.org/software/systemd/man/systemd.network.html">systemd&rsquo;s documentation site</a>.</p>
]]></content></item><item><title>Test Fedora 22 at Rackspace with Ansible</title><link>https://major.io/2015/03/24/test-fedora-22-at-rackspace-with-ansible/</link><pubDate>Tue, 24 Mar 2015 13:55:08 +0000</pubDate><guid>https://major.io/2015/03/24/test-fedora-22-at-rackspace-with-ansible/</guid><description>Fedora 22 will be arriving soon and it&amp;rsquo;s easy to test on Rackspace&amp;rsquo;s cloud with my Ansible playbook:
https://github.com/major/ansible-rax-fedora22 As with the previous playbook I created for Fedora 21, this playbook will ensure your Fedora 21 instance is fully up to date and then perform the upgrade to Fedora 22.
WARNING: It&amp;rsquo;s best to use this playbook against a non-production system. Fedora 22 is an alpha release at the time of this post&amp;rsquo;s writing.</description><content type="html"><![CDATA[<p><a href="/wp-content/uploads/2012/01/fedorainfinity.png"><!-- raw HTML omitted --></a>Fedora 22 will be <a href="http://fedoraproject.org/wiki/Releases/22/Schedule">arriving soon</a> and it&rsquo;s easy to test on Rackspace&rsquo;s cloud with my Ansible playbook:</p>
<ul>
<li><a href="https://github.com/major/ansible-rax-fedora22">https://github.com/major/ansible-rax-fedora22</a></li>
</ul>
<p>As with the <a href="https://github.com/major/ansible-rax-fedora21">previous playbook</a> I created for Fedora 21, this playbook will ensure your Fedora 21 instance is fully up to date and then perform the upgrade to Fedora 22.</p>
<p><strong><!-- raw HTML omitted -->WARNING<!-- raw HTML omitted -->: It&rsquo;s best to use this playbook against a non-production system. Fedora 22 is an alpha release at the time of this post&rsquo;s writing.</strong></p>
<p>This playbook should work well against other servers and virtual machines from other providers but there are a few things that are Rackspace-specific cleanups that might not apply to other servers.</p>
]]></content></item><item><title>Xerox ColorQube 9302 and Linux</title><link>https://major.io/2015/03/15/xerox-colorqube-9302-and-linux/</link><pubDate>Mon, 16 Mar 2015 02:23:07 +0000</pubDate><guid>https://major.io/2015/03/15/xerox-colorqube-9302-and-linux/</guid><description>I do a bunch of Linux-related tasks daily. Some are difficult and others are easy. Printing has always been my nemesis.
Some printers offer up highly standardized methods for printing. For example, many HP printers simply work with JetDirect and PCL 5. However, the quirkier ones that require plenty of transformations before paper starts rolling can be tricky.
We have some Xerox ColorQube printers at the office and they require some proprietary software to get them printing under Linux.</description><content type="html"><![CDATA[<p>I do a bunch of Linux-related tasks daily. Some are difficult and others are easy. Printing has always been my nemesis.</p>
<p>Some printers offer up highly standardized methods for printing. For example, many HP printers simply work with JetDirect and PCL 5. However, the quirkier ones that require plenty of transformations before paper starts rolling can be tricky.</p>
<p>We have some Xerox ColorQube printers at the office and they require some proprietary software to get them printing under Linux. To get started, you&rsquo;ll need a <a href="http://www.support.xerox.com/support/colorqube-9300-series/downloads/enus.html?operatingSystem=linux&amp;fileLanguage=en">Linux printer driver for the Xerox ColorQube 9200 series</a>.</p>
<p>Once you&rsquo;ve downloaded the RPM (or DEB), install it:</p>
<pre><code>sudo rpm -Uvh Xeroxv5Pkg-Linuxx86_64-5.15.551.3277.rpm
</code></pre><p>Start the Xerox Printer Manager:</p>
<pre><code>sudo xeroxprtmgr
</code></pre><p>You should have a screen like this:</p>
<p><a href="/wp-content/uploads/2015/03/Xerox-Printer-Manager_006.png"><!-- raw HTML omitted --></a></p>
<p>Press the double down arrow button at the top (it&rsquo;s the one on the left), and then press the button at the top right of the next window that looks like rectangles stacked on top of one another. Choose <strong>Manual Install</strong> from the menu that appears.</p>
<p><a href="/wp-content/uploads/2015/03/Selection_008.png"><!-- raw HTML omitted --></a></p>
<p>In the next menu, enter a nickname for the printer, the printer&rsquo;s IP address, and select the correct printer model from the list. The printer should be properly configured in your CUPS system afterwards:</p>
<p><a href="/wp-content/uploads/2015/03/Printers_009.png"><!-- raw HTML omitted --></a></p>
<p>Any new print jobs set to the printer will cause the Xerox printer manager to pop up. This gives you the opportunity to customize your job (collating, stapling, etc) and you can also use secure print (which I highly recommend).</p>
]]></content></item><item><title>Using play/pause buttons in Chrome with GNOME 3</title><link>https://major.io/2015/02/20/using-playpause-buttons-in-chrome-with-gnome-3/</link><pubDate>Fri, 20 Feb 2015 14:21:44 +0000</pubDate><guid>https://major.io/2015/02/20/using-playpause-buttons-in-chrome-with-gnome-3/</guid><description>I wrote a post last summer about preventing Chrome from stealing the media buttons (like play, pause, previous track and next track) from OS X. Now that I&amp;rsquo;m using Linux regularly and I fell in love with Google Play Music All Access, I found that GNOME was stealing the media keys from Chrome.
The fix is quite simple. Press the SUPER key (Windows key or Mac Command key), type settings, and press enter.</description><content type="html"><![CDATA[<p>I <a href="/2014/07/30/playpause-button-stopped-working-in-os-x-mavericks/">wrote a post last summer</a> about preventing Chrome from stealing the media buttons (like play, pause, previous track and next track) from OS X. Now that I&rsquo;m using Linux regularly and I <a href="/2014/12/29/two-months-google-play-music-access/">fell in love with Google Play Music All Access</a>, I found that GNOME was stealing the media keys from Chrome.</p>
<p>The fix is quite simple. Press the SUPER key (Windows key or Mac Command key), type settings, and press enter. Click on <strong>Keyboard</strong> and then on the <strong>Shortcuts</strong> tab. You should now see something like this:</p>
<p><a href="/wp-content/uploads/2015/02/Keyboard_002.png"><!-- raw HTML omitted --></a></p>
<p>Click on each entry that shows <em>Disabled</em> above. After clicking on the entry, press your backspace key to clear the shortcut. If you&rsquo;re using a Mac keyboard, that&rsquo;s your oddly-named delete key that sits right above the pipe/backslash key.</p>
<p>You should be set to go once they&rsquo;re all cleared out. If you disabled the media keys in Chrome, <a href="/2014/07/30/playpause-button-stopped-working-in-os-x-mavericks/">go to this post</a> and do all of the steps in reverse. ;)</p>
]]></content></item><item><title>Rotate GNOME 3’s wallpaper with systemd user units and timers</title><link>https://major.io/2015/02/11/rotate-gnome-3s-wallpaper-systemd-user-units-timers/</link><pubDate>Wed, 11 Feb 2015 14:23:03 +0000</pubDate><guid>https://major.io/2015/02/11/rotate-gnome-3s-wallpaper-systemd-user-units-timers/</guid><description>NOTE: This works in Fedora 21, but not in Fedora 22. Review this post for the fixes.
GNOME 3 has improved by leaps and bounds since its original release and it&amp;rsquo;s my daily driver window manager on my Linux laptop. Even with all of these improvements, there&amp;rsquo;s still no built-in way to rotate wallpaper (that I&amp;rsquo;ve found).
There are some extensions, like BackSlide, that enable background rotation on a time interval.</description><content type="html"><![CDATA[<p><em>NOTE: This works in Fedora 21, but not in Fedora 22. Review <a href="/2015/06/23/fedora-22-and-rotating-gnome-wallpaper-with-systemd-timers/">this post</a> for the fixes.</em></p>
<p><a href="http://www.gnome.org/gnome-3/">GNOME 3</a> has improved by leaps and bounds since its original release and it&rsquo;s my daily driver window manager on my Linux laptop. Even with all of these improvements, there&rsquo;s still no built-in way to rotate wallpaper (that I&rsquo;ve found).</p>
<p>There are some extensions, like <a href="https://extensions.gnome.org/extension/543/backslide/">BackSlide</a>, that enable background rotation on a time interval. Fedora 21 uses GNOME 3.14 and the current BackSlide version is incompatible. BackSlide&rsquo;s interface is fairly useful but I wanted something different.</p>
<p>One of systemd&rsquo;s handy features is the ability to set up <a href="http://www.freedesktop.org/wiki/Software/systemd/">systemd</a> unit files on a per-user basis. Every user can create unit files in their home directory and tell systemd to begin using those.</p>
<h3 id="getting-started">Getting started</h3>
<p>We first need a script that can rotate the background based on files in a particular directory. All of my wallpaper images are in <code>~/Pictures/wallpapers</code>. I adjusted this script that I found on GitHub so that it searches through files in my wallpaper directory and picks one at random to use:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#719e07">#!/bin/bash
</span><span style="color:#719e07"></span>
<span style="color:#268bd2">walls_dir</span><span style="color:#719e07">=</span><span style="color:#268bd2">$HOME</span>/Pictures/Wallpapers
<span style="color:#268bd2">selection</span><span style="color:#719e07">=</span><span style="color:#719e07">$(</span>find <span style="color:#268bd2">$walls_dir</span> -type f -name <span style="color:#2aa198">&#34;*.jpg&#34;</span> -o -name <span style="color:#2aa198">&#34;*.png&#34;</span> | shuf -n1<span style="color:#719e07">)</span>
gsettings <span style="color:#b58900">set</span> org.gnome.desktop.background picture-uri <span style="color:#2aa198">&#34;file://</span><span style="color:#268bd2">$selection</span><span style="color:#2aa198">&#34;</span>
</code></pre></div><p>I tossed this script into <code>~/bin/rotate_bg.sh</code> and made it executable with <code>chmod +x ~/bin/rotate_bg.sh</code>. Before you go any further, run the script manually in a terminal to verify that your background rotates to another image.</p>
<h3 id="preparing-the-systemd-service-unit-file">Preparing the systemd service unit file</h3>
<p>You&rsquo;ll need to create a user-level systemd service file directory if it doesn&rsquo;t exist already:</p>
<pre><code>mkdir ~/.config/systemd/user/
</code></pre><p>Drop this file into <code>~/.config/systemd/user/gnome-background-change.service</code>:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ini" data-lang="ini"><span style="color:#719e07">[Unit]</span>
Description<span style="color:#719e07">=</span><span style="color:#2aa198">Rotate GNOME background</span>

<span style="color:#719e07">[Service]</span>
Type<span style="color:#719e07">=</span><span style="color:#2aa198">oneshot</span>
Environment<span style="color:#719e07">=</span><span style="color:#2aa198">DISPLAY=:0</span>
ExecStart<span style="color:#719e07">=</span><span style="color:#2aa198">/usr/bin/bash /home/[USERNAME]/bin/rotate_bg.sh</span>

<span style="color:#719e07">[Install]</span>
WantedBy<span style="color:#719e07">=</span><span style="color:#2aa198">basic.target</span>
</code></pre></div><p>This unit file tells systemd that we have a oneshot script that will exit when it&rsquo;s finished. In addition, we also give the environment details to systemd so that it&rsquo;s aware of our existing X session.</p>
<p>Don&rsquo;t enable or start the service file yet. We will let our timer handle that part.</p>
<h3 id="setting-a-timer">Setting a timer</h3>
<p>Systemd&rsquo;s concept of <a href="http://www.freedesktop.org/software/systemd/man/systemd.timer.html">timers</a> is pretty detailed. You have plenty of control over how and when you want a particular service to run. We need a simple calendar-based timer (much like cron) that will start up our service from the previous step.</p>
<p>Drop this into <code>~/.config/systemd/user/gnome-background-change.timer</code>:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ini" data-lang="ini"><span style="color:#719e07">[Unit]</span>
Description<span style="color:#719e07">=</span><span style="color:#2aa198">Rotate GNOME wallpaper timer</span>

<span style="color:#719e07">[Timer]</span>
OnCalendar<span style="color:#719e07">=</span><span style="color:#2aa198">*:0/5</span>
Persistent<span style="color:#719e07">=</span><span style="color:#2aa198">true</span>
Unit<span style="color:#719e07">=</span><span style="color:#2aa198">gnome-background-change.service</span>

<span style="color:#719e07">[Install]</span>
WantedBy<span style="color:#719e07">=</span><span style="color:#2aa198">gnome-background-change.service</span>
</code></pre></div><p>We&rsquo;re telling systemd that we want this timer to run every five minutes and we want to start our service unit file from the previous step. The <code>Persistent</code> line tells systemd that we want this unit file run if the last run was missed. For example, if you log in at 7:02AM, we don&rsquo;t want to wait until 7:05AM to rotate the background. We can rotate it immediately after login.</p>
<p>If you&rsquo;d like a different interval, be sure to review systemd&rsquo;s <a href="http://www.freedesktop.org/software/systemd/man/systemd.time.html">time syntax</a> for the <code>OnCalendar</code> line. It&rsquo;s a little quirky if you&rsquo;re used to working with crontabs but it&rsquo;s very powerful once you understand it.</p>
<p>Now we can enable and start the timer:</p>
<pre><code>systemctl --user enable gnome-background-change.timer
systemctl --user start gnome-background-change.timer
</code></pre><h3 id="checking-our-work">Checking our work</h3>
<p>You can use systemctl to query the timer we just activated:</p>
<pre><code>$ systemctl --user list-timers
NEXT                         LEFT          LAST                         PASSED  UNIT                          ACTIVATES
Wed 2015-02-11 08:15:00 CST  3min 53s left Wed 2015-02-11 08:10:49 CST  16s ago gnome-background-change.timer gnome-background-change.service
</code></pre><p>In my case, this shows that the background rotation service last ran 16 seconds ago. It will run again in just under four minutes. If you find that the service runs but your wallpaper doesn&rsquo;t change, try running <code>journalctl -xe</code> to see if your service is throwing any errors.</p>
<h3 id="additional-reading">Additional reading</h3>
<p>This is just the tip of the iceberg of what systemd can do with user unit files and timers. the Arch Linux wiki has some awesome documentation about <a href="https://wiki.archlinux.org/index.php/Systemd/User">user unit files</a> and <a href="https://wiki.archlinux.org/index.php/Systemd/Timers">timers</a>. Check out the other timers that already exist on your system for more ideas.</p>
]]></content></item><item><title>Lessons learned from a kernel bisection</title><link>https://major.io/2015/02/09/lessons-learned-kernel-bisection/</link><pubDate>Mon, 09 Feb 2015 14:39:08 +0000</pubDate><guid>https://major.io/2015/02/09/lessons-learned-kernel-bisection/</guid><description>[][1]I&amp;rsquo;m far from being a kernel developer, but I found myself staring down a [peculiar touchpad problem][2] with my new Dell XPS 13. Before kernel 3.17, the touchpad showed up as a standard PS/2 mouse, which certainly wasn&amp;rsquo;t ideal. That robbed the pad of its multi-touch capabilities. Kernel 3.17 added the right support for the pad but freezes began to occur somewhere between 3.17 and 3.19.
Bisecting It became apparent that bisecting the kernel would be required.</description><content type="html"><![CDATA[<p>[<!-- raw HTML omitted -->][1]I&rsquo;m far from being a kernel developer, but I found myself staring down a [peculiar touchpad problem][2] with my new Dell XPS 13. Before kernel 3.17, the touchpad showed up as a standard PS/2 mouse, which certainly wasn&rsquo;t ideal. That robbed the pad of its multi-touch capabilities. Kernel 3.17 added the right support for the pad but freezes began to occur somewhere between 3.17 and 3.19.</p>
<h2 id="bisecting">Bisecting</h2>
<p>It became apparent that bisecting the kernel would be required. If you&rsquo;re not familiar with [bisection][3], it&rsquo;s a process than can help you narrow down where a particular piece of software picked up a bug. You tell git which revision you know is good and you also tell it which revision has a problem. Git will pick a revision right in the middle and let you re-test. If the test is good, you mark the revision as good and git scoots to the middle between the two known good revisions. The same thing happens if you mark the revision as a bad one.</p>
<p>You&rsquo;ll eventually find yourself staring down fewer and fewer commits until you isolate the commit that is causing problems. From there, you&rsquo;ll need to write a new patch to fix the bug or consider reverting the problematic patch entirely.</p>
<h2 id="lessons-learned">Lessons learned</h2>
<p>Making mistakes during a kernel bisection are quite painful since the build times are fairly extensive. Kernel builds on my laptop took about a half hour and a 32-core Rackspace Cloud Server still took about 10 minutes to compile and package the kernel.</p>
<h3 id="come-up-with-a-solid-test-plan">Come up with a solid test plan</h3>
<p>Before you get started, define a good test plan so that you know what a good or bad revision should look like. In my case, the touchpad froze when I applied more than one finger to the touchpad or tried to do multi-finger taps and clicks. It&rsquo;s even better if you can figure out a way to run a script to test the revision. If you can do that, git can automated the bisection for you and you&rsquo;ll be done really quickly.</p>
<h3 id="build-the-project-consistently">Build the project consistently</h3>
<p>Ensure that you build the software project the same way each time. In my case, I was careful to use the same exact kernel config file and use the same script to build the kernel for each round of bisection. Introducing changes in the build routine could sway your results and cause you to mislabel a good or bad revision.</p>
<h3 id="write-the-upcoming-revisions-to-a-file">Write the upcoming revisions to a file</h3>
<p>You can protect yourself from many mistakes by writing the list of revisions in your bisection to a file. That would allow you to come back to the bisection after a mistake and pick up where you left off. You could use something like this:</p>
<pre><code>

That file will help in case you accidentally run a `git bisect reset` or delete the repository. I cannot confirm or deny that anything like that happened during my work. :)

 [1]: /wp-content/uploads/2015/02/Git-Logo-1788C.png
 [2]: /2015/02/03/linux-support-dell-xps-13-9343-2015-model/
 [3]: http://git-scm.com/docs/git-bisect
</code></pre>]]></content></item><item><title>Using ZoneMinder with a Logitech C270 webcam</title><link>https://major.io/2015/02/07/using-zoneminder-logitech-c270-webcam/</link><pubDate>Sun, 08 Feb 2015 04:04:08 +0000</pubDate><guid>https://major.io/2015/02/07/using-zoneminder-logitech-c270-webcam/</guid><description>For those of you in the market for a cheap webcam for videoconferencing or home surveillance, the Logitech C270 is hard to beat at about $20-25 USD. It can record video at 1280×960 and it&amp;rsquo;s fairly good at low light levels. The white balance gets a bit off when it&amp;rsquo;s bright in the room but hey — this camera is cheap.
ZoneMinder can monitor multiple cameras connected via USB or network.</description><content type="html"><![CDATA[<p>For those of you in the market for a cheap webcam for videoconferencing or home surveillance, the <a href="http://www.logitech.com/en-us/product/hd-webcam-c270">Logitech C270</a> is hard to beat at about $20-25 USD. It can record video at 1280×960 and it&rsquo;s fairly good at low light levels. The white balance gets a bit off when it&rsquo;s bright in the room but hey — this camera is cheap.</p>
<p><a href="http://www.zoneminder.com/">ZoneMinder</a> can monitor multiple cameras connected via USB or network. Setting up the C270 with ZoneMinder is relatively straightforward. (Getting ZoneMinder installed and running is well outside the scope of this post.)</p>
<h2 id="adjust-groups">Adjust groups</h2>
<p>If a user wants to access the webcam in Linux, they must be in the video group. On my system, ZoneMinder runs as the apache user. I needed to add the apache user to the video group:</p>
<pre><code>usermod -G video apache
</code></pre><h2 id="configuring-the-c270">Configuring the C270</h2>
<p>After clicking <strong>Add New Monitor</strong>, here&rsquo;s the data for each tab:</p>
<p>General Tab:</p>
<ul>
<li>Source Type: Local</li>
<li>Function: Modect</li>
</ul>
<p>Source:</p>
<ul>
<li>Device Path: /dev/video0</li>
<li>Capture Method: Video For Linux version2</li>
<li>Device Format: PAL</li>
<li>Capture Palette: YUYV</li>
<li>Capture Width: 1280</li>
<li>Capture Height: 960</li>
</ul>
<p>The width and height settings are suggestions. You can crank them down to something like 640×480 if you&rsquo;d like to save disk space or get a higher frame rate.</p>
<p>Once you save the configuration and the window disappears, you should see <strong>/dev/video0 (0)</strong> turn green in the ZoneMinder web interface. If it&rsquo;s red, there may be a different permissions issue to solve or your ZoneMinder instance might be running as a different user than you expected. If the text is yellow/orange, go back and check your camera configuration settings in the ZoneMinder interface.</p>
]]></content></item><item><title>Linux support for the Dell XPS 13 9343 (2015 model)</title><link>https://major.io/2015/02/03/linux-support-dell-xps-13-9343-2015-model/</link><pubDate>Tue, 03 Feb 2015 15:23:24 +0000</pubDate><guid>https://major.io/2015/02/03/linux-support-dell-xps-13-9343-2015-model/</guid><description>I&amp;rsquo;M ALL DONE:I&amp;rsquo;m not working on Linux compatibility for the XPS 13 any longer. I&amp;rsquo;ve purchased a Lenovo X1 Carbon (3rd gen) and that&amp;rsquo;s my preferred laptop. More on this change later.
I&amp;rsquo;ve been looking for a good laptop to run Linux for a while now and my new Dell XPS 13 9343 has arrived. It was released at CES in 2015 and it received quite a lot of attention for packing a large amount of pixels into a very small laptop frame with excellent battery life.</description><content type="html"><![CDATA[<p><img src="https://major.io/wp-content/uploads/2015/02/dellXPS13-9343_2.jpg" alt="1"></p>
<p><em><!-- raw HTML omitted --><!-- raw HTML omitted -->I&rsquo;M ALL DONE:<!-- raw HTML omitted --><!-- raw HTML omitted --> I&rsquo;m not working on Linux compatibility for the XPS 13 any longer. I&rsquo;ve purchased a Lenovo X1 Carbon (3rd gen) and that&rsquo;s my preferred laptop. More on this change later.</em></p>
<hr>
<p>I&rsquo;ve been looking for a good laptop to run Linux for a while now and my new Dell XPS 13 9343 has arrived. It was released at CES in 2015 and it received quite a lot of attention for packing a large amount of pixels into a very small laptop frame with excellent battery life. Ars Technica has a <a href="http://arstechnica.com/gadgets/2015/01/hands-on-dell-xps-13-packs-a-13-inch-screen-into-an-11-inch-laptop/">great overall review</a> of the device.</p>
<p>Linux support has been historically good on the previous generation XPS 13&rsquo;s and a <a href="http://en.community.dell.com/techcenter/b/techcenter/archive/2015/01/27/you-asked-for-it-ubuntu-officially-on-the-precision-m3800-worldwide">blog post from Dell</a> suggests that the latest revision will have good support as well. For a deep dive on the hardware inside the laptop, review <a href="https://gist.github.com/semenko/60015029e13c1de65ff6">this GitHub Gist</a>.</p>
<p>After wiping Windows 8.1 off the laptop, I started with the Fedora 21 installation. If you want to run Linux on one of these laptops, here&rsquo;s what you need to know:</p>
<h2 id="the-good">The good</h2>
<p>All of the most basic devices work just fine. The display, storage, and peripheral connections (USB, SD card slot, mini DisplayPort) all work out of the box in Linux 3.18.5 with Fedora 21. The display looks great with GNOME 3&rsquo;s default HiDPI settings and it&rsquo;s very readable with the default font sizes without HiDPI (although this is a bit subjective).</p>
<p>The webcam works without any additional configuration the video quality is excellent.</p>
<p>The wireless card in the laptop I received is a BCM4352:</p>
<pre><code>02:00.0 Network controller: Broadcom Corporation BCM4352 802.11ac Wireless Network Adapter (rev 03)
</code></pre><p>It&rsquo;s possible to get this card working with the b43 kernel modules but I&rsquo;ve had better luck with the binary blob STA drivers from Broadcom. There are <a href="http://www.cyberciti.biz/faq/fedora-linux-install-broadcom-wl-sta-wireless-driver-for-bcm43228/">plenty of guides</a> out there to help you install the kernel module for your Fedora kernel. I&rsquo;ve had great network performance with the binary driver.</p>
<p>Some users are seeing Intel wireless cards in their Dell XPS 13&rsquo;s, especially in Europe. Opening the laptop for service <a href="http://www.myfixguide.com/manual/dell-xps-13-9343-disassembly/">isn&rsquo;t terribly difficult</a> and you could replace the bluetooth/wireless card with a different one.</p>
<p><strong>PRO TIP:</strong> If you&rsquo;re seeing errors in your journald logs about NetworkManager being unable to scan for access points, be sure to hit the wireless switch key on your keyboard (Fn-F12) to enable the card. This had me stumped for about 45 minutes. There&rsquo;s an option in the BIOS to disable the switch and let the OS control the wireless card.</p>
<p>The special keyboard buttons (volume up/down, brightness up/down) all work flawlessly.</p>
<h2 id="the-bad">The bad</h2>
<p>The touchpad and keyboard are on the I2C bus and this creates some problems. Many users have reported that keys on the keyboard seem to repeat themselves while you&rsquo;re typing and the touchpad has an issue where X stops receiving input from it. However, when the touchpad seems to freeze, the kernel still sees data coming from the device (verified with evtest and evemu-record).</p>
<p>There are some open bugs and discussion about the touchpad issues:</p>
<ul>
<li><a href="http://www.reddit.com/r/linux/comments/2u0jjd/linux_support_is_terrible_on_the_new_dell_xps_13/">Linux Support is Terrible on the New Dell XPS 13 (2015)</a> [Reddit]</li>
<li><a href="https://bugs.launchpad.net/ubuntu/+source/linux/+bug/1416601">touchpad does not respond to tap-to-clicks in I2C mode in Ubuntu 15.04 on 2015 XPS 13 (9343)</a> [Launchpad Bug]</li>
<li><a href="https://bugzilla.redhat.com/show_bug.cgi?id=1188439">Dell XPS 13 9343 (2015) touchpad freeze</a> [Red Hat Bug]</li>
</ul>
<p>You can connect up a mouse and keyboard to the laptop and work around those issues. However, dragging around some big peripherals with such a small laptop isn&rsquo;t a great long-term solution. Some users suggested blacklisting the i2c_hid module so that the touchpad shows up as a plain PS/2 touchpad but I&rsquo;m still seeing freezes even after making that change.</p>
<p>If you&rsquo;re having one of those “touchpad on the I2C bus?” moments like I had, read Synaptics' <a href="http://www.synaptics.com/en/intertouch.php">brief page about Intertouch</a>. Using the I2C bus saves power, reduces USB port consumption, and allows for more powerful multi-touch gestures.</p>
<p>Oddly enough, the touchscreen is an ELAN Touchscreen and it runs over USB. It suffers from the same freezes that the touchpad does.</p>
<h2 id="the-ugly">The ugly</h2>
<p>Sound is a big problem. The microphone, speakers and headphone port don&rsquo;t work under 3.18.5 and 3.19.0-rc7. The audio device is a ALC3263 from RealTek and it should use the same module as the RT286. However, the probing still fails and the module can&rsquo;t be used. The <a href="https://bugs.launchpad.net/ubuntu/+source/linux/+bug/1413446/comments/13">module code</a> seems to be correct but the probing still fails.</p>
<p>There&rsquo;s an open bug on Launchpad about the problem:</p>
<ul>
<li><a href="https://bugs.launchpad.net/ubuntu/+source/linux/+bug/1413446">Audio broken on 2015 XPS 13 (9343) in I2S mode in Ubuntu 14.10/15.04</a> [Launchpad bug]</li>
<li><a href="https://bugzilla.redhat.com/show_bug.cgi?id=1188741">No sound on Dell XPS 13 9343 (2015 model)</a> [Red Hat bug]</li>
<li><a href="https://bugzilla.kernel.org/show_bug.cgi?id=93361">broadwell-audio: rt286 device appears, no sound (Dell XPS 13 9343)</a> [Linux kernel bug]</li>
</ul>
<p>I connected up an old Syba USB audio device to the USB port and was able to get sound immediately. This is also a horrible workaround.</p>
<h2 id="what-now">What now?</h2>
<p>From what I gather, Dell is extremely eager to make Linux work on the new XPS 13 and we should see some movement on these bugs soon. I&rsquo;m still doing a bunch of testing on my own with kernel 3.19 and I&rsquo;ll be keeping this page updated as news becomes available.</p>
<p>If you know much about the I2C bus or about the sound devices in this laptop and you have some time available to help, just let me know where to send the beer. ;)</p>
<h2 id="latest-updates">Latest updates</h2>
<h4 id="2015-02-03">2015-02-03</h4>
<p>Added Red Hat bug link for sound issues.</p>
<h4 id="2015-02-05">2015-02-05</h4>
<p>The touchpad bug has been reduced to a kernel issue. Recordings from evemu-record look fine when they&rsquo;re played back in X. Users reported in Launchpad and in the Red Hat bug that kernel 3.16 works perfectly but 3.17 doesn&rsquo;t. A kernel bisection will most likely be required to find the patch that broke the touchpad.</p>
<p>Many users find that adding <code>acpi.os=&quot;!Windows 2013&quot;</code> to the kernel boot line will bring the audio card online after 1-3 reboots. Apparently there is some level of state information stored in memory that requires a few reboots to clear it. I haven&rsquo;t verified this yet.</p>
<h4 id="2015-02-06">2015-02-06</h4>
<p>Kernel bisect for the touchpad issue is underway. Every 3.16.x kernel I built would keep the trackpad in PS/2 mode and that&rsquo;s not helpful at all. There&rsquo;s no multi-finger taps/clicks/gestures. 3.17.0 works perfectly, however. My gut says something broke down between 3.17.0 and 3.18.0 but it might actually be closer to 3.17.4 since Fedora 21&rsquo;s initial kernel is 3.17.4 (and the touchpad doesn&rsquo;t work well with it).</p>
<p>A post was made on <a href="http://bartongeorge.net/2015/02/05/update-dell-xps-13-laptop-developer-edition-sputnik-gen-4/">Barton&rsquo;s Blog</a> yesterday about Dell being aware of the Linux issues. <em>(Thanks to Chris' comment below!)</em></p>
<p>After about 35 kernel builds during the most frustrating git bisect of my life, I found the <a href="https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=d1c7e29e8d276c669e8790bb8be9f505ddc48888">problematic patch</a>. The <a href="https://bugzilla.redhat.com/show_bug.cgi?id=1188439#c25">Red Hat bug</a> is updated now and I&rsquo;m hoping that someone with a detailed knowledge of this part of the kernel can make sense of it:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-diff" data-lang="diff">From d1c7e29e8d276c669e8790bb8be9f505ddc48888 Mon Sep 17 00:00:00 2001
From: Gwendal Grignou &lt;gwendal@chromium.org&gt;
Date: Thu, 11 Dec 2014 16:02:45 -0800
Subject: HID: i2c-hid: prevent buffer overflow in early IRQ

Before -&gt;start() is called, bufsize size is set to HID_MIN_BUFFER_SIZE,
64 bytes. While processing the IRQ, we were asking to receive up to
wMaxInputLength bytes, which can be bigger than 64 bytes.

Later, when -&gt;start is run, a proper bufsize will be calculated.

Given wMaxInputLength is said to be unreliable in other part of the
code, set to receive only what we can even if it results in truncated
reports.

Signed-off-by: Gwendal Grignou &lt;gwendal@chromium.org&gt;
Reviewed-by: Benjamin Tissoires &lt;benjamin.tissoires@redhat.com&gt;
Cc: stable@vger.kernel.org
Signed-off-by: Jiri Kosina &lt;jkosina@suse.cz&gt;

<span style="color:#cb4b16">diff --git a/drivers/hid/i2c-hid/i2c-hid.c b/drivers/hid/i2c-hid/i2c-hid.c
</span><span style="color:#cb4b16">index 747d544..9c014803b4 100644
</span><span style="color:#cb4b16"></span><span style="color:#dc322f">--- a/drivers/hid/i2c-hid/i2c-hid.c
</span><span style="color:#dc322f"></span><span style="color:#719e07">+++ b/drivers/hid/i2c-hid/i2c-hid.c
</span><span style="color:#719e07"></span><span style="color:#268bd2">@@ -369,7 +369,7 @@ static int i2c_hid_hwreset(struct i2c_client *client)
</span><span style="color:#268bd2"></span> static void i2c_hid_get_input(struct i2c_hid *ihid)
 {
 	int ret, ret_size;
<span style="color:#dc322f">-	int size = le16_to_cpu(ihid-&gt;hdesc.wMaxInputLength);
</span><span style="color:#dc322f"></span><span style="color:#719e07">+	int size = ihid-&gt;bufsize;
</span><span style="color:#719e07"></span>
 	ret = i2c_master_recv(ihid-&gt;client, ihid-&gt;inbuf, size);
 	if (ret != size) {
</code></pre></div><p>I reverted the patch in Linux 3.19-rc7 and built the kernel. The touchpad works flawlessly. However, simply reverting the patch probably isn&rsquo;t the best idea long term. ;)</p>
<h4 id="2015-02-07">2015-02-07</h4>
<p>The <a href="https://bugs.launchpad.net/ubuntu/+source/linux/+bug/1413446/+attachment/4313687/+files/rt288.patch">audio patch</a> mentioned in the <a href="https://bugs.launchpad.net/ubuntu/+source/linux/+bug/1413446#30">Launchpad bug report</a> didn&rsquo;t work for me on Linux 3.19-rc7.</p>
<h4 id="2015-02-10">2015-02-10</h4>
<p>Progress is still being made on the touchpad in the <a href="https://bugzilla.redhat.com/show_bug.cgi?id=1188439#c25">Red Hat bug</a> ticket. If you can live with the pad working as PS/2, you can get sound by adding <code>acpi_osi=&quot;!Windows 2013&quot;</code> to your kernel command line. Once you do that, you&rsquo;ll need to:</p>
<ol>
<li>Do a warm reboot</li>
<li>Wait for the OS to boot, then do a full poweroff</li>
<li>Boot the laptop, then do a full poweroff</li>
<li>Sound should now be working</li>
</ol>
<p>If sound still isn&rsquo;t working, you may need to install <code>pavucontrol</code>, the PulseAudio volume controller, and disable the HDMI sound output that is built into the Broadwell chip.</p>
<p>This obviously isn&rsquo;t a long-term solution, but it&rsquo;s a fair workaround.</p>
<h4 id="2015-02-11">2015-02-11</h4>
<p>There is now a <a href="https://bugzilla.redhat.com/attachment.cgi?id=990188">patch</a> that you can apply to 3.18 or 3.19 kernels that eliminates the trackpad freeze:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-diff" data-lang="diff">From 2a2aa272447d0ad4340c73db91bd8e995f6a0c3f Mon Sep 17 00:00:00 2001
From: Benjamin Tissoires &lt;benjamin.tissoires@redhat.com&gt;
Date: Tue, 10 Feb 2015 12:40:13 -0500
Subject: [PATCH] HID: multitouch: force release of touches when i2c
 communication is not reliable

The Dell XPS 13 9343 (2015) shows that from time to time, i2c_hid misses
some reports from the touchpad. This can lead to a freeze of the cursor
in user space when the missing report contains a touch release information.

Win 8 devices should have a contact count reliable, to we can safely
release all touches that has not been seen in the current report.

Signed-off-by: Benjamin Tissoires &lt;benjamin.tissoires@redhat.com&gt;
<span style="color:#dc322f">---
</span><span style="color:#dc322f"></span> drivers/hid/hid-multitouch.c | 8 ++++++++
 1 file changed, 8 insertions(+)

<span style="color:#cb4b16">diff --git a/drivers/hid/hid-multitouch.c b/drivers/hid/hid-multitouch.c
</span><span style="color:#cb4b16">index f65e78b..48b051e 100644
</span><span style="color:#cb4b16"></span><span style="color:#dc322f">--- a/drivers/hid/hid-multitouch.c
</span><span style="color:#dc322f"></span><span style="color:#719e07">+++ b/drivers/hid/hid-multitouch.c
</span><span style="color:#719e07"></span><span style="color:#268bd2">@@ -1021,6 +1021,14 @@ static int mt_probe(struct hid_device *hdev, const struct hid_device_id *id)
</span><span style="color:#268bd2"></span> 	if (id-&gt;vendor == HID_ANY_ID &amp;&amp; id-&gt;product == HID_ANY_ID)
 		td-&gt;serial_maybe = true;

<span style="color:#719e07">+	if ((id-&gt;group == HID_GROUP_MULTITOUCH_WIN_8) &amp;&amp; (hdev-&gt;bus == BUS_I2C))
</span><span style="color:#719e07">+		/*
</span><span style="color:#719e07">+		 * Some i2c sensors are not completely reliable with the i2c
</span><span style="color:#719e07">+		 * communication. Force release of unseen touches in a report
</span><span style="color:#719e07">+		 * to prevent bad behavior from user space.
</span><span style="color:#719e07">+		 */
</span><span style="color:#719e07">+		td-&gt;mtclass.quirks |= MT_QUIRK_NOT_SEEN_MEANS_UP;
</span><span style="color:#719e07">+
</span><span style="color:#719e07"></span> 	ret = hid_parse(hdev);
 	if (ret != 0)
 		return ret;
</code></pre></div><p>I&rsquo;ve tested it against 3.19-rc7 as well as Fedora&rsquo;s 3.18.5. However, tapping still doesn&rsquo;t work yet with more than one finger. The touchpad jumps around a bit when you apply two fingers to it.</p>
<h4 id="2015-02-12">2015-02-12</h4>
<p>Rene commented below that he <a href="http://mailman.alsa-project.org/pipermail/alsa-devel/2015-February/087462.html">found a post in alsa devel with a patch</a> for the “Dell Dino” that looks like it might help with the i2c audio issues. Another kernel maintainer replied and asked for some of the code to be rewritten to make it easier to handle audio quirks. UPDATE: Audio patch didn&rsquo;t work.</p>
<p>We&rsquo;ve created an IRC channel on Freenode: #xps13.</p>
<p>There&rsquo;s an interesting <a href="https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=2cc3f2347022969f00a429951ce489d35a9b4ea8">kernel patch mentioning “Dell Dino”</a> that is line for inclusion in 3.20-rc1. Someone in IRC found “Dell Dino” mentioned on a <a href="http://www.dell.com/us/business/p/xps-13-9343-laptop/fs">Dell business purchase page</a>. The board name from dmidecode in the patch is <strong>0144P8</strong> but that doesn&rsquo;t match other known board names. My i5-5200U with touch is <strong>0TM99H</strong> while a user with a non-touch i5 has a board name of <strong>OTRX4F</strong>. Other i5 touch models have the same board name as mine. All BIOS revisions found so far are A00 (the latest on Dell&rsquo;s site).</p>
<p>A <a href="http://paste.fedoraproject.org/185053/23803691/">probe for the rt286 module</a> looks like it starts to happen and then it fails (skip to line 795):</p>
<pre><code>[    4.141189] rt286 i2c-INT343A:00: probe
[    4.141245] i2c i2c-8: master_xfer[0] W, addr=0x1c, len=4
[    4.141246] i2c i2c-8: master_xfer[1] R, addr=0x1c, len=4
[    4.141249] i2c_designware INT3432:00: i2c_dw_xfer: msgs: 2
[    4.141389] i2c_designware INT3432:00: Standard-mode HCNT:LCNT = 432:507
[    4.141391] i2c_designware INT3432:00: Fast-mode HCNT:LCNT = 72:160
[    4.141662] i2c_designware INT3432:00: i2c_dw_isr:  Synopsys DesignWare I2C adapter enabled= 0x1 stat=0x10
[    4.141670] i2c_designware INT3433:00: i2c_dw_isr:  Synopsys DesignWare I2C adapter enabled= 0x1 stat=0x0
[    4.141695] i2c_designware INT3432:00: i2c_dw_isr:  Synopsys DesignWare I2C adapter enabled= 0x1 stat=0x750
[    4.141703] i2c_designware INT3433:00: i2c_dw_isr:  Synopsys DesignWare I2C adapter enabled= 0x1 stat=0x0
[    4.141965] i2c_designware INT3432:00: i2c_dw_handle_tx_abort: slave address not acknowledged (7bit mode)
[    4.141968] rt286 i2c-INT343A:00: Device with ID register 0 is not rt286
[    4.160506] i2c-core: driver [rt286] registered
</code></pre><p><strong><!-- raw HTML omitted -->2015-02-16<!-- raw HTML omitted --></strong></p>
<p>I received an email from a Realtek developer about the sound card in the XPS:</p>
<blockquote>
<p>I see “rt286 i2c-INT343A:00: Device with ID register 0 is not rt286” in the log. It means there are something wrong when the driver is trying to read the device id of codec. I believe that is due to I2C read/write issue. ALC3263 is a dual mode (I2S and HDA) codec. And BIOS will decide which mode according to OS type. So, if you want to use i2s mode, you need to configure your BIOS to set ALC3263 to I2S mode.</p>
</blockquote>
<p>After poring through the <a href="https://github.com/major/xps-13-9343-dsdt">DSDT and other ACPI tables</a> over the weekend (and building way too many kernels with overriden DSDT&rsquo;s), it sounds like a BIOS update may be required for the sound card to function properly. The <a href="https://github.com/major/xps-13-9343-dsdt/blob/master/DSDT.dsl#L8700">sound devices</a> specified in the DSDT that are on the i2c bus are only activated after a BUNCH of checks succeed. One of them is the check of <code>OSYS</code>, the system&rsquo;s operating system. Setting <code>acpi_osi=&quot;Windows 2013&quot;</code> does flip <code>OSYS</code> to <code>0x07DD</code>, but that&rsquo;s only part of the fix. There are other variables checked, like <code>CODS</code> (that shows up very often) that are instantiated early in the DSDT but I can&rsquo;t find them ever being set to a value anywhere in the DSDT code. These variables equal zero by default and that disables critical parts of the sound device.</p>
<p>My take: This laptop is going to need a BIOS update of some sort before we can get sound working properly in Linux with an i2c touchpad. If someone is more skilled with DSDT&rsquo;s than I am, I&rsquo;ll be glad to share all of my work that I&rsquo;ve tried so far. As for now, I&rsquo;m going to be waiting eagerly for some type of firmware update from Dell.</p>
<p><strong><!-- raw HTML omitted -->2015-02-17<!-- raw HTML omitted --></strong></p>
<p>There&rsquo;s some progress on the sound card in Linux! After building the latest commits from <a href="https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/log/">linux.git&rsquo;s master branch</a>, my XPS started showing a device called “broadwell-rt286” in pavucontrol. It showed up as a normal audio device but it had no output support, only input. I tried to enable the microphone but I couldn&rsquo;t record any sound.</p>
<p>I found a <a href="https://bugzilla.kernel.org/show_bug.cgi?id=92061#c24">kernel bug</a> from a ThinkPad Helix 2 user with a very similar hardware setup. Their rt286 device is on the I2S bus with a Haswell SoC. Their fix was to copy over the latest firmware binaries from <a href="http://git.kernel.org/cgit/linux/kernel/git/firmware/linux-firmware.git">linux-firmware.git</a> and reboot. I did the same and an output device suddenly showed up in pavucontrol after a reboot.</p>
<p>When I played sounds via aplay, canberra-gtk-play, and rhythmbox, I could see the signal level fluctuating in pavucontrol on the broadwell-rt286 device. However, I couldn&rsquo;t hear the sound through the speakers. I connected headphones and I couldn&rsquo;t hear any sound there either.</p>
<p>There&rsquo;s now a <a href="https://bugzilla.kernel.org/show_bug.cgi?id=93361">kernel bug ticket</a> open for the sound issue.</p>
<p>Stay tuned for a BIOS update with a potential keyboard repeat fix. It&rsquo;s already been talked about in IRC as a potential A01 release sssssssssssoon:</p>
<blockquote>
<p>someone asked about the fix for the repeating keypresses. yes, it was traced back to the source and will be fixed on all affected Dell platforms soon</p>
</blockquote>
<blockquote>
<p>I just saw that the one for 9343 was promoted to our factories so should be up on support.dell.com any day now as BIOS A01</p>
</blockquote>
<p>You can get notifications about driver releases for the XPS on <a href="http://www.dell.com/support/home/us/en/04/product-support/product/xps-13-9343-laptop/drivers">Dell&rsquo;s site</a>.</p>
<p><strong><!-- raw HTML omitted -->2015-03-04<!-- raw HTML omitted --></strong></p>
<p><!-- raw HTML omitted -->Sound on the I2S bus is working in Linux 4.0-rc2!<!-- raw HTML omitted --> <em>See note from 2015-03-08 below.</em> I was too exhausted last night for a full write-up, but here&rsquo;s the gist of what I did:</p>
<p>First off, build 4.0-rc2 with all of the available I2C and ALSA SoC modules. I haven&rsquo;t narrowed down which modules are critical quite yet. Once you&rsquo;ve built the kernel and rebooted into it, run <code>alsamixer</code> and choose the <code>broadwell-rt286</code> card. Hold the right arrow key until you go all the way to the right of the alsamixer display and press M to unmute the last control there. You should now be able to turn up the volume and play some test sounds.</p>
<p>Luckily, no update for linux-firmware is required. Also, there&rsquo;s no need for any ALSA UCM files as I had originally thought.</p>
<p>Stay tuned for a more in-depth write-up soon.</p>
<p><strong><!-- raw HTML omitted -->2015-03-08<!-- raw HTML omitted --></strong></p>
<p>After a few more reboots, I can&rsquo;t get sound working again. I&rsquo;m wondering if I had an errant <code>acpi_osi</code> setting somewhere during my testing that brought sound up on the HDA bus. :/</p>
]]></content></item><item><title>Helpful, low-FUD information security sites, mailing lists, and blogs</title><link>https://major.io/2015/01/08/helpful-low-fud-information-security-sites-mailing-lists-blogs/</link><pubDate>Thu, 08 Jan 2015 13:55:43 +0000</pubDate><guid>https://major.io/2015/01/08/helpful-low-fud-information-security-sites-mailing-lists-blogs/</guid><description>Keeping current with the latest trends and technologies in the realm of information security is critical and there are many options to choose from. However, as with any content on the internet, it takes some effort to find sites with a good signal-to-noise ratio. Information security is a heavily FUD-laden industry and I&amp;rsquo;ve taken some time to compile a list of helpful sites.
General sites Cryptanalys.is: https://cryptanalys.is/ Linux Weekly News (subscription optional but highly recommended): http://lwn.</description><content type="html"><![CDATA[<p><img src="/wp-content/uploads/2015/01/bookshelf-flickr-stevehuang7-e1420725220602.jpg" alt="1"></p>
<p>Keeping current with the latest trends and technologies in the realm of information security is critical and there are many options to choose from. However, as with any content on the internet, it takes some effort to find sites with a good signal-to-noise ratio. Information security is a heavily <a href="https://en.wikipedia.org/wiki/Fear,_uncertainty_and_doubt">FUD</a>-laden industry and I&rsquo;ve taken some time to compile a list of helpful sites.</p>
<h3 id="general-sites">General sites</h3>
<ul>
<li>Cryptanalys.is: <a href="https://cryptanalys.is/">https://cryptanalys.is/</a></li>
<li>Linux Weekly News <em>(subscription optional but highly recommended)</em>: <a href="http://lwn.net/">http://lwn.net/</a></li>
<li>Reddit&rsquo;s r/netsec: <a href="https://www.reddit.com/r/netsec">https://www.reddit.com/r/netsec</a></li>
<li>SANS Internet Storm Center: <a href="https://isc.sans.edu/">https://isc.sans.edu/</a></li>
<li>Wired: Threat Level: <a href="http://www.wired.com/category/threatlevel/">http://www.wired.com/category/threatlevel/</a></li>
</ul>
<h3 id="blogs">Blogs</h3>
<ul>
<li>Andy Ellis' Blog (CSO @ Akamai): <a href="https://blogs.akamai.com/author/andy-ellis/">https://blogs.akamai.com/author/andy-ellis/</a></li>
<li>Bruce Schneier&rsquo;s Blog: <a href="https://www.schneier.com/">https://www.schneier.com/</a></li>
<li>Imperial Violet (good SSL/crypto knowledge): <a href="https://www.imperialviolet.org/">https://www.imperialviolet.org/</a></li>
<li>Network Security Blog: <a href="http://www.mckeay.net/">http://www.mckeay.net/</a></li>
<li>Red Hat&rsquo;s Security Blog: <a href="https://securityblog.redhat.com/">https://securityblog.redhat.com/</a></li>
<li>TaoSecurity: <a href="http://taosecurity.blogspot.com/">http://taosecurity.blogspot.com/</a></li>
</ul>
<h3 id="mailing-lists">Mailing Lists</h3>
<ul>
<li>Apple Product Security ML: <a href="https://lists.apple.com/mailman/listinfo/security-announce/">https://lists.apple.com/mailman/listinfo/security-announce/</a></li>
<li>Full Disclosure List: <a href="http://seclists.org/fulldisclosure/">http://seclists.org/fulldisclosure/</a></li>
</ul>
<h3 id="humor-come-on-we-need-it">Humor (come on, we need it)</h3>
<ul>
<li>Security Reactions <em>(occasionally NSFW)</em>: <a href="http://securityreactions.tumblr.com/">http://securityreactions.tumblr.com/</a></li>
<li>DNS Reactions <em>(also occasionally NSFW)</em>: <a href="http://dnsreactions.tumblr.com/">http://dnsreactions.tumblr.com/</a></li>
</ul>
<hr>
<p>Many thanks to my <a href="http://rackspace.jobs/">coworkers</a> for helping to compile the list. If you have any others that you really enjoy, let me know! I&rsquo;ll be glad to add them to the post.</p>
<p><em>Photo Credit: <a href="https://www.flickr.com/photos/25400462@N04/2393673332/">stevehuang7</a> via <a href="http://compfight.com">Compfight</a> <a href="https://creativecommons.org/licenses/by-nc-nd/2.0/">cc</a></em></p>
]]></content></item><item><title>Two months with Google Play Music All Access</title><link>https://major.io/2014/12/29/two-months-google-play-music-access/</link><pubDate>Mon, 29 Dec 2014 21:26:37 +0000</pubDate><guid>https://major.io/2014/12/29/two-months-google-play-music-access/</guid><description>After using Spotify for a couple of months, I decided to try Google Play Music All Access. My experience was quite good from the start:
Music selection My music tastes include some very popular artists as well as some less popular ones. I found that Spotify and Google Play almost always had what I was looking for. Some artists only have portions of their catalog available for streaming and I found the selection of music in both products to be identical.</description><content type="html"><![CDATA[<p>After using Spotify for a couple of months, I decided to try <a href="https://play.google.com/about/music/allaccess/#/">Google Play Music All Access</a>. My experience was quite good from the start:</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<h3 id="music-selection">Music selection</h3>
<p>My music tastes include some very popular artists as well as some less popular ones. I found that Spotify and Google Play almost always had what I was looking for. Some artists only have portions of their catalog available for streaming and I found the selection of music in both products to be identical.</p>
<p>One feature of Google Play that I really enjoy is the ability to upload music. It&rsquo;s somewhat similar to iTunes Match. You can upload up to 20,000 tracks and then you can stream them to any of your devices after that. This really helps if you have some obscure music stored locally or if there&rsquo;s an album from one of your favorite artists that isn&rsquo;t available for streaming. Any music that you actually <em>purchase</em> from Google isn&rsquo;t included in that total. All of that is streamable anywhere, anytime. If there is music that you want to buy and keep forever, you can buy it and keep it without continuing your All Access subscription.</p>
<h3 id="interface">Interface</h3>
<p>There are several options in the interface depending on how you want to listen to your music. If you know of an artist or album you want to listen to, you can go straight there and begin listening. However, Google offers many options to discover new music.</p>
<p>One option is to find your favorite artist, album, or track and click <strong>Play Radio</strong>. Google tries to find very similar music to what you selected and it has an uncanny ability to get it right most of the time. It&rsquo;s almost creepy how well it finds the right music to go along with what you selected. I&rsquo;ve found a slew of new music this way simply by finding an artist I like and then playing the radio station from there.</p>
<p>There are also curated lists from other users and from Google. In addition, you can choose your mood or situation and let Google suggest some things. For example, Google lines up different selections depending on the time of day. Here&rsquo;s what I get during the day on a workday:</p>
<p><img src="/wp-content/uploads/2014/12/google_music_mood_selections.png" alt="2"></p>
<p>Selecting one of those options leads to a submenu where you can hone in on a genre of music. From there, you&rsquo;re offered three radio stations that meet your criteria with explanations of the music as well as a sample of the album art.</p>
<p>The interface is snappy in Chrome and rarely throws any errors. Various rich clients for Windows, Mac, and Linux exist if you&rsquo;re not a fan of playing music in your browser. As an added bonus, you get to watch music videos of tracks from Google Play Music on Youtube without ads.</p>
<h3 id="mobile">Mobile</h3>
<p>As you might expect, the Android integration is superb. Playing music on Wi-Fi or 4G is quick and you can broadcast music to Bluetooth receivers or Chromecast devices from the interface without much fuss. The basic music controls, like play/pause and prev/next track, are all available even from the lock screen.</p>
<p>I listen to music quite a bit in the car and you get three options for audio quality when you&rsquo;re off Wi-Fi. The lowest quality is pretty horrible but the middle and high settings are quite good. The middle setting seems to shave off 20-30% of the bandwidth requirements of the high setting but it still sounds reasonable.</p>
<p>You have a few options for offline caching. Any album, artist or track can be saved to your mobile device on demand. You can also create playlists and set those playlists to always be kept offline. Your mobile device will automatically download the music you add to those playlists within a few minutes. That&rsquo;s handy if you add music to playlists at work and then want to listen offline in the car while you drive home.</p>
<h3 id="improvements">Improvements</h3>
<p>You can only <em>stream</em> on one device at any one time. Offline playlists are excluded from that restriction but it would be nice to be able to stream to more than one device for an additional fee.</p>
<p>The &ldquo;Feeling Lucky&rdquo; radio station tries to guess what I like but it often seems to choose one genre of music. I&rsquo;m probably an oddball since I bounce between quite a few different genres of music but this has caused me to avoid using that feature.</p>
<h3 id="conclusion">Conclusion</h3>
<p>I&rsquo;m definitely a promoter of Google Play Music All Access. At only $10/month (plus some tax), it&rsquo;s much cheaper than what I was spending to purchase albums regularly and it allows me to access a huge supply of music from wherever I&rsquo;m located without loading files onto my mobile devices' small storage volumes.</p>
]]></content></item><item><title>Eight years at Rackspace</title><link>https://major.io/2014/12/18/eight-years-rackspace/</link><pubDate>Thu, 18 Dec 2014 14:00:38 +0000</pubDate><guid>https://major.io/2014/12/18/eight-years-rackspace/</guid><description/><content type="html">&lt;!-- raw HTML omitted -->
&lt;!-- raw HTML omitted --></content></item><item><title>Try out LXC with an Ansible playbook</title><link>https://major.io/2014/12/17/try-lxc-ansible-playbook/</link><pubDate>Wed, 17 Dec 2014 13:50:26 +0000</pubDate><guid>https://major.io/2014/12/17/try-lxc-ansible-playbook/</guid><description>The world of containers is constantly evolving lately. The latest turn of events involves the CoreOS developers when they announced Rocket as an alternative to Docker. However, LXC still lingers as a very simple path to begin using containers.
When I talk to people about LXC, I often hear people talk about how difficult it is to get started with LXC. After all, Docker provides an easy-to-use image downloading function that allows you to spin up multiple different operating systems in Docker containers within a few minutes.</description><content type="html"><![CDATA[<p><a href="/wp-content/uploads/2014/08/image-ansible.png"><!-- raw HTML omitted --></a>The world of containers is constantly evolving lately. The latest turn of events involves the CoreOS developers when they announced <a href="https://coreos.com/blog/rocket/">Rocket</a> as an alternative to <a href="https://www.docker.com/">Docker</a>. However, <a href="https://linuxcontainers.org/">LXC</a> still lingers as a very simple path to begin using containers.</p>
<p>When I talk to people about LXC, I often hear people talk about how difficult it is to get started with LXC. After all, Docker provides an easy-to-use image downloading function that allows you to spin up multiple different operating systems in Docker containers within a few minutes. It also comes with a daemon to help you manage your images and your containers.</p>
<p>Managing LXC containers using the basic LXC tools isn&rsquo;t terribly easy - I&rsquo;ll give you that. However, managing LXC through <a href="https://libvirt.org/drvlxc.html">libvirt</a> makes the process much easier. I <a href="/2014/04/21/launch-secure-lxc-containers-on-fedora-20-using-selinux-and-svirt/">wrote a little about this</a> earlier in the year.</p>
<p>I decided to turn the LXC container deployment process into an <a href="https://github.com/major/ansible-lxc">Ansible playbook</a> that you can use to automatically spawn an LXC container on any server or virtual machine. At the moment, only Fedora 20 and 21 are supported. I plan to add CentOS 7 and Debian support soon.</p>
<p>Clone the repository to get started:</p>
<pre><code>git clone https://github.com/major/ansible-lxc.git
cd ansible-lxc
ansible-playbook -i hosts playbook.yml
</code></pre><p>If you&rsquo;re running the playbook on the actual server or virtual machine where you want to run LXC, there&rsquo;s no need to alter the <code>hosts</code> file. You will need to adjust it if you&rsquo;re running your playbook from a remote machine.</p>
<p>As the playbook runs, it will install all of the necessary packages and begin assembling a Fedora 21 chroot. It will register the container with libvirt and do some basic configuration of the chroot so that it will work as a container. You&rsquo;ll end up with a running Fedora 21 LXC container that is using the built-in default NAT network created by libvirt. The playbook will print out the IP address of the container at the end. The default password for root is <em>fedora</em>. I wouldn&rsquo;t recommend leaving that for a production use container. ;)</p>
<p>All of the normal <code>virsh</code> commands should work on the container. For example:</p>
<pre><code># Stop the container gracefully
virsh shutdown fedora21
# Start the container
virsh start fedora21
</code></pre><p>Feel free to install the virt-manager tool and manage everything via a GUI locally or via X forwarding:</p>
<pre><code>yum -y install virt-manager dejavu* xorg-x11-xauth
# OPTIONAL: For a better looking virt-manager interface, install these, too
yum -y install gnome-icon-theme gnome-themes-standard
</code></pre>]]></content></item><item><title>Install sysstat on Fedora 21</title><link>https://major.io/2014/12/12/install-sysstat-fedora-21/</link><pubDate>Fri, 12 Dec 2014 17:55:57 +0000</pubDate><guid>https://major.io/2014/12/12/install-sysstat-fedora-21/</guid><description>One of the first tools I learned about after working with Red Hat was sysstat. It can write down historical records about your server at regular intervals. This can help you diagnose CPU usage, RAM usage, or network usage problems. In addition, sysstat also provides some handy command line utilities like vmstat, iostat, and pidstat that give you a live view of what your system is doing.
On Debian-based systems (including Ubuntu), you install the sysstat package and enable it with a quick edit to /etc/default/sysstat and the cron job takes it from there.</description><content type="html"><![CDATA[<p>One of the first tools I learned about after working with Red Hat was <a href="http://sebastien.godard.pagesperso-orange.fr/">sysstat</a>. It can write down historical records about your server at regular intervals. This can help you diagnose CPU usage, RAM usage, or network usage problems. In addition, sysstat also provides some handy command line utilities like <a href="http://linux.die.net/man/8/vmstat">vmstat</a>, <a href="http://linux.die.net/man/1/iostat">iostat</a>, and <a href="http://linux.die.net/man/1/pidstat">pidstat</a> that give you a live view of what your system is doing.</p>
<p>On Debian-based systems (including Ubuntu), you install the sysstat package and enable it with a quick edit to <code>/etc/default/sysstat</code> and the cron job takes it from there. CentOS and Fedora systems call the collector process using a cron job in <code>/etc/cron.d</code> and it&rsquo;s enabled by default.</p>
<p>Fedora 21 comes with sysstat 11 and there are now systemd unit files to control the collection and management of stats. You can find the unit files by listing the files in the sysstat RPM:</p>
<pre><code>$ rpm -ql sysstat | grep systemd
/usr/lib/systemd/system/sysstat-collect.service
/usr/lib/systemd/system/sysstat-collect.timer
/usr/lib/systemd/system/sysstat-summary.service
/usr/lib/systemd/system/sysstat-summary.timer
/usr/lib/systemd/system/sysstat.service
</code></pre><p>These services and timers <strong>aren&rsquo;t enabled by default</strong> in Fedora 21. If you run <code>sar</code> after installing sysstat, you&rsquo;ll see something like this:</p>
<pre><code># sar
Cannot open /var/log/sa/sa12: No such file or directory
Please check if data collecting is enabled
</code></pre><p>All you need to do is enable and start the main sysstat service:</p>
<pre><code>systemctl enable sysstat
systemctl start sysstat
</code></pre><p>From there, systemd will automatically call for collection and management of the statistics using its <a href="http://www.freedesktop.org/software/systemd/man/systemd.timer.html">internal timers</a>. Opening up <code>/usr/lib/systemd/system/sysstat-collect.timer</code> reveals the following:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ini" data-lang="ini"><span style="color:#586e75"># /usr/lib/systemd/system/sysstat-collect.timer</span>
<span style="color:#586e75"># (C) 2014 Tomasz Torcz &lt;tomek@pipebreaker.pl&gt;</span>
<span style="color:#586e75">#</span>
<span style="color:#586e75"># sysstat-11.0.0 systemd unit file:</span>
<span style="color:#586e75">#        Activates activity collector every 10 minutes</span>

<span style="color:#719e07">[Unit]</span>
Description<span style="color:#719e07">=</span><span style="color:#2aa198">Run system activity accounting tool every 10 minutes</span>

<span style="color:#719e07">[Timer]</span>
OnCalendar<span style="color:#719e07">=</span><span style="color:#2aa198">*:00/10</span>

<span style="color:#719e07">[Install]</span>
WantedBy<span style="color:#719e07">=</span><span style="color:#2aa198">sysstat.service</span>
</code></pre></div><p>The timer unit file ensures that the sysstat-collect.service is called every 10 minutes based on the real time provided by the system clock. (There are other options to set timers based on relative time of when the server booted or when a user logged into the system). The familiar <code>sa1</code> command appears in <code>/usr/lib/systemd/system/sysstat-collect.service</code>:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ini" data-lang="ini"><span style="color:#586e75"># /usr/lib/systemd/system/sysstat-collect.service</span>
<span style="color:#586e75"># (C) 2014 Tomasz Torcz &lt;tomek@pipebreaker.pl&gt;</span>
<span style="color:#586e75">#</span>
<span style="color:#586e75"># sysstat-11.0.0 systemd unit file:</span>
<span style="color:#586e75">#        Collects system activity data</span>
<span style="color:#586e75">#        Activated by sysstat-collect.timer unit</span>

<span style="color:#719e07">[Unit]</span>
Description<span style="color:#719e07">=</span><span style="color:#2aa198">system activity accounting tool</span>
Documentation<span style="color:#719e07">=</span><span style="color:#2aa198">man:sa1(8)</span>

<span style="color:#719e07">[Service]</span>
Type<span style="color:#719e07">=</span><span style="color:#2aa198">oneshot</span>
User<span style="color:#719e07">=</span><span style="color:#2aa198">root</span>
ExecStart<span style="color:#719e07">=</span><span style="color:#2aa198">/usr/lib64/sa/sa1 1 1</span>
</code></pre></div>]]></content></item><item><title>Send weechat notifications via Pushover</title><link>https://major.io/2014/12/05/send-weechat-notifications-via-pushover/</link><pubDate>Fri, 05 Dec 2014 16:11:13 +0000</pubDate><guid>https://major.io/2014/12/05/send-weechat-notifications-via-pushover/</guid><description>IRC is my main communication mechanism and I&amp;rsquo;ve gradually moved from graphical clients, to irssi and then to weechat. Text-based IRC removes quite a few distractions for me and it allows me to get access to my IRC communications from anything that can act as an ssh client.
I wanted a better way to get notifications when people send me messages and I&amp;rsquo;m away from my desk. Pushover is a great service that will take notification data via an API and blast it out to various devices.</description><content type="html"><![CDATA[<p>IRC is my main communication mechanism and I&rsquo;ve gradually moved from graphical clients, to <a href="http://irssi.org/">irssi</a> and then to <a href="https://weechat.org/">weechat</a>. Text-based IRC removes quite a few distractions for me and it allows me to get access to my IRC communications from anything that can act as an ssh client.</p>
<p>I wanted a better way to get notifications when people send me messages and I&rsquo;m away from my desk. <a href="https://pushover.net/">Pushover</a> is a great service that will take notification data via an API and blast it out to various devices. Once you configure your account, just install the mobile application on your device and you&rsquo;re ready to go.</p>
<p>Connecting weechat to Pushover is quite easy thanks to the <a href="http://weechat.org/scripts/source/pushover.pl.html/">pushover.pl</a> script. Go into your main weechat console (usually by pressing META/ALT/OPTION-1 on your keyboard) and install it:</p>
<pre><code>/script install pushover.pl
</code></pre><p>There are quite a few variables to configure. You can get a list of them by typing:</p>
<pre><code>/set plugins.var.perl.push*
</code></pre><p>You&rsquo;ll need two pieces of information to configure the plugin:</p>
<ul>
<li><strong>User key</strong>: The user key is displayed on your main account page when you login at Pushover.</li>
<li><strong>Application key</strong>: Click on <em>Register an Application</em> towards the bottom or <a href="https://pushover.net/apps/build">use this direct link</a>.</li>
</ul>
<p>Now you&rsquo;re ready to configure the plugin:</p>
<pre><code>/set plugins.var.perl.pushover.token [YOUR PUSHOVER APPLICATION TOKEN]
/set plugins.var.perl.pushover.user [YOUR USER KEY]
/set plugins.var.perl.pushover.enabled on
</code></pre><p>You can test it out quickly by using <a href="https://webchat.freenode.net/">Freenode&rsquo;s web chat</a> to send yourself a private message from another account.</p>
]]></content></item><item><title>Trust an IP address with firewalld’s rich rules</title><link>https://major.io/2014/11/24/trust-ip-address-firewallds-rich-rules/</link><pubDate>Mon, 24 Nov 2014 14:44:09 +0000</pubDate><guid>https://major.io/2014/11/24/trust-ip-address-firewallds-rich-rules/</guid><description>Managing firewall rules with iptables can be tricky at times. The rule syntax itself isn&amp;rsquo;t terribly difficult but you can quickly run into problems if you don&amp;rsquo;t save your rules to persistent storage after you get your firewall configured. Things can also get out of hand quickly if you run a lot of different tables with jumps scattered through each.
Why FirewallD? FirewallD&amp;rsquo;s goal is to make this process a bit easier by adding a daemon to the mix.</description><content type="html"><![CDATA[<p>Managing firewall rules with iptables can be tricky at times. The rule syntax itself isn&rsquo;t terribly difficult but you can quickly run into problems if you don&rsquo;t save your rules to persistent storage after you get your firewall configured. Things can also get out of hand quickly if you run a lot of different tables with jumps scattered through each.</p>
<h4 id="why-firewalld">Why FirewallD?</h4>
<p><a href="https://fedoraproject.org/wiki/FirewallD">FirewallD&rsquo;s</a> goal is to make this process a bit easier by adding a daemon to the mix. You can send firewall adjustment requests to the daemon and it handles the iptables syntax for you. It can also write firewall configurations to disk. It&rsquo;s especially useful on laptops since you can quickly jump between different firewall configurations based on the network you&rsquo;re using. You might run a different set of firewall rules at a coffee shop than you would run at home.</p>
<p>Adding a trusted IP address to a device running firewalld requires the use of <a href="https://fedoraproject.org/wiki/Features/FirewalldRichLanguage#Handle_rich_rules_with_the_command_line_client">rich rules</a>.</p>
<h4 id="an-example">An example</h4>
<p>Consider a situation where you have a server and you want to allow unrestricted connectivity to that server from a bastion or from your home internet connection. First off, determine your default zone (which is most likely &ldquo;public&rdquo; unless you&rsquo;ve changed it to something else):</p>
<pre><code># firewall-cmd --get-default-zone
public
</code></pre><p>We will use 11.22.33.44 as our example IP address. Let&rsquo;s add the rich rule:</p>
<pre><code>firewall-cmd --zone=public --add-rich-rule='rule family=&quot;ipv4&quot; source address=&quot;11.22.33.44&quot; accept'
</code></pre><p>Let&rsquo;s break down what we&rsquo;re asking firewalld to do. We&rsquo;re asking to allow IPv4 connectivity from 11.22.33.44 to all ports on the server and we&rsquo;re asking for that rule to be added to the <em>public</em> (default) zone. If you list the contents of your <em>public</em> zone, it should look like this:</p>
<pre><code># firewall-cmd --list-all --zone=public
public (default, active)
  interfaces: eth0
  sources:
  services: dhcpv6-client mdns ssh
  ports:
  masquerade: no
  forward-ports:
  icmp-blocks:
  rich rules:
	rule family=&quot;ipv4&quot; source address=&quot;11.22.33.44&quot; accept
</code></pre>]]></content></item><item><title>A response to Infoworld’s confusing article about Fedora</title><link>https://major.io/2014/11/19/response-infoworlds-confusing-article-fedora/</link><pubDate>Wed, 19 Nov 2014 14:24:57 +0000</pubDate><guid>https://major.io/2014/11/19/response-infoworlds-confusing-article-fedora/</guid><description>Working with the Fedora community is something I really enjoy in my spare time and I was baffled by a article I saw in Infoworld earlier last week. Here&amp;rsquo;s a link:
Red Hat confuses Linux users with latest Fedora moves
The article dives into the productization of Fedora 21 that hopes to deliver a better experience for workstation, server, and cloud users. The article suggests that Red Hat drove Fedora development and that the goals of Red Hat and Fedora are closely aligned.</description><content type="html"><![CDATA[<p>Working with the <a href="https://fedoraproject.org/join-fedora">Fedora community</a> is something I really enjoy in my spare time and I was baffled by a article I saw in Infoworld earlier last week. Here&rsquo;s a link:</p>
<p><a href="http://www.infoworld.com/article/2843687/linux/red-hat-fedora-confuses-linux-users.html">Red Hat confuses Linux users with latest Fedora moves</a></p>
<p>The article dives into the <a href="https://fedoraproject.org/wiki/Releases/21/FeatureList#Fedora_21_Products">productization of Fedora 21</a> that hopes to deliver a better experience for workstation, server, and cloud users. The article suggests that Red Hat drove Fedora development and that the goals of Red Hat and Fedora are closely aligned.</p>
<p>That couldn&rsquo;t be further from the truth.</p>
<p>I was heavily involved with the changes as a Fedora board member in 2013 and we had many lively discussions about which products should be offered, the use cases for each, and how development would proceed for each product. <a href="https://fedoraproject.org/wiki/Fedora_Engineering_Steering_Committee">FESCo</a> and the working groups <a href="http://fedoramagazine.org/fesco-announces-acceptance-of-fedora-next-prds/">trudged through the process</a> and worked diligently to ensure that users and developers weren&rsquo;t alienated by the process. It was impressive to see so many people from different countries, companies and skill levels come together and change the direction of the project into a more modern form.</p>
<p>Some of those board members, FESCo members, and working group members worked for Red Hat at the time. Based on the discussions, it was obvious to me that these community members wanted to make changes to improve the project based on their own personal desires. I never heard a mention of &ldquo;Red Hat wants to do…&rdquo; or &ldquo;this doesn&rsquo;t align with <!-- raw HTML omitted -->…&rdquo; during any part of the process. It was entirely community driven.</p>
<p>Some projects and products from Fedora eventually make it into the Red Hat product list (<a href="http://rhelblog.redhat.com/2014/04/15/rhel-7-rc-and-atomic-host/">Red Hat Atomic</a> is an example) but that usually involves Red Hat bringing a community effort under their umbrella and adding formal processes so they can offer it to their customers (and support it).</p>
<p>Fedora&rsquo;s community is vibrant, independent, and welcoming. If anyone is ever confused by the actions of the community, there are many great ways to <a href="https://fedoraproject.org/join-fedora">join the community</a> and learn more.</p>
]]></content></item><item><title>Test Fedora 21 at Rackspace with Ansible</title><link>https://major.io/2014/10/03/easily-test-fedora-21-rackspace-cloud/</link><pubDate>Fri, 03 Oct 2014 20:24:19 +0000</pubDate><guid>https://major.io/2014/10/03/easily-test-fedora-21-rackspace-cloud/</guid><description>Fedora 21 reached Alpha status last month and will reach beta status at the end of October. There are plenty of new features planned for the final release.
You can test Fedora 21 now using Rackspace&amp;rsquo;s Cloud Servers. I&amp;rsquo;ve assembled a small Ansible playbook that will automate the upgrade process from Fedora 20 to 21 and it takes around 7-9 minutes to complete.
The Ansible playbook is on GitHub along with instructions: ansible-rax-fedora21</description><content type="html"><![CDATA[<p><a href="/wp-content/uploads/2012/01/fedorainfinity.png"><!-- raw HTML omitted --></a>Fedora 21 reached Alpha status last month and will <a href="https://fedoraproject.org/wiki/Releases/21/Schedule">reach beta status</a> at the end of October. There are plenty of <a href="https://fedoraproject.org/wiki/Releases/21/ChangeSet">new features planned</a> for the final release.</p>
<p>You can test Fedora 21 now using Rackspace&rsquo;s Cloud Servers. I&rsquo;ve assembled a small Ansible playbook that will automate the upgrade process from Fedora 20 to 21 and it takes around 7-9 minutes to complete.</p>
<p>The Ansible playbook is on GitHub along with instructions: <a href="https://github.com/major/ansible-rax-fedora21">ansible-rax-fedora21</a></p>
]]></content></item><item><title>Xen’s XSA-108 patch and Fedora</title><link>https://major.io/2014/10/02/xens-xsa-108-patch-fedora/</link><pubDate>Thu, 02 Oct 2014 12:39:11 +0000</pubDate><guid>https://major.io/2014/10/02/xens-xsa-108-patch-fedora/</guid><description>Xen&amp;rsquo;s latest vulnerability, XSA-108, has generated a lot of buzz over the last week. Most of the attention has come from the reboot notifications from large cloud providers (including my employer).
The vulnerability allows a user within a guest to potentially read memory from another guest or the hypervisor itself. The window of available memory is small but it could be read many times over - much like how the Heartbleed vulnerability was exploited.</description><content type="html"><![CDATA[<p><a href="/wp-content/uploads/2012/06/xen_logo_small.png"><!-- raw HTML omitted --></a>Xen&rsquo;s latest vulnerability, <a href="http://xenbits.xen.org/xsa/advisory-108.html">XSA-108</a>, has generated a lot of buzz over the last week. Most of the attention has come from the reboot notifications from large cloud providers (including <a href="http://www.rackspace.com/blog/an-apology/">my employer</a>).</p>
<p>The vulnerability allows a user within a guest to potentially read memory from another guest or the hypervisor itself. The window of available memory is small but it could be read many times over - much like how the <a href="http://heartbleed.com/">Heartbleed</a> vulnerability was exploited. In some situations, these actions could cause the guest or the hypervisor to crash.</p>
<p>The fix involves a <a href="http://xenbits.xen.org/xsa/xsa108.patch">small patch</a> to the Xen hypervisor kernel. The patch is essentially a one-liner since the write operation was merely a no-op already.</p>
<p>Thanks to the efforts of Michael Young, new packages are in testing for Fedora 19, 20, and 21:</p>
<ul>
<li><a href="http://koji.fedoraproject.org/koji/buildinfo?buildID=582124">xen-4.2.5-3.fc19</a></li>
<li><a href="http://koji.fedoraproject.org/koji/buildinfo?buildID=582115">xen-4.3.3-3.fc20</a></li>
<li><a href="http://koji.fedoraproject.org/koji/buildinfo?buildID=582102">xen-4.4.1-6.fc21</a></li>
</ul>
<p>If you&rsquo;d like to test these packages now, you can install koji and download the RPM&rsquo;s directly:</p>
<pre><code>yum -y install koji
koji download-build --arch=x86_64 xen-4.2.5-3.fc19  # For Fedora 19
koji download-build --arch=x86_64 xen-4.3.3-3.fc20  # For Fedora 20
koji download-build --arch=x86_64 xen-4.4.1-6.fc21  # For Fedora 21
</code></pre><p>Use yum or rpm to install the new packages. Some servers may need to install all of the downloaded RPM&rsquo;s or only a portion of them. All of that depends on which Xen-related packages were installed already.</p>
<p>After testing, <em>please leave karma</em> in <a href="https://admin.fedoraproject.org/updates/xen">Bodhi</a> on the appropriate package page.</p>
]]></content></item><item><title>Apache’s mod_proxy, mod_ssl, and BitTorrent Sync</title><link>https://major.io/2014/09/27/apaches-mod_proxy-mod_ssl-bittorrent-sync/</link><pubDate>Sun, 28 Sep 2014 02:08:18 +0000</pubDate><guid>https://major.io/2014/09/27/apaches-mod_proxy-mod_ssl-bittorrent-sync/</guid><description>BitTorrent Sync allows you to keep files synchronized between multiple computers or mobile devices. It&amp;rsquo;s a handy way to do backups, share files with friends, or automate the movement of data from device to device. It comes with a web frontend, called the Web UI, that allows for connections over HTTP or HTTPS.
Using HTTP across the internet to administer Sync seems totally absurd, so I decided to enable HTTPS. I quickly realized two things:</description><content type="html"><![CDATA[<p><img src="/wp-content/uploads/2014/09/picjumbo.com_IMG_6970-e1411869964358.jpg" alt="1"></p>
<p><a href="https://www.getsync.com/">BitTorrent Sync</a> allows you to keep files synchronized between multiple computers or mobile devices. It&rsquo;s a handy way to do backups, share files with friends, or automate the movement of data from device to device. It comes with a web frontend, called the Web UI, that allows for connections over HTTP or HTTPS.</p>
<p>Using HTTP across the internet to administer Sync seems totally absurd, so I decided to enable HTTPS. I quickly realized two things:</p>
<ul>
<li>My SSL certificates were now specified in Apache and Sync</li>
<li>Sync&rsquo;s Web UI is relatively slow with SSL enabled (especially over higher latency links)</li>
</ul>
<p>I really wanted to keep things simple by wedging Sync into my existing Apache configuration using mod_proxy.  That was easier said than done since the Web UI has some hard-coded paths for certain assets, like CSS and Javascript.  After quite a bit of trial end error, this configuration works well:</p>
<pre><code>ProxyPass /btsync http://127.0.0.1:8888
ProxyPassReverse /btsync http://127.0.0.1:8888
ProxyHTMLURLMap http://127.0.0.1:8888 /btsync
Redirect permanent /gui /btsync/gui
</code></pre><p>The <em>ProxyPass</em> and <em>ProxyPassReverse</em> lines tell Apache where to proxy the requests and it also tells Apache to make requests <em>on behalf of</em> the browser making the request. The <em>ProxyHTMLURLMap</em> directive tells Apache that any requests to <code>/btsync</code> from a client browser should be translated as a request to the root directory (<code>/</code>) of the Web UI. The last line redirects hard-coded requests to <code>/gui</code> up to <code>/btsync/gui</code> instead.</p>
<p>When your configuration is in place, be sure to run a configuration check (<code>httpd -S</code>) and reload the Apache daemon. If you&rsquo;d like to access your application at a different URI, just replace <code>/btsync</code> in the example configuration with that URI.</p>
<p>Once all this is done, I&rsquo;m able to access Sync at <code>https://example.com/btsync</code> and Apache handles all of the backend requests properly. On some distributions, you may find that <em>mod_proxy_html</em> isn&rsquo;t installed by default. You&rsquo;ll need to install it if you want to use <em>ProxyHTMLURLMap</em> in your configuration. For Fedora users, just install it via yum:</p>
<pre><code>yum install mod_proxy_html
</code></pre><p><!-- raw HTML omitted -->Photo: <!-- raw HTML omitted -->Old Vintage Railway<!-- raw HTML omitted --> by <!-- raw HTML omitted -->Viktor Hanacek<!-- raw HTML omitted --><!-- raw HTML omitted --></p>
]]></content></item><item><title>HOWTO: Time Warner Cable and IPv6</title><link>https://major.io/2014/09/11/howto-time-warner-cable-ipv6/</link><pubDate>Thu, 11 Sep 2014 14:43:03 +0000</pubDate><guid>https://major.io/2014/09/11/howto-time-warner-cable-ipv6/</guid><description>Time Warner has gradually rolled out IPv6 connectivity to their Road Runner customers over the past couple of years and it started appearing on my home network earlier this year. I had some issues getting the leases to renew properly after they expired (TWC&amp;rsquo;s default lease length appears to be seven days) and there were some routing problems that cropped up occasionally. However, over the past month, things seem to have settled down on TWC&amp;rsquo;s San Antonio network.</description><content type="html"><![CDATA[<p><a href="/wp-content/uploads/2014/09/logo-top.png"><!-- raw HTML omitted --></a></p>
<p>Time Warner has <a href="http://www.twcableuntangled.com/2014/03/what-is-ipv6-twc-upgrades-the-internet/">gradually rolled</a> out <a href="https://en.wikipedia.org/wiki/IPv6">IPv6 connectivity</a> to their Road Runner customers over the past couple of years and it started appearing on my home network earlier this year.  I had some issues getting the leases to renew properly after they expired (TWC&rsquo;s default lease length appears to be seven days) and there were some routing problems that cropped up occasionally.  However, over the past month, things seem to have settled down on TWC&rsquo;s San Antonio network.</p>
<h3 id="do-you-have-ipv6-yet">Do you have IPv6 yet?</h3>
<p>Before you make any adjustments to your network, I&rsquo;d recommend connecting your computer directly to the cable modem briefly to see if you can get an IPv6 address via <a href="https://en.wikipedia.org/wiki/IPv6#Stateless_address_autoconfiguration_.28SLAAC.29">stateless autoconfiguration</a> (SLAAC).  You&rsquo;ll only get one IPv6 address via SLAAC, but we can get a bigger network block later on (keep reading).  Check your computer&rsquo;s network status to see if you received an IPv6 address.  If you have one, try accessing <a href="http://ipv6.google.com/">ipv6.google.com</a>.  You can always check <a href="http://ipv6.icanhazip.com">ipv6.icanhazip.com</a> or <a href="http://ipv6.icanhaztraceroute.com">ipv6.icanhaztraceroute.com</a> as well.</p>
<p>There&rsquo;s a chance your computer didn&rsquo;t get an IPv6 address while directly connected to the cable modem.  Here are some possible solutions:</p>
<ul>
<li>Power off the cable modem for 30 seconds, then plug it back in and see if your computer gets an address</li>
<li>Ensure you have one of TWC&rsquo;s <a href="http://www.timewarnercable.com/en/support/internet/topics/buy-your-modem.html">approved modems</a>. <em>(Bear in mind that not all of these modems support IPv6.)</em></li>
<li>Verify that your computer has IPv6 enabled. <em>(Instructions for <a href="http://windows.microsoft.com/en-us/windows/ipv6-faq">Windows</a>, <a href="http://support.apple.com/kb/HT4667">Mac</a> and <a href="http://www.linux.com/learn/tutorials/428331-ipv6-crash-course-for-linux">Linux</a> are available.)</em></li>
</ul>
<h3 id="but-i-want-more-addresses">But I want more addresses</h3>
<p>If you were able to get an IPv6 address, it&rsquo;s now time to allocate a network block for yourself and begin using it!  We will request an allocation via <a href="https://en.wikipedia.org/wiki/DHCPv6">DHCPv6</a>.  Every router is a little different, but the overall concept is the same.  Your router will request an allocation on the network and receive that allocation from Time Warner&rsquo;s network.  From there, your router will assign that block to an interface (most likely your LAN, more on that in a moment) and begin handing our IPv6 addresses to devices in your home.</p>
<p>By default, TWC hands out <a href="https://en.wikipedia.org/wiki/IPv6_subnetting_reference">/64 allocations</a> regardless of what you request via DHCPv6.  <!-- raw HTML omitted -->I had some success in late 2013 when I requested a /56 but it appears that allocations of that size aren&rsquo;t available any longer.  Sure, a /64 allocation is gigantic (bigger than the entire IPv4 address space), but getting a /56 would allow you to assign multiple /64 allocations to different interfaces.<!-- raw HTML omitted --> <strong>See the last section of this post on how to get a /56 allocation.</strong>  Splitting /64&rsquo;s into smaller subnets is a bad idea.</p>
<h3 id="lets-talk-security">Let&rsquo;s talk security</h3>
<p>IPv6 eliminates the need for <a href="https://en.wikipedia.org/wiki/Network_address_translation">network address translation</a> (NAT).  This means that by the time you finish this howto, each device in your network with have a publicly accessible internet address.  Also, bear in mind that with almost all network devices, firewall rules and ACL&rsquo;s that are configured with IPv4 will have no effect on IPv6.  This means that you&rsquo;ll end up with devices on your network with all of their ports exposed to the internet.</p>
<p>In Linux, be sure to use <a href="http://ipset.netfilter.org/ip6tables.man.html">ip6tables</a> (via <a href="https://fedoraproject.org/wiki/FirewallD">firewalld</a>, if applicable).  For other network devices, review their firewall configuration settings to see how you can filter IPv6 traffic.  <strong>This is a critical step.  Please don&rsquo;t skip it.</strong></p>
<p>On my Mikrotik device, I have a separate IPv6 firewall interface that I can configure.  Here is my default ruleset:</p>
<pre><code>/ipv6 firewall filter
/ipv6 firewall filter
add chain=input connection-state=related
add chain=input connection-state=established
add chain=forward connection-state=established
add chain=input in-interface=lanbridge
add chain=forward connection-state=related
add chain=input dst-port=546 protocol=udp
add chain=input protocol=icmpv6
add chain=forward protocol=icmpv6
add chain=forward out-interface=ether1-gateway
add action=drop chain=input
add action=drop chain=forward
</code></pre><p>The first five rules ensure that only related or established connections can make it to my internal LAN. I allow UDP 546 for DHCPv6 connectivity and I&rsquo;m allowing all ICMPv6 traffic to the router and internal devices. Finally, I allow all of my devices inside the network to talk to the internet and block the remainder of the unmatched traffic.</p>
<h3 id="configuring-the-router">Configuring the router</h3>
<p>It&rsquo;s no secret that I&rsquo;m a big fan of <a href="https://www.roc-noc.com/Mikrotik-Desktop-Routers/">Mikrotik</a> devices and I&rsquo;ll guide you through the setup of IPv6 on the Mikrotik in this post.  <strong>Before starting this step, ensure that your firewall is configured (see previous section).</strong></p>
<p>On the Mikrotik, just add a simple DHCPv6 configuration. I&rsquo;ll call mine &lsquo;twc&rsquo;:</p>
<pre><code>/ipv6 dhcp-client
add add-default-route=yes interface=ether1-gateway pool-name=twc
</code></pre><p>After that, you should see an allocation pop up within a few seconds (run <code>ipv6 dhcp-client print</code>):</p>
<pre><code>#    INTERFACE     STATUS        PREFIX                                      EXPIRES-AFTER
0    ether1-gat... bound         2605:xxxx:xxxx:xxxx::/64                    6d9h15m45s
</code></pre><p>Check that a new address pool was allocated by running <code>ipv6 pool print</code>:</p>
<pre><code>#   NAME      PREFIX                                      PREFIX-LENGTH EXPIRES-AFTER
0 D twc       2605:xxxx:xxxx:xxxx::/64                               64 6d9h13m33s
</code></pre><p>You can now assign that address pool to an interface. Be sure to assign the block to your LAN interface. In my case, that&rsquo;s called <em>lanbridge</em>:</p>
<pre><code>/ipv6 address
add address=2605:xxxx:xxxx:xxxx:: from-pool=twc interface=lanbridge
</code></pre><p>By default, the Mikrotik device will now begin announcing that network allocation on your internal network. Some of your devices may already be picking up IPv6 addresses via SLAAC! Try accessing the Google or icanhazip IPv6 addresses from earlier in the post.</p>
<p>Checking a Linux machine for IPv6 connectivity is easy. Here&rsquo;s an example from a Fedora 20 server I have at home:</p>
<pre><code>$ ip -6 addr
2: em1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qlen 1000
    inet6 2605:xxxx:xxxx:xxxx:xxxx:xxxx:xxxx:xxxx/64 scope global mngtmpaddr dynamic
       valid_lft 2591998sec preferred_lft 604798sec
    inet6 2605:xxxx:xxxx:xxxx:xxxx:xxxx:xxxx:xxxx/64 scope global deprecated mngtmpaddr dynamic
       valid_lft 1871064sec preferred_lft 0sec
</code></pre><p>If you only see an address that starts with <em>fe80</em>, that&rsquo;s your <a href="https://en.wikipedia.org/wiki/Link-local_address">link local</a> address. It&rsquo;s not an address that can be accessed from the internet.</p>
<h3 id="troubleshooting">Troubleshooting</h3>
<p>If you run into some problems or your router can&rsquo;t pull an allocation via DHCPv6, try the troubleshooting steps from the first section of this post.</p>
<p>Getting assistance from Time Warner is a real challenge. Everyone I&rsquo;ve contacted via phone or Twitter has not been able to help and many of them don&rsquo;t even know what IPv6 is. I was even told &ldquo;we have plenty of regular IPv4 addresses left, don&rsquo;t worry&rdquo; when I asked for help. Even my unusual methods haven&rsquo;t worked:</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>My old <a href="http://www.timewarnercable.com/en/residential-home/support/faqs/faqs-equipment-and-instruction-manuals/modems/motorola/motorola-surfboard-sbg6580.html">SBG6580</a> that was issued by Time Warner wouldn&rsquo;t ever do IPv6 reliably. I ended up buying a <a href="http://www.newegg.com/Product/Product.aspx?Item=N82E16825122015">SB6121</a> and I was able to get IPv6 connectivity fairly easily. The SB6121 only does 172mb/sec down - I&rsquo;ll be upgrading it if TWC MAXX shows up in San Antonio.</p>
<h3 id="get-a-56">Get a /56</h3>
<p>You can get a /56 block of IP addresses from Time Warner by adding <code>prefix-hint=::/56</code> onto your IPv6 dhcp client configuration. You&rsquo;ll need to carve out some /64 subnets on your own for your internal network and that&rsquo;s outside the scope of this post. The prefix hint configuration isn&rsquo;t available in the graphical interface or on the web (at the time of this post&rsquo;s writing).</p>
]]></content></item><item><title>Asus Maximus VI Gene – Error 55</title><link>https://major.io/2014/08/22/asus-maximus-vi-gene-error-55/</link><pubDate>Fri, 22 Aug 2014 14:20:23 +0000</pubDate><guid>https://major.io/2014/08/22/asus-maximus-vi-gene-error-55/</guid><description>It&amp;rsquo;s been quite a while since I built a computer but I decided to give it a try for a new hypervisor/NAS box at home. I picked up an Asus Maximus VI Gene motherboard since it had some good parts installed and it seems to work well with Linux. This was my first time doing water cooling for the CPU and I picked up a Seidon 240M after getting some recommendations.</description><content type="html"><![CDATA[<p>It&rsquo;s been quite a while since I built a computer but I decided to give it a try for a new hypervisor/NAS box at home. I picked up an <a href="http://www.asus.com/Motherboards/MAXIMUS_VI_GENE/">Asus Maximus VI Gene</a> motherboard since it had some good parts installed and it seems to work well with Linux. This was my first time doing water cooling for the CPU and I picked up a <a href="http://www.coolermaster.com/cooling/cpu-liquid-cooler/seidon-240m/">Seidon 240M</a> after getting some recommendations.</p>
<h3 id="rubber-hits-the-road">Rubber hits the road</h3>
<p>Once everything was in the box and the power was applied, I was stuck with an error code. There&rsquo;s a two-digit LCD display on the motherboard that rapidly flips between different codes during boot-up. If it stays on a code for a while and you don&rsquo;t get any display output, you have a problem. For me, this Asus Q code was 55.</p>
<p>The manual says it means that RAM isn&rsquo;t installed. I pulled out my four sticks of RAM and reseated all of them. I still got the same error. After reading a bunch of forum posts, I ran through a lot of troubleshooting steps:</p>
<ul>
<li>Reseat the RAM</li>
<li>Try one stick of RAM and add more until the error comes back</li>
<li>Reseat the CPU cooler (at least three times)</li>
<li>Reseat the CPU (at least three times)</li>
<li>Upgrade the BIOS</li>
<li>Clear the CMOS</li>
<li>Curse loudly, <a href="http://www.shiner.com/">drink a beer</a>, and come back</li>
</ul>
<p>I still had error 55 and wasn&rsquo;t going anywhere fast. After some further testing, I found that if I left the two RAM slots next to the CPU empty, the system would boot. If I put any RAM in the two left RAM slots (A1 and A2), the system wouldn&rsquo;t boot. Here&rsquo;s an excerpt from the manual:</p>
<p><img src="/wp-content/uploads/2014/08/asus_mb.png" alt="mobo_schematic">
CPU is on the left. RAM slots are A1, A2, B1, B2, left to right.</p>
<h3 id="fine-tuning-the-google-search">Fine-tuning the Google search</h3>
<p>I adjusted my Google terms to include &ldquo;A1 A2 slots&rdquo; and found more posts talking about CPU coolers being installed incorrectly. Mine had to be correct - I installed it four times! I decided to try re-installing it one last time.</p>
<p>When I removed the CPU cooler from the CPU, I noticed something strange. There are four standoffs around the CPU that the cooler would attach to with screws. Those standoffs screwed into posts that connected to a bracket on the back of the motherboard.</p>
<p><img src="/wp-content/uploads/2014/08/IMG_20140821_223505.jpg" alt="standoffs"> The lower two standoffs are highlighted.</p>
<p>I removed the two standoffs that were closest to the A1/A2 RAM slots and noticed something peculiar. One side of the standoff had a black coating that seemed a bit tacky while the other side of the standoff was bare metal. Three of the standoffs had the black side down (against the board) while one had the black side up. I unscrewed that standoff and found that the bare metal side was wedged firmly onto some connections that run from the CPU to the A1/A2 RAM slots. <em>Could this be the issue?</em></p>
<h3 id="eureka">Eureka</h3>
<p>After double-checking all of the CPU cooler standoffs and attaching the cooler to the board, I crossed my fingers and hit the power button. The machine shot through POST and I was staring down a Fedora logo that quickly led to a GDM login.</p>
<p><img src="/wp-content/uploads/2014/08/asus_mb_cooler_back.jpg" alt="culprit"> The culprit</p>
<p>I don&rsquo;t talk about hardware too often on the blog, but I certainly hopes this helps someone else who is desperately trying to find a solution.</p>
]]></content></item><item><title>Audit RHEL/CentOS 6 security benchmarks with ansible</title><link>https://major.io/2014/08/19/audit-rhelcentos-6-security-benchmarks-ansible/</link><pubDate>Tue, 19 Aug 2014 12:00:35 +0000</pubDate><guid>https://major.io/2014/08/19/audit-rhelcentos-6-security-benchmarks-ansible/</guid><description>Securing critical systems isn&amp;rsquo;t easy and that&amp;rsquo;s why security benchmarks exist. Many groups and communities distribute recommendations for securing servers, including NIST, the US Department of Defense (DoD), and the Center for Internet Security (CIS).
Although NIST and DoD are catching up quickly with newer OS releases, I&amp;rsquo;ve found that the CIS benchmarks are updated very regularly. CIS distributes auditing tools (with paid memberships) that require Java and they&amp;rsquo;re cumbersome to use, especially on servers where Java isn&amp;rsquo;t normally installed.</description><content type="html"><![CDATA[<p><a href="/wp-content/uploads/2014/08/image-ansible.png"><!-- raw HTML omitted --></a>Securing critical systems isn&rsquo;t easy and that&rsquo;s why security benchmarks exist. Many groups and communities distribute recommendations for securing servers, including <a href="https://web.nvd.nist.gov/view/ncp/repository">NIST</a>, the <a href="http://iase.disa.mil/stigs/Pages/index.aspx">US Department of Defense (DoD)</a>, and the <a href="http://benchmarks.cisecurity.org/downloads/">Center for Internet Security (CIS)</a>.</p>
<p>Although NIST and DoD are catching up quickly with newer OS releases, I&rsquo;ve found that the CIS benchmarks are updated very regularly. CIS distributes auditing tools (with paid memberships) that require Java and they&rsquo;re cumbersome to use, especially on servers where Java isn&rsquo;t normally installed.</p>
<h3 id="a-better-way-to-audit-security-benchmarks">A better way to audit security benchmarks</h3>
<p>I set out to create an Ansible playbook that would allow users to audit and (carefully!) remediate servers. The result is <a href="https://github.com/major/cis-rhel-ansible">on GitHub</a>. <em>Before we go any further, I&rsquo;d just like to state that I&rsquo;m not affiliated with CIS in any way and this repository hasn&rsquo;t been endorsed by CIS. Use it at your own risk.</em></p>
<p>Getting the playbook onto a machine is easy:</p>
<pre><code>git clone https://github.com/major/cis-rhel-ansible.git
</code></pre><p><!-- raw HTML omitted -->PLEASE review the <!-- raw HTML omitted -->README<!-- raw HTML omitted --> and <!-- raw HTML omitted -->NOTES<!-- raw HTML omitted --> files in the GitHub repository prior to running the playbook.<!-- raw HTML omitted --></p>
<p><a href="/wp-content/uploads/2014/08/What-Did-You-Do-Chris-Farley-Gif.gif"><!-- raw HTML omitted --></a></p>
<p><!-- raw HTML omitted -->Seriously. I mean it. This playbook could knock production environments offline.<!-- raw HTML omitted --></p>
<p>The tasks are split into sections (just like the CIS benchmarks themselves) and each section is split into Level 1 and 2 requirements.</p>
<h3 id="benchmark-levels">Benchmark levels</h3>
<p>Level 1 requirements provide good security improvements without a tremendous amount of intrusion into production workloads. With that said, they can still cause issues.</p>
<p>Level 2 requirements provide stronger security improvements but they can adversely affect production server environments. This is where you find things like SELinux, AIDE (including disabling prelinking), and some kernel tweaks for IPv6.</p>
<h3 id="how-to-use-it">How to use it</h3>
<p>I strongly recommend some dry runs with Ansible&rsquo;s <a href="http://docs.ansible.com/playbooks_checkmode.html">check mode</a> before trying to modify a production system. Also, you can run the playbook against a freshly-installed system and then deploy your applications on top of it. Find out what breaks and disable certain benchmarks that get in the way.</p>
<p>The entire playbook takes less than a minute to run locally on a Rackspace Performance Cloud Server. Your results may vary over remote ssh connections, but I was seeing the playbooks complete over ssh within three to four minutes.</p>
<p>You can also review the variables file to find all the knobs you need to get more aggressive in your audits. If you spot something potentially destructive that needs a variable added, let me know (or submit a pull request).</p>
<h3 id="its-open-source">It&rsquo;s open source</h3>
<p>The entire repository is licensed under <a href="https://www.apache.org/licenses/LICENSE-2.0.html">Apache License 2.0</a>, so please feel free to submit issues, pull requests, or patches.</p>
]]></content></item><item><title>Start Jenkins on Fedora 20</title><link>https://major.io/2014/08/13/get-jenkins-start-fedora-20/</link><pubDate>Wed, 13 Aug 2014 14:39:52 +0000</pubDate><guid>https://major.io/2014/08/13/get-jenkins-start-fedora-20/</guid><description>Installing Jenkins on Fedora 20 is quite easy thanks to the available Red Hat packages, but I ran into problems when I tried to start Jenkins. Here are the installation steps I followed:
wget -O /etc/yum.repos.d/jenkins.repo http://pkg.jenkins-ci.org/redhat/jenkins.repo rpm --import http://pkg.jenkins-ci.org/redhat/jenkins-ci.org.key yum -y install jenkins systemctl enable jenkins systemctl start jenkins Your first error will show up if Java isn&amp;rsquo;t installed. You can fix that by installing Java:
yum -y install java-1.</description><content type="html"><![CDATA[<p>Installing Jenkins on Fedora 20 is quite easy thanks to the <a href="http://pkg.jenkins-ci.org/redhat/">available Red Hat packages</a>, but I ran into problems when I tried to start Jenkins. Here are the installation steps I followed:</p>
<pre><code>wget -O /etc/yum.repos.d/jenkins.repo http://pkg.jenkins-ci.org/redhat/jenkins.repo
rpm --import http://pkg.jenkins-ci.org/redhat/jenkins-ci.org.key
yum -y install jenkins
systemctl enable jenkins
systemctl start jenkins
</code></pre><p>Your first error will show up if Java isn&rsquo;t installed. You can fix that by installing Java:</p>
<pre><code>yum -y install java-1.7.0-openjdk-headless
</code></pre><p>After installing Java, Jenkins still refused to start. Nothing showed up in the command line or via <code>journalctl -xn</code>, so I jumped into the Jenkins log file (found at <code>/var/log/jenkins/jenkins.log</code>):</p>
<pre><code>Aug 13, 2014 2:21:44 PM org.eclipse.jetty.util.log.JavaUtilLog info
INFO: jetty-8.y.z-SNAPSHOT
Aug 13, 2014 2:21:46 PM org.eclipse.jetty.util.log.JavaUtilLog info
INFO: NO JSP Support for , did not find org.apache.jasper.servlet.JspServlet
</code></pre><p>My Java knowledge is relatively limited, so I tossed the JSP error message into Google. A <a href="https://stackoverflow.com/questions/3521654/missing-jsp-support-in-jetty-or-confusing-log-message">stackoverflow</a> thread was the first result and it talked about a possible misconfiguration with Jetty. I tried their trick of using the OPTIONS environment variable, but that didn&rsquo;t work.</p>
<p>Then I realized that there wasn&rsquo;t a Jetty package installed on my server. Ouch. The installation continues:</p>
<pre><code>yum -y install jetty-jsp
</code></pre><p>Jenkins could now get off the ground and I saw the familiar log messages that I&rsquo;m more accustomed to seeing:</p>
<pre><code>Aug 13, 2014 2:24:26 PM hudson.WebAppMain$3 run
INFO: Jenkins is fully up and running
</code></pre><p>Much of these problems could stem from the fact that Jenkins RPM&rsquo;s are built to suit a wide array of system versions and the dependencies aren&rsquo;t configured correctly. My hope is that the <a href="https://fedoraproject.org/wiki/Changes/Jenkins">Jenkins project for Fedora 21</a> will alleviate some of these problems and give the user a better experience.</p>
]]></content></item><item><title>httpry 0.1.8 available for RHEL and CentOS 7</title><link>https://major.io/2014/08/13/httpry-rhel-centos-7/</link><pubDate>Wed, 13 Aug 2014 13:20:28 +0000</pubDate><guid>https://major.io/2014/08/13/httpry-rhel-centos-7/</guid><description>Red Hat Enterprise Linux and CentOS 7 users can now install httpry 0.1.8 in EPEL 7 Beta. The new httpry version is also available for RHEL/CentOS 6 and supported Fedora versions (19, 20, 21 branched, and rawhide).
Configuring EPEL on a RHEL/CentOS server is easy. Follow the instructions on EPEL&amp;rsquo;s site and install the epel-release RPM that matches your OS release version.
If you haven&amp;rsquo;t used httpry before, check the output on Jason Bittel&amp;rsquo;s site.</description><content type="html"><![CDATA[<p>Red Hat Enterprise Linux and CentOS 7 users can now install <a href="https://github.com/jbittel/httpry">httpry</a> <a href="https://github.com/jbittel/httpry/blob/master/doc/ChangeLog">0.1.8</a> in EPEL 7 Beta. The new httpry version is also available for RHEL/CentOS 6 and supported Fedora versions (19, 20, 21 branched, and rawhide).</p>
<p>Configuring EPEL on a RHEL/CentOS server is easy. <a href="https://fedoraproject.org/wiki/EPEL#How_can_I_use_these_extra_packages.3F">Follow the instructions</a> on EPEL&rsquo;s site and install the epel-release RPM that matches your OS release version.</p>
<p>If you haven&rsquo;t used httpry before, <a href="http://dumpsterventures.com/jason/httpry/">check the output</a> on Jason Bittel&rsquo;s site. It&rsquo;s a handy way to watch almost any type of HTTP server and see the traffic in an easier to read (and easier to grep) format.</p>
]]></content></item><item><title>Quickly post gists to GitHub Enterprise and github.com</title><link>https://major.io/2014/08/08/use-gist-gem-github-enterprise-github-com/</link><pubDate>Fri, 08 Aug 2014 21:13:07 +0000</pubDate><guid>https://major.io/2014/08/08/use-gist-gem-github-enterprise-github-com/</guid><description>The gist gem from GitHub allows you to quickly post text into a GitHub gist. You can use it with the public github.com site but you can also configure it to work with a GitHub Enterprise installation.
To get started, add two aliases to your ~/.bashrc:
alias gist=&amp;quot;gist -c&amp;quot; alias workgist=&amp;quot;GITHUB_URL=https://github.mycompany.com gist -c&amp;quot; The -c will copy the link to the gist to your keyboard whenever you use the gist tool on the command line.</description><content type="html"><![CDATA[<p><a href="/wp-content/uploads/2014/08/github.png"><!-- raw HTML omitted --></a></p>
<p>The <a href="https://github.com/defunkt/gist">gist gem</a> from GitHub allows you to quickly post text into a GitHub gist. You can use it with the public github.com site but you can also <a href="https://github.com/defunkt/gist#github-enterprise">configure it</a> to work with a GitHub Enterprise installation.</p>
<p>To get started, add two aliases to your <code>~/.bashrc</code>:</p>
<pre><code>alias gist=&quot;gist -c&quot;
alias workgist=&quot;GITHUB_URL=https://github.mycompany.com gist -c&quot;
</code></pre><p>The <code>-c</code> will copy the link to the gist to your keyboard whenever you use the gist tool on the command line. Now, go through the login process with each command after sourcing your updated <code>~/.bashrc</code>:</p>
<pre><code>source ~/.bashrc
gist --login
(follow the prompts to auth and get an oauth token from github.com)
workgist --login
(follow the prompts to auth and get an oauth token from GitHub Enterprise)
</code></pre><p>You&rsquo;ll now be able to use both aliases quickly from the command line:</p>
<pre><code>cat boring_public_data.txt | gist
cat super_sensitive_company_script.sh | workgist
</code></pre>]]></content></item><item><title>icanhazip.com blocked by Websense</title><link>https://major.io/2014/08/06/icanhazip-com-blocked-websense/</link><pubDate>Thu, 07 Aug 2014 01:16:54 +0000</pubDate><guid>https://major.io/2014/08/06/icanhazip-com-blocked-websense/</guid><description>UPDATE 2014-08-07: Websense emailed me to say that the site has been reviewed and found to be safe. It may take some time for all of their products to receive the updated classification.
Quite a few emails and IRC messages hit my screen today about icanhazip.com being blocked by Websense products. The report on Websense&amp;rsquo;s site claims shows that the site is part of a bot network: The URL analyzed is currently compromised to serve malicious content to visitors.</description><content type="html"><![CDATA[<p><em><strong>UPDATE 2014-08-07:</strong> Websense emailed me to say that the site has been reviewed and found to be safe. It may take some time for all of their products to receive the updated classification.</em></p>
<hr>
<p>Quite a few emails and IRC messages hit my screen today about <a href="/icanhazip-com-faq/" title="icanhazip.com FAQ">icanhazip.com</a> being blocked by Websense products. The report on Websense&rsquo;s site claims shows that the site is part of a bot network: <em>The URL analyzed is currently compromised to serve malicious content to visitors.</em></p>
<p>Here are some screenshots from the report:</p>
<p><a href="/wp-content/uploads/2014/08/icanhazip-websense-01.png"><!-- raw HTML omitted --></a></p>
<p><a href="/wp-content/uploads/2014/08/icanhazip-websense-02.png"><!-- raw HTML omitted --></a></p>
<p>I reached out to Websense on Twitter and via their site. In the report I sent to them, I explained how the site works, gave them a link to the FAQ, and directed them to several blog posts from this site about icanhazip.com. This response from Websense hit my inbox late today:</p>
<blockquote>
<p>Hello,</p>
<p>The site you submitted has been reviewed and determined to pose security risk. At this time, the site is not safe for browsing and is appropriately classified under the following category:</p>
<p>hxxp://icanhazip.com/ - Bot Networks</p>
<p>Researcher Notes: according to our findings, this site in question is embedded with Dyzap campaign malware.</p>
<p>For additional details related to this threat, please refer to the following source: <a href="https://www.bluecoat.com/security-blog/2014-08-01/dyzap-campaign-employs-freshly-minted-domains-and-other-tricks">https://www.bluecoat.com/security-blog/2014-08-01/dyzap-campaign-employs-freshly-minted-domains-and-other-tricks</a></p>
<p>The site will resume its content-based categorization, once it has been determined to no longer be a security risk.</p>
<p>For further investigation, please contact the website administrator.</p>
<p>If you have any questions and/or need any additional information, please let us know.</p>
<p>Thank you for your inquiry,</p>
<p>Lorna</p>
</blockquote>
<blockquote>
<p>Websense Labs</p>
</blockquote>
<p>Here&rsquo;s what I know:</p>
<ul>
<li>The application that serves up the icanhazip services <strong>is not compromised</strong></li>
<li>The virtual machine on which the application resides <strong>is not compromised</strong></li>
<li>The application is returning valid data with <strong>no evidence of serving malware</strong></li>
</ul>
<p>If Websense wishes to claim that the site is being used by malware, I can certainly believe that. However, if they claim the site is serving malicious content or actively participating in attacks in any way, I&rsquo;ve found no evidence that supports that claim.</p>
<p>I&rsquo;ll be reaching out to Websense again for additional details and to clear up the report listing on the website. If anyone knows of a way for me to identify this malware traffic and block it from accessing icanhazip.com, please let me know. My GPG key is <a href="https://pgp.mit.edu/pks/lookup?op=get&amp;search=0x9653FDDC6DC99178">available</a>.</p>
]]></content></item><item><title>Unexpected predictable network naming with systemd</title><link>https://major.io/2014/08/06/unexpected-predictable-network-naming-systemd/</link><pubDate>Wed, 06 Aug 2014 21:09:34 +0000</pubDate><guid>https://major.io/2014/08/06/unexpected-predictable-network-naming-systemd/</guid><description>While using a Dell R720 at work today, we stumbled upon a problem where the predictable network device naming with systemd gave us some unpredictable results. The server has four onboard network ports (two 10GbE and two 1GbE) and an add-on 10GbE card with two additional ports.
Running lspci gives this output:
# lspci | grep Eth 01:00.0 Ethernet controller: Intel Corporation Ethernet Controller 10-Gigabit X540-AT2 (rev 01) 01:00.1 Ethernet controller: Intel Corporation Ethernet Controller 10-Gigabit X540-AT2 (rev 01) 08:00.</description><content type="html"><![CDATA[<p><img src="/wp-content/uploads/2014/08/3240995967_04d7888d5c_o-e1407359174321.jpg" alt="1"></p>
<p>While using a Dell R720 at work today, we stumbled upon a problem where the <a href="http://www.freedesktop.org/wiki/Software/systemd/PredictableNetworkInterfaceNames/">predictable network device naming with systemd</a> gave us some unpredictable results. The server has four onboard network ports (two 10GbE and two 1GbE) and an add-on 10GbE card with two additional ports.</p>
<p>Running <em>lspci</em> gives this output:</p>
<pre><code># lspci | grep Eth
01:00.0 Ethernet controller: Intel Corporation Ethernet Controller 10-Gigabit X540-AT2 (rev 01)
01:00.1 Ethernet controller: Intel Corporation Ethernet Controller 10-Gigabit X540-AT2 (rev 01)
08:00.0 Ethernet controller: Intel Corporation I350 Gigabit Network Connection (rev 01)
08:00.1 Ethernet controller: Intel Corporation I350 Gigabit Network Connection (rev 01)
42:00.0 Ethernet controller: Intel Corporation Ethernet Controller 10-Gigabit X540-AT2 (rev 01)
42:00.1 Ethernet controller: Intel Corporation Ethernet Controller 10-Gigabit X540-AT2 (rev 01)
</code></pre><p>If you&rsquo;re not familiar with that output, it says:</p>
<ul>
<li>Two 10GbE ports on PCI bus 1 (ports 0 and 1)</li>
<li>Two 1GbE ports on PCI bus 8 (ports 0 and 1)</li>
<li>Two 10GbE ports on PCI bus 42 (ports 0 and 1)</li>
</ul>
<p>When the system boots up, the devices are named based on <a href="http://cgit.freedesktop.org/systemd/systemd/tree/src/udev/udev-builtin-net_id.c#n35">systemd-udevd&rsquo;s criteria</a>. Our devices looked like this after boot:</p>
<pre><code># ip addr | egrep ^[0-9]
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default
2: enp8s0f0: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc noop state DOWN group default qlen 1000
3: enp8s0f1: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc noop state DOWN group default qlen 1000
4: enp1s0f0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP group default qlen 1000
5: enp1s0f1: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc noop state DOWN group default qlen 1000
6: enp66s0f0: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc noop state DOWN group default qlen 1000
7: enp66s0f1: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc noop state DOWN group default qlen 1000
</code></pre><p>Devices 2-5 make sense since they&rsquo;re on PCI buses 1 and 8. However, our two-port NIC on PCI bus 42 has suddenly been named 66. We rebooted the server with the <em>rd.udev.debug</em> kernel command line to display debug messages from systemd-udevd during boot. That gave us this:</p>
<pre><code># journalctl | grep enp66s0f0
systemd-udevd[471]: renamed network interface eth0 to enp66s0f0
systemd-udevd[471]: NAME 'enp66s0f0' /usr/lib/udev/rules.d/80-net-setup-link.rules:13
systemd-udevd[471]: changing net interface name from 'eth0' to 'enp66s0f0'
systemd-udevd[471]: renamed netif to 'enp66s0f0'
systemd-udevd[471]: changed devpath to '/devices/pci0000:40/0000:40:02.0/0000:42:00.0/net/enp66s0f0'
</code></pre><p>So the system sees that the enp66s0f0 device is actually on PCI bus 42. What gives? A quick trip to #systemd on Freenode caused a facepalm:</p>
<pre><code>mhayden | weird, udev shows it on pci bus 42 but yet names it 66
    jwl | 0x42 = 66
</code></pre><p>I didn&rsquo;t expect to see hex. Sure enough, converting 42 in hex to decimal yields 66:</p>
<pre><code>$ printf &quot;%d\n&quot; 0x42
66
</code></pre><p>That also helps to explain why the devices on buses 1 and 8 were unaffected. Converting 1 and 8 in hex to decimal gives 1 and 8. If you&rsquo;re new to hex, this <a href="http://ascii.cl/conversion.htm">conversion table</a> may help.</p>
<p><em>Photo Credit: <!-- raw HTML omitted -->mindfieldz<!-- raw HTML omitted --> via <!-- raw HTML omitted -->Compfight<!-- raw HTML omitted --> <!-- raw HTML omitted -->cc<!-- raw HTML omitted --></em></p>
]]></content></item><item><title>Play/pause button stopped working in OS X Mavericks</title><link>https://major.io/2014/07/30/playpause-button-stopped-working-in-os-x-mavericks/</link><pubDate>Wed, 30 Jul 2014 14:31:04 +0000</pubDate><guid>https://major.io/2014/07/30/playpause-button-stopped-working-in-os-x-mavericks/</guid><description>My play/pause button mysteriously stopped working in iTunes and VLC mysteriously this week on my laptop. It affected the previous track and next track buttons as well. It turns out that my Google Music extension in Chrome stole the keyboard bindings after the extension updated this week.
If your buttons stopped working as well, follow these steps to check your keyboard shortcuts in Chrome:
Choose Preferences in the Chrome menu in the menu bar Click Extensions in the left sidebar Scroll all the way to the bottom of the page Click Keyboard Shortcuts Look at the key bindings in the Google Play Music section Your shortcuts might look like the ones shown here in an Apple support forum.</description><content type="html"><![CDATA[<p><img src="/wp-content/uploads/2014/07/apple_keyboard_from_flickr-e1406730573439.jpg" alt="1"></p>
<p>My play/pause button mysteriously stopped working in iTunes and VLC mysteriously this week on my laptop. It affected the previous track and next track buttons as well. It turns out that my Google Music extension in Chrome stole the keyboard bindings after the extension updated this week.</p>
<p>If your buttons stopped working as well, follow these steps to check your keyboard shortcuts in Chrome:</p>
<ul>
<li>Choose <strong>Preferences</strong> in the <strong>Chrome</strong> menu in the menu bar</li>
<li>Click <strong>Extensions</strong> in the left sidebar</li>
<li>Scroll all the way to the bottom of the page</li>
<li>Click <strong>Keyboard Shortcuts</strong></li>
<li>Look at the key bindings in the <strong>Google Play Music</strong> section</li>
</ul>
<p>Your shortcuts might look like the ones shown here in an <a href="https://discussions.apple.com/message/25754447#25754447">Apple support forum</a>. Click each box with the X to clear each key binding or click on the key binding box itself to bind it to another key combination. If you do that, it should end up like this:</p>
<p><a href="/wp-content/uploads/2014/07/google-play-music-keyboard-shortcut.png"><!-- raw HTML omitted --></a></p>
<p>You also have the options of switching the shortcuts to only work within Chrome by using the drop down menus to the right of the key binding boxes.</p>
<p><!-- raw HTML omitted -->Photo Credit: <!-- raw HTML omitted -->Andrew*<!-- raw HTML omitted --> via <!-- raw HTML omitted -->Compfight<!-- raw HTML omitted --> <!-- raw HTML omitted -->cc<!-- raw HTML omitted --><!-- raw HTML omitted --></p>
]]></content></item><item><title>Adventures in live booting Linux distributions</title><link>https://major.io/2014/07/29/adventures-in-live-booting-linux-distributions/</link><pubDate>Tue, 29 Jul 2014 13:05:54 +0000</pubDate><guid>https://major.io/2014/07/29/adventures-in-live-booting-linux-distributions/</guid><description>We&amp;rsquo;re all familiar with live booting Linux distributions. Almost every Linux distribution under the sun has a method for making live CD&amp;rsquo;s, writing live USB sticks, or booting live images over the network. The primary use case for some distributions is on a live medium (like KNOPPIX).
However, I embarked on an adventure to look at live booting Linux for a different use case. Sure, many live environments are used for demonstrations or installations - temporary activities for a desktop or a laptop.</description><content type="html"><![CDATA[<p>We&rsquo;re all familiar with live booting Linux distributions. Almost every Linux distribution under the sun has a method for making live CD&rsquo;s, writing live USB sticks, or booting live images over the network. The primary use case for some distributions is on a live medium (like <a href="http://www.knoppix.org/">KNOPPIX</a>).</p>
<p>However, I embarked on an adventure to look at live booting Linux for a different use case. Sure, many live environments are used for demonstrations or installations - temporary activities for a desktop or a laptop. My goal was to find a way to boot a large fleet of servers with live images. These would need to be long-running, stable, feature-rich, and highly configurable live environments.</p>
<p>Finding off the shelf solutions wasn&rsquo;t easy. Finding cross-platform off the shelf solutions for live booting servers was even harder. I worked on a solution with a coworker to create a cross-platform live image builder that we hope to open source soon. (I&rsquo;d do it sooner but the code is horrific.) ;)</p>
<h4 id="debian-jessie-testing">Debian jessie (testing)</h4>
<p>First off, we took a look at Debian&rsquo;s <a href="http://live.debian.net/">Live Systems project</a>. It consists of two main parts: something to build live environments, and something to help live environments boot well off the network. At the time of this writing, the live build process leaves a lot to be desired. There&rsquo;s a peculiar tree of directories that are required to get started and the documentation isn&rsquo;t terribly straightforward. Although there&rsquo;s a bunch of documentation available, it&rsquo;s difficult to follow and it seems to skip some critical details. <em>(In all fairness, I&rsquo;m an experienced Debian user but I haven&rsquo;t gotten into the innards of Debian package/system development yet. My shortcomings there could be the cause of my problems.)</em></p>
<p>The second half of the Live Systems project consist of multiple packages that help with the initial boot and configuration of a live instance. These tools work <strong>extremely well</strong>. Version 4 (currently in alpha) has tools for doing all kinds of system preparation very early in the boot process and it&rsquo;s compatible with SysVinit or systemd. The live images boot up with a simple <a href="https://en.wikipedia.org/wiki/SquashFS">SquashFS</a> (mounted read only) and they use AUFS to add on a writeable filesystem that stays in RAM. Reads and writes to the RAM-backed filesystem are extremely quick and you don&rsquo;t run into a brick wall when the filesystem fills up (more on that later with Fedora).</p>
<h4 id="ubuntu-1404">Ubuntu 14.04</h4>
<p>Ubuntu uses <a href="https://help.ubuntu.com/community/LiveCDCustomizationFromScratch">casper</a> which seems to precede Debian&rsquo;s Live Systems project or it could be a fork (please correct me if I&rsquo;m incorrect). Either way, it seemed a bit less mature than Debian&rsquo;s project and left a lot to be desired.</p>
<h4 id="fedora-and-centos">Fedora and CentOS</h4>
<p>Fedora 20 and CentOS 7 are very close in software versions and they use the same mechanisms to boot live images. They use <a href="https://dracut.wiki.kernel.org/index.php/Main_Page">dracut</a> to create the initramfs and there are a set of <a href="https://git.kernel.org/cgit/boot/dracut/dracut.git/tree/modules.d/90dmsquash-live">dmsquash modules</a> that handle the setup of the live image. The <a href="https://git.kernel.org/cgit/boot/dracut/dracut.git/tree/modules.d/90livenet">livenet</a> module allows the live images to be pulled over the network during the early part of the boot process.</p>
<p>Building the live images is a little tricky. You&rsquo;ll find good <a href="https://fedoraproject.org/wiki/LiveOS_image">documentation and tools</a> for standard live bootable CD&rsquo;s and USB sticks, but booting a server isn&rsquo;t as straightforward. Dracut expects to find a <a href="https://fedoraproject.org/wiki/LiveOS_image#Operating_system_file_systems">squashfs which contains a filesystem image</a>. When the live image boots, that filesystem image is connected to a loopback device and mounted read-only. A snapshot is made via device mapper that gives you a small overlay for adding data to the live image.</p>
<p>This overlay comes with some caveats. Keeping tabs on how quickly the overlay is filling up can be tricky. Using tools like <em>df</em> is insufficient since device mapper snapshots are concerned with blocks. As you write 4k blocks in the overlay, you&rsquo;ll begin to fill the snapshot, just as you would with an LVM snapshot. When the snapshot fills up and there are no blocks left, the filesystem in RAM becomes corrupt and unusable. There are some tricks to force it back online but I didn&rsquo;t have much luck when I tried to recover. The only solution I could find was to hard reboot.</p>
<h4 id="arch">Arch</h4>
<p>The ArchLinux live boot environments seem very similar to the ones I saw in Fedora and CentOS. All of them use dracut and systemd, so this makes sense. Arch once used a project called <a href="https://wiki.archlinux.org/index.php/larch">Larch</a> to create live environments but it&rsquo;s fallen out of support due to AUFS2 being removed (according to the <a href="https://wiki.archlinux.org/index.php/larch#Installation">wiki page</a>).</p>
<p>Although I didn&rsquo;t build a live environment with Arch, I booted one of their live ISO&rsquo;s and found their live environment to be much like Fedora and CentOS. There was a device mapper snapshot available as an overlay and once it&rsquo;s full, you&rsquo;re in trouble.</p>
<h4 id="opensuse">OpenSUSE</h4>
<p>The path to live booting an OpenSUSE image seems quite different. The live squashfs is mounted read only onto <em>/read-only</em>. An ext3 filesystem is created in RAM and is mounted on <em>/read-write</em>. From there, <a href="https://kernel.googlesource.com/pub/scm/linux/kernel/git/mszeredi/vfs/+/overlayfs.current/Documentation/filesystems/overlayfs.txt">overlayfs</a> is used to lay the writeable filesystem on top of the read-only squashfs. You can still fill up the overlay filesystem and cause some temporary problems, but you can back out those errant files and still have a useable live environment.</p>
<p>Here&rsquo;s the problem: overlayfs was given the green light for <em>consideration</em> in the Linux kernel <a href="https://lwn.net/Articles/542709/">by Linus in 2013</a>. It&rsquo;s been proposed for several kernel releases and it didn&rsquo;t make it into 3.16 (which will be released soon). OpenSUSE has wedged overlayfs into their kernel tree just as Debian and Ubuntu have wedged AUFS into theirs.</p>
<h4 id="wrap-up">Wrap-up</h4>
<p>Building highly customized live images isn&rsquo;t easy and running them in production makes it more challenging. Once the upstream kernel has a stable, solid, stackable filesystem, it should be much easier to operate a live environment for extended periods. There has been a parade of stackable filesystems over the years (remember <a href="http://funionfs.apiou.org/?lng=en">funion-fs</a>?) but I&rsquo;ve been told that overlayfs seems to be a solid contender. I&rsquo;ll keep an eye out for those kernel patches to land upstream but I&rsquo;m not going to hold my breath quite yet.</p>
]]></content></item><item><title>X11 forwarding request failed on channel 0</title><link>https://major.io/2014/07/24/x11-forwarding-request-failed-on-channel-0/</link><pubDate>Thu, 24 Jul 2014 19:24:32 +0000</pubDate><guid>https://major.io/2014/07/24/x11-forwarding-request-failed-on-channel-0/</guid><description>Forwarding X over ssh is normally fairly straightforward when you have the correct packages installed. I have another post about the errors that appear when you&amp;rsquo;re missing the xorg-x11-xauth (CentOS, Fedora, RHEL) or xauth (Debian, Ubuntu) packages.
Today&amp;rsquo;s error was a bit different. Each time I accessed a particular Debian server via ssh with X forwarding requested, I saw this:
$ ssh -YC myserver.example.com X11 forwarding request failed on channel 0 The xauth package was installed and I found a .</description><content type="html"><![CDATA[<p>Forwarding X over ssh is normally fairly straightforward when you have the correct packages installed. I have <a href="/2012/07/14/x-forwarding-over-ssh-woes-display-is-not-set/">another post</a> about the errors that appear when you&rsquo;re missing the <em>xorg-x11-xauth</em> (CentOS, Fedora, RHEL) or <em>xauth</em> (Debian, Ubuntu) packages.</p>
<p>Today&rsquo;s error was a bit different. Each time I accessed a particular Debian server via ssh with X forwarding requested, I saw this:</p>
<pre><code>$ ssh -YC myserver.example.com
X11 forwarding request failed on channel 0
</code></pre><p>The <em>xauth</em> package was installed and I found a <em>.Xauthority</em> file in root&rsquo;s home directory. Removing the <em>.Xauthority</em> file and reconnecting via ssh didn&rsquo;t help. After some searching, I stumbled upon a <a href="https://gist.github.com/adrianratnapala/1324845">GitHub gist</a> that had some suggestions for fixes.</p>
<p>On this particular server, IPv6 was disabled. That caused the error. The quickest fix was to restrict sshd to IPv4 only by adding this line to <em>/etc/ssh/sshd_config</em>:</p>
<pre><code>AddressFamily inet
</code></pre><p>I restarted the ssh daemon and I was able to forward X applications over ssh once again.</p>
]]></content></item><item><title>Etsy reminds us that information security is an active process</title><link>https://major.io/2014/07/22/etsy-reminds-us-that-information-security-is-an-active-process/</link><pubDate>Tue, 22 Jul 2014 13:06:23 +0000</pubDate><guid>https://major.io/2014/07/22/etsy-reminds-us-that-information-security-is-an-active-process/</guid><description>I&amp;rsquo;m always impressed with the content published by folks at Etsy and Ben Hughes' presentation from DevOpsDays Minneapolis 2014 is no exception.
Ben adds some levity to the topic of information security with some hilarious (but relevant) images and reminds us that security is an active process that everyone must practice. Everyone plays a part - not just traditional corporate security employees.
I&amp;rsquo;ve embedded the presentation here for your convenience:</description><content type="html"><![CDATA[<p>I&rsquo;m always impressed with the content published by folks at <a href="http://codeascraft.com/">Etsy</a> and <a href="https://twitter.com/benjammingh">Ben Hughes'</a> <a href="https://speakerdeck.com/barnbarn/handmade-security-at-etsy">presentation from DevOpsDays Minneapolis 2014</a> is no exception.</p>
<p>Ben adds some levity to the topic of information security with some hilarious (but relevant) images and reminds us that security is an active process that everyone must practice. Everyone plays a part - not just traditional corporate security employees.</p>
<p>I&rsquo;ve embedded the presentation here for your convenience:</p>
<p>Here&rsquo;s a link to the original presentation on SpeakerDeck:</p>
<ul>
<li><a href="https://speakerdeck.com/barnbarn/handmade-security-at-etsy">Handmade security at Etsy</a></li>
</ul>
]]></content></item><item><title>icanhazip and CORS</title><link>https://major.io/2014/07/21/icanhazip-and-cors/</link><pubDate>Mon, 21 Jul 2014 14:31:45 +0000</pubDate><guid>https://major.io/2014/07/21/icanhazip-and-cors/</guid><description>I received an email from an icanhazip.com user last week about enabling cross-origin resource sharing. He wanted to use AJAX calls on a different site to pull data from icanhazip.com and use it for his visitors.
Those headers are now available for all requests to the services provided by icanhazip.com! Here&amp;rsquo;s what you&amp;rsquo;ll see:
$ curl -i icanhazip.com --- Access-Control-Allow-Origin: * Access-Control-Allow-Methods: GET ---</description><content type="html"><![CDATA[<p>I received an email from an <a href="/icanhazip-com-faq/" title="icanhazip.com FAQ">icanhazip.com</a> user last week about <a href="http://enable-cors.org/index.html">enabling cross-origin resource sharing</a>. He wanted to use AJAX calls on a different site to pull data from icanhazip.com and use it for his visitors.</p>
<p>Those headers are now available for all requests to the services provided by icanhazip.com! Here&rsquo;s what you&rsquo;ll see:</p>
<pre><code>$ curl -i icanhazip.com
---
Access-Control-Allow-Origin: *
Access-Control-Allow-Methods: GET
---
</code></pre>]]></content></item><item><title>AVC: denied dyntransition from sshd</title><link>https://major.io/2014/07/03/avc-denied-dyntransition-from-sshd/</link><pubDate>Thu, 03 Jul 2014 19:52:51 +0000</pubDate><guid>https://major.io/2014/07/03/avc-denied-dyntransition-from-sshd/</guid><description>I&amp;rsquo;ve been working with some Fedora environments in chroots and I ran into a peculiar SELinux AVC denial a short while ago:
avc: denied { dyntransition } for pid=809 comm=&amp;quot;sshd&amp;quot; scontext=system_u:system_r:kernel_t:s0 tcontext=system_u:system_r:sshd_net_t:s0 tclass=process The ssh daemon is running on a non-standard port but I verified that the port is allowed with semanage port -l. The target context of sshd_net_t from the AVC seems sensible for the ssh daemon. I started to wonder if a context wasn&amp;rsquo;t applied correctly to the sshd excutable itself, so I checked within the chroot:</description><content type="html"><![CDATA[<p>I&rsquo;ve been working with some Fedora environments in chroots and I ran into a peculiar SELinux AVC denial a short while ago:</p>
<pre><code>avc:  denied  { dyntransition } for  pid=809 comm=&quot;sshd&quot; scontext=system_u:system_r:kernel_t:s0 tcontext=system_u:system_r:sshd_net_t:s0 tclass=process
</code></pre><p>The ssh daemon is running on a non-standard port but I verified that the port is allowed with <code>semanage port -l</code>. The target context of <em>sshd_net_t</em> from the AVC seems sensible for the ssh daemon. I started to wonder if a context wasn&rsquo;t applied correctly to the sshd excutable itself, so I checked within the chroot:</p>
<pre><code># ls -alZ /usr/sbin/sshd
-rwxr-xr-x. 1 root root system_u:object_r:sshd_exec_t:SystemLow 652816 May 15 03:56 /usr/sbin/sshd
</code></pre><p>That&rsquo;s what it should be. I double-checked my running server (which booted a squashfs containing the chroot) and saw something wrong:</p>
<pre><code># ls -alZ /usr/sbin/sshd
-rwxr-xr-x. root root system_u:object_r:file_t:s0      /usr/sbin/sshd
</code></pre><p>How did <em>file_t</em> get there? It turns out that I was using rsync to drag data out of the chroot and I forgot to use the <code>--xattrs</code> argument with rsync.</p>
]]></content></item><item><title>Install Debian packages without starting daemons</title><link>https://major.io/2014/06/26/install-debian-packages-without-starting-daemons/</link><pubDate>Thu, 26 Jun 2014 20:39:44 +0000</pubDate><guid>https://major.io/2014/06/26/install-debian-packages-without-starting-daemons/</guid><description>My work at Rackspace has involved working with a bunch of Debian chroots lately. One problem I had was that daemons tried to start in the chroot as soon as I installed them. That created errors and made my ansible output look terrible.
If you&amp;rsquo;d like to prevent daemons from starting after installing a package, just toss a few lines into /usr/sbin/policy-rc.d:
cat &amp;gt; /usr/sbin/policy-rc.d &amp;lt;&amp;lt; EOF #!/bin/sh echo &amp;#34;All runlevel operations denied by policy&amp;#34; &amp;gt;&amp;amp;2 exit 101 EOF Now, install any packages that you need and the daemons will remain stopped until you start them (or reboot the server).</description><content type="html"><![CDATA[<p>My work at Rackspace has involved working with a bunch of Debian chroots lately. One problem I had was that daemons tried to start in the chroot as soon as I installed them. That created errors and made my ansible output look terrible.</p>
<p>If you&rsquo;d like to prevent daemons from starting after installing a package, just toss a few lines into <em>/usr/sbin/policy-rc.d</em>:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">cat &gt; /usr/sbin/policy-rc.d <span style="color:#2aa198">&lt;&lt; EOF
</span><span style="color:#2aa198">#!/bin/sh
</span><span style="color:#2aa198">echo &#34;All runlevel operations denied by policy&#34; &gt;&amp;2
</span><span style="color:#2aa198">exit 101
</span><span style="color:#2aa198">EOF</span>
</code></pre></div><p>Now, install any packages that you need and the daemons will remain stopped until you start them (or reboot the server). Be sure to remove the policy file you added once you&rsquo;re done installing your packages.</p>
<hr>
<p>This seems like a good opportunity to get on a soapbox about automatically starting daemons. ;)</p>
<p>I still have a very difficult time understanding why Debian-based distributions start daemons as soon as the package is installed. Having an option to enable this might be useful for some situations, but this <strong>shouldn&rsquo;t be the default</strong>.</p>
<p>You end up with situations like the one in this <a href="https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=723080">puppet bug report</a>. The daemon shouldn&rsquo;t start until you&rsquo;re ready to configure it and use it. However, the logic is that the daemon is so horribly un-configured that it shouldn&rsquo;t hurt anything if starts immediately. So why start the daemon at all?</p>
<p>When I run the command <em>apt-get install</em> or <em>yum install</em>, I expect that packages will be installed to disk and nothing more. Even the <a href="https://en.wiktionary.org/wiki/install">definition of the English word &ldquo;install&rdquo;</a> talks about “preparing” something for use, not actually using it:</p>
<blockquote>
<p>To connect, set up or prepare something for use</p>
</blockquote>
<p><strong>If I install an electrical switch at home, I don&rsquo;t install it in the ON position with my circuit breaker in the ON position.</strong> I install it with everything off, verify my work, ensure that it fits in place, and then I apply power. The installation and actual <strong>use</strong> of the new switch are two completely separate activities with additional work required in between.</p>
<p>I strongly urge the Debian community to consider switching to a mechanism where daemons don&rsquo;t start until the users configure them properly and are ready to use them. This makes configuration management much easier, improves security, and provides consistency with almost every other Linux distribution.</p>
]]></content></item><item><title>Get colorful ansible output in Jenkins</title><link>https://major.io/2014/06/25/get-colorful-ansible-output-in-jenkins/</link><pubDate>Wed, 25 Jun 2014 21:32:18 +0000</pubDate><guid>https://major.io/2014/06/25/get-colorful-ansible-output-in-jenkins/</guid><description>Working with ansible is enjoyable, but it&amp;rsquo;s a little bland when you use it with Jenkins. Jenkins doesn&amp;rsquo;t spawn a TTY and that causes ansible to skip over the code that outputs status lines with colors. The fix is relatively straightforward.
First, install the AnsiColor Plugin on your Jenkins node.
Once that&amp;rsquo;s done, edit your Jenkins job so that you export ANSIBLE_FORCE_COLOR=true before running ansible:
export ANSIBLE_FORCE_COLOR=true ansible-playbook -i hosts site.</description><content type="html"><![CDATA[<p>Working with ansible is enjoyable, but it&rsquo;s a little bland when you use it with Jenkins. Jenkins doesn&rsquo;t spawn a TTY and that causes ansible to skip over the code that outputs status lines with colors. The fix is relatively straightforward.</p>
<p>First, install the <a href="https://wiki.jenkins-ci.org/display/JENKINS/AnsiColor+Plugin">AnsiColor Plugin</a> on your Jenkins node.</p>
<p>Once that&rsquo;s done, edit your Jenkins job so that you export <em>ANSIBLE_FORCE_COLOR=true</em> before running ansible:</p>
<pre><code>export ANSIBLE_FORCE_COLOR=true
ansible-playbook -i hosts site.yml
</code></pre><p>If your ansible playbook requires sudo to run properly on your local host, be sure to use the <em>-E</em> option with sudo so that your environment variables are preserved when your job runs. For example:</p>
<pre><code>export ANSIBLE_FORCE_COLOR=true
sudo -E ansible-playbook -i hosts site.yml
</code></pre><p><strong>HOLD UP:</strong> As <a href="http://twitter.com/SamJSharpe/status/481921454263787520">Sam Sharpe</a> reminded me, the better way to handle environment variables with sudo is to add them to <em>env_keep</em> in your sudoers file (use <code>visudo</code> to edit it):</p>
<pre><code>Defaults        env_reset
Defaults        env_keep += &quot;ANSIBLE_FORCE_COLOR&quot;
</code></pre><p>Adding it to <em>env_keep</em> is a more secure method and you won&rsquo;t need the <em>-E</em> any longer on the command line.</p>
<p>While you&rsquo;re on the configuration page for your Jenkins job, look for <em>Color ANSI Console Output</em> under the <em>Build Environment</em> section. Enable it and ensure <em>xterm</em> is selected in the drop-down box.</p>
<p>Save your new configuration and run your job again. You should have some awesome colors in your console output when your ansible job runs.</p>
]]></content></item><item><title>Performance benchmarks: KVM vs. Xen</title><link>https://major.io/2014/06/22/performance-benchmarks-kvm-vs-xen/</link><pubDate>Sun, 22 Jun 2014 17:00:14 +0000</pubDate><guid>https://major.io/2014/06/22/performance-benchmarks-kvm-vs-xen/</guid><description>After having some interesting discussions last week around KVM and Xen performance improvements over the past years, I decided to do a little research on my own. The last complete set of benchmarks I could find were from the Phoronix Haswell tests in 2013. There were some other benchmarks from 2011 but those were hotly debated due to the Xen patches headed into kernel 3.0.
The 2011 tests had a good list of benchmarks and I&amp;rsquo;ve done my best to replicate that list here three years later.</description><content type="html"><![CDATA[<p>After having some interesting discussions last week around KVM and Xen performance improvements over the past years, I decided to do a little research on my own. The last complete set of benchmarks I could find were from the <a href="http://www.phoronix.com/scan.php?page=article&amp;item=intel_haswell_virtualization&amp;num=1">Phoronix Haswell tests in 2013</a>. There were <a href="http://blog.xen.org/index.php/2011/11/29/baremetal-vs-xen-vs-kvm-redux/">some other benchmarks from 2011</a> but those were hotly debated due to the Xen patches headed into kernel 3.0.</p>
<p>The 2011 tests had a <a href="http://blog.xen.org/wp-content/uploads/2011/11/overview.png">good list of benchmarks</a> and I&rsquo;ve done my best to replicate that list here three years later. I&rsquo;ve removed two or three of the benchmark tests because they didn&rsquo;t run well without extra configuration or they took an extremely long time to run.</p>
<h4 id="testing-environment">Testing environment</h4>
<p>My testing setup consists of two identical SuperMicro servers. Both have a single <a href="http://ark.intel.com/products/52269/Intel-Xeon-Processor-E3-1220-8M-Cache-3_10-GHz?q=e3-1220">Intel Xeon E3-1220</a> (four cores, 3.10GHz), 24GB Kingston DDR3 RAM, and four Western Digital RE-3 160GB drives in a RAID 10 array. BIOS versions are identical.</p>
<p>All of the tests were run in Fedora 20 (with SELinux enabled) for the hosts and the virtual machines. Very few services were left running during the tests. Here are the relevant software versions:</p>
<ul>
<li>Kernel: 3.14.8</li>
<li>For KVM: qemu-kvm 1.6.2</li>
<li>For Xen: xen 4.3.2</li>
</ul>
<p>All root filesystems are XFS with the default configuration. Virtual machines were created using virt-manager using the default configuration available for KVM and Xen. Virtual disks used raw images and were allotted 8GB RAM with 4 virtual CPU&rsquo;s. Xen guests used <a href="http://wiki.xen.org/wiki/Xen_Linux_PV_on_HVM_drivers">PVHVM</a>.</p>
<h4 id="caveats">Caveats</h4>
<p>One might argue that Fedora&rsquo;s parent owner, Red Hat, puts a significant amount of effort into maintaining and improving KVM within their distribution. Red Hat hasn&rsquo;t made significant contributions to Xen in years and they <a href="http://www.infoworld.com/d/virtualization/red-hat-releases-first-kvm-support-rhel-54-376">made the switch to KVM back in 2009</a>. I&rsquo;ve left this out of scope for these tests, but it&rsquo;s still something worth considering.</p>
<p>Also, contention was tightly controlled and minimized. On most virtualized servers, you&rsquo;re going to have multiple virtual machines fighting for CPU time, disk I/O, and access to the network. These tests didn&rsquo;t take that type of activity into consideration. One hypervisor might have poor performance at low contention but then perform much better than its competitors when contention for resources is high.</p>
<p>These tests were performed only on Intel CPU&rsquo;s. Results may vary on AMD and ARM.</p>
<h4 id="results">Results</h4>
<p>The tests against the bare metal servers served as a baseline for the virtual machine tests. The deviation in performance between the two servers without virtualization was at 0.51% or less.</p>
<p>KVM&rsquo;s performance fell within 1.5% of bare metal in almost all tests. Only two tests fell outside that variance. One of those tests was the 7-Zip test where KVM was 2.79% slower than bare metal. Oddly enough, KVM was 4.11% faster than bare metal with the PostMark test (which simulates a really busy mail server). I re-ran the PostMark tests again on both servers and those results fell within 1% of my original test results. I&rsquo;ll be digging into this a bit more as my knowledge of virtio&rsquo;s internals isn&rsquo;t terribly deep.</p>
<p>Xen&rsquo;s performance varied more from bare metal than KVM. Three tests with Xen came within 2.5% of bare metal speeds but the remainder were much slower. The PostMark test was 14.41% slower in Xen than bare metal and I found that result surprising. I re-ran the test and the results during the second run were within 2% of my original results. KVM&rsquo;s best performing CPU test, the MAFFT alignment, was Xen&rsquo;s second worst.</p>
<p>I&rsquo;ve provided a short summary table here with the final results:</p>
<!-- raw HTML omitted -->
<pre><code>&lt;th class=&quot;column-2&quot;&gt;
  Best Value
&lt;/th&gt;

&lt;th class=&quot;column-3&quot;&gt;
  Bare Metal
&lt;/th&gt;

&lt;th class=&quot;column-4&quot;&gt;
  KVM
&lt;/th&gt;

&lt;th class=&quot;column-5&quot;&gt;
  Xen
&lt;/th&gt;
</code></pre>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<pre><code>&lt;td class=&quot;column-2&quot;&gt;
  lower
&lt;/td&gt;

&lt;td class=&quot;column-3&quot;&gt;
  35.35
&lt;/td&gt;

&lt;td class=&quot;column-4&quot;&gt;
  35.66
&lt;/td&gt;

&lt;td class=&quot;column-5&quot;&gt;
  36.13
&lt;/td&gt;
</code></pre>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<pre><code>&lt;td class=&quot;column-2&quot;&gt;
  lower
&lt;/td&gt;

&lt;td class=&quot;column-3&quot;&gt;
  230.02
&lt;/td&gt;

&lt;td class=&quot;column-4&quot;&gt;
  232.44
&lt;/td&gt;

&lt;td class=&quot;column-5&quot;&gt;
  235.89
&lt;/td&gt;
</code></pre>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<pre><code>&lt;td class=&quot;column-2&quot;&gt;
  lower
&lt;/td&gt;

&lt;td class=&quot;column-3&quot;&gt;
  160
&lt;/td&gt;

&lt;td class=&quot;column-4&quot;&gt;
  162
&lt;/td&gt;

&lt;td class=&quot;column-5&quot;&gt;
  167.5
&lt;/td&gt;
</code></pre>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<pre><code>&lt;td class=&quot;column-2&quot;&gt;
  higher
&lt;/td&gt;

&lt;td class=&quot;column-3&quot;&gt;
  3026
&lt;/td&gt;

&lt;td class=&quot;column-4&quot;&gt;
  2991.5
&lt;/td&gt;

&lt;td class=&quot;column-5&quot;&gt;
  2856
&lt;/td&gt;
</code></pre>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<pre><code>&lt;td class=&quot;column-2&quot;&gt;
  higher
&lt;/td&gt;

&lt;td class=&quot;column-3&quot;&gt;
  7374833.5
&lt;/td&gt;

&lt;td class=&quot;column-4&quot;&gt;
  7271833.5
&lt;/td&gt;

&lt;td class=&quot;column-5&quot;&gt;
  6911167
&lt;/td&gt;
</code></pre>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<pre><code>&lt;td class=&quot;column-2&quot;&gt;
  higher
&lt;/td&gt;

&lt;td class=&quot;column-3&quot;&gt;
  49548
&lt;/td&gt;

&lt;td class=&quot;column-4&quot;&gt;
  48899.5
&lt;/td&gt;

&lt;td class=&quot;column-5&quot;&gt;
  46653.5
&lt;/td&gt;
</code></pre>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<pre><code>&lt;td class=&quot;column-2&quot;&gt;
  higher
&lt;/td&gt;

&lt;td class=&quot;column-3&quot;&gt;
  397.68
&lt;/td&gt;

&lt;td class=&quot;column-4&quot;&gt;
  393.95
&lt;/td&gt;

&lt;td class=&quot;column-5&quot;&gt;
  388.25
&lt;/td&gt;
</code></pre>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<pre><code>&lt;td class=&quot;column-2&quot;&gt;
  higher
&lt;/td&gt;

&lt;td class=&quot;column-3&quot;&gt;
  12467.5
&lt;/td&gt;

&lt;td class=&quot;column-4&quot;&gt;
  12129.5
&lt;/td&gt;

&lt;td class=&quot;column-5&quot;&gt;
  11879
&lt;/td&gt;
</code></pre>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<pre><code>&lt;td class=&quot;column-2&quot;&gt;
  lower
&lt;/td&gt;

&lt;td class=&quot;column-3&quot;&gt;
  7.78
&lt;/td&gt;

&lt;td class=&quot;column-4&quot;&gt;
  7.795
&lt;/td&gt;

&lt;td class=&quot;column-5&quot;&gt;
  8.42
&lt;/td&gt;
</code></pre>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<pre><code>&lt;td class=&quot;column-2&quot;&gt;
  higher
&lt;/td&gt;

&lt;td class=&quot;column-3&quot;&gt;
  3.3
&lt;/td&gt;

&lt;td class=&quot;column-4&quot;&gt;
  3.285
&lt;/td&gt;

&lt;td class=&quot;column-5&quot;&gt;
  3.125
&lt;/td&gt;
</code></pre>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<pre><code>&lt;td class=&quot;column-2&quot;&gt;
  higher
&lt;/td&gt;

&lt;td class=&quot;column-3&quot;&gt;
  3667
&lt;/td&gt;

&lt;td class=&quot;column-4&quot;&gt;
  3824
&lt;/td&gt;

&lt;td class=&quot;column-5&quot;&gt;
  3205
&lt;/td&gt;
</code></pre>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>If you&rsquo;d like to see the full data, feel free to <a href="https://docs.google.com/spreadsheets/d/1kmudbOjCDUgfw76b8qP2GqNqF1ddlTOKyOjc0GmNOIE/edit?usp=sharing">review the spreadsheet on Google Docs</a>.</p>
<h4 id="conclusion">Conclusion</h4>
<p>Based on this testing environment, KVM is almost always within 2% of bare metal performance. Xen fell within 2.5% of bare metal performance in three out of ten tests but often had a variance of up to 5-7%. Although KVM performed much better with the PostMark test, there was only one I/O test run in this group of tests and more testing is required before a clear winner in disk I/O could be found.</p>
<p>As for me, I&rsquo;d like to look deeper into how KVM and Xen handle disk I/O and why their results were so different. I may also run some tests under contention to see if one hypervisor can deal with that stress with better performance.</p>
<p>I&rsquo;d encourage readers to review the list of benchmark tests available in the <a href="http://www.phoronix-test-suite.com/">Phoronix test suite</a> and find some that emulate portions of their normal workloads. If your workloads are low CPU and high I/O in nature, look for some of the I/O stress tests in the suite. On the other hand, if you do a lot of audio/video transcoding, try some of the x264 or mp3 tests within the suite.</p>
<p><em>UPDATE: <a href="https://twitter.com/comstud/status/480785742730252288">Chris Behrens pointed out</a> that I neglected to mention the type of virtual machine I tested with Xen. I used PVHVM for the tests as it&rsquo;s the fastest performing option for Linux guests on Xen 4.3. Keep in mind that PVH is available in Xen 4.4 but that version of Xen isn&rsquo;t available in Fedora 20 at this time.</em></p>
]]></content></item><item><title>Getting Dell’s racadm working in Fedora 20</title><link>https://major.io/2014/06/20/getting-dells-racadm-working-in-fedora-20/</link><pubDate>Fri, 20 Jun 2014 14:39:19 +0000</pubDate><guid>https://major.io/2014/06/20/getting-dells-racadm-working-in-fedora-20/</guid><description>Dell provides the racadm software on Linux that allows you to manage Dell hardware from a Linux system. Getting it installed on a very modern distribution like Fedora 20 isn&amp;rsquo;t supported, but here are some steps that might help you along the way:
First off, go to Dell&amp;rsquo;s site and review the racadm download instructions. I&amp;rsquo;d recommend following the Remote RACADM instructions so that you can manage multiple systems from your Fedora installation.</description><content type="html"><![CDATA[<p>Dell provides the <code>racadm</code> software on Linux that allows you to manage Dell hardware from a Linux system. Getting it installed on a very modern distribution like Fedora 20 isn&rsquo;t supported, but here are some steps that might help you along the way:</p>
<p>First off, go to Dell&rsquo;s site and review the <a href="http://en.community.dell.com/techcenter/systems-management/w/wiki/3205.racadm-command-line-interface-for-drac.aspx">racadm download instructions</a>. I&rsquo;d recommend following the <em>Remote RACADM</em> instructions so that you can manage multiple systems from your Fedora installation. You&rsquo;ll be looking for a download with the text <em>Linux Remote Access Utilities</em> in the name. At the time of this post&rsquo;s writing, the filename is <code>OM-MgmtStat-Dell-Web-LX-7.4.0-866_A00.tar.gz</code>.</p>
<p>Un-tar the file and you&rsquo;ll get two directories dumped out into your working directory: <em>docs</em> and <em>linux</em>:</p>
<pre><code>tar xvzf OM-MgmtStat-Dell-Web-LX-7.4.0-866_A00.tar.gz
cd linux/rac/RHEL6/x86_64/
yum localinstall *.rpm
</code></pre><p>That should install all of the software you need. There weren&rsquo;t any dependencies to install on my Fedora workstation but yum should take care of these for you if you have a more minimal installation.</p>
<p>Once that&rsquo;s done, close your shell and re-open it. You should be able to run <code>racadm</code> from your terminal. You&rsquo;ll probably get an error like this if you run it:</p>
<pre><code>ERROR: Failed to initialize transport
</code></pre><p>Running <code>strace</code> reveals that racadm is looking for libssl.so but can&rsquo;t find it. Fix that by installing <code>openssl-devel</code>:</p>
<pre><code>yum -y install openssl-devel
</code></pre><p>Now you should be able to run <code>racadm</code> and configure your servers!</p>
]]></content></item><item><title>Fixing broken DNS lookups in spamassassin</title><link>https://major.io/2014/06/20/fixing-broken-dns-lookups-in-spamassassin/</link><pubDate>Fri, 20 Jun 2014 13:20:56 +0000</pubDate><guid>https://major.io/2014/06/20/fixing-broken-dns-lookups-in-spamassassin/</guid><description>I talked about the joys of running my own mail server last week only to find that my mail server was broken yesterday. Spamassassin stopped doing DNS lookups for RBL and SPF checks.
I had one of these moments:
My logs looked like this:
plugin: eval failed: available_nameservers: No DNS servers available! plugin: eval failed: available_nameservers: No DNS servers available! rules: failed to run NO_DNS_FOR_FROM RBL test, skipping: (available_nameservers: [.</description><content type="html"><![CDATA[<p>I talked about the <a href="https://twitter.com/majorhayden/status/479250665311457281">joys of running my own mail server</a> last week only to find that my mail server was broken yesterday. Spamassassin stopped doing DNS lookups for <a href="https://en.wikipedia.org/wiki/DNSBL">RBL</a> and <a href="https://en.wikipedia.org/wiki/Sender_Policy_Framework">SPF</a> checks.</p>
<p>I had one of these moments:</p>
<p><a href="/wp-content/uploads/2014/06/neil_patrick_harris_sigh.gif"><!-- raw HTML omitted --></a></p>
<p>My logs looked like this:</p>
<pre><code>plugin: eval failed: available_nameservers: No DNS servers available!
plugin: eval failed: available_nameservers: No DNS servers available!
rules: failed to run NO_DNS_FOR_FROM RBL test, skipping:
 (available_nameservers: [...] No DNS servers available!)
 (available_nameservers: [...] No DNS servers available!
</code></pre><p>My /etc/resolv.conf was correct and had two valid DNS servers listed. Also, the permissions set on /etc/resolv.conf were reasonable (0644) and the SELinux context applied to the file was appropriate (net_conf_t). Everything else on the system was able to resolve DNS records properly. Even an strace on the spamd process showed it reading /etc/resolv.conf successfully!</p>
<p>It was Google time. I put some snippets of my error output into the search bar and found a <a href="https://issues.apache.org/SpamAssassin/show_bug.cgi?id=7057">spamassassin bug report</a>. Mark Martinec found the root cause of the bug:</p>
<blockquote>
<p>Net::DNS version 0.76 changed the field name holding a set of nameservers in a Net::DNS::Resolver object: it used to be &lsquo;nameservers&rsquo;, but is now split into two fields: &lsquo;nameserver4&rsquo; and &lsquo;nameserver6&rsquo;.</p>
<p>Mail/SpamAssassin/DnsResolver.pm relied on the internal field name of a Net::DNS::Resolver object to obtain a default list of recursive name servers, so the change in Net::DNS broke that.</p>
</blockquote>
<p>The <a href="https://svn.apache.org/viewvc/spamassassin/trunk/lib/Mail/SpamAssassin/DnsResolver.pm?r1=1603518&amp;r2=1603517&amp;pathrev=1603518">patch from the bug report</a> worked just fine on my Fedora 20 mail server. Be sure to restart spamd after making the change.</p>
<p>There&rsquo;s a <a href="https://bugzilla.redhat.com/show_bug.cgi?id=1111586">Fedora bug report</a> as well.</p>
<p><em>If anyone is interested, I plan to write up my email configuration on Fedora soon for other folks to use. I might even make some ansible playbooks for it. ;)</em></p>
<p><em>Fedora update: Fedora&rsquo;s spamassassin package has been <a href="https://admin.fedoraproject.org/updates/spamassassin-3.4.0-7.fc20">updated to 3.4.0-7 and it fixes two bugs</a>. You&rsquo;ll find it in the stable repositories in a few days.</em></p>
]]></content></item><item><title>Configure remote syslog for XenServer via the command line</title><link>https://major.io/2014/06/03/configure-remote-syslog-for-xenserver-via-the-command-line/</link><pubDate>Tue, 03 Jun 2014 17:55:59 +0000</pubDate><guid>https://major.io/2014/06/03/configure-remote-syslog-for-xenserver-via-the-command-line/</guid><description>Citrix has some helpful documentation online about configuring remote syslog support for XenServer using the XenCenter GUI. However, if you need to do this via configuration management or scripts, using a GUI isn&amp;rsquo;t an option.
Getting it done via the command line is relatively easy:
HOSTUUID=`xe host-list --minimal` SYSLOGHOST=syslog.example.com xe host-param-set uuid=${HOSTUUID} logging:syslog_destination=${SYSLOGHOST} xe host-syslog-reconfigure host-uuid=${HOSTUUID} Removing the configuration and going back to only local logging is easy as well:</description><content type="html"><![CDATA[<p>Citrix has some <a href="https://support.citrix.com/article/CTX119496">helpful documentation</a> online about configuring remote syslog support for XenServer using the XenCenter GUI. However, if you need to do this via configuration management or scripts, using a GUI isn&rsquo;t an option.</p>
<p>Getting it done via the command line is relatively easy:</p>
<pre><code>HOSTUUID=`xe host-list --minimal`
SYSLOGHOST=syslog.example.com
xe host-param-set uuid=${HOSTUUID} logging:syslog_destination=${SYSLOGHOST}
xe host-syslog-reconfigure host-uuid=${HOSTUUID}
</code></pre><p>Removing the configuration and going back to only local logging is easy as well:</p>
<pre><code>HOSTUUID=`xe host-list --minimal`
xe host-param-clear uuid=${HOSTUUID} param-name=logging
xe host-syslog-reconfigure host-uuid=${HOSTUUID}
</code></pre>]]></content></item><item><title>Enjoy supernova 1.0.0</title><link>https://major.io/2014/05/27/enjoy-supernova-1-0-0/</link><pubDate>Tue, 27 May 2014 13:19:31 +0000</pubDate><guid>https://major.io/2014/05/27/enjoy-supernova-1-0-0/</guid><description>The first supernova commit to GitHub appeared just over two years ago. It&amp;rsquo;s been stable for quite some time, but it&amp;rsquo;s made it to version 1.0.0 today!
As always, you can get supernova from PyPi using pip:
pip install -U supernova All of the documentation has moved to ReadTheDocs and it looks much better than the giant README file in GitHub that served as the documentation for so long. Thanks to everyone that has committed code, found bugs, or called out my inability to write Python!</description><content type="html"><![CDATA[<p>The <a href="https://github.com/major/supernova/commit/2e225bd20b1c1385b972f7e931b9af3ab6419a88">first supernova commit</a> to GitHub appeared just over two years ago. It&rsquo;s been stable for quite some time, but it&rsquo;s made it to version 1.0.0 today!</p>
<p>As always, you can get supernova from PyPi using pip:</p>
<pre><code>pip install -U supernova
</code></pre><p>All of the <a href="http://supernova.readthedocs.org/">documentation has moved</a> to ReadTheDocs and it looks much better than the giant README file in GitHub that served as the documentation for so long. Thanks to everyone that has committed code, found bugs, or called out my inability to write Python!</p>
]]></content></item><item><title>Evade the Breach</title><link>https://major.io/2014/05/24/evade-the-breach/</link><pubDate>Sat, 24 May 2014 18:36:48 +0000</pubDate><guid>https://major.io/2014/05/24/evade-the-breach/</guid><description>This post appeared on the Rackspace Bloglast week and I copied it here so that readers of this blog will see it.You&amp;rsquo;ve heard it before: information security isn&amp;rsquo;t easy. There&amp;rsquo;s no perfect security policy or piece of technology that will protect your business from all attacks. However, security is a process and processes can always be improved.
Last month, the great folks at Accruent invited me to talk about this topic at the annual Accruent Insights 2014 conference held in Austin, Texas.</description><content type="html"><![CDATA[<p><!-- raw HTML omitted -->This post appeared on the <!-- raw HTML omitted -->Rackspace Blog<!-- raw HTML omitted --> last week and I copied it here so that readers of this blog will see it.<!-- raw HTML omitted --></p>
<p>You&rsquo;ve heard it before: information security isn&rsquo;t easy. There&rsquo;s no perfect security policy or piece of technology that will protect your business from all attacks. However, security is a process and processes can always be improved.</p>
<p>Last month, the great folks at <a href="http://www.accruent.com/">Accruent</a> invited me to talk about this topic at the annual <a href="http://www.accruent.com/conference/insights">Accruent Insights 2014 conference</a> held in Austin, Texas. Their users wanted to know more about the Target breach and the Heartbleed attack, as well as strategies for strengthening their security safeguards against unknown threats.</p>
<p>To understand these threats, it&rsquo;s important to have a good grasp of the basic concepts around information security. Businesses don&rsquo;t exist to be secure; they exist to build innovative products, create relationships with customers and provide a great work environment for their employees. Security must be woven into the processes that drive a business forward. There&rsquo;s no finish line for security and it&rsquo;s rarely successful when it&rsquo;s bolted on as an afterthought.</p>
<p>Donald Rumsfeld delivered an unexpectedly cohesive summary of modern information security back in 2002 <a href="https://en.wikipedia.org/wiki/There_are_known_knowns">when reporters asked him</a> about the lack of evidence surrounding Iraq and weapons of mass destruction:</p>
<blockquote>
<p>Reports that say there&rsquo;s - that something hasn&rsquo;t happened are always interesting to me, because as we know, there are known knowns; there are things that we know that we know. We also know there are known unknowns; that is to say we know there are some things we do not know. But there are also unknown unknowns, the ones we don&rsquo;t know we don&rsquo;t know.</p>
<p>-Donald Rumsfeld, United States Secretary of Defense</p>
</blockquote>
<p>Rumsfeld probably didn&rsquo;t know it at the time, but he summarized the challenges of information security in a few sentences. There are things we know will be problematic (a known known) and we must fix them or prepare ourselves for the damage they may cause. There are other things that we don&rsquo;t know enough about (a known unknown) and we must learn more about them. The last group, the unknown unknowns, is the most challenging. If you&rsquo;re looking for a good example of these, just examine the Heartbleed attack.</p>
<p>Dealing with all of these attacks requires a multi-layer approach: preventative, detective and corrective.</p>
<p>The preventative layer reduces your chances of being breached. If you lock your doors or close your blinds when you leave your home, then you already understand the value of the preventative layer. Making the attacker&rsquo;s job more difficult reduces the chance that they will target you. Let&rsquo;s face it: most attackers are looking for an easy target. Going after a hard target means there&rsquo;s a greater risk of getting caught.</p>
<p>However, there are situations where someone has targeted your business individually, and they will do whatever it takes to get what they want. It&rsquo;s critical to detect that activity as soon as it occurs. At home, we set our security alarms and join neighborhood watch programs. These measures will alert us to attacks that make it through our preventative layers. Businesses might use intrusion detection systems or log monitoring solutions in their defensive layer.</p>
<p>When all else fails, the corrective layer is the last line of defense. This layer consists of the things you must do to remove a threat and return everything back to normal. For property owners, examples of the corrective layer include calling the police, purchasing homeowner&rsquo;s insurance or acquiring firearms. These mechanisms are much more costly, and they require thought before they&rsquo;re used.</p>
<p>Each layer gives you a feedback loop for the previous layers. For example, if someone breaks in through a window and takes your TV, you may invest in better detective layers (like an alarm system with a glass break sensor) or preventative layers (like thorny bushes in front of your windows).</p>
<p>If these layers make sense, then you understand defense in depth and risk management. Defense in depth requires you to assume the worst and build more layers of defense (think about castles). Risk management involves identifying and avoiding risk. If you have heirloom jewelry at home, you might place it in fire safe. You&rsquo;ve just practiced defense in depth (the jewelry is in a locked safe in a locked house) and risk management (there&rsquo;s a high impact to you if the jewelry is stolen and you reduced the risk).</p>
<p>In summary, good security practice stems from exactly that: practice security each day and make it part of your normal business processes. Security improvements must be made with changes to people, process and technology. The businesses that truly excel in information security are those that insulate themselves from risk-internal and external-with effective preventative, detective and corrective layers.</p>
<p>If you&rsquo;d like to review the presentation slides from the Accruent Insights conference, you can <a href="http://bit.ly/evadethebreach">download the big PDF</a> or <a href="http://www.slideshare.net/MajorHayden/accruent-insights-2014-20140428-v8-final">find them on SlideShare</a>.</p>
<p><em>I&rsquo;m always trying to get better at presenting so please feel free to send me some constructive criticism. ;)</em></p>
]]></content></item><item><title>Switching to systemd on Debian jessie</title><link>https://major.io/2014/05/20/switching-to-systemd-on-debian-jessie/</link><pubDate>Tue, 20 May 2014 13:47:33 +0000</pubDate><guid>https://major.io/2014/05/20/switching-to-systemd-on-debian-jessie/</guid><description>It seems like everyone is embracing systemd these days. It&amp;rsquo;s been in Fedora since 2011 and it&amp;rsquo;s already in the RHEL 7 release candidate. Arch Linux and Gentoo have it as well. Debian got on board with the jessie release (which is currently in testing).
Switching from old SysVinit to systemd in Debian jessie is quite simple. For the extremely cautious system administrators, you can follow Debian&amp;rsquo;s guide and test systemd before you make the full cutover.</description><content type="html"><![CDATA[<p><a href="/wp-content/uploads/2014/05/Debian-icon.png"><!-- raw HTML omitted --></a>It seems like everyone is embracing systemd these days. It&rsquo;s been in Fedora since 2011 and it&rsquo;s already in the RHEL 7 release candidate. Arch Linux and Gentoo have it as well. Debian got on board with the jessie release (which is currently in testing).</p>
<p>Switching from old SysVinit to systemd in Debian jessie is quite simple. For the extremely cautious system administrators, you can <a href="https://wiki.debian.org/systemd">follow Debian&rsquo;s guide and test systemd</a> before you make the full cutover.</p>
<p>However, I&rsquo;ve had great results with making the jump in one pass:</p>
<pre><code>apt-get update
apt-get install systemd systemd-sysv
reboot
</code></pre><p>After you reboot, you might notice <code>/sbin/init</code> still hanging out in your process list:</p>
<pre><code># ps aufx | grep init
root         1  0.0  0.1  45808  3820 ?        Ss   08:16   0:00 /sbin/init
</code></pre><p>That&rsquo;s actually a symlink to systemd:</p>
<pre><code># ls -al /sbin/init
lrwxrwxrwx 1 root root 20 Mar 19 13:15 /sbin/init -&gt; /lib/systemd/systemd
</code></pre><p>You also have journald for quick access to logs:</p>
<pre><code># journalctl -u cron
-- Logs begin at Tue 2014-05-20 08:16:21 CDT, end at Tue 2014-05-20 08:31:20 CDT. --
May 20 08:16:24 jessie-auditd-2 /usr/sbin/cron[837]: (CRON) INFO (pidfile fd = 3)
May 20 08:16:24 jessie-auditd-2 cron[774]: Starting periodic command scheduler: cron.
May 20 08:16:24 jessie-auditd-2 systemd[1]: Started LSB: Regular background program processing daemon.
May 20 08:16:24 jessie-auditd-2 /usr/sbin/cron[842]: (CRON) STARTUP (fork ok)
May 20 08:16:24 jessie-auditd-2 /usr/sbin/cron[842]: (CRON) INFO (Running @reboot jobs)
May 20 08:17:01 jessie-auditd-2 CRON[990]: pam_unix(cron:session): session opened for user root by (uid=0)
May 20 08:17:01 jessie-auditd-2 /USR/SBIN/CRON[991]: (root) CMD (   cd / &amp;&amp; run-parts --report /etc/cron.hourly)
</code></pre>]]></content></item><item><title>Text missing in chrome on Linux</title><link>https://major.io/2014/05/17/text-missing-in-chrome-on-linux/</link><pubDate>Sun, 18 May 2014 04:33:14 +0000</pubDate><guid>https://major.io/2014/05/17/text-missing-in-chrome-on-linux/</guid><description>I&amp;rsquo;m in the process of trying Fedora 20 on my retina MacBook and I ran into a peculiar issue with Chrome. Some sites would load up normally and I could read everything on the page. Other sites would load up and only some of the text would be displayed. Images were totally unaffected.
It wasn&amp;rsquo;t this way on the initial installation of Fedora but it cropped up somewhere along the way as I installed software.</description><content type="html"><![CDATA[<p>I&rsquo;m in the process of trying Fedora 20 on my retina MacBook and I ran into a peculiar issue with Chrome. Some sites would load up normally and I could read everything on the page. Other sites would load up and only some of the text would be displayed. Images were totally unaffected.</p>
<p>It wasn&rsquo;t this way on the initial installation of Fedora but it cropped up somewhere along the way as I installed software. Changing the configuration within Chrome wasn&rsquo;t an option - I couldn&rsquo;t even see any text on the configuration pages!</p>
<p>The only commonality I could find is that all pages that specified their own web fonts (like the pages on this site) loaded up perfectly. Everything was visible. However, on sites that tend to use whatever font is available in the browser (sites that specify a font family), the text was missing. A good example was <a href="http://avherald.com/">The Aviation Herald</a>.</p>
<p>I remembered installing some Microsoft core fonts via <a href="https://satya164.github.io/fedy/">Fedy</a> and I added in some patched powerline fonts to work with tmux. A quick check of the SELinux troubleshooter alerted me to the problem: the new fonts had the wrong SELinux labels applied and Chrome wasn&rsquo;t allowed to access them.</p>
<p>I decided to relabel the whole filesystem:</p>
<pre><code>restorecon -Rv /
</code></pre><p>The restorecon output was line after line of fonts that I had installed earlier in the evening. Once it finished running, I started Chrome and it was working just as I had expected.</p>
]]></content></item><item><title>Thanks to TrueAbility for featuring me on their blog!</title><link>https://major.io/2014/05/14/thanks-to-trueability-for-featuring-me-on-their-blog/</link><pubDate>Wed, 14 May 2014 18:27:22 +0000</pubDate><guid>https://major.io/2014/05/14/thanks-to-trueability-for-featuring-me-on-their-blog/</guid><description>The fine folks at TrueAbility were kind enough to post an interview with me on their blog this week. They have an ongoing set of posts titled &amp;ldquo;Profile of a Linux Pro&amp;rdquo; and those posts are a good way to see how other people have learned about Linux and built a career around it.</description><content type="html"><![CDATA[<p><a href="/wp-content/uploads/2014/05/TrueAbility_final_logo-16.jpg"><!-- raw HTML omitted --></a>The fine folks at <a href="https://trueability.com/">TrueAbility</a> were kind enough to <a href="http://blog.trueability.com/2014/05/profile-of-a-linux-pro-major-hayden/">post an interview with me</a> on their blog this week. They have an <a href="http://blog.trueability.com/category/profile-of-a-linux-pro/">ongoing set of posts</a> titled &ldquo;Profile of a Linux Pro&rdquo; and those posts are a good way to see how other people have learned about Linux and built a career around it.</p>
]]></content></item><item><title>Testing grouped environment support for supernova</title><link>https://major.io/2014/05/04/testing-grouped-environment-support-for-supernova/</link><pubDate>Mon, 05 May 2014 02:43:04 +0000</pubDate><guid>https://major.io/2014/05/04/testing-grouped-environment-support-for-supernova/</guid><description>I&amp;rsquo;ve added some grouped environment support for supernova tonight. This allows you to run the same action across a group of environments listed in your supernova configuration file. To give you an idea of how this works, I have two environments in my supernova file called prodord and prodiad. Both of them are my production environments at Rackspace:
[prodord] SUPERNOVA_GROUP=prodrax OS_AUTH_URL=https://identity.api.rackspacecloud.com/v2.0/ OS_AUTH_SYSTEM=rackspace OS_REGION_NAME=ORD OS_TENANT_NAME=USE_KEYRING[&amp;#39;prodcloudauthuser&amp;#39;] OS_PROJECT_ID=USE_KEYRING[&amp;#39;prodcloudauthuser&amp;#39;] OS_PASSWORD=USE_KEYRING[&amp;#39;prodcloudauthpass&amp;#39;] OS_PROJECT_ID=[my account number] [prodiad] SUPERNOVA_GROUP=prodrax OS_AUTH_URL=https://identity.</description><content type="html"><![CDATA[<p>I&rsquo;ve added some grouped environment support for supernova tonight. This allows you to run the same action across a group of environments listed in your supernova configuration file. To give you an idea of how this works, I have two environments in my supernova file called <em>prodord</em> and <em>prodiad</em>. Both of them are my production environments at Rackspace:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ini" data-lang="ini"><span style="color:#719e07">[prodord]</span>
SUPERNOVA_GROUP<span style="color:#719e07">=</span><span style="color:#2aa198">prodrax</span>
OS_AUTH_URL<span style="color:#719e07">=</span><span style="color:#2aa198">https://identity.api.rackspacecloud.com/v2.0/</span>
OS_AUTH_SYSTEM<span style="color:#719e07">=</span><span style="color:#2aa198">rackspace</span>
OS_REGION_NAME<span style="color:#719e07">=</span><span style="color:#2aa198">ORD</span>
OS_TENANT_NAME<span style="color:#719e07">=</span><span style="color:#2aa198">USE_KEYRING[&#39;prodcloudauthuser&#39;]</span>
OS_PROJECT_ID<span style="color:#719e07">=</span><span style="color:#2aa198">USE_KEYRING[&#39;prodcloudauthuser&#39;]</span>
OS_PASSWORD<span style="color:#719e07">=</span><span style="color:#2aa198">USE_KEYRING[&#39;prodcloudauthpass&#39;]</span>
OS_PROJECT_ID<span style="color:#719e07">=</span><span style="color:#2aa198">[my account number]</span>

<span style="color:#719e07">[prodiad]</span>
SUPERNOVA_GROUP<span style="color:#719e07">=</span><span style="color:#2aa198">prodrax</span>
OS_AUTH_URL<span style="color:#719e07">=</span><span style="color:#2aa198">https://identity.api.rackspacecloud.com/v2.0/</span>
OS_AUTH_SYSTEM<span style="color:#719e07">=</span><span style="color:#2aa198">rackspace</span>
OS_REGION_NAME<span style="color:#719e07">=</span><span style="color:#2aa198">IAD</span>
OS_TENANT_NAME<span style="color:#719e07">=</span><span style="color:#2aa198">USE_KEYRING[&#39;prodcloudauthuser&#39;]</span>
OS_PROJECT_ID<span style="color:#719e07">=</span><span style="color:#2aa198">USE_KEYRING[&#39;prodcloudauthuser&#39;]</span>
OS_PASSWORD<span style="color:#719e07">=</span><span style="color:#2aa198">USE_KEYRING[&#39;prodcloudauthpass&#39;]</span>
OS_PROJECT_ID<span style="color:#719e07">=</span><span style="color:#2aa198">[my account number]</span>
</code></pre></div><p>You might notice the <code>SUPERNOVA_GROUP</code> configuration option there. That allows me to put these environments into a group called <em>prodrax</em>. I can make requests to them as if they were one environment:</p>
<pre><code>$ supernova prodrax keypair-list
[SUPERNOVA] Running nova against prodord...
+------------------+-------------------------------------------------+
| Name             | Fingerprint                                     |
+------------------+-------------------------------------------------+
| personal_servers | 05💿ff:f1:c8:04:7d:74:d8:44:be:9b:a1:12:e0:b8 |
+------------------+-------------------------------------------------+
[SUPERNOVA] Running nova against prodiad...
+------------------+-------------------------------------------------+
| Name             | Fingerprint                                     |
+------------------+-------------------------------------------------+
| personal_servers | 05💿ff:f1:c8:04:7d:74:d8:44:be:9b:a1:12:e0:b8 |
+------------------+-------------------------------------------------+
</code></pre><p>This allows you to list instances across multiple environments or take actions against instances in multiple environments. To begin testing the functionality, simply add <code>SUPERNOVA_GROUP=&lt;em&gt;group_name&lt;/em&gt;</code> to your commonly used environments and use the group name in place of the environment name when you use supernova.</p>
<p>The new functionality isn&rsquo;t in PyPi just yet, but you can get the changes directly from the <a href="https://github.com/major/supernova">GitHub repo</a>:</p>
<pre><code>git clone https://github.com/major/supernova.git
cd supernova
python setup.py install
</code></pre>]]></content></item><item><title>Small update for supernova 0.9.6</title><link>https://major.io/2014/05/01/small-update-for-supernova-0-9-6/</link><pubDate>Thu, 01 May 2014 14:32:13 +0000</pubDate><guid>https://major.io/2014/05/01/small-update-for-supernova-0-9-6/</guid><description>A supernova user ran into a tough problem where supernova didn&amp;rsquo;t seem to obey a configuration within the supernova configuration file. After checking python module versions and re-checking the configuration file a multitude of times, we discovered that there were variables defined in the user&amp;rsquo;s ~/.bash_profile that were not overwritten. It&amp;rsquo;s key to remember how supernova works:
supernova copies your current environment variables into a dictionary any configuration options from the supernova configuration are applied to the dictionary the dictionary is used to set environment variables only for the subprocess that runs nova (or the executable of your choice) your original environment variables are left unaltered.</description><content type="html"><![CDATA[<p>A supernova user ran into a tough problem where supernova didn&rsquo;t seem to obey a configuration within the supernova configuration file. After checking python module versions and re-checking the configuration file a multitude of times, we discovered that there were variables defined in the user&rsquo;s <code>~/.bash_profile</code> that were not overwritten. It&rsquo;s key to remember how supernova works:</p>
<ol>
<li>supernova copies your current environment variables into a dictionary</li>
<li>any configuration options from the supernova configuration are applied to the dictionary</li>
<li>the dictionary is used to set environment variables only for the subprocess that runs nova (or the executable of your choice)</li>
<li>your original environment variables are left unaltered.</li>
</ol>
<p>A weird situation comes up if you set a variable in your environment (like <code>NOVA_RAX_AUTH</code>) but you don&rsquo;t set that as a configuration option within your supernova configuration file. That environment variable will carry over into your supernova subprocess and it will be used within that process.</p>
<p>You can fix it by clearing the conflicting environment variables from your environment or ensuring that they&rsquo;re not set in the first place. You can also set explicit configuration variables inside your supernova configuration file that will overwrite an environment variable that is set automatically in your environment.</p>
<p>With the release of supernova 0.9.6, you&rsquo;ll be warned if any potential conflicts exist in your environment variables:</p>
<pre><code>$ supernova prod list
________________________________________________________________________________
*WARNING* Found existing environment variables that may cause conflicts:
  - NOVA_VARIABLE_DOESNT_EXIST
  - OS_THIS_DOESNT_EXIST
________________________________________________________________________________
</code></pre><p>You can get supernova via <a href="https://pypi.python.org/pypi?:action=display&amp;name=supernova&amp;version=0.9.6">PyPi</a> or on <a href="https://github.com/major/supernova">GitHub</a>.</p>
]]></content></item><item><title>Helpful Linux I/O stack diagram</title><link>https://major.io/2014/04/30/helpful-linux-io-stack-diagram/</link><pubDate>Wed, 30 Apr 2014 15:03:46 +0000</pubDate><guid>https://major.io/2014/04/30/helpful-linux-io-stack-diagram/</guid><description>During one of my regular trips to reddit, I stumbled upon an amazingly helpful Linux I/O stack diagram:
It&amp;rsquo;s quite comprehensive and it can really help if you&amp;rsquo;re digging through a bottleneck and you&amp;rsquo;re not quite sure where to look. The original diagram is available in multiple formats from Thomas Krenn&amp;rsquo;s website.
If you combine that with this slide from Brendan Gregg&amp;rsquo;s Linux Performance Analysis and Tools presentation from Scale 11x, you can attack performance problems with precision:</description><content type="html"><![CDATA[<p>During one of my regular trips to reddit, I stumbled upon an amazingly helpful Linux I/O stack diagram:</p>
<p><img src="/wp-content/uploads/2014/04/linux-io-stack-diagram_v1.0.png" alt="1"></p>
<p>It&rsquo;s quite comprehensive and it can really help if you&rsquo;re digging through a bottleneck and you&rsquo;re not quite sure where to look. The original diagram is available in multiple formats <a href="http://www.thomas-krenn.com/en/wiki/Linux_I/O_Stack_Diagram">from Thomas Krenn&rsquo;s website</a>.</p>
<p>If you combine that with this slide from <a href="https://twitter.com/brendangregg">Brendan Gregg&rsquo;s</a> <em><a href="http://www.slideshare.net/brendangregg/linux-performance-analysis-and-tools">Linux Performance Analysis and Tools</a></em> presentation from Scale 11x, you can attack performance problems with precision:</p>
<p><img src="/wp-content/uploads/2014/04/scalelinuxperformance-130224171331-phpapp01-dragged.png" alt="5"></p>
]]></content></item><item><title>supernova 0.9.5 is available</title><link>https://major.io/2014/04/29/supernova-0-9-5-is-available/</link><pubDate>Tue, 29 Apr 2014 13:14:54 +0000</pubDate><guid>https://major.io/2014/04/29/supernova-0-9-5-is-available/</guid><description>I just pushed supernova 0.9.5 to pypi and it&amp;rsquo;s now available for installation using pip. You can get the latest version by running:
pip install --upgrade supernova Some of the new features include the ability to use suprernova with other executables, like glance. Place a configuration option within your ~/.supernova file that looks like this:
OS_EXECUTABLE=/usr/bin/glance Once you do that, supernova will package up all of your environment variables as it normally would, but it will call glance instead of nova.</description><content type="html"><![CDATA[<p>I just pushed <a href="https://pypi.python.org/pypi/supernova/0.9.5">supernova 0.9.5 to pypi</a> and it&rsquo;s now available for installation using pip. You can get the latest version by running:</p>
<pre><code>pip install --upgrade supernova
</code></pre><p>Some of the new features include the ability to use suprernova <a href="https://github.com/major/supernova/commit/60f78f16f1c433fa6c9d4c5196e20778005dea7a">with other executables</a>, like <a href="http://docs.openstack.org/developer/glance/">glance</a>. Place a configuration option within your ~/.supernova file that looks like this:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ini" data-lang="ini">OS_EXECUTABLE<span style="color:#719e07">=</span><span style="color:#2aa198">/usr/bin/glance</span>
</code></pre></div><p>Once you do that, supernova will package up all of your environment variables as it normally would, but it will call glance instead of nova. Don&rsquo;t worry, nova is still the default unless you specify a different executable.</p>
<p>Installation instructions are improved and some folks were kind enough to fix some PEP8 issues. Visit the <a href="https://github.com/major/supernova">github project page</a> for the changes that went into this release.</p>
]]></content></item><item><title>Configure static IP addresses for Project Atomic’s KVM image</title><link>https://major.io/2014/04/23/configure-static-ip-addresses-for-project-atomics-kvm-image/</link><pubDate>Wed, 23 Apr 2014 15:14:39 +0000</pubDate><guid>https://major.io/2014/04/23/configure-static-ip-addresses-for-project-atomics-kvm-image/</guid><description>Amid all of the Docker buzz at the Red Hat Summit, Project Atomic was launched. It&amp;rsquo;s a minimalistic Fedora 20 image with a few tweaks, including rpm-ostree and geard.
There are great instructions on the site for firing up a test instance under KVM but my test server doesn&amp;rsquo;t have a DHCP server on its network. You can use Project Atomic with static IP addresses fairly easily:
Create a one-line /etc/sysconfig/network:</description><content type="html"><![CDATA[<p>Amid all of the Docker buzz at the Red Hat Summit, <a href="http://www.projectatomic.io/">Project Atomic</a> was launched. It&rsquo;s a minimalistic Fedora 20 image with a <a href="http://www.projectatomic.io/docs/gettingstarted/">few tweaks</a>, including <a href="http://rpm-ostree.cloud.fedoraproject.org/#/">rpm-ostree</a> and <a href="https://openshift.github.io/geard/">geard</a>.</p>
<p>There are <a href="http://www.projectatomic.io/download/">great instructions</a> on the site for firing up a test instance under KVM but my test server doesn&rsquo;t have a DHCP server on its network. You can use Project Atomic with static IP addresses fairly easily:</p>
<p>Create a one-line <code>/etc/sysconfig/network</code>:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ini" data-lang="ini">NETWORKING<span style="color:#719e07">=</span><span style="color:#2aa198">yes</span>
</code></pre></div><p>Drop in a basic network configuration into <code>/etc/sysconfig/network-scripts/ifcfg-eth0</code>:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ini" data-lang="ini">DEVICE<span style="color:#719e07">=</span><span style="color:#2aa198">eth0</span>
IPADDR<span style="color:#719e07">=</span><span style="color:#2aa198">10.127.92.32</span>
NETMASK<span style="color:#719e07">=</span><span style="color:#2aa198">255.255.255.0</span>
GATEWAY<span style="color:#719e07">=</span><span style="color:#2aa198">10.127.92.1</span>
ONBOOT<span style="color:#719e07">=</span><span style="color:#2aa198">yes</span>
</code></pre></div><p>All that&rsquo;s left is to set DNS servers and a hostname:</p>
<pre><code>echo &quot;nameserver 8.8.8.8&quot; &gt; /etc/resolv.conf
hostnamectl set-hostname myatomichost.example.com
</code></pre><p>Bring up the network interface:</p>
<pre><code>ifup eth0
</code></pre><p>Of course, you could do all of this via the <code>nmcli</code> tool if you prefer to go that route.</p>
]]></content></item><item><title>Launch secure LXC containers on Fedora 20 using SELinux and sVirt</title><link>https://major.io/2014/04/21/launch-secure-lxc-containers-on-fedora-20-using-selinux-and-svirt/</link><pubDate>Tue, 22 Apr 2014 04:11:00 +0000</pubDate><guid>https://major.io/2014/04/21/launch-secure-lxc-containers-on-fedora-20-using-selinux-and-svirt/</guid><description>Getting started with LXC is a bit awkward and I&amp;rsquo;ve assembled this guide for anyone who wants to begin experimenting with LXC containers in Fedora 20. As an added benefit, you can follow almost every step shown here when creating LXC containers on Red Hat Enterprise Linux 7 Beta (which is based on Fedora 19).
You&amp;rsquo;ll need a physical machine or a VM running Fedora 20 to get started. (You could put a container in a container, but things get a little dicey with that setup.</description><content type="html"><![CDATA[<p><img src="/wp-content/uploads/2013/07/selinux-penguin-new_medium.png" alt="1"></p>
<p>Getting started with <a href="https://en.wikipedia.org/wiki/LXC">LXC</a> is a bit awkward and I&rsquo;ve assembled this guide for anyone who wants to begin experimenting with LXC containers in Fedora 20. As an added benefit, you can follow almost every step shown here when creating LXC containers on <a href="https://access.redhat.com/site/products/Red_Hat_Enterprise_Linux/Get-Beta">Red Hat Enterprise Linux 7</a> Beta (which is based on Fedora 19).</p>
<p>You&rsquo;ll need a physical machine or a VM running Fedora 20 to get started. <!-- raw HTML omitted -->(You could put a container in a container, but things get a little dicey with that setup. Let&rsquo;s just avoid talking about nested containers for now. No, really, I shouldn&rsquo;t have even brought it up. Sorry about that.)<!-- raw HTML omitted --></p>
<h3 id="prep-work">Prep Work</h3>
<p>Start by updating all packages to the latest versions available:</p>
<pre><code>yum -y upgrade
</code></pre><p>Verify that SELinux is in enforcing mode by running <code>getenforce</code>. If you see <em>Disabled</em> or <em>Permissive</em>, get SELinux into enforcing mode with a quick configuration change:</p>
<pre><code>sed -i 's/^SELINUX=.*/SELINUX=enforcing/' /etc/selinux/config
</code></pre><p>I recommend installing <code>setroubleshoot-server</code> to make it easier to find the root cause of AVC denials:</p>
<pre><code class="language-yum" data-lang="yum"></code></pre><p>Reboot now. This will ensure that SELinux comes up in enforcing mode (verify that with <code>getenforce</code> after reboot) and it ensures that auditd starts up sedispatch (for setroubleshoot).</p>
<h3 id="install-management-libraries-and-utilities">Install management libraries and utilities</h3>
<p>Let&rsquo;s grab libvirt along with LXC support and a basic NAT networking configuration.</p>
<pre><code>yum -y install libvirt-daemon-lxc libvirt-daemon-config-network
</code></pre><p>Launch libvirtd via systemd and ensure that it always comes up on boot. This step will also adjust firewalld for your containers and ensure that dnsmasq is serving up IP addresses via DHCP on your default NAT network.</p>
<pre><code>systemctl start libvirtd.service
systemctl enable libvirtd.service
</code></pre><h3 id="bootstrap-our-container">Bootstrap our container</h3>
<p>Installing packages into the container&rsquo;s filesystem will take some time.</p>
<pre><code>yum -y --installroot=/var/lib/libvirt/filesystems/fedora20 --releasever=20 --nogpg install systemd passwd yum fedora-release vim-minimal openssh-server procps-ng iproute net-tools dhclient
</code></pre><p>This step fills in the filesystem with the necessary packages to run a Fedora 20 container. We now need to tell libvirt about the container we&rsquo;ve just created.</p>
<pre><code>virt-install --connect lxc:// --name fedora20 --ram 512 --filesystem /var/lib/libvirt/filesystems/fedora20/,/
</code></pre><p>At this point, libvirt will know enough about the container to start it and you&rsquo;ll be connected to the console of the container! We need to adjust some configuration files within the container to use it properly. Detach from the console with CTRL-].</p>
<p>Let&rsquo;s stop the container so we can make some adjustments.</p>
<pre><code>virsh -c lxc:// shutdown fedora20
</code></pre><h3 id="get-the-container-ready-for-production">Get the container ready for production</h3>
<p>Hop into your container and set a root password.</p>
<pre><code>chroot /var/lib/libvirt/filesystems/fedora20 /bin/passwd root
</code></pre><p>We will be logging in as root via the console occasionally and we need to allow that access.</p>
<pre><code>echo &quot;pts/0&quot; &gt;&gt; /var/lib/libvirt/filesystems/fedora20/etc/securetty
</code></pre><p>Since we will be using our NAT network with our auto-configured dnsmasq server (thanks to libvirt), we can configure a simple DHCP setup for eth0:</p>
<pre><code>cat &lt; &lt; EOF &gt; /var/lib/libvirt/filesystems/fedora20/etc/sysconfig/network
NETWORKING=yes
EOF
cat &lt; &lt; EOF &gt; /var/lib/libvirt/filesystems/fedora20/etc/sysconfig/network-scripts/ifcfg-eth0
BOOTPROTO=dhcp
ONBOOT=yes
DEVICE=eth0
EOF
</code></pre><p>Using ssh makes the container a lot easier to manage, so let&rsquo;s ensure that it starts when the container boots. (You could do this via systemctl after logging in at the console, but I&rsquo;m lazy.)</p>
<pre><code>chroot /var/lib/libvirt/filesystems/fedora20/
ln -s /usr/lib/systemd/system/sshd.service /etc/systemd/system/multi-user.target.wants/
exit
</code></pre><h3 id="launch">Launch!</h3>
<p>Cross your fingers and launch the container.</p>
<pre><code>virsh -c lxc:// start --console fedora20
</code></pre><p>You&rsquo;ll be attached to the console during boot but don&rsquo;t worry, hold down CTRL-] to get back to your host prompt. Check the dnsmasq leases to find your container&rsquo;s IP address and you can login as root over ssh.</p>
<pre><code>cat /var/lib/libvirt/dnsmasq/default.leases
</code></pre><h3 id="security">Security</h3>
<p>After logging into your container via ssh, check the process labels within the container:</p>
<pre><code># ps aufxZ
LABEL                           USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
system_u:system_r:virtd_lxc_t:s0-s0:c0.c1023 root 1 0.0  1.3 47444 3444 ?      Ss   03:18   0:00 /sbin/init
system_u:system_r:virtd_lxc_t:s0-s0:c0.c1023 root 18 0.0  2.0 43016 5368 ?     Ss   03:18   0:00 /usr/lib/systemd/systemd-journald
system_u:system_r:virtd_lxc_t:s0-s0:c0.c1023 root 38 0.4  7.8 223456 20680 ?   Ssl  03:18   0:00 /usr/bin/python -Es /usr/sbin/firewalld -
system_u:system_r:virtd_lxc_t:s0-s0:c0.c1023 root 40 0.0  0.7 26504 2084 ?     Ss   03:18   0:00 /usr/sbin/smartd -n -q never
system_u:system_r:virtd_lxc_t:s0-s0:c0.c1023 root 41 0.0  0.4 19268 1252 ?     Ss   03:18   0:00 /usr/sbin/irqbalance --foreground
system_u:system_r:virtd_lxc_t:s0-s0:c0.c1023 root 44 0.0  0.6 34696 1636 ?     Ss   03:18   0:00 /usr/lib/systemd/systemd-logind
system_u:system_r:virtd_lxc_t:s0-s0:c0.c1023 root 46 0.0  1.8 267500 4832 ?    Ssl  03:18   0:00 /sbin/rsyslogd -n
system_u:system_r:virtd_lxc_t:s0-s0:c0.c1023 dbus 47 0.0  0.6 26708 1680 ?     Ss   03:18   0:00 /bin/dbus-daemon --system --address=syste
system_u:system_r:virtd_lxc_t:s0-s0:c0.c1023 rpc 54 0.0  0.5 41992 1344 ?      Ss   03:18   0:00 /sbin/rpcbind -w
system_u:system_r:virtd_lxc_t:s0-s0:c0.c1023 root 55 0.0  0.3 25936 924 ?      Ss   03:18   0:00 /usr/sbin/atd -f
system_u:system_r:virtd_lxc_t:s0-s0:c0.c1023 root 56 0.0  0.5 22728 1488 ?     Ss   03:18   0:00 /usr/sbin/crond -n
system_u:system_r:virtd_lxc_t:s0-s0:c0.c1023 root 60 0.0  0.2 6412 784 pts/0   Ss+  03:18   0:00 /sbin/agetty --noclear -s console 115200
system_u:system_r:virtd_lxc_t:s0-s0:c0.c1023 root 74 0.0  3.2 339808 8456 ?    Ssl  03:18   0:00 /usr/sbin/NetworkManager --no-daemon
system_u:system_r:virtd_lxc_t:s0-s0:c0.c1023 root 394 0.0  5.9 102356 15708 ?  S    03:18   0:00  \_ /sbin/dhclient -d -sf /usr/libexec/nm
system_u:system_r:virtd_lxc_t:s0-s0:c0.c1023 polkitd 83 0.0  4.4 514792 11548 ? Ssl 03:18   0:00 /usr/lib/polkit-1/polkitd --no-debug
system_u:system_r:virtd_lxc_t:s0-s0:c0.c1023 rpcuser 110 0.0  0.6 46564 1824 ? Ss   03:18   0:00 /sbin/rpc.statd
system_u:system_r:virtd_lxc_t:s0-s0:c0.c1023 root 111 0.0  1.3 82980 3620 ?    Ss   03:18   0:00 /usr/sbin/sshd -D
system_u:system_r:virtd_lxc_t:s0-s0:c0.c1023 root 409 0.0  1.9 131576 5084 ?   Ss   03:18   0:00  \_ sshd: root@pts/1
system_u:system_r:virtd_lxc_t:s0-s0:c0.c1023 root 413 0.0  0.9 115872 2592 pts/1 Ss 03:18   0:00      \_ -bash
system_u:system_r:virtd_lxc_t:s0-s0:c0.c1023 root 438 0.0  0.5 123352 1344 pts/1 R+ 03:19   0:00          \_ ps aufxZ
system_u:system_r:virtd_lxc_t:s0-s0:c0.c1023 root 411 0.0  0.8 44376 2252 ?    Ss   03:18   0:00 /usr/lib/systemd/systemd --user
system_u:system_r:virtd_lxc_t:s0-s0:c0.c1023 root 412 0.0  0.5 66828 1328 ?    S    03:18   0:00  \_ (sd-pam)
system_u:system_r:virtd_lxc_t:s0-s0:c0.c1023 root 436 0.0  0.4 21980 1144 ?    Ss   03:19   0:00 /usr/lib/systemd/systemd-hostnamed
</code></pre><p>You&rsquo;ll notice something interesting if you run <code>getenforce</code> now within the container — SELinux is disabled. Actually, it&rsquo;s not really disabled. The processing of SELinux policy is done on the host. The container isn&rsquo;t able to see what&rsquo;s going on outside of its own files and processes. The <a href="http://libvirt.org/drvlxc.html#security">libvirt documentation for LXC</a> hints at the importance of this isolation:</p>
<blockquote>
<p>A suitably configured UID/GID mapping is a pre-requisite to making containers secure, in the absence of sVirt confinement.</p>
</blockquote>
<blockquote>
<p>In the absence of the “user” namespace being used, containers cannot be considered secure against exploits of the host OS. The sVirt SELinux driver provides a way to secure containers even when the “user” namespace is not used. The cost is that writing a policy to allow execution of arbitrary OS is not practical. The SELinux sVirt policy is typically tailored to work with an simpler application confinement use case, as provided by the “libvirt-sandbox” project.</p>
</blockquote>
<p>This leads to something really critical to understand:</p>
<h3 id="containers-dont-contain">Containers don&rsquo;t contain</h3>
<p>Dan Walsh has a <a href="https://danwalsh.livejournal.com/30565.html">great post</a> that goes into the need for sVirt and the protections it can provide when you need to be insulated from potentially dangerous virtual machines or containers. If a user is root inside a container, they&rsquo;re root on the host as well. <!-- raw HTML omitted -->(There&rsquo;s an exception: <!-- raw HTML omitted -->UID namespaces<!-- raw HTML omitted -->. But let&rsquo;s not talk about that now. Oh great, first it was nested containers and now I brought up UID namespaces. Sorry again.)<!-- raw HTML omitted --></p>
<p>Dan&rsquo;s talk about securing containers hasn&rsquo;t popped up on the <a href="http://www.redhat.com/summit/2014/presentations/">Red Hat Summit presentations</a> page quite yet but here are some notes that I took and then highlighted:</p>
<ul>
<li>Containers don&rsquo;t contain. The kernel doesn&rsquo;t know about containers. Containers simply use kernel subsystems to carve up namespaces for applications.</li>
<li>Containers on Linux aren&rsquo;t complete. Don&rsquo;t compare directly to Solaris zones yet.</li>
<li>Running containers without Mandatory Access Control (MAC) systems like SELinux or AppArmor opens the door for full system compromise via untrusted applications and users within containers.</li>
</ul>
<p>Using MAC gives you one extra barrier to keep a malicious container from getting higher levels of access to the underlying host. There&rsquo;s always a chance that a kernel exploit could bypass MAC but it certainly raises the level of difficulty for an attacker and allows server operators extra time to react to alerts.</p>
]]></content></item><item><title>DevOps and enterprise inertia</title><link>https://major.io/2014/04/17/devops-and-enterprise-inertia/</link><pubDate>Thu, 17 Apr 2014 17:46:25 +0000</pubDate><guid>https://major.io/2014/04/17/devops-and-enterprise-inertia/</guid><description>As I wait in the airport to fly back home from this year&amp;rsquo;s Red Hat Summit, I&amp;rsquo;m thinking back over the many conversations I had over breakfast, over lunch, and during the events. One common theme that kept cropping up was around bringing DevOps to the enterprise. I stumbled upon Mathias Meyer&amp;rsquo;s post, The Developer is Dead, Long Live the Developer, and I was inspired to write my own.
Before I go any further, here&amp;rsquo;s my definition of DevOps: it&amp;rsquo;s a mindset shift where everyone is responsible for the success of the customer experience.</description><content type="html"><![CDATA[<p>As I wait in the airport to fly back home from this year&rsquo;s <a href="http://www.redhat.com/summit/2014/presentations/">Red Hat Summit</a>, I&rsquo;m thinking back over the many conversations I had over breakfast, over lunch, and during the events. One common theme that kept cropping up was around bringing DevOps to the enterprise. I stumbled upon <a href="https://twitter.com/roidrage">Mathias Meyer&rsquo;s</a> post, <a href="http://www.paperplanes.de/2014/4/17/the-developer-is-dead.html">The Developer is Dead, Long Live the Developer</a>, and I was inspired to write my own.</p>
<p>Before I go any further, here&rsquo;s my definition of DevOps: <em>it&rsquo;s a mindset shift where everyone is responsible for the success of the customer experience</em>. The success (and failure) of the project rests on everyone involved. If it goes well, everyone celebrates and looks for ways to highlight what worked well. If it fails, everyone gets involved to bring it back on track. Doing this correctly means that your usage of &ldquo;us&rdquo; and &ldquo;them&rdquo; should decrease sharply.</p>
<h3 id="the-issue-at-hand">The issue at hand</h3>
<p>One of the conference attendees told me that he and his technical colleagues are curious about trying DevOps but their organization isn&rsquo;t set up in a way to make it work. On top of that, very few members of the teams knew about the concept of continuous delivery and only one or two people knew about tools that are commonly used to practice it.</p>
<p>I dug deeper and discovered that they have outages just like any other company and they treat outages as an operations problem primarily.  Operations teams don&rsquo;t get much sleep and they get frustrated with poorly written code that is difficult to deploy, upgrade, and maintain.  Feedback loops with the development teams are relatively non-existent since the development teams report into a different portion of the business.  His manager knows that something needs to change but his manager wasn&rsquo;t sure how to change it.</p>
<p>His company certainly isn&rsquo;t unique.  My advice for him was to start a three step process:</p>
<h3 id="step-1-start-a-conversation-around-responsibility">Step 1: Start a conversation around responsibility.</h3>
<p>Leaders need to understand that the customer experience is key and that experience depends on much more than just uptime. This applies to products and systems that support internal users within your company and those that support your external customers.</p>
<p>Imagine if you called for pizza delivery and received a pizza without any cheese. You drive back to the pizza place to show the manager the partial pizza you received. The manager turns to the employees and they point to the person assigned to putting toppings on the pizza. They might say: &ldquo;It&rsquo;s his fault, I did my part and put it in the oven.&rdquo; The delivery driver might say: &ldquo;Hey, I did what I was supposed to and I delivered the pizza. It&rsquo;s not my fault.&rdquo;</p>
<p>All this time, you, the customer, are stuck holding a half made pizza. Your experience is awful.</p>
<p>Looking back, the person who put the pizza in the oven should have asked why it was only partially made. The delivery driver should have asked about it when it was going into the box. Most important of all, the manager should have turned to the employees and put the responsibility on all of them to make it right.</p>
<h3 id="step-2-foster-collaboration-via-cross-training">Step 2: Foster collaboration via cross-training.</h3>
<p>Once responsibility is shared, everyone within the group needs some knowledge of what other members of the group do. This is most obvious with developers and operations teams. Operations teams need to understand what the applications do and where their weak points are. Developers need to understand resource constraints and how to deploy their software. They don&rsquo;t need to become experts but they need to know enough overlapping knowledge to build a strong, healthy feedback loop.</p>
<p>This cross-training must include product managers, project managers, and leaders. Feedback loops between these groups will only be successful if they can speak some of the language of the other groups.</p>
<h3 id="step-3-dont-force-tooling">Step 3: Don&rsquo;t force tooling.</h3>
<p>Use the tools that make the most sense to the groups that need to use them. Just because a particular software tool helps another company collaborate or deploy software more reliably doesn&rsquo;t mean it will have a positive impact on your company.</p>
<p>Watch out for the &ldquo;<a href="https://en.wikipedia.org/wiki/Sunk_costs">sunk cost</a>&rdquo; fallacy as well. <a href="http://nealford.com/">Neal Ford</a> talked about this during a <a href="http://nealford.com/downloads/Agile_Architecture_and_Design(Neal_Ford).pdf">talk at the Red Hat Summit</a> and how it can really stunt the growth of a high performing team.</p>
<h3 id="summary">Summary</h3>
<p>The big takeaway from this post is that making the mindset shift is the first and most critical step if you want to use the DevOps model in a large organization. The first results you&rsquo;ll see will be in morale and camaraderie. That builds momentum faster than anything else and will carry teams into the idea of shared responsibility and ownership.</p>
]]></content></item><item><title>Upgrade OpenSSL, then upgrade WordPress</title><link>https://major.io/2014/04/10/upgrade-openssl-then-upgrade-wordpress/</link><pubDate>Thu, 10 Apr 2014 13:06:33 +0000</pubDate><guid>https://major.io/2014/04/10/upgrade-openssl-then-upgrade-wordpress/</guid><description>The internet has been buzzing about the heartbleed OpenSSL vulnerability but another critical update came out this week: WordPress 3.8.2. The update fixes two CVE&amp;rsquo;s and a few other security issues.
eWeek has an informative article with additional details on the update.</description><content type="html"><![CDATA[<p>The internet has been buzzing about the <a href="http://heartbleed.com/">heartbleed OpenSSL vulnerability</a> but another critical update came out this week: <a href="https://wordpress.org/news/2014/04/wordpress-3-8-2/">WordPress 3.8.2</a>. The <a href="http://codex.wordpress.org/Version_3.8.2">update fixes</a> two CVE&rsquo;s and a few other security issues.</p>
<p>eWeek has an <a href="http://www.eweek.com/security/wordpress-customers-receive-automatic-security-updates.html/">informative article</a> with additional details on the update.</p>
]]></content></item><item><title>openssl heartbleed updates for Fedora 19 and 20</title><link>https://major.io/2014/04/07/openssl-heartbleed-updates-for-fedora-19-and-20/</link><pubDate>Tue, 08 Apr 2014 01:18:19 +0000</pubDate><guid>https://major.io/2014/04/07/openssl-heartbleed-updates-for-fedora-19-and-20/</guid><description>The openssl heartbleed bug has made the rounds today and there are two new testing builds or openssl out for Fedora 19 and 20:
Fedora 19 Fedora 20 Both builds are making their way over into the updates-testingstable repository thanks to some quick testing and karma from the Fedora community.
If the stable updates haven&amp;rsquo;t made it into your favorite mirror yet, you can live on the edge and grab the koji builds:</description><content type="html"><![CDATA[<p><a href="/wp-content/uploads/2014/04/heartbleed.png"><!-- raw HTML omitted --></a>The <a href="http://heartbleed.com/">openssl heartbleed bug</a> has made the rounds today and there are two new testing builds or openssl out for Fedora 19 and 20:</p>
<ul>
<li><a href="https://admin.fedoraproject.org/updates/openssl-1.0.1e-37.fc19.1">Fedora 19</a></li>
<li><a href="https://admin.fedoraproject.org/updates/openssl-1.0.1e-37.fc20.1">Fedora 20</a></li>
</ul>
<p>Both builds are making their way over into the <!-- raw HTML omitted -->updates-testing<!-- raw HTML omitted --> <strong>stable</strong> repository thanks to some quick testing and karma from the Fedora community.</p>
<p>If the stable updates haven&rsquo;t made it into your favorite mirror yet, you can live on the edge and grab the koji builds:</p>
<h3 id="for-fedora-19-x86_64">For Fedora 19 x86_64:</h3>
<pre><code>yum -y install koji
koji download-build --arch=x86_64 openssl-1.0.1e-37.fc19.1
yum localinstall openssl-1.0.1e-37.fc19.1.x86_64.rpm
</code></pre><h3 id="for-fedora-20-x86_64">For Fedora 20 x86_64:</h3>
<pre><code>yum -y install koji
koji download-build --arch=x86_64 openssl-1.0.1e-37.fc20.1
yum localinstall openssl-1.0.1e-37.fc20.1.x86_64.rpm
</code></pre><p>Be sure to replace <em>x86_64</em> with <em>i686</em> for 32-bit systems or <em>armv7hl</em> for ARM systems (Fedora 20 only). If your system has <code>openssl-libs</code> or other package installed, be sure to install those with yum as well.</p>
<p>Kudos to <a href="https://fedoraproject.org/wiki/User:Ausil">Dennis Gilmore</a> for the hard work and to the Fedora community for the quick tests.</p>
]]></content></item><item><title>Detect proxies with icanhazproxy</title><link>https://major.io/2014/04/03/detect-proxies-with-icanhazproxy/</link><pubDate>Thu, 03 Apr 2014 15:52:58 +0000</pubDate><guid>https://major.io/2014/04/03/detect-proxies-with-icanhazproxy/</guid><description>You can already detect proxy servers using icanhazip.com by accessing the service on port 80, 81, and 443. If you compare your results and you see different IP addresses, there&amp;rsquo;s most likely a proxy in the way.
To make things easier, I&amp;rsquo;ve launched icanhazproxy.com. It&amp;rsquo;s available on ports 80, 81 and 443 as well. If you choose to access it on port 443, you&amp;rsquo;ll get a certificate for icanhazip.com that you&amp;rsquo;ll need to ignore.</description><content type="html"><![CDATA[<p>You can already detect proxy servers using <a href="http://icanhazip.com">icanhazip.com</a> by accessing the service on port <a href="http://icanhazip.com">80</a>, <a href="http://icanhazip.com:81">81</a>, and <a href="https://icanhazip.com">443</a>. If you compare your results and you see different IP addresses, there&rsquo;s most likely a proxy in the way.</p>
<p>To make things easier, I&rsquo;ve launched <a href="http://icanhazproxy.com/">icanhazproxy.com</a>. It&rsquo;s available on ports <a href="http://icanhazproxy.com/">80</a>, <a href="http://icanhazproxy.com:81/">81</a> and <a href="https://icanhazproxy.com/">443</a> as well. If you choose to access it on port 443, you&rsquo;ll get a certificate for icanhazip.com that you&rsquo;ll need to ignore.</p>
<p>You&rsquo;ll get one of two possible responses:</p>
<p><strong>200/OK with JSON output:</strong> When a proxy is detected, you&rsquo;ll receive a 200 response and any proxy-related headers will be returned in JSON format. Here&rsquo;s a sample:</p>
<pre><code>$ curl icanhazproxy.com
{&quot;via&quot;: &quot;1.1 proxy.example.com 0A065C93&quot;}
</code></pre><p>You may receive multiple headers via JSON, so please be prepared for that.</p>
<p><strong>204/NO CONTENT and empty response:</strong> No common proxy headers were detected. Here&rsquo;s an example:</p>
<pre><code>$ curl -si http://icanhazproxy.com:81/ | head -n1
HTTP/1.1 204 NO CONTENT
</code></pre><p>If you get this response but you know there&rsquo;s a proxy in the way, let me know. I may need to look for extra headers or other items in the request.</p>
<p>Go try it out and send me your feedback!</p>
]]></content></item><item><title>Xen hackathon coming up in London</title><link>https://major.io/2014/03/27/xen-hackathon-coming-up-in-london/</link><pubDate>Thu, 27 Mar 2014 13:20:34 +0000</pubDate><guid>https://major.io/2014/03/27/xen-hackathon-coming-up-in-london/</guid><description>If you enjoy using Xen, join members of the Xen Project community and Rackspace at the Xen Hackathon in London. The two day event starts on May 29th.
Use these links to get more information:
Hackathon announcement and travel/venue/registration information Discussion topics You don&amp;rsquo;t need to be a developer to join the event. It&amp;rsquo;s a great networking opportunity and you can take time to learn more about virtualization and how Xen works under the hood.</description><content type="html"><![CDATA[<p><a href="/wp-content/uploads/2014/03/MA0139_N60_medium.jpg"><!-- raw HTML omitted --></a></p>
<p>If you enjoy using Xen, join members of the Xen Project community and Rackspace at the Xen Hackathon in London. The two day event starts on May 29th.</p>
<p>Use these links to get more information:</p>
<ul>
<li><a href="http://blog.xen.org/index.php/2014/03/27/rackspace-hosts-xen-project-hackathon-may-29-30-in-london/">Hackathon announcement and travel/venue/registration information</a></li>
<li><a href="http://wiki.xen.org/wiki/Hackathon/May2014#Topics_to_Discuss.2C_Code.2C_Work_on.2C_..._at_the_Hackathon">Discussion topics</a></li>
</ul>
<p>You don&rsquo;t need to be a developer to join the event. It&rsquo;s a great networking opportunity and you can take time to learn more about virtualization and how Xen works under the hood.</p>
]]></content></item><item><title>Docker, trusted builds, and Fedora 20</title><link>https://major.io/2014/03/26/docker-trusted-builds-and-fedora-20/</link><pubDate>Wed, 26 Mar 2014 05:17:58 +0000</pubDate><guid>https://major.io/2014/03/26/docker-trusted-builds-and-fedora-20/</guid><description>Docker is a hot topic in the Linux world at the moment and I decided to try out the new trusted build process. Long story short, you put your Dockerfile along with any additional content into your GitHub repository, link your GitHub account with Docker, and then fire off a build. The Docker index labels it as &amp;ldquo;trusted&amp;rdquo; since it was build from source files in your repository.
I set off to build a Dockerfile to provision a container that would run all of the icanhazip services.</description><content type="html"><![CDATA[<p><a href="/wp-content/uploads/2014/03/docker-whale.png"><!-- raw HTML omitted --></a></p>
<p><a href="/wp-content/uploads/2012/01/fedorainfinity.png"><!-- raw HTML omitted --></a>Docker is a hot topic in the Linux world at the moment and I decided to try out the new <a href="http://blog.docker.io/2013/11/introducing-trusted-builds/">trusted build process</a>. Long story short, you put your Dockerfile along with any additional content into your GitHub repository, link your GitHub account with Docker, and then fire off a build. The Docker index labels it as &ldquo;trusted&rdquo; since it was build from source files in your repository.</p>
<p>I set off to build a Dockerfile to provision a container that would run all of the <a href="/icanhazip-com-faq/">icanhazip</a> services. Getting httpd running was a little tricky, but I soon had a <a href="https://github.com/major/icanhaz/blob/master/docker/Dockerfile">working Dockerfile</a> that built and ran successfully on Fedora 20.</p>
<p>The trusted build process kicked off without much fuss and I found myself waiting for a couple of hours for my job to start. I was sad to see an error after waiting so long:</p>
<pre><code>Installing : httpd-2.4.7-3.fc20.x86_64
error: unpacking of archive failed on file /usr/sbin/suexec: cpio: cap_set_file
</code></pre><p>Well, that&rsquo;s weird. It turns out that <code>cap_set_file</code> is part of libcap that sets filesystem capabilities based on the POSIX.1e standards. You can read up on capabilities in the <a href="https://www.kernel.org/pub/linux/libs/security/linux-privs/kernel-2.2/capfaq-0.2.txt">Linux kernel capabilities FAQ</a>. <em>(Special thanks to Andrew Clayton getting me pointed in the right direction there.)</em></p>
<p><a href="http://fedoraproject.org/wiki/User:Goldmann">Marek Goldmann</a> ran into this problem back in September 2013 and opened a <a href="https://bugzilla.redhat.com/show_bug.cgi?id=1012952">bug report</a>. Marek <a href="https://bugzilla.redhat.com/attachment.cgi?id=804061&amp;action=diff">proposed a change</a> to the Docker codebase that would remove setfcap from the list of banned capabilities in the LXC template used by docker. Another workaround would be to use the <code>-privileged</code> option to perform a build in privileged mode (available in docker 0.6+).</p>
<p>Both of those workarounds are unavailable when doing trusted builds with docker&rsquo;s index. Sigh.</p>
<p>I fired off an email to Docker&rsquo;s support staff and received a quick reply:</p>
<blockquote>
<p>Major,</p>
<p>We are aware of this issue, and we are currently working on a fix, and we hope to have something we can start testing this week. I&rsquo;m not sure when we will be able to roll out the fix, but we are hoping soon. Until then, there isn&rsquo;t anything you can do to work around it. Sorry for the inconvenience.</p>
<p>If anything changes, we will be sure to let you know.</p>
<p>Ken</p>
</blockquote>
<p>It wasn&rsquo;t the answer I wanted but it&rsquo;s good to know that the issue is being worked. In the meantime, I&rsquo;ll push an untrusted build of the icanhazip Docker container up to the index for everyone to enjoy.</p>
<p>Stay tuned for updates.</p>
<p><em><strong>UPDATED 2014-08-08:</strong> Per Thomas' comment below, this has been <a href="https://github.com/docker/docker/pull/5930">fixed upstream</a>.</em></p>
]]></content></item><item><title>Show originating IP address in Apple Mail</title><link>https://major.io/2014/03/18/show-originating-ip-address-in-apple-mail/</link><pubDate>Tue, 18 Mar 2014 14:20:13 +0000</pubDate><guid>https://major.io/2014/03/18/show-originating-ip-address-in-apple-mail/</guid><description>I&amp;rsquo;ve received some very sophisticated phishing emails lately and I was showing some of them to my coworkers. One of my coworkers noticed that my Apple Mail client displays the X-Originating-IP header for all of the emails I receive.
You can enter that IP into a whois search and get a better idea of who sent you the message without diving into the headers. If someone that regularly exchanges email with me suddenly has an originating IP in another country that would be unusual for them to travel to, I can approach the message with more caution.</description><content type="html"><![CDATA[<p>I&rsquo;ve received some very sophisticated phishing emails lately and I was showing some of them to my coworkers. One of my coworkers noticed that my Apple Mail client displays the X-Originating-IP header for all of the emails I receive.</p>
<p>You can enter that IP into a whois search and get a better idea of who sent you the message without diving into the headers. If someone that regularly exchanges email with me suddenly has an originating IP in another country that would be unusual for them to travel to, I can approach the message with more caution.</p>
<p>Enabling this feature in Mail is a quick process:</p>
<ol>
<li>Click on the <strong>Mail</strong> menu, then <strong>Preferences</strong></li>
<li>Go to the <strong>Viewing</strong> tab</li>
<li>Click the drop down menu next to <strong>Show header detail</strong> and choose <strong>Custom</strong></li>
<li>Click the <strong>plus (+)</strong> and type <code>X-Originating-IP</code></li>
<li>Click <strong>OK</strong> and close the <strong>Preferences</strong> window</li>
</ol>
<p>This should work in Apple Mail from OS X 10.6 through 10.9. You can also search your email for messages from certain IP addresses. Just start typing <code>X-Originating-IP: 123.234...</code> into the search field and watch the results appear.</p>
]]></content></item><item><title>Annoying security requests highlight company silos</title><link>https://major.io/2014/03/10/annoying-security-requests-highlight-company-silos/</link><pubDate>Mon, 10 Mar 2014 13:39:53 +0000</pubDate><guid>https://major.io/2014/03/10/annoying-security-requests-highlight-company-silos/</guid><description>I stumbled upon this video earlier today via Tripwire&amp;rsquo;s Twitter feed:
Some of the requests are hilarious, obviously, but many of them highlight a critical problem. In organizations where security is one department, silos develop and the &amp;ldquo;us versus them&amp;rdquo; mentality sets in quickly.
For organizations to grow and maintain security, the ownership of security and process maturity must be spread throughout the organization. Traditional corporate security teams simply cannot carry this burden alone.</description><content type="html"><![CDATA[<p>I stumbled upon this video earlier today via <a href="http://twitter.com/TripwireInc/status/443016198905921536">Tripwire&rsquo;s Twitter feed</a>:</p>
<p><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --></p>
<p>Some of the requests are hilarious, obviously, but many of them highlight a critical problem. In organizations where security is one department, silos develop and the &ldquo;us versus them&rdquo; mentality sets in quickly.</p>
<p>For organizations to grow and maintain security, the ownership of security and process maturity must be spread throughout the organization. Traditional corporate security teams simply cannot carry this burden alone. Security teams should be looked to as subject matter experts and consultants for critical projects. The business should be as eager to engage security experts as the security experts should be to engage the rest of the business.</p>
<p>Lopsided security ownership quickly leads to comments like the ones in the video.</p>
]]></content></item><item><title>virt-manager: ‘NoneType’ object has no attribute ‘cpus’</title><link>https://major.io/2014/03/06/virt-manager-nonetype-object-has-no-attribute-cpus/</link><pubDate>Thu, 06 Mar 2014 18:44:58 +0000</pubDate><guid>https://major.io/2014/03/06/virt-manager-nonetype-object-has-no-attribute-cpus/</guid><description>After upgrading my Fedora 20 Xen hypervisor to virt-manager 1.0.0, I noticed that I couldn&amp;rsquo;t open the console or VM details for any of my guests. Running virt-manager --debug gave me the following traceback:
Traceback (most recent call last): File &amp;quot;/usr/share/virt-manager/virtManager/engine.py&amp;quot;, line 803, in _show_vm_helper details = self._get_details_dialog(uri, uuid) File &amp;quot;/usr/share/virt-manager/virtManager/engine.py&amp;quot;, line 760, in _get_details_dialog obj = vmmDetails(con.get_vm(uuid)) File &amp;quot;/usr/share/virt-manager/virtManager/details.py&amp;quot;, line 530, in __init__ self.init_details() File &amp;quot;/usr/share/virt-manager/virtManager/details.py&amp;quot;, line 990, in init_details for name in [c.</description><content type="html"><![CDATA[<p>After upgrading my Fedora 20 Xen hypervisor to virt-manager 1.0.0, I noticed that I couldn&rsquo;t open the console or VM details for any of my guests. Running <code>virt-manager --debug</code> gave me the following traceback:</p>
<pre><code>Traceback (most recent call last):
  File &quot;/usr/share/virt-manager/virtManager/engine.py&quot;, line 803, in _show_vm_helper
    details = self._get_details_dialog(uri, uuid)
  File &quot;/usr/share/virt-manager/virtManager/engine.py&quot;, line 760, in _get_details_dialog
    obj = vmmDetails(con.get_vm(uuid))
  File &quot;/usr/share/virt-manager/virtManager/details.py&quot;, line 530, in __init__
    self.init_details()
  File &quot;/usr/share/virt-manager/virtManager/details.py&quot;, line 990, in init_details
    for name in [c.model for c in cpu_values.cpus]:
AttributeError: 'NoneType' object has no attribute 'cpus'
[Tue, 04 Mar 2014 22:13:31 virt-manager 21019] DEBUG (error:84) error dialog message:
summary=Error launching details: 'NoneType' object has no attribute 'cpus'
details=Error launching details: 'NoneType' object has no attribute 'cpus'
</code></pre><p>I <a href="https://bugzilla.redhat.com/show_bug.cgi?id=1072704">opened a bug report</a> and the fix was <a href="https://git.fedorahosted.org/cgit/virt-manager.git/commit/?id=b078ba8c3d69b62fe748d9182babef8971914277">committed upstream</a> today. If you want to make these updates to your Fedora 20 server before the update package is available, just snag the <a href="http://koji.fedoraproject.org/koji/buildinfo?buildID=502966">three RPM&rsquo;s from koji</a> and install them:</p>
<pre><code>mkdir /tmp/virt-manager
cd /tmp/virt-manager
wget http://kojipkgs.fedoraproject.org/packages/virt-manager/1.0.0/4.fc20/noarch/virt-install-1.0.0-4.fc20.noarch.rpm
wget http://kojipkgs.fedoraproject.org/packages/virt-manager/1.0.0/4.fc20/noarch/virt-manager-1.0.0-4.fc20.noarch.rpm
wget http://kojipkgs.fedoraproject.org/packages/virt-manager/1.0.0/4.fc20/noarch/virt-manager-common-1.0.0-4.fc20.noarch.rpm
yum localinstall *.rpm
</code></pre><p><strong>UPDATE:</strong> Thanks to Cole&rsquo;s comment below, you can actually pull in the RPM&rsquo;s using koji directly:</p>
<pre><code>koji download-build virt-manager-1.0.0-4.fc20
</code></pre>]]></content></item><item><title>Installing Xen on Fedora 20</title><link>https://major.io/2014/02/27/installing-xen-on-fedora-20/</link><pubDate>Fri, 28 Feb 2014 03:43:27 +0000</pubDate><guid>https://major.io/2014/02/27/installing-xen-on-fedora-20/</guid><description>I&amp;rsquo;ve written about installing Xen on Fedora 19 and earlier versions on this blog before. Let&amp;rsquo;s tackle it on Fedora 20.
Start with the Xen hypervisor and the basic toolset first:
yum -y install xen xen-hypervisor xen-libs xen-runtime systemctl enable xend.service systemctl enable xendomains.service Get GRUB2 in order:
# grep ^menuentry /boot/grub2/grub.cfg | cut -d &amp;quot;'&amp;quot; -f2 Fedora, with Linux 3.13.4-200.fc20.x86_64 Fedora, with Linux 0-rescue-c9dcecb251df472fbc8b4e620a749f6d Fedora, with Xen hypervisor # grub2-set-default 'Fedora, with Xen hypervisor' # grub2-editenv list saved_entry=Fedora, with Xen hypervisor # grub2-mkconfig -o /boot/grub2/grub.</description><content type="html"><![CDATA[<p><a href="/wp-content/uploads/2012/06/xen_logo_small.png"><!-- raw HTML omitted --></a>I&rsquo;ve written about <a href="/2013/06/02/installing-the-xen-hypervisor-on-fedora-19/">installing Xen on Fedora 19</a> and earlier versions on this blog before. Let&rsquo;s tackle it on Fedora 20.</p>
<p>Start with the Xen hypervisor and the basic toolset first:</p>
<pre><code>yum -y install xen xen-hypervisor xen-libs xen-runtime
systemctl enable xend.service
systemctl enable xendomains.service
</code></pre><p>Get GRUB2 in order:</p>
<pre><code># grep ^menuentry /boot/grub2/grub.cfg | cut -d &quot;'&quot; -f2
Fedora, with Linux 3.13.4-200.fc20.x86_64
Fedora, with Linux 0-rescue-c9dcecb251df472fbc8b4e620a749f6d
Fedora, with Xen hypervisor
# grub2-set-default 'Fedora, with Xen hypervisor'
# grub2-editenv list
saved_entry=Fedora, with Xen hypervisor
# grub2-mkconfig -o /boot/grub2/grub.cfg
</code></pre><p>Now reboot. When the server restarts, verify that Xen is running:</p>
<pre><code># xm dmesg | head
 __  __            _  _    _____  _    ___    __      ____   ___
 \ \/ /___ _ __   | || |  |___ / / |  / _ \  / _| ___|___ \ / _ \
  \  // _ \ '_ \  | || |_   |_ \ | |_| (_) || |_ / __| __) | | | |
  /  \  __/ | | | |__   _| ___) || |__\__, ||  _| (__ / __/| |_| |
 /_/\_\___|_| |_|    |_|(_)____(_)_|    /_(_)_|  \___|_____|\___/

(XEN) Xen version 4.3.1 (mockbuild@[unknown]) (gcc (GCC) 4.8.2 20131212 (Red Hat 4.8.2-7)) debug=n Thu Feb  6 16:52:58 UTC 2014
(XEN) Latest ChangeSet:
(XEN) Bootloader: GRUB 2.00
(XEN) Command line: placeholder
</code></pre><p>As I&rsquo;ve mentioned before, I enjoy using virt-manager to manage my VM&rsquo;s. Let&rsquo;s get started:</p>
<pre><code>yum -y install virt-manager dejavu* xorg-x11-xauth
yum -y install libvirt-daemon-driver-network libvirt-daemon-driver-storage libvirt-daemon-xen
systemctl enable libvirtd.service
systemctl start libvirtd.service
</code></pre><p>By this point, you have the Xen hypervisor running and you have VM management tools available from virt-manager and libvirt. Enjoy!</p>
]]></content></item><item><title>icanhaz more domains: icanhazepoch.com</title><link>https://major.io/2014/02/21/icanhaz-more-domains-icanhazepoch-com/</link><pubDate>Sat, 22 Feb 2014 02:41:06 +0000</pubDate><guid>https://major.io/2014/02/21/icanhaz-more-domains-icanhazepoch-com/</guid><description>Sometimes I can&amp;rsquo;t help myself. I like to buy domains and use them for interesting activities. This tweet cropped up in my stream tonight:
You can give it a try now: icanhazepoch.com
$ curl icanhazepoch.com 1393036551 I&amp;rsquo;m doing my best to keep the server time in sync, so it should be relatively reliable. However, I wouldn&amp;rsquo;t recommend using it for launching satellites or timing anything where someone&amp;rsquo;s life is held in the balance.</description><content type="html"><![CDATA[<p>Sometimes I can&rsquo;t help myself. I like to buy domains and use them for interesting activities. This tweet cropped up in my stream tonight:</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>You can give it a try now: <a href="http://icanhazepoch.com">icanhazepoch.com</a></p>
<pre><code>$ curl icanhazepoch.com
1393036551
</code></pre><p>I&rsquo;m doing my best to keep the server time in sync, so it should be relatively reliable. However, I wouldn&rsquo;t recommend using it for launching satellites or timing anything where someone&rsquo;s life is held in the balance. If you don&rsquo;t know why this warning is here, <a href="http://www.ntp.org/ntpfaq/NTP-s-sw-clocks-quality.htm">read up on NTP jitter</a>.</p>
<p>Thanks to <a href="http://twitter.com/claco">@claco</a> for the idea and to <a href="http://www.namecheap.com/?aff=7214">Namecheap</a> for fueling my domain purchasing addiction.</p>
]]></content></item><item><title>MySQLTuner v1.3.0</title><link>https://major.io/2014/02/21/mysqltuner-v1-3-0/</link><pubDate>Sat, 22 Feb 2014 02:30:53 +0000</pubDate><guid>https://major.io/2014/02/21/mysqltuner-v1-3-0/</guid><description>It&amp;rsquo;s been three long years since the last MySQLTuner release but you&amp;rsquo;ll now find version 1.3.0 available on GitHub. You can get it from the git repository or via these extremely simple methods:
wget -O mysqltuner.pl mysqltuner.pl wget --trust-server-names mysqltuner.pl There are a bunch of new features and fixes that you can find in the list of commits from today (2014-02-21). Some of the bigger adjustments include:
Basic support for MariaDB 10.</description><content type="html"><![CDATA[<p>It&rsquo;s been three long years since the last MySQLTuner release but you&rsquo;ll now find <a href="https://github.com/major/MySQLTuner-perl/releases/tag/v1.3.0">version 1.3.0</a> available on GitHub. You can get it from the <a href="https://github.com/major/MySQLTuner-perl">git repository</a> or via these extremely simple methods:</p>
<pre><code>wget -O mysqltuner.pl mysqltuner.pl
wget --trust-server-names mysqltuner.pl
</code></pre><p>There are a bunch of new features and fixes that you can find in the <a href="https://github.com/major/MySQLTuner-perl/commits/master">list of commits from today</a> (2014-02-21). Some of the bigger adjustments include:</p>
<ul>
<li>Basic support for MariaDB 10.x</li>
<li>A more flexible storage engine display</li>
<li>Better support for darwin, solaris, and BSD variants</li>
<li>Version checking is removed until I can find a better method</li>
<li>Fixed a divide by zero error with a key_buffer_size set to 0</li>
<li>Custom paths to <code>mysqladmin</code> are now supported with a command line parameter</li>
</ul>
<p>I&rsquo;d still like to convert this script over to python and make it installable from pypi. That&rsquo;s a work in progress.</p>
]]></content></item><item><title>Puppy Linux, icanhazip, and tin foil hats</title><link>https://major.io/2014/02/09/puppy-linux-icanhazip-and-tin-foil-hats/</link><pubDate>Mon, 10 Feb 2014 04:04:27 +0000</pubDate><guid>https://major.io/2014/02/09/puppy-linux-icanhazip-and-tin-foil-hats/</guid><description>I figured that the Puppy Linux and icanhazip.com fiasco was over, but I was wrong:
After a quick visit to the forums, I found the debate stirred up again. Various users were wondering if their internet connections were somehow compromised or if a remote American network was somehow spying on their internet traffic. Others wondered if some secretive software was added to the Puppy Linux distribution that was calling out to the site.</description><content type="html"><![CDATA[<p>I figured that the <a href="/2012/08/04/privacy-and-icanhazip-com/">Puppy Linux and icanhazip.com fiasco was over</a>, but I was wrong:</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>After a quick visit to the forums, <a href="http://murga-linux.com/puppy/viewtopic.php?t=90151">I found the debate stirred up again</a>. Various users were wondering if their internet connections were somehow compromised or if a remote American network was somehow spying on their internet traffic. Others wondered if some secretive software was added to the Puppy Linux distribution that was calling out to the site.</p>
<p>Fortunately, quite a few users on the forum showed up <a href="http://murga-linux.com/puppy/viewtopic.php?p=748928#748928">to explain</a> that Puppy Linux has a built-in feature to figure out a user&rsquo;s external IP address to help them get started with their system after it boots. <a href="http://murga-linux.com/puppy/viewtopic.php?p=749399#749399">Another user</a> was kind enough to <a href="http://lifehacker.com/5785602/find-your-public-ip-anywhere-with-icanhazipcom">dig up the Lifehacker post</a> about icanhazip from 2011.</p>
<p>Many users on the forum were still dissatisfied. Many of them turned their questions to maintainers of the distribution (which is where those questions should go), but many others felt that icanhazip was the source of the problem. Some of them felt so strongly that they called my hosting provider via telephone to curse at them. Here&rsquo;s a snippet of an email I received from my colocation provider:</p>
<blockquote>
<p>I had an interesting call from someone today said that 216.69.252.101 was showing up on his computer. Sounded kind of [omitted] and called me a <em>*\</em>* ******…</p>
</blockquote>
<p>Let&rsquo;s get three things straight:</p>
<ol>
<li>I&rsquo;m a huge supporter of everything Linux, including Puppy Linux. I don&rsquo;t hold a grudge against the project for what a minority of their users do.</li>
<li>I don&rsquo;t collect data when users visit icanhazip.com other than standard Apache logs. No cookies are used.</li>
<li>I run these applications on my own time, with my own money, and my own resources.</li>
</ol>
<p>Before I forget, thanks to all of the folks who came forward in the forums to explain what was actually happening and defend the work I&rsquo;ve done. I&rsquo;m tremendously flattered to receive that kind of support.</p>
]]></content></item><item><title>Be an inspiration, not an impostor</title><link>https://major.io/2014/02/04/be-an-inspiration-not-an-impostor/</link><pubDate>Wed, 05 Feb 2014 03:44:18 +0000</pubDate><guid>https://major.io/2014/02/04/be-an-inspiration-not-an-impostor/</guid><description>Many of the non-technical posts on the blog are inspired by the comments of others. I stumbled upon this tweet after it was retweeted by someone I follow:
The link in the tweet takes you to a blog post from Erika Owens about impostor syndrome. Erika touches on that uncomfortable feeling that some of us feel when we&amp;rsquo;re surrounded by other people from our field of study or work. These three sentences hit home for me:</description><content type="html"><![CDATA[<p>Many of the non-technical posts on the blog are inspired by the comments of others. I stumbled upon this tweet after it was retweeted by someone I follow:</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>The link in the tweet takes you to a <a href="http://www.niemanlab.org/2013/12/the-year-to-eradicate-imposter-syndrome/">blog post from Erika Owens about impostor syndrome</a>. Erika touches on that uncomfortable feeling that some of us feel when we&rsquo;re surrounded by other people from our field of study or work. These three sentences hit home for me:</p>
<blockquote>
<p>At first, I thought people were just being modest. But it soon became clear that people were reluctant to recognize in themselves the same traits that awed them in other people. This dynamic holds people back while also overtaxing the limited number of anointed experts.</p>
</blockquote>
<p><a href="https://twitter.com/annegentle">Anne Gentle</a> gave a presentation at this year&rsquo;s offsite for leaders at Rackspace and talked about the challenges of defeating impostor syndrome while attending male-dominated technical conferences. She talked about a portion of these challenges in her <a href="http://opensource.com/business/14/2/women-of-openstack-conference-group">Women of OpenStack</a> post.</p>
<p>I&rsquo;ve struggled with this from time to time with various groups. Sure, I have some deep technical knowledge and experience in some areas, but I don&rsquo;t always feel like the expert in those areas. One thing I&rsquo;ve come to realize is that when you&rsquo;re invited to talk to a group or asked to write an article, <em>you&rsquo;re being asked because the community has identified you as an expert</em>.</p>
<p>&ldquo;Expert&rdquo; is always a relative term. Toss me in a room with Windows system administrators and I can provide an expert level of guidance around the Linux kernel. If <a href="https://en.wikipedia.org/wiki/Linus_torvalds">Linus</a> or <a href="https://en.wikipedia.org/wiki/Greg_Kroah-Hartman">Greg Kroah-Hartman</a> walk in the door, I&rsquo;d certainly defer to them. I&rsquo;d definitely offer up an opinion if asked or if I disagreed with something that was being said (even if an &ldquo;expert&rdquo; said it). With that said, I&rsquo;ve spoken with Linus and Greg in person and they seem to understand this well. They leave gaps in conversation and defer to their peers to ensure that the experts around them get time in the spotlight.</p>
<p>Here&rsquo;s where the rubber meets the road: when you embrace your expertise and share it with others, <em>you <strong>inspire</strong> them</em>.</p>
<p>What happens when you inspire others? They&rsquo;re more eager to talk. They&rsquo;re more eager to listen. <strong>They&rsquo;re more eager to learn more and embrace their inner expert.</strong></p>
<p>This process isn&rsquo;t easy. Read through my post on why <a href="/2012/03/30/why-technical-people-should-blog-but-dont/">technical people should blog, but don&rsquo;t</a>. You&rsquo;ll need to understand that you&rsquo;ll be wrong from time to time and that you&rsquo;ll need to do some homework when you&rsquo;re asked for an expert opinion. You&rsquo;ll also need to learn when and how to say &ldquo;I don&rsquo;t know, but I&rsquo;ll find out the answer.&rdquo;</p>
<p>The next time you feel like you know less than the other people in the room, speak up. You&rsquo;ll probably be an inspiration to many in the room who feel like an impostor and they&rsquo;ll want to follow your lead.</p>
]]></content></item><item><title>Hierarchy of DevOps Needs from DevOps Weekly</title><link>https://major.io/2014/02/02/hierarchy-of-devops-needs-from-devops-weekly/</link><pubDate>Mon, 03 Feb 2014 02:30:40 +0000</pubDate><guid>https://major.io/2014/02/02/hierarchy-of-devops-needs-from-devops-weekly/</guid><description>I&amp;rsquo;ve made posts about the DevOps Weekly mailing list before. If you haven&amp;rsquo;t signed up yet, do so now. You&amp;rsquo;ll thank me later.
Aaron Suggs' Hierarchy of DevOps Needs gives a great summary of the building blocks of a solid development culture and how they relate to one another. I laughed a bit when I saw his pyramid because I&amp;rsquo;ve seen many development groups build the pyramid upside down in the past.</description><content type="html"><![CDATA[<p><em>I&rsquo;ve made posts about the DevOps Weekly mailing list before. If you haven&rsquo;t signed up yet, <a href="http://devopsweekly.com/">do so now</a>. You&rsquo;ll thank me later.</em></p>
<p>Aaron Suggs' <a href="https://www.kickstarter.com/backing-and-hacking/hierarchy-of-devops-needs">Hierarchy of DevOps Needs</a> gives a great summary of the building blocks of a solid development culture and how they relate to one another. I laughed a bit when I saw his pyramid because I&rsquo;ve seen many development groups build the pyramid upside down in the past.</p>
<p>At the core of DevOps is that <em>everyone</em> owns the result. Developers, operations folks, product managers, and leaders are all responsible for the success or failure of a development project. If the developers are happy and writing code that keeps the operations engineers up all night, that&rsquo;s not DevOps. On the other hand, if the operations team is slowing down deployments or building inconsistent environments, then they&rsquo;re not taking ownership of the results.</p>
]]></content></item><item><title>supernova 0.9.0 is now available</title><link>https://major.io/2014/01/13/supernova-0-9-0-is-now-available/</link><pubDate>Mon, 13 Jan 2014 20:28:44 +0000</pubDate><guid>https://major.io/2014/01/13/supernova-0-9-0-is-now-available/</guid><description>You&amp;rsquo;ll find version 0.9.0 of supernova on GitHub.
Here are the big changes:
A fix for supernova with python 2.6 (thanks to Sandy Walsh)
You can now use supernova with executables other than nova (thanks to Dan Krause)
Bash completion support (thanks to Cory Wright)
Version 0.9.0 is already on PyPi as well. Enjoy!</description><content type="html"><![CDATA[<p>You&rsquo;ll find <a href="https://github.com/major/supernova/releases/tag/v0.9.0">version 0.9.0</a> of supernova on <a href="https://github.com/major/supernova">GitHub</a>.</p>
<p>Here are the big changes:</p>
<ul>
<li>
<p>A <a href="https://github.com/major/supernova/commit/2e5e00ac97254ef7d03f2b61e649fe2b6476faa4">fix</a> for supernova with python 2.6 (thanks to <a href="https://github.com/SandyWalsh">Sandy Walsh</a>)</p>
</li>
<li>
<p>You can now <a href="https://github.com/major/supernova/pull/31">use supernova with executables</a> other than nova (thanks to <a href="https://github.com/dankrause">Dan Krause</a>)</p>
</li>
<li>
<p><a href="https://github.com/major/supernova/pull/10">Bash completion support</a> (thanks to <a href="https://github.com/corywright">Cory Wright</a>)</p>
</li>
</ul>
<p>Version 0.9.0 is <a href="https://pypi.python.org/pypi/supernova/0.9.0">already on PyPi</a> as well. Enjoy!</p>
]]></content></item><item><title>CentOS and Red Hat join forces</title><link>https://major.io/2014/01/08/centos-and-red-hat-join-forces/</link><pubDate>Wed, 08 Jan 2014 20:23:52 +0000</pubDate><guid>https://major.io/2014/01/08/centos-and-red-hat-join-forces/</guid><description>I&amp;rsquo;d like to congratulate the CentOS project on their big news yesterday. The CentOS website received a refresh and there&amp;rsquo;s a new concept of variants being proposed. After talking to Karanbir Singh and Robyn Bergeron yesterday, it was obvious that they&amp;rsquo;re both really excited about the changes.
If you&amp;rsquo;re looking for more details on the changes, here are some handy links:
Robyn Bergeron&amp;rsquo;s blog post Red Hat and CentOS FAQ The media coverage has some entertaining headlines:</description><content type="html"><![CDATA[<p>I&rsquo;d like to congratulate the CentOS project on their <a href="http://lists.centos.org/pipermail/centos-announce/2014-January/020100.html">big news yesterday</a>. The <a href="https://www.centos.org/">CentOS website</a> received a refresh and there&rsquo;s a <a href="https://www.centos.org/variants/">new concept of variants</a> being proposed. After talking to <a href="http://wiki.centos.org/KaranbirSingh">Karanbir Singh</a> and <a href="https://fedoraproject.org/wiki/User:Rbergero">Robyn Bergeron</a> yesterday, it was obvious that they&rsquo;re both really excited about the changes.</p>
<p>If you&rsquo;re looking for more details on the changes, here are some handy links:</p>
<ul>
<li><a href="http://networkedblogs.com/SDdaz">Robyn Bergeron&rsquo;s blog post</a></li>
<li><a href="http://community.redhat.com/centos-faq/">Red Hat and CentOS FAQ</a></li>
</ul>
<p>The media coverage has some entertaining headlines:</p>
<ul>
<li><a href="http://www.wired.com/wiredenterprise/2014/01/redhat-centos/">Hell Freezes Over in Linux Land as Red Hat Makes Nice With Its Clone</a></li>
<li><a href="http://arstechnica.com/information-technology/2014/01/red-hat-and-centos-become-voltron-build-free-operating-system-together/">Red Hat and CentOS become Voltron, build free operating system together</a></li>
</ul>
]]></content></item><item><title>nf_conntrack: table full, dropping packet</title><link>https://major.io/2014/01/07/nf-conntrack-table-full-dropping-packet/</link><pubDate>Tue, 07 Jan 2014 20:22:01 +0000</pubDate><guid>https://major.io/2014/01/07/nf-conntrack-table-full-dropping-packet/</guid><description>I was doing some testing with apachebench and received some peculiar results:
[608487.317284] nf_conntrack: table full, dropping packet [608487.708916] nf_conntrack: table full, dropping packet [608488.010236] nf_conntrack: table full, dropping packet I&amp;rsquo;ve seen this problem before and I tried to fix it by adjusting /proc/sys/net/ipv4/ip_conntrack_max as I did back in 2008. However, Fedora 20 doesn&amp;rsquo;t have the same structure in /proc under kernel 3.12.
The fix is to adjust /proc/sys/net/netfilter/nf_conntrack_max instead:</description><content type="html"><![CDATA[<p>I was doing some testing with apachebench and received some peculiar results:</p>
<pre><code>[608487.317284] nf_conntrack: table full, dropping packet
[608487.708916] nf_conntrack: table full, dropping packet
[608488.010236] nf_conntrack: table full, dropping packet
</code></pre><p>I&rsquo;ve <a href="/2008/01/24/ip_conntrack-table-full-dropping-packet/">seen this problem before</a> and I tried to fix it by adjusting /proc/sys/net/ipv4/ip_conntrack_max as I did back in 2008. However, Fedora 20 doesn&rsquo;t have the same structure in /proc under kernel 3.12.</p>
<p>The fix is to adjust /proc/sys/net/netfilter/nf_conntrack_max instead:</p>
<pre><code>echo 256000 &gt; /proc/sys/net/netfilter/nf_conntrack_max
</code></pre><p>After a quick test, apachebench was back to normal. You can make the change permanent and test it with:</p>
<pre><code>echo &quot;net.netfilter.nf_conntrack_max = 256000&quot; &gt;&gt; /etc/sysctl.conf
sysctl -p
</code></pre><p>There are some handy connection tracking tools available in the conntrack-tools package. Take a look at the man page for conntrack and you&rsquo;ll find ways to review and flush the connection tracking table.</p>
]]></content></item><item><title>Learn octal file permissions easily with stat</title><link>https://major.io/2013/12/10/learn-octal-file-permissions-easily-with-stat/</link><pubDate>Tue, 10 Dec 2013 13:41:40 +0000</pubDate><guid>https://major.io/2013/12/10/learn-octal-file-permissions-easily-with-stat/</guid><description>My SANS classmates were learning how to set and recognize file permissions on a Linux server and we realized it would be helpful to display the octal value of the permissions next to the normal rwx display. Fortunately, a quick search revealed that stat could deliver this information:
# stat -c &amp;quot;%a %A %n&amp;quot; /usr/sbin/* | head 755 -rwxr-xr-x /usr/sbin/accessdb 755 -rwxr-xr-x /usr/sbin/acpid 755 -rwxr-xr-x /usr/sbin/addgnupghome 755 -rwxr-xr-x /usr/sbin/addpart 777 lrwxrwxrwx /usr/sbin/adduser 755 -rwxr-xr-x /usr/sbin/agetty 755 -rwxr-xr-x /usr/sbin/alternatives 755 -rwxr-xr-x /usr/sbin/anacron 755 -rwxr-xr-x /usr/sbin/apachectl 755 -rwxr-xr-x /usr/sbin/applygnupgdefaults The first octal digit (for setuid, setgid, and sticky) is left off for any files without those bits set.</description><content type="html"><![CDATA[<p>My SANS classmates were learning how to set and recognize file permissions on a Linux server and we realized it would be helpful to display the octal value of the permissions next to the normal <em>rwx</em> display. Fortunately, a quick search revealed that <code>stat</code> could deliver this information:</p>
<pre><code># stat -c &quot;%a %A %n&quot; /usr/sbin/* | head
755 -rwxr-xr-x /usr/sbin/accessdb
755 -rwxr-xr-x /usr/sbin/acpid
755 -rwxr-xr-x /usr/sbin/addgnupghome
755 -rwxr-xr-x /usr/sbin/addpart
777 lrwxrwxrwx /usr/sbin/adduser
755 -rwxr-xr-x /usr/sbin/agetty
755 -rwxr-xr-x /usr/sbin/alternatives
755 -rwxr-xr-x /usr/sbin/anacron
755 -rwxr-xr-x /usr/sbin/apachectl
755 -rwxr-xr-x /usr/sbin/applygnupgdefaults
</code></pre><p>The first octal digit (for setuid, setgid, and sticky) is left off for any files without those bits set.</p>
]]></content></item><item><title>Information security nuggets from DevOps Weekly #150</title><link>https://major.io/2013/11/17/information-security-nuggets-from-devops-weekly-150/</link><pubDate>Sun, 17 Nov 2013 21:31:10 +0000</pubDate><guid>https://major.io/2013/11/17/information-security-nuggets-from-devops-weekly-150/</guid><description>Keeping an eye out for the DevOps Weekly email is something I&amp;rsquo;ve enjoyed since it started at the end of 2010. It&amp;rsquo;s usually chock full of tips for systems engineers, developers, managers, or anyone who is focused on environments that utilize continuous integration and deployment strategies. Quite a few of the tips are totally relevant for information security professionals who are looking for an edge at work.
This week, there are four links worth reviewing if you work in information security:</description><content type="html"><![CDATA[<p>Keeping an eye out for the <a href="http://devopsweekly.com/">DevOps Weekly email</a> is something I&rsquo;ve enjoyed since it started at the end of 2010. It&rsquo;s usually chock full of tips for systems engineers, developers, managers, or anyone who is focused on environments that utilize continuous integration and deployment strategies. Quite a few of the tips are totally relevant for information security professionals who are looking for an edge at work.</p>
<p>This week, there are four links worth reviewing if you work in information security:</p>
<ul>
<li><a href="http://www.sitespeed.io/">sitespeed.io</a></li>
<li><a href="http://www.threedrunkensysadsonthe.net/2013/11/burnout-recovery-and-honesty/">Burnout, Recovery and Honesty</a></li>
<li><a href="http://flowcon.org/dl/flowcon-sanfran-2013/slides/JamesDeLuccia_SuccessfullyEstablishingAndRepresentingDevOpsInAnAudit.pdf">Audits of High Deployment Environments</a> [PDF]</li>
<li><a href="http://www.danielmiessler.com/study/tcpdump/">A tcpdump Primer</a></li>
</ul>
<p>The idea behind <a href="http://www.sitespeed.io/">sitespeed.io</a> is to monitor an application&rsquo;s performance through deployments. Availability is critical to security (although it&rsquo;s often de-prioritized until you feel the pain) and it can signal an attack in process. Performance degradation over time could allow the application to be knocked offline from smaller attacks.</p>
<p><a href="http://www.threedrunkensysadsonthe.net/2013/11/burnout-recovery-and-honesty/">Burnout, Recovery and Honesty</a> is an anecdote from an IT worker about how their job changed their personal and home life. It&rsquo;s worth a read so that you can catch the warning signs of burnout within yourself and your coworkers.</p>
<p>Bringing continuous deployments to large companies is challenging due to the number of compliance and regulatory programs. A great slide deck called <a href="http://flowcon.org/dl/flowcon-sanfran-2013/slides/JamesDeLuccia_SuccessfullyEstablishingAndRepresentingDevOpsInAnAudit.pdf">Audits of High Deployment Environments</a> covers some of the basic strategies for how to deal with these challenges.</p>
<p>Finally, my favorite nugget from this week&rsquo;s newsletter is the the <a href="http://www.danielmiessler.com/study/tcpdump/">tcpdump primer</a>. It&rsquo;s a great resource for people who have never used tcpdump or for those of us who have only used some of the basic functionality. You&rsquo;ll be able to get more data out of tcpdump with less fuss after reading the post.</p>
]]></content></item><item><title>One year in information security</title><link>https://major.io/2013/11/13/one-year-in-information-security/</link><pubDate>Wed, 13 Nov 2013 15:15:12 +0000</pubDate><guid>https://major.io/2013/11/13/one-year-in-information-security/</guid><description>Going to the dark side. Those were my first thoughts about taking an information security role one year ago. One year later, the situation seems much brighter than I expected.
This role has taught me more about how our business operates, how we set priorities, and how to respond to a setback. I&amp;rsquo;ve been fortunate enough to meet some extremely intelligent people along the way. Some of them frighten me with their descriptions of past experiences or their adversaries.</description><content type="html"><![CDATA[<p>Going to the dark side.  Those were my first thoughts about taking an information security role one year ago.  One year later, the situation seems much brighter than I expected.</p>
<p>This role has taught me more about how our business operates, how we set priorities, and how to respond to a setback.  I&rsquo;ve been fortunate enough to meet some extremely intelligent people along the way.  Some of them frighten me with their descriptions of past experiences or their adversaries.  Other people spin a different tale about mature, consistent information security programs that deliver value to the business.</p>
<p>I was asked by a coworker last week to talk about three things I&rsquo;ve learned over the past year as I transitioned from the world of managing Linux servers, wrangling software deployments, and writing python to a heavy focus on information security at a macro level.  This post is a response to that request.</p>
<p>Without further ado, here are the three biggest lessons I&rsquo;ve learned over the past year:</p>
<p><strong>Double down on what motivates people</strong></p>
<p>It&rsquo;s easy to focus on the critics when you work in a corporate security department.  They wrestle with you on anything that causes any changes in their day-to-day work.  My immediate reaction was an angry one: &ldquo;Why don&rsquo;t they <em>get it</em>?&rdquo;  In most cases, they did get it; but the demands placed on them to complete a task or launch a product was the top priority.  It didn&rsquo;t take long before I was frustrated.</p>
<p>Luckily, I found a copy of <a href="http://heathbrothers.com/books/switch/"><em>Switch</em></a> on a bookshelf and took it home to read it.  One of the big takeaways from the book is to look for your partners in the business when you&rsquo;re focused on the critics.  Find the people who are motivated to take security seriously and take them out to lunch.  Learn about their background and their previous experiences.  Discover why security is a priority for them and why it motivates them to change.  Once you find out what motivates them, double down on that motivation when you spot a critic.  It won&rsquo;t work every time (you certainly can&rsquo;t please everyone), but it has paid dividends for me.</p>
<p>Keep in mind that what motivates one person might not motivate the next person.  You may find someone who embraces security because they&rsquo;ve worked through a serious breach in the past.  Retelling that story to another person might not have the same impact, but it may lead to a higher-level discussion around the value of the change you&rsquo;re trying to drive.</p>
<p>If you work in an environment with highly technical people, always remember to talk about the &ldquo;why&rdquo; behind the change.  In most of my communication, I generally start with the &ldquo;why&rdquo; or &ldquo;what&rsquo;s broken&rdquo; first.  Get them to agree that something is broken and you can lead them to your desired solution.  That initial agreement builds trust and it allows you to revert back to a common ground in case the conversation goes astray. A former manager taught me this method and it works extremely well.</p>
<p>I prefer to talk about things you should do rather than the things you shouldn&rsquo;t, but there are two critical things I have to mention.  Don&rsquo;t spread fear, uncertainty and doubt (FUD).  Also, certifications don&rsquo;t make you an expert.  When people feel that you&rsquo;re constantly throwing out doomsday scenarios or you&rsquo;re grandstanding with alphabet soup after your name, they&rsquo;ll become desensitized to your message.  The difficulty involved with changing their minds is then ratcheted up another level.</p>
<p><strong>Evade analysis paralysis by painting with broad strokes</strong></p>
<p>It&rsquo;s easy to sweat the small stuff in information security.  If you don&rsquo;t believe me, just look at your average vulnerability scan report.  Once you filter out all of the false positives and irrelevant vulnerabilities, you&rsquo;re left with a final few items that are worth an additional review.  Scan reports of multiple systems (or subnets) can get out of hand quickly.  Sure, those vulnerabilities should be fixed, but think about how you can take a bigger picture approach to the problem.</p>
<p>Another favorite book of mine is <a href="http://itrevolution.com/books/phoenix-project-devops-book/"><em>The Phoenix Project</em></a>.  It&rsquo;s an adaptation of <em>The Goal</em> that is specific to IT workers.  The main character is suddenly promoted after his superiors are relieved and he is overwhelmed with tons of IT problems big and small.  After a good dose of firefighting and tactical work, he discovers that the problems plaguing his department are very broad.  Change management, documentation, and resource contention were completely out of control.  He comes to terms with the problems and realizes that he won&rsquo;t succeed unless he steps back and looks at the big picture.  There&rsquo;s also an amazing CISO character in the book and he goes through the same transformation.</p>
<p>Instead of focusing on the small battles, focus on the war.  Look for ways to drive consistency first.  If nobody has set the bar for security within your organization, set it.  Start out with something simple and partner with your supporters in the business to gain buy-in.  Setting a standard does something interesting to humans: we don&rsquo;t want anything we maintain to be called &ldquo;substandard.&rdquo;</p>
<p>Find ways to weave your standards in with the business in helpful ways.  Write scripts.  Do demos.  Figure out which configuration management software they use and try to build your standards into the existing frameworks.  Talk to them on their turf and in their terms.  When they hear you speaking their language and integrating with their tools, they will be much more eager to collaborate with you.  That&rsquo;s a great time to deliver your message and weave security into the fabric of their project.</p>
<p>Building consistency will take time depending on the maturity of the organization.  As it builds, raise the bar with the help of your supporters.  Do it gradually and closely monitor the effects.  Businesses constantly do this with software development cycles and uptime improvements.  Implementing security is no different.</p>
<p><strong>Drive self-reliance by making them part of the process</strong></p>
<p>One of my peers said it best: &ldquo;Everyone is part of the security team.  We all play a part.&rdquo;  Getting people to feel that they&rsquo;re responsible for security isn&rsquo;t easy, but you can make an impact by explaining the &ldquo;why&rdquo; behind your changes, partnering with standards, and keeping an open door policy.</p>
<p>Security teams need to maintain a feedback loop with the business.  I feel like I say this constantly: &ldquo;We won&rsquo;t have a security team if we never launch a product.&rdquo;  There&rsquo;s always going to be a situation where something launches with vulnerabilities (whether known or unknown) and the business accepts the risk.  Don&rsquo;t dwell on that; you&rsquo;re sweating the small stuff.  Instead, think about helping the business avoid that risk in the future.  Should we develop a standard?  Is our testing process rigorous enough?  Do we need more detailed training for developers or engineers?</p>
<p>That feedback loop must include open and frank discussions about failures without a rush to blame.  My favorite example of this thought process is a post from <a href="https://twitter.com/allspaw">John Allspaw</a> titled <a href="http://www.kitchensoap.com/2013/09/30/learning-from-failure-at-etsy/"><em>Learning from Failure at Etsy</em></a>.  A business can drive accountability without needing to place blame.  If you&rsquo;ve ever gone through a <a href="http://en.wikipedia.org/wiki/Ishikawa_diagram">fishbone diagram</a> or you&rsquo;ve answered the <a href="http://en.wikipedia.org/wiki/5_Whys">Five Why&rsquo;s</a>, you know what I&rsquo;m talking about.  Trace it back to the original failure and you&rsquo;ll most often find a process or a technology problem and not a people problem.</p>
<p>If an IT team can&rsquo;t be honest with a security team because they fear punishment or shaming, then they won&rsquo;t share the real problems.  This could be disastrous for a security team since they&rsquo;re operating with only a portion of the real story.  The opposite is also true: security teams must feel comfortable sharing their failures.  Healthy feedback loops like these build trust and engagement.  That leads to more process improvements and fewer failures.  If there&rsquo;s anything I&rsquo;ve learned about security teams, it&rsquo;s that we don&rsquo;t want to fail.</p>
<p><strong>Conclusion</strong></p>
<p>This post might read a bit more pedantic than I intended, but I hope you find it useful.  Much of it applies to more than just information security.  Think about where you work in your company and which groups you find yourself at odds with daily.  Learn what motivates your supporters, paint with broad strokes, and make everyone part of the process.</p>
<p>You might find more in common with them than you ever expected.</p>
]]></content></item><item><title>Speed up your Fedora PXE installations by hosting the stage2 installer locally</title><link>https://major.io/2013/11/03/speed-up-your-fedora-pxe-installations-by-hosting-the-stage2-installer-locally/</link><pubDate>Sun, 03 Nov 2013 17:04:33 +0000</pubDate><guid>https://major.io/2013/11/03/speed-up-your-fedora-pxe-installations-by-hosting-the-stage2-installer-locally/</guid><description>In my previous post about installing Fedora via PXE, I forgot to mention a big time saver for the installation. A Fedora PXE installation requires a few different things:
initial ramdisk (initrd.img) kernel (vmlinuz) installation repository If you only specify an installation repository, then Anaconda tries to drag down a 214MB squashfs.img file in each installation. You can host this file locally by recreating a portion of a Fedora repo&amp;rsquo;s structure and dropping two files into it.</description><content type="html"><![CDATA[<p>In my previous post about <a href="/2013/07/23/pxe-boot-fedora-19-using-a-mikrotik-firewall/">installing Fedora via PXE</a>, I forgot to mention a big time saver for the installation. A Fedora PXE installation requires a few different things:</p>
<ul>
<li>initial ramdisk (<code>initrd.img</code>)</li>
<li>kernel (<code>vmlinuz</code>)</li>
<li>installation repository</li>
</ul>
<p>If you only specify an installation repository, then Anaconda tries to drag down a 214MB squashfs.img file in each installation. You can host this file locally by recreating a portion of a Fedora repo&rsquo;s structure and dropping two files into it.</p>
<p>Do the following in a directory that can be served up via HTTP:</p>
<pre><code>mkdir -p fedora/releases/19/Fedora/x86_64/os/LiveOS/
cd fedora/releases/19/Fedora/x86_64/os/LiveOS/
wget http://mirror.rackspace.com/fedora/releases/19/Fedora/x86_64/os/LiveOS/squashfs.img
cd ..
wget http://mirror.rackspace.com/fedora/releases/19/Fedora/x86_64/os/.treeinfo
</code></pre><p>Your files are now ready. Go back to your tftp server and adjust your <code>pxelinux.0/default</code> file:</p>
<pre><code>label linux
  menu label Install Fedora 19 guest
  kernel vmlinuz
  append initrd=initrd.img inst.stage2=http://localwebserver.example.com/fedora/releases/19/Fedora/x86_64/os/ inst.repo=http://mirror.rackspace.com/fedora/releases/19/Fedora/x86_64/os/ ks=http://example.com/kickstart.ks ip=eth0:dhcp
</code></pre><p>This should speed up your installations by a large amount (unless your internet connection is much faster than mine).</p>
]]></content></item><item><title>Guide to securing apache</title><link>https://major.io/2013/10/22/guide-to-securing-apache/</link><pubDate>Tue, 22 Oct 2013 12:30:51 +0000</pubDate><guid>https://major.io/2013/10/22/guide-to-securing-apache/</guid><description>I stumbled upon a helpful guide to securing an apache server via Reddit&amp;rsquo;s /r/netsec subreddit. Without further ado, here&amp;rsquo;s a link to the guide:
Apache web server hardening &amp;amp; security guide The guide covers the simplest changes, like reducing ServerTokens output and eliminating indexes, all the way up through configuring mod_security and using the SpiderLabs GitHub repository to add additional rules.
If you&amp;rsquo;d like a more in-depth post about installing mod_security, I&amp;rsquo;d recommend this one from Tecmint.</description><content type="html"><![CDATA[<p>I stumbled upon a helpful guide to securing an apache server via Reddit&rsquo;s <a href="http://reddit.com/r/netsec">/r/netsec subreddit</a>. Without further ado, here&rsquo;s a link to the guide:</p>
<ul>
<li><a href="http://www.chandank.com/webservers/apache/apache-web-server-hardening-security">Apache web server hardening &amp; security guide</a></li>
</ul>
<p>The guide covers the simplest changes, like reducing ServerTokens output and eliminating indexes, all the way up through configuring mod_security and using the <a href="https://github.com/SpiderLabs/owasp-modsecurity-crs/">SpiderLabs GitHub repository</a> to add additional rules.</p>
<p>If you&rsquo;d like a more in-depth post about installing mod_security, I&rsquo;d recommend <a href="http://www.tecmint.com/protect-apache-using-mod_security-and-mod_evasive-on-rhel-centos-fedora/">this one from Tecmint</a>.</p>
<p>Oh, and as always, don&rsquo;t forget about SELinux. :)</p>
<p><strong>UPDATE:</strong> Thanks to <a href="http://twitter.com/matrixtek">@matrixtek</a> for mentioning <a href="https://wiki.mozilla.org/Security/Server_Side_TLS">Mozilla&rsquo;s recommendations specific to TLS</a>.</p>
]]></content></item><item><title>I’m on the hunt for experienced security analysts!</title><link>https://major.io/2013/10/14/im-on-the-hunt-for-experienced-security-analysts/</link><pubDate>Tue, 15 Oct 2013 02:17:13 +0000</pubDate><guid>https://major.io/2013/10/14/im-on-the-hunt-for-experienced-security-analysts/</guid><description>Want to work for a company that finds new approaches to traditional IT problems? Do you want to work with a team that provides Fanatical Support through keeping customer data safe?
Our Global Security Services team is looking for experienced security analysts who can take network analysis and malware defense to the next level.
The responsibilities of the role include:
Monitor global NIDS, Firewall, and log correlation tools for potential threats Initiate escalation procedure to counteract potential threats/vulnerabilities Provide Incident remediation and prevention documentation Document and conform to processes related to security monitoring Provide performance metrics as necessary Provide customer service that exceeds our customers expectations Experience with tools such as Wireshark, Hex Rays, IDA Pro or Hex workshop.</description><content type="html"><![CDATA[<p><a href="/wp-content/uploads/2013/10/rackspace_logo.png"><!-- raw HTML omitted --></a>Want to work for a company that finds new approaches to traditional IT problems? Do you want to work with a team that provides Fanatical Support through keeping customer data safe?</p>
<p>Our Global Security Services team is looking for experienced security analysts who can take network analysis and malware defense to the next level.</p>
<p>The responsibilities of the role include:</p>
<ul>
<li>Monitor global NIDS, Firewall, and log correlation tools for potential threats</li>
<li>Initiate escalation procedure to counteract potential threats/vulnerabilities</li>
<li>Provide Incident remediation and prevention documentation</li>
<li>Document and conform to processes related to security monitoring</li>
<li>Provide performance metrics as necessary</li>
<li>Provide customer service that exceeds our customers expectations</li>
<li>Experience with tools such as Wireshark, Hex Rays, IDA Pro or Hex workshop.</li>
<li>Experienced in computer security incident response activities</li>
<li>Advanced capability to analyze malware, including: worms, viruses, trojans, rootkits and bots</li>
<li>Research and identify key indicators of malicious activities on the network and end user workstations</li>
</ul>
<p>Do you have experience you can bring to a high-performance team? <a href="http://rfer.us/RAXTaD5bf">This might be the career for you.</a></p>
<p>Feel free to reach out if you want to know more. Any security analyst worth their salt should be able to find out how to reach me. ;)</p>
]]></content></item><item><title>Thanks to ISACA’s San Antonio chapter!</title><link>https://major.io/2013/09/24/thanks-to-isacas-san-antonio-chapter/</link><pubDate>Wed, 25 Sep 2013 03:29:30 +0000</pubDate><guid>https://major.io/2013/09/24/thanks-to-isacas-san-antonio-chapter/</guid><description>I had the pleasure of speaking to ISACA&amp;rsquo;s San Antonio chapter today about cloud security. The luncheon sold out and they had some good questions for me.
Much of the questions were centered around the increased risks associated with cloud and how to store data securely in cloud environments. After going through some analogies to explain what makes cloud hosting different, I explained how the risks in cloud are very similar to the risks you face in hosting anything outside your company&amp;rsquo;s four walls.</description><content type="html"><![CDATA[<p><a href="http://major.io/wp-content/uploads/2013/09/isaca_logo.jpg"><!-- raw HTML omitted --></a>I had the pleasure of speaking to <a href="https://www.isaca.org/Pages/default.aspx">ISACA&rsquo;s</a> <a href="http://www.isaca.org/chapters6/San-Antonio/Pages/default.aspx">San Antonio chapter</a> today about cloud security. The luncheon sold out and they had some good questions for me.</p>
<p>Much of the questions were centered around the increased risks associated with cloud and how to store data securely in cloud environments. After going through some analogies to explain what makes cloud hosting different, I explained how the risks in cloud are very similar to the risks you face in hosting anything outside your company&rsquo;s four walls. You still have to deal with a provider who potentially can access your data and the pace of cloud is much faster. I covered the <a href="http://www.slideshare.net/gmccance/cern-data-centre-evolution">&ldquo;cattle vs. pets&rdquo; idea</a> and talked about treating cloud resources as large groups of servers with managed configurations as opposed to physical servers that are handled individually.</p>
<p>As for storing data securely, I urged the audience to analyze how a provider handles their data. For example, if you use a cloud storage solution and you&rsquo;re never asked about encryption keys, a good question to ask is &ldquo;who holds the key to encrypt and decrypt my data?&rdquo; When it comes down to it, your data isn&rsquo;t safe anywhere. Even if it&rsquo;s on paper behind four locked doors, there are people who would have access to that paper at some point in time. You must understand your threats and stack up defenses to thwart the most dangerous (and probable) threats that you&rsquo;ll face.</p>
<p>If you&rsquo;d like a copy of the presentation, <a href="https://app.box.com/s/slm4wr95bq1kil2ndhgh">feel free to download it</a>.</p>
]]></content></item><item><title>One month using a Linux laptop at work: Back to the Mac</title><link>https://major.io/2013/09/22/one-month-using-a-linux-laptop-at-work-back-to-the-mac/</link><pubDate>Mon, 23 Sep 2013 02:38:59 +0000</pubDate><guid>https://major.io/2013/09/22/one-month-using-a-linux-laptop-at-work-back-to-the-mac/</guid><description>This post has been a bit delayed, but I want to follow up on the post I wrote last month about moving from OS X to Linux at work. I started out with a Lenovo Thinkpad X1 Carbon along with Fedora 19 and KDE. Although most things went really well, there were a few deal-breakers that sent me back to the Mac.
Just to give you an idea of my daily workflow, much of my day revolved around my calendar and email.</description><content type="html"><![CDATA[<p>This post has been a bit delayed, but I want to follow up on the <a href="/2013/08/26/moving-from-os-x-to-linux-day-one/">post I wrote last month</a> about moving from OS X to Linux at work. I started out with a Lenovo Thinkpad X1 Carbon along with Fedora 19 and KDE. Although most things went really well, there were a few deal-breakers that sent me back to the Mac.</p>
<p>Just to give you an idea of my daily workflow, much of my day revolved around my calendar and email. As much as I don&rsquo;t like to have my life revolve around a calendar, that&rsquo;s the way it can be at times. This means I need quick access to handle and generate invitations but I also need speedy access to entire email threads and email searches. On top of all that, I review and edit many documents. The majority of the documents I handle are fairly simple but there are some very complex ones as well. Outside of those tasks, I log into remote servers via ssh/RDP, manage social connections (IM, twitter, and IRC), and surf the web.</p>
<p>Without further ado, here are the top three things that (regrettably) pushed me back to OS X at work:</p>
<p><strong>Email management</strong></p>
<p>Connecting to Exchange at work gives me quite a few options:</p>
<ul>
<li>Thunderbird + davmail</li>
<li>Thunderbird + IMAP/POP</li>
<li>Thunderbird + Exquilla</li>
<li>Evolution + EWS</li>
<li>Evolution + IMAP/POP</li>
<li>Claws Mail + IMAP/POP</li>
</ul>
<p>The best method I found was Thunderbird plus <a href="https://exquilla.zendesk.com/home">Exquilla</a>. The performance was quite good and the GAL search worked decently. Thunderbird&rsquo;s keyboard shortcuts were intuitive and easy to begin using regularly with a few days' use. Even with the global indexer enabled, Thunderbird&rsquo;s overall performance was just fine on the X1.</p>
<p>My main gripes showed up when following large email threads on mailing lists or trying to find replies to a message I&rsquo;d send previously. The <a href="https://addons.mozilla.org/en-us/thunderbird/addon/gmail-conversation-view/">Thunderbird Conversations</a> extension helped to an extent, but it really mangled up the UI. Searching the global index was unpredictable. I knew an email was sitting in my inbox but the search function didn&rsquo;t return the message. Even in situations where I knew I&rsquo;d received hundreds of emails from the same sender, the global indexer sometimes couldn&rsquo;t find any of them.</p>
<p>The Claws UI was too minimalistic and Evolution, although feature packed, really chewed up the CPU on the X1 and drained the battery.</p>
<p><strong>Calendar management</strong></p>
<p>After trying Thunderbird with davmail, Thunderbird with <a href="http://www.1st-setup.nl/wordpress/?page_id=133">1st setup&rsquo;s extension</a>, and Evolution with EWS, I was horribly frustrated. My calendar was a mess and some of the applications started marking meetings I&rsquo;d previously accepted as tentative. It confused the meeting organizers and even confused some of the attendees of meetings that I&rsquo;d scheduled.</p>
<p>Inviting other coworkers to meetings led to unpredictable results. Sometimes I could see their free/busy times but most times I couldn&rsquo;t. Getting contacts from the GAL into the invitations sometimes worked and sometimes didn&rsquo;t. In situations where an emergency get-together was required, this became extremely annoying.</p>
<p>My last resort was to keep OWA open in Chrome all day and use it for all of my calendaring. That worked quite well but it meant flipping between Thunderbird and OWA to handle invitations. I&rsquo;d considered using OWA for email as well, but it lacked the functionality I needed for GPG signing among other things.</p>
<p><strong>Microsoft Office compatibility</strong></p>
<p>LibreOffice&rsquo;s work on compatibility was impressive, but it still falls well short of the native Microsoft Office applications. Excel documents with pivot tables were often mangled and Word documents with any complex formatting adjustments were left unreadable. I&rsquo;m not a fan of PowerPoint, but I handle those documents regularly and LibreOffice did an acceptable job.</p>
<p>If you don&rsquo;t have to worry with Office documents at your job, then you might think this is a silly requirement. However, I need quick access to review and edit these documents as I don&rsquo;t like these tasks to occupy my day. I like to get in, get out, and get back to what I&rsquo;m good at doing.</p>
<p><strong>Summary</strong></p>
<p>All in all, I could use Linux as my daily laptop OS if I wasn&rsquo;t so dependent on my calendar, email, and Office documents. It would definitely be a good choice for me if I was still doing heavy development and system administration. Linux has indeed come a long way (I&rsquo;ve said it before) and the stability is impressive. Even during heavy usage periods, I never had a crash in X and hardly ever had a screen flicker. Adding monitors via DVI and USB (DisplayPort) was extremely easy in KDE and I was able to connect to projectors almost as easily as I can in OS X.</p>
<p>I still have the X1 and I&rsquo;m using it for other projects at home. The laptop itself is fantastic and I&rsquo;m eager to see when Lenovo starts adding Haswell chips to the remainder of the Thinkpad line.</p>
]]></content></item><item><title>Keeping bwm-ng 0.6 functional on Fedora 19</title><link>https://major.io/2013/09/19/keeping-bwm-ng-0-6-functional-on-fedora-19/</link><pubDate>Fri, 20 Sep 2013 02:51:31 +0000</pubDate><guid>https://major.io/2013/09/19/keeping-bwm-ng-0-6-functional-on-fedora-19/</guid><description>If you run bwm-ng and you&amp;rsquo;ve run a yum upgrade lately on Fedora 19, you have probably seen this:
---&amp;gt; Package libstatgrab.x86_64 0:0.17-4.fc19 will be updated --&amp;gt; Processing Dependency: libstatgrab.so.6()(64bit) for package: bwm-ng-0.6-10.fc19.x86_64 --&amp;gt; Finished Dependency Resolution Error: Package: bwm-ng-0.6-10.fc19.x86_64 (@fedora) Requires: libstatgrab.so.6()(64bit) Removing: libstatgrab-0.17-4.fc19.x86_64 (@fedora) libstatgrab.so.6()(64bit) Updated By: libstatgrab-0.90-1.fc19.x86_64 (updates) ~libstatgrab.so.9()(64bit) You could try using --skip-broken to work around the problem You could try running: rpm -Va --nofiles --nodigest The error message mentions that libstatgrab needs to be updated to version 0.</description><content type="html"><![CDATA[<p>If you run <a href="http://www.gropp.org/?id=projects&amp;sub=bwm-ng">bwm-ng</a> and you&rsquo;ve run a <code>yum upgrade</code> lately on Fedora 19, you have probably seen this:</p>
<pre><code>---&gt; Package libstatgrab.x86_64 0:0.17-4.fc19 will be updated
--&gt; Processing Dependency: libstatgrab.so.6()(64bit) for package: bwm-ng-0.6-10.fc19.x86_64
--&gt; Finished Dependency Resolution
Error: Package: bwm-ng-0.6-10.fc19.x86_64 (@fedora)
           Requires: libstatgrab.so.6()(64bit)
           Removing: libstatgrab-0.17-4.fc19.x86_64 (@fedora)
               libstatgrab.so.6()(64bit)
           Updated By: libstatgrab-0.90-1.fc19.x86_64 (updates)
              ~libstatgrab.so.9()(64bit)
 You could try using --skip-broken to work around the problem
 You could try running: rpm -Va --nofiles --nodigest
</code></pre><p>The error message mentions that libstatgrab needs to be updated to version 0.90 (released in August) but bwm-ng requires version 0.17 of libstatgrab. I&rsquo;ve emailed the author of bwm-ng to ask if he plans to update it to use the newer libstatgrab version but I haven&rsquo;t heard back yet. Two Fedora bugs are open for the package in Red Hat&rsquo;s Bugzilla.</p>
<p>There are two available workarounds:</p>
<p><strong>Skip the libstatgrab update just this one time</strong></p>
<p>You can skip the libstatgrab update for one run of yum by doing the following:</p>
<pre><code>yum upgrade --skip-broken
</code></pre><p>However, this error will pop up again the next time you run an upgrade with yum. It will also derail your automatic updates with yum-updatesd (if you use it).</p>
<p><strong>Exclude the libstatgrab package from updates</strong></p>
<p>In your <code>/etc/yum.conf</code>, add this line:</p>
<pre><code>exclude=libstatgrab
</code></pre><p>That will prevent libstatgrab from receiving any updates until you remove it from the exclude line. Of course, when Fedora 20 rolls around, this line could cause problems.</p>
]]></content></item><item><title>This is why we can’t have nice things: malware callbacks to icanhazip.com</title><link>https://major.io/2013/09/17/this-is-why-we-cant-have-nice-things-malware-callbacks-to-icanhazip-com/</link><pubDate>Tue, 17 Sep 2013 19:54:54 +0000</pubDate><guid>https://major.io/2013/09/17/this-is-why-we-cant-have-nice-things-malware-callbacks-to-icanhazip-com/</guid><description>I figured it would only be a matter of time until people used icanhazip.com for evil deeds. Someone linked me to these Sophos analysis pages:
Detailed Analysis - Troj/Agent-ADRF
Detailed Analysis - Troj/Mdrop-FIM
I&amp;rsquo;ve worked with a couple of these vendors to get my site removed from their products' blacklists but I&amp;rsquo;m not sure how long that will stay in place. I&amp;rsquo;d really like to hunt down user agents so that I can deny requests from certain malicious scripts and other malware but I don&amp;rsquo;t have enough intel on the malware to get started.</description><content type="html"><![CDATA[<p>I figured it would only be a matter of time until people used <a href="http://icanhazip.com">icanhazip.com</a> for evil deeds. Someone linked me to these Sophos analysis pages:</p>
<p><a href="https://secure2.sophos.com/en-us/threat-center/threat-analyses/viruses-and-spyware/Troj~Agent-ADRF/detailed-analysis.aspx">Detailed Analysis - Troj/Agent-ADRF</a></p>
<p><a href="https://secure2.sophos.com/en-us/threat-center/threat-analyses/viruses-and-spyware/Troj~Mdrop-FIM/detailed-analysis.aspx">Detailed Analysis - Troj/Mdrop-FIM</a></p>
<p>I&rsquo;ve worked with a couple of these vendors to get my site removed from their products' blacklists but I&rsquo;m not sure how long that will stay in place. I&rsquo;d really like to hunt down user agents so that I can deny requests from certain malicious scripts and other malware but I don&rsquo;t have enough intel on the malware to get started. If anyone is able to help, please let me know.</p>
]]></content></item><item><title>Need an edge at work? Learn accounting and finance.</title><link>https://major.io/2013/09/16/need-an-edge-at-work-learn-accounting-and-finance/</link><pubDate>Tue, 17 Sep 2013 04:06:18 +0000</pubDate><guid>https://major.io/2013/09/16/need-an-edge-at-work-learn-accounting-and-finance/</guid><description>I spent two days last week in a class called &amp;ldquo;Accounting and Finance for Non-Financial Managers&amp;rdquo; at UT Austin&amp;rsquo;s Texas Executive Education program. The assigned reading (a book of the same name as the class) was informative but I still felt like it was too advanced for me right off the bat.
My main goal for the class was to learn how my role can have a financial impact as well as an information security impact.</description><content type="html"><![CDATA[<p><img src="/wp-content/uploads/2013/09/finance_and_accounting_book_cover.jpg" alt="1"></p>
<p>I spent two days last week in a class called &ldquo;<a href="https://www.mccombs.utexas.edu/ExecED/Accounting-and-Finance.aspx">Accounting and Finance for Non-Financial Managers</a>&rdquo; at UT Austin&rsquo;s <a href="https://www.mccombs.utexas.edu/execed">Texas Executive Education</a> program. The <a href="http://books.google.com/books/about/Finance_and_Accounting_for_Nonfinancial.html?id=SiS3NAEACAAJ">assigned reading</a> (a book of the same name as the class) was informative but I still felt like it was too advanced for me right off the bat.</p>
<p>My main goal for the class was to learn how my role can have a financial impact as well as an information security impact. It&rsquo;s fairly common for people who work in information security to provide additional evidence that their recommendations are sound. After all, we may be recommending something that impacts productivity, communication, or the financial bottom line.</p>
<p>The class itself was superb. We started with accounting on the first day and we were surprised to see how much we all actually knew about accounting already. <a href="http://www.mccombs.utexas.edu/ExecED/Participant-Resources/Faculty/Hirst.aspx">Dr. Hirst</a> explained how accounting is an art more than a science and that learning the vocabulary would allow us to understand more of what&rsquo;s happening within our own company.</p>
<p>He took the time to pull some balance sheets from 10K&rsquo;s of various companies represented in the room by their employees. We were able to dissect end of year balance sheets, income statements, and cash flow statements from companies like <a href="http://www.solvaychemicals.com/">Solvay Chemicals</a>, <a href="http://www.omnicell.com/">Omnicell</a>, <a href="http://apple.com">Apple</a>, and <a href="http://rackspace.com">Rackspace</a>. Dr. Hirst took us through several accounting failures and this helped to not only make it more real, but it drove home the idea that proper accounting is integral to the success of the firm. I&rsquo;d never realized how <a href="http://en.wikipedia.org/wiki/MCI_Inc.#Accounting_scandals">Worldcom fell apart</a>, but he was able to summarize it in accounting terms in a few sentences.</p>
<p>The second day was centered around finance and <a href="https://www.mccombs.utexas.edu/ExecED/Participant-Resources/Faculty/Nolen.aspx">Dr. Nolen</a> led the class. He gave us a model (the <a href="http://en.wikipedia.org/wiki/Return_on_equity#The_DuPont_formula">DuPont Formula</a>) for understanding a firm&rsquo;s return on equity that made sense. As he broke the model apart, he showed us what all of our C-level executives care about:</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<ul>
<li>CEO: return on equity</li>
<li>COO: asset efficiency (net income over assets)</li>
<li>CFO: leverage (assets over shareholder equity)</li>
</ul>
<p>In short, the CEO is looking to bring more profitability from less investment, the COO is looking to increase sales with fewer assets, and the CFO is looking for borrowing leverage to increase assets without increasing shareholder equity.</p>
<p>We also learned about the right and wrong times to raise capital and how to manage the cost of capital. The most head-scratching part of the course for me was around <a href="http://en.wikipedia.org/wiki/Net_present_value">net present value</a>. Long story short, the whole idea behind NPV is that a dollar gained a year from now is worth less than one gained today (think about inflation and what you could do with that dollar today before next year).</p>
<p>Dr. Nolen reminded us that although creative finance people often get promoted, creative accountants usually find themselves in jail. Also, the banks always get paid back first before shareholders.</p>
<p>So how does this all tie back into information security?</p>
<p>You have the potential to improve your firm&rsquo;s finances through information security improvements. As you reduce risk to the firm, you might find that you need to purchase less insurance, or the potential for fines for losing data might decrease. That reduces your liabilities and increases your return on equity.</p>
<p>On the flip side, if you&rsquo;re able to talk to your customers about the advances in information security that your company has taken, you might end up increasing sales. Offering additional security products or security enhancements to existing products could also increase revenue.</p>
<p>If you get the opportunity to spend some of your training budget next February, try to get in on this class at UT Austin. The pace is fast and the knowledge is extremely useful. Knowing what&rsquo;s going on behind the scenes in your company&rsquo;s finance and accounting departments may give you the edge to push your next project to completion.</p>
]]></content></item><item><title>Moving from OS X to Linux: Day One</title><link>https://major.io/2013/08/26/moving-from-os-x-to-linux-day-one/</link><pubDate>Tue, 27 Aug 2013 03:05:46 +0000</pubDate><guid>https://major.io/2013/08/26/moving-from-os-x-to-linux-day-one/</guid><description>The thought of using Linux as a manager in a highly Windows- and Mac-centric corporate environment isn&amp;rsquo;t something to be taken lightly. Integrating with Active Directory, wrangling email with Microsoft Exchange, and taming quirky Microsoft office documents can be a challenge even with a well-equipped Mac. I decided to make a change after using a Mac at Rackspace for six years.
Let&amp;rsquo;s get one thing straight: I&amp;rsquo;m not a Windows or Mac basher.</description><content type="html"><![CDATA[<p>The thought of using Linux as a manager in a highly Windows- and Mac-centric corporate environment isn&rsquo;t something to be taken lightly. Integrating with Active Directory, wrangling email with Microsoft Exchange, and taming quirky Microsoft office documents can be a challenge even with a well-equipped Mac. I decided to make a change after using a Mac at Rackspace for six years.</p>
<p>Let&rsquo;s get one thing straight: I&rsquo;m not a Windows or Mac basher. Windows 7 has been a solid performer for me and OS X has an amazing UI (and a vibrant community around it). I can&rsquo;t make any sense out of Windows 8, but I&rsquo;ve heard some positive things about it on tablets.</p>
<p>My main goal for switching to Linux is to reduce clutter. I moved away from the iPhone to Android last year because the Android gave me finer-grained controls over my phone and allowed me to troubleshoot my own problems. The Mac was working well for me, but as each release passed, it seems like more things were out of my control and I was constantly notified of something that my computer wanted me to do.</p>
<p><a href="http://major.io/wp-content/uploads/2013/08/Tux.png"><!-- raw HTML omitted --></a>While at this year&rsquo;s Red Hat Summit, I saw someone using Linux on a laptop and I asked: &ldquo;How do you survive on Linux at your office?&rdquo; He confided that his office is extremely Windows-centric and that it was tough to overcome in the beginning. When I asked why he stuck with Linux, he smiled and responded quickly: &ldquo;When I use Linux, I feel like I can do my work without being bothered. Reducing clutter has saved me a ton of time.&rdquo;</p>
<p>In an effort to free up my time at work for the important stuff, I&rsquo;m moving to Linux. I&rsquo;m hoping that the move is permanent, but time will tell. If you&rsquo;re eager to make the same change, here&rsquo;s the workflow I&rsquo;m using:</p>
<p><strong>Hardware</strong></p>
<p><a href="http://shop.lenovo.com/us/en/laptops/thinkpad/x-series/x1-carbon/">Thinkpad X1 Carbon</a>. It has a decent screen, a fantastic keyboard, good battery life, and it&rsquo;s very light. Extra displays are connected with mini-DisplayPort and that allows me to use the Mac DisplayPort dongles that I find laying around all over the place. There&rsquo;s no ethernet adapter, but you can pick up a USB 2.0 Gigabit adapter for $25 or less.</p>
<p>One nice benefit is that almost every piece of hardware is recognized within Linux. The only hangup is the fingerprint reader (due to proprietary firmware). That can be fixed but I&rsquo;m too lazy to go down that road at the moment.</p>
<p>One of my favorite parts of the Thinkpad is the mouse buttons <em>above</em> the trackpad. As a Mac user, I sometimes find myself highlighting the wrong piece of text or rolling backwards and forwards to get the right selection. I&rsquo;m able to hold the left mouse button with my left hand while using the touchpad with my right. It feels awkward at first but it&rsquo;s extremely quick and accurate once you get it right.</p>
<p><strong>Distribution and Desktop Environment.</strong></p>
<p>I chose Fedora 19 with KDE. Some folks prefer Kubuntu (Ubuntu&rsquo;s KDE release) or Linux Mint&rsquo;s KDE release, but I&rsquo;m a bit biased towards Fedora as I enjoy RPM/yum and I&rsquo;m involved in the Fedora community.</p>
<p>KDE makes sense for me because it&rsquo;s feature-rich and the Qt-based applications are well-designed. GNOME 3 has an interface that just doesn&rsquo;t make sense to me, but GNOME 3&rsquo;s new classic mode shows a lot of potential. Cinnamon is a good alternative if you really enjoy GNOME applications. XFCE is good if you&rsquo;re on older hardware or if you prefer something very lightweight.</p>
<p><strong>Microsoft Exchange email</strong></p>
<p>Exchange can even be a challenge on Windows, so don&rsquo;t expect a cakewalk in Linux. My preferred method is to use <a href="http://www.mozilla.org/en-US/thunderbird/">Thunderbird</a> and <a href="http://davmail.sourceforge.net/download.html">Davmail</a>. Davmail is a translation layer that handles the Exchange connectivity (via OWA/EWS) and it serves up POP, IMAP, SMTP, LDAP, and CalDav to applications on your machine. Point Davmail to your OWA server and then configure Thunderbird to talk to Davmail. One downside is that Davmail can become a bit CPU-hungry at times and may drag down a battery on a laptop.</p>
<p>The latest release of <a href="https://projects.gnome.org/evolution/">Evolution</a> for GNOME has an exchange-ews connector that works relatively well with newer versions of Exchange. There are still some bugs and missing features, especially around starring/flagging emails. The performance could be better, but it seems to perform slightly better than using Davmail. Evolution&rsquo;s UI was too clunky for me to use and it seemed to have significant lags when fetching email.</p>
<p>If you&rsquo;re not eager to mess with a fat client, just use Outlook Web Access in your favorite browser. Beware that OWA detects Chrome on Linux and presents you with the awful &ldquo;light&rdquo; interface for OWA. Add a <a href="https://chrome.google.com/webstore/detail/user-agent-switcher-for-c/djflhoibgkdhkhhcedjiklpkjnoahfmg">user agent spoofing extension</a> to Chrome and masquerade as Chrome on Windows or Mac. You&rsquo;ll get the rich OWA interface that makes things much easier.</p>
<p><strong>Microsoft Exchange calendar</strong></p>
<p>Getting calendaring right with Exchange seems to be more difficult than email. My preferred method is to use OWA to manage calendaring. As long as you set your user agent correctly (see previous paragraph), it works flawlessly.</p>
<p>Fat client users should look at Evolution&rsquo;s calendaring capabilities. I found it to still be pretty buggy and complex recurring invitations were often botched in the interface. Coworkers reported not seeing confirmation responses for me on certain invitations while others reported receiving multiple acceptances for the same invitation.</p>
<p>Another option is to use Thunderbird with Davmail via CalDav. This was as buggy as Evolution and it was excruciatingly slow.</p>
<p><strong>Microsoft Office</strong></p>
<p>LibreOffice copes well with the majority of the documents I need to edit. I took some time to bring over some of the most commonly used fonts from my Mac and I picked up the Windows fonts via <a href="http://satya164.github.io/fedorautils/">fedorautils</a>. The Calligra office suite in KDE fulfills a lot of the additional needs (like a Visio and Project replacement).</p>
<p>However, there are those times when you need a little more from your Office applications. I have a Windows 7 VM running in VirtualBox when I need it for some Office heavy lifting. Another option is to use Office365&rsquo;s web interface for the common Office applications. If your organization has SharePoint, some of the licenses allow you to have SkyDrive access within your organization and that includes the web-based Office applications as well.</p>
<p><strong>RSS feeds</strong></p>
<p>Ever since Google Reader&rsquo;s demise, I&rsquo;ve switched to <a href="http://tt-rss.org/redmine/projects/tt-rss/wiki">Tiny Tiny RSS</a> running on a cheap VM. I can access the RSS feeds via any browser or via applications on my Nexus 4.</p>
<p><strong>IM and IRC</strong></p>
<p><a href="http://www.pidgin.im/">Pidgin</a> has been my go-to choice for instant messaging ever since I used GAIM. I&rsquo;ve heard good things about telepathy/empathy but the UI didn&rsquo;t make much sense to me. For IRC, <a href="http://konversation.kde.org/">Konversation</a> is a clear GUI winner with <a href="http://www.irssi.org/">irssi</a> being my favorite in the terminal.</p>
<p><strong>Twitter</strong></p>
<p>As you probably know, I like to use Twitter, so this was critical to my workflow. I use <a href="https://chrome.google.com/webstore/detail/tweetdeck/hbdpomandigafcibbmofojjchbcdagbl?hl=en-US">TweetDeck&rsquo;s Chrome application</a> because it uses the streaming API and gives me plenty of one-click functionality.</p>
<p><strong>Music</strong></p>
<p>iTunes was hard to live without, but <a href="http://www.clementine-player.org/">Clementine</a> filled my needs well. It has built-in internet music services that are easy to use. I&rsquo;m a Digitally Imported subscriber and I was able to log in via Clementine and access the premium streams. The podcast management isn&rsquo;t perfect but it&rsquo;s certainly a decent replacement for iTunes. It can monitor certain directories for new music and automatically populate itself with a playlist based on the music it finds.</p>
<p><strong>Networking</strong></p>
<p>All of my required VPN capabilities worked right out of the box, including OpenVPN and Cisco VPN&rsquo;s via VPNC. I can join 802.1x-protected wireless and wired networks with ease. Every USB to ethernet adapter I&rsquo;ve tried has worked right out of the box without any additional configuration needed. IPv6 connectivity works just fine (as expected).</p>
<p><strong>Summary</strong></p>
<p>With one day on Linux under my belt, I&rsquo;m glad I made the change. I&rsquo;m able to sit down with my work laptop and use it for what I want to do with it: work. Sure, there are still notification popups from time to time, but they&rsquo;re either notifications that I&rsquo;ve configured intentionally or my laptop is trying to tell me something that I really need to know. So far, the switch has caused me to think about my software in a more minimalistic way. I regularly have my browser, IM client, and IRC client open - that&rsquo;s all. I&rsquo;m hoping that less clutter and fewer applications lead to better focus and increased productivity.</p>
]]></content></item><item><title>Get a rock-solid Linux touchpad configuration for the Lenovo X1 Carbon</title><link>https://major.io/2013/08/24/get-a-rock-solid-linux-touchpad-configuration-for-the-lenovo-x1-carbon/</link><pubDate>Sat, 24 Aug 2013 20:28:35 +0000</pubDate><guid>https://major.io/2013/08/24/get-a-rock-solid-linux-touchpad-configuration-for-the-lenovo-x1-carbon/</guid><description>The X1 Carbon&amp;rsquo;s touchpad has been my nemesis in Linux for quite some time because of its high sensitivity. I&amp;rsquo;d often find the cursor jumping over a few pixels each time I tried to tap to click. This was aggravating at first, but then I found myself closing windows when I wanted them minimized or confirming something in a dialog that I didn&amp;rsquo;t want to confirm.
Last December, I wrote a post about some fixes.</description><content type="html"><![CDATA[<p><a href="http://major.io/wp-content/uploads/2013/08/X1_closed.jpg"><!-- raw HTML omitted --></a>The X1 Carbon&rsquo;s touchpad has been my nemesis in Linux for quite some time because of its high sensitivity. I&rsquo;d often find the cursor jumping over a few pixels each time I tried to tap to click. This was aggravating at first, but then I found myself closing windows when I wanted them minimized or confirming something in a dialog that I didn&rsquo;t want to confirm.</p>
<p>Last December, <a href="/2012/12/28/handy-settings-for-the-touchpadclickpad-in-the-lenovo-x1-carbon/">I wrote a post about some fixes</a>. However, as I force myself to migrate to Linux (no turning back this time) again, my fixes didn&rsquo;t work well enough. I <a href="http://code.technically.us/post/50837506478/senistive-touchpads-and-ubuntu">stumbled upon a post</a> about the X1&rsquo;s touchpad and how an Ubuntu user found a configuration file that seemed to work well.</p>
<p>Just as a timesaver, I&rsquo;ve reposted his configuration here:</p>
<pre><code># softlink this file into:
# /usr/share/X11/xorg.conf.d

# and prevent the settings app from overwriting our settings:
# gsettings set org.gnome.settings-daemon.plugins.mouse active false


Section &quot;InputClass&quot;
    Identifier &quot;nathan touchpad catchall&quot;
    MatchIsTouchpad &quot;on&quot;
    MatchDevicePath &quot;/dev/input/event*&quot;
    Driver &quot;synaptics&quot;

    # three fingers for the middle button
    Option &quot;TapButton3&quot; &quot;2&quot;
    # drag lock
    Option &quot;LockedDrags&quot; &quot;1&quot;
    # accurate tap-to-click!
    Option &quot;FingerLow&quot; &quot;50&quot;
    Option &quot;FingerHigh&quot; &quot;55&quot;

    # prevents too many intentional clicks
    Option &quot;PalmDetect&quot; &quot;0&quot;

    # &quot;natural&quot; vertical and horizontal scrolling
    Option &quot;VertTwoFingerScroll&quot; &quot;1&quot;
    Option &quot;VertScrollDelta&quot; &quot;-75&quot;
    Option &quot;HorizTwoFingerScroll&quot; &quot;1&quot;
    Option &quot;HorizScrollDelta&quot; &quot;-75&quot;

    Option &quot;MinSpeed&quot; &quot;1&quot;
    Option &quot;MaxSpeed&quot; &quot;1&quot;

    Option &quot;AccelerationProfile&quot; &quot;2&quot;
    Option &quot;ConstantDeceleration&quot; &quot;4&quot;
EndSection
</code></pre><p>Many many thanks to <a href="https://twitter.com/n8han">Nathan Hamblen</a> for assembling this configuration and offering it out to the masses on his blog.</p>
]]></content></item><item><title>drbd 8.4.3 package on the way for Fedora 19</title><link>https://major.io/2013/08/02/drbd-8-4-3-package-on-the-way-for-fedora-19/</link><pubDate>Fri, 02 Aug 2013 16:57:00 +0000</pubDate><guid>https://major.io/2013/08/02/drbd-8-4-3-package-on-the-way-for-fedora-19/</guid><description>There&amp;rsquo;s a drbd-8.4.3-1 package waiting in the testing repository for Fedora 19. The DRBD kernel module for kernel 3.10 is up to 8.4.3 but the client tools within Fedora 19 (currently at 8.4.2) should work fine. The API versions are the same for both the kernel modules and user tools.
If you&amp;rsquo;re eager to see a changelog between 8.4.2 and 8.4.3, check DRBD&amp;rsquo;s git repository. The majority of the changes are within the kernel module itself.</description><content type="html"><![CDATA[<p>There&rsquo;s a <a href="https://admin.fedoraproject.org/updates/drbd-8.4.3-1.fc19">drbd-8.4.3-1 package</a> waiting in the testing repository for Fedora 19. The DRBD kernel module for kernel 3.10 is up to 8.4.3 but the client tools within Fedora 19 (currently at 8.4.2) should work fine. The API versions are the same for both the kernel modules and user tools.</p>
<p>If you&rsquo;re eager to see a changelog between 8.4.2 and 8.4.3, check <a href="http://git.drbd.org/gitweb.cgi?p=drbd-8.4.git;a=blob_plain;f=ChangeLog;hb=HEAD">DRBD&rsquo;s git repository</a>. The majority of the changes are within the kernel module itself.</p>
<p>Please help me test the package if you can spare some time. Thanks!</p>
]]></content></item><item><title>PXE boot Fedora 19 using a Mikrotik firewall</title><link>https://major.io/2013/07/23/pxe-boot-fedora-19-using-a-mikrotik-firewall/</link><pubDate>Tue, 23 Jul 2013 21:47:33 +0000</pubDate><guid>https://major.io/2013/07/23/pxe-boot-fedora-19-using-a-mikrotik-firewall/</guid><description>Outside of the RHCA exams, I haven&amp;rsquo;t configured a PXE system for my personal needs. A colleague demoed his PXE setup for me and I was hooked. Once I realized how much time I could save when I&amp;rsquo;m building and tearing down virtual machines, it made complete sense. This post will show you how to configure PXE and tftpd in Mikrotik&amp;rsquo;s RouterOS to boot and install Fedora 19 (as well as provide rescue environments).</description><content type="html"><![CDATA[<p>Outside of the RHCA exams, I haven&rsquo;t configured a <a href="http://en.wikipedia.org/wiki/Preboot_Execution_Environment">PXE</a> system for my personal needs. A colleague demoed his PXE setup for me and I was hooked. Once I realized how much time I could save when I&rsquo;m building and tearing down virtual machines, it made complete sense. This post will show you how to configure PXE and tftpd in <a href="http://major.io/?s=mikrotik">Mikrotik&rsquo;s RouterOS</a> to boot and install Fedora 19 (as well as provide rescue environments).</p>
<p>The first thing you&rsquo;ll need are a few files from a working Fedora installation. Install the <code>syslinux-tftpboot</code> package and grab the following files:</p>
<pre><code>/tftpboot/pxelinux.0
/tftpboot/vesamenu.c32
</code></pre><p>You&rsquo;ll also need a <a href="http://mirrors.kernel.org/fedora/releases/19/Fedora/x86_64/os/images/pxeboot/vmlinuz">vmlinuz</a> and <a href="http://mirrors.kernel.org/fedora/releases/19/Fedora/x86_64/os/images/pxeboot/initrd.img">initrd.img</a> file from your favorite Fedora mirror (use the linked text here for F19 x86_64 or look in the <code>os/images/pxeboot</code> directory on the mirror for your architecture).</p>
<p>When you have your four files, create a directory on the Mikrotik via FTP called <strong>tftp</strong>, and upload those to your Mikrotik. Your directory should look something like this:</p>
<pre><code> ls tftp/
-rw-rw----   1 root     root       155792 Jul 23 00:01 vesamenu.c32
-rw-rw----   1 root     root      5055896 Jul 22 23:41 vmlinuz
-rw-rw----   1 root     root     32829968 Jul 22 23:42 initrd.img
-rw-rw----   1 root     root        26460 Jul 22 23:37 pxelinux.0
</code></pre><p>Within the <strong>tftp</strong> directory, make a directory called <strong>pxelinux.cfg</strong>. Add a file called <strong>default</strong> inside the pxelinux.cfg directory with these contents:</p>
<pre><code>default vesamenu.c32
prompt 0
timeout 600

display boot.msg

label linux
  menu label ^Install or upgrade an existing system
  kernel vmlinuz
  append initrd=initrd.img repo=http://mirrors.kernel.org/fedora/releases/19/Fedora/x86_64/os/ ks=http://example.com/kickstart.ks ip=eth0:dhcp
label vesa
  menu label Install system with ^basic video driver
  kernel vmlinuz
  append initrd=initrd.img xdriver=vesa nomodeset
label rescue
  menu label ^Rescue installed system
  menu default
  kernel vmlinuz
  append initrd=initrd.img repo=http://mirrors.kernel.org/fedora/releases/19/Fedora/x86_64/os/ rescue ip=eth0:dhcp
label local
  menu label Boot from ^local drive
  localboot 0xffff
</code></pre><p>Be sure to adjust the <code>ip=</code> <code>and</code> <code>repo=</code> arguments to fit your server. Keep in mind that from Fedora 17 on, you&rsquo;ll need to use the <a href="https://fedoraproject.org/wiki/Dracut/Options#Network">dracut syntax</a> for anaconda boot options. Once that&rsquo;s done, you&rsquo;re ready to configure the Mikrotik firewall, so get logged into the firewall over ssh.</p>
<p>We need to set some network options for our Mikrotik&rsquo;s DHCP server:</p>
<pre><code>/ip dhcp-server network
set 0 boot-file-name=pxelinux.0 next-server=192.168.25.1
</code></pre><p>The value for <code>next-server=</code> should be the gateway address for your internal network (the Mikrotik&rsquo;s internal IP).</p>
<p>Next, we need to configure the tftp server so that it serves up files to our internal network:</p>
<pre><code>/ip tftp
add ip-addresses=192.168.25.0/24 real-filename=tftp/pxelinux.0 req-filename=pxelinux.0
add ip-addresses=192.168.25.0/24 real-filename=tftp/pxelinux.cfg/default req-filename=pxelinux.cfg/default
add ip-addresses=192.168.25.0/24 real-filename=tftp/vmlinuz req-filename=vmlinuz
add ip-addresses=192.168.25.0/24 real-filename=tftp/vesamenu.c32 req-filename=vesamenu.c32
add ip-addresses=192.168.25.0/24 real-filename=tftp/initrd.img req-filename=initrd.img
</code></pre><p>Now it&rsquo;s time to test it! If you&rsquo;re using a physical machine, double check your BIOS to verify that PXE boot is enabled for your ethernet interface. Most modern chipsets have support for it, but be sure to check that it&rsquo;s enabled. You may have to reboot after enabling it in the BIOS for the ethernet BIOS to be included.</p>
<p>If you&rsquo;re using a virtual machine, just start up virt-manager and choose <em>Network Boot (PXE)</em> from the installation options:</p>
<p><img src="/wp-content/uploads/2013/07/virt-manager-pxe.png" alt="6"></p>
<p>Once the VM boots, you&rsquo;ll be sent straight to the PXE boot screen:</p>
<p><img src="/wp-content/uploads/2013/07/pxetest-Virtual-Machine.png" alt="7"></p>
<p><strong>TAKE NOTE!</strong> In the pxelinux.cfg/default file, I set rescue mode to boot as the default option. This will prevent a situation where you forget to remove PXE from a system&rsquo;s boot order and accidentally re-kickstart over the live system.</p>
<p>The installer should now boot up normally and you can install your Fedora system via kickstart or via the anaconda interface.</p>
]]></content></item><item><title>Come and get your SELinux shirts!</title><link>https://major.io/2013/07/18/come-and-get-your-selinux-shirts/</link><pubDate>Thu, 18 Jul 2013 12:14:45 +0000</pubDate><guid>https://major.io/2013/07/18/come-and-get-your-selinux-shirts/</guid><description>After my podcast interview at the 2013 Red Hat Summit, Dan Walsh posted a photo of himself in the SELinux shirt that I gave him at the Summit:
Needless to say, I was flooded with requests for shirts after that. Someone suggested using the Overpass font and I have new shirts ready for purchase on Spreadshirt.
I set all the prices as low as the vendor will allow and I&amp;rsquo;m not making any profit for each purchase.</description><content type="html"><![CDATA[<p>After my <a href="/2013/06/19/my-interview-on-the-dave-and-gunnar-show/">podcast interview</a> at the 2013 Red Hat Summit, Dan Walsh posted a photo of himself in the SELinux shirt that I gave him at the Summit:</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>Needless to say, I was flooded with requests for shirts after that. Someone suggested using the <a href="http://fedoraproject.org/wiki/Overpass_Fonts">Overpass font</a> and I have <a href="http://mhayden.spreadshirt.com/">new shirts ready for purchase</a> on Spreadshirt.</p>
<p><a href="http://major.io/wp-content/uploads/2013/07/men-s-selinux-shirt-gildan-181.png"><!-- raw HTML omitted --></a></p>
<p>I set all the prices as low as the vendor will allow and I&rsquo;m not making any profit for each purchase. I&rsquo;ve seen over 20 orders come through already. There are two options for men and women so you can opt for a cheaper shirt or a higher quality one. You can even customize the color of the shirt!</p>
<p><a href="http://mhayden.spreadshirt.com/">Go buy a few!</a></p>
]]></content></item><item><title>A humble farewell to Seth Vidal</title><link>https://major.io/2013/07/09/a-humble-farewell-to-seth-vidal/</link><pubDate>Wed, 10 Jul 2013 02:58:36 +0000</pubDate><guid>https://major.io/2013/07/09/a-humble-farewell-to-seth-vidal/</guid><description>I was shocked to see Robyn Bergeron&amp;rsquo;s email today about Seth Vidal&amp;rsquo;s passing. He was the victim of a hit and run accident while he was cycling last night. The suspect has turned himself in as of tonight.
I first met Seth at FUDCon Tempe back in 2011. We had talked off and on via email and IRC about cloud-related topics. He was interested in how we assembled our cloud offering at Rackspace and I was eager to talk to him about building cloud images and handling mirrors.</description><content type="html"><![CDATA[<p><a href="http://major.io/wp-content/uploads/2013/07/11-1.jpg"><!-- raw HTML omitted --></a>I was shocked to see <a href="https://lists.fedoraproject.org/pipermail/announce/2013-July/003174.html">Robyn Bergeron&rsquo;s email</a> today about Seth Vidal&rsquo;s passing. He was the <a href="http://durham.io/2013/07/09/seth-vidal-creator-of-yum-open-source-software-killed-in-bike-accident-off-hillandale-rd/">victim of a hit and run accident</a> while he was cycling last night. The suspect has <a href="http://www.heraldsun.com/news/localnews/x177810618/Driver-arrested-in-cyclist-hit-and-run">turned himself in</a> as of tonight.</p>
<p>I first met Seth at <a href="https://fedoraproject.org/wiki/Archive:FUDCon:Tempe_2011?rd=FUDCon:Tempe_2011">FUDCon Tempe</a> back in 2011. We had talked off and on via email and IRC about cloud-related topics. He was interested in how we assembled our cloud offering at Rackspace and I was eager to talk to him about building cloud images and handling mirrors. I gave him a compliment about yum and how handy it was. He thanked me, shrugged it off humbly, and then wanted to talk more about my work at Rackspace on our Cloud Servers offering.</p>
<p>He had some criticisms for our product and he delivered them in such a way that they were open ended. It wasn&rsquo;t like &ldquo;Rackspace&rsquo;s cloud is terrible, I can&rsquo;t use it&rdquo;, but instead, his point was &ldquo;I like your stuff - just not in its current state - so how can you set things up so my stuff will work?&rdquo; Although I tried to do the same when talking about Fedora and Xen (which wasn&rsquo;t a match made in heaven at the time), I don&rsquo;t think I was nearly as effective as Seth was.</p>
<p>We talked every so often after that, mostly on IRC, about changing in the cloud environment. He&rsquo;d elbow me about using Xen over KVM and I&rsquo;d elbow him about using Eucalyptus over OpenStack. We&rsquo;d have a good volley and eventually we&rsquo;d make fun of each other&rsquo;s stance in the conversation. I learned a lot from Seth on how to handle disagreements in the open source world and he was always a good sounding board when I had a good idea. Well, sometimes I thought it was a good idea but he quickly reminded me that I could do better. ;)</p>
<p>Seth: I bid you farewell and safe passage to wherever you go from here. I&rsquo;m not a religious guy, and we never talked religion together, but if there&rsquo;s a good place a guy like you could go, I&rsquo;m sure you&rsquo;re on the way there now. If I&rsquo;m able to be half the technologist you were on your worst day, I&rsquo;d say I&rsquo;ve accomplished something pretty amazing. I&rsquo;ll cut this short here, Seth, because I need to use yum to get some servers updated. Thanks again.</p>
]]></content></item><item><title>Boot VM’s with virt-manager and libvirt with ISO’s stored remotely via samba/cifs</title><link>https://major.io/2013/07/06/boot-vms-with-virt-manager-and-libvirt-with-isos-stored-remotely-via-sambacifs/</link><pubDate>Sun, 07 Jul 2013 01:51:10 +0000</pubDate><guid>https://major.io/2013/07/06/boot-vms-with-virt-manager-and-libvirt-with-isos-stored-remotely-via-sambacifs/</guid><description>Pairing virt-manager with KVM makes booting new VM&amp;rsquo;s pretty darned easy. I have a QNAP NAS at home with a bunch of ISO&amp;rsquo;s stored in share available to guests and I wanted to use that with libvirt to boot new VM&amp;rsquo;s. (By the way, if you&amp;rsquo;re looking for an off-the-shelf NAS that is built with solid hardware and pretty reliable software, try one of the QNAP devices. You still get access to many of the usual commands that you would normally find on a Linux box for emergencies.</description><content type="html"><![CDATA[<p><img src="http://major.io/wp-content/uploads/2013/07/qnap.jpg" alt="1"></p>
<p>Pairing <a href="http://virt-manager.org/">virt-manager</a> with KVM makes booting new VM&rsquo;s pretty darned easy. I have a <a href="http://www.qnap.com/en/?lang=en&amp;sn=822&amp;c=1655&amp;sc=1656&amp;t=1660&amp;n=6703">QNAP NAS</a> at home with a bunch of ISO&rsquo;s stored in share available to guests and I wanted to use that with libvirt to boot new VM&rsquo;s. (By the way, if you&rsquo;re looking for an off-the-shelf NAS that is built with solid hardware and pretty reliable software, try one of the QNAP devices. You still get access to many of the usual commands that you would normally find on a Linux box for emergencies. More on that in a later post.)</p>
<p>The first step was creating a mountpoint and configuring the mount in /etc/fstab:</p>
<pre><code># mkdir /mnt/iso
# grep qemu /etc/passwd
qemu❌107:107:qemu user:/:/sbin/nologin
# echo &quot;//qnap/ISO /mnt/iso cifs    _netdev,guest,uid=107,gid=107,defaults 0 0&quot; &gt;&gt; /etc/fstab
# mount /mnt/iso
</code></pre><p>My QNAP is already in /etc/hosts so I didn&rsquo;t need to specify the IP in the file. Adding <code>_netdev</code> ensures that the network will be up before the mount is made. The <code>guest</code> option ensures that I won&rsquo;t be prompted for credentials and the <code>uid=107,gid=107</code> mounts the share as the qemu user. If you forget this, virt-manager will throw some ugly permissions errors from libvirt.</p>
<p>From there, I had another permissions error and I suspected that SELinux was preventing libvirt from accessing the files in the share. A quick check of /var/log/messages revealed that I was right:</p>
<pre><code>Jul  6 16:12:51 nuc1 setroubleshoot: SELinux is preventing /usr/bin/qemu-system-x86_64 from open access on the file /mnt/iso/livecd.iso. For complete SELinux messages. run sealert -l c1c80b2c-b5df-4114-86c7-ffee98274552
</code></pre><p>Here&rsquo;s the output from sealert:</p>
<pre><code># sealert -l c1c80b2c-b5df-4114-86c7-ffee98274552
SELinux is preventing /usr/bin/qemu-system-x86_64 from open access on the file /mnt/iso/livecd.iso.

*****  Plugin catchall_boolean (89.3 confidence) suggests  *******************

If you want to allow virt to use samba
Then you must tell SELinux about this by enabling the 'virt_use_samba' boolean.
You can read 'None' man page for more details.
Do
setsebool -P virt_use_samba 1
</code></pre><p>The fix is a quick one:</p>
<pre><code># setsebool -P virt_use_samba 1
</code></pre><p>You should be all set after that. Press “Browse Local” in virt-manager when you look for your ISO to boot the virtual machine and navigate over to /mnt/iso for your list of ISO&rsquo;s.</p>
]]></content></item><item><title>Confine untrusted users (including your children) with SELinux</title><link>https://major.io/2013/07/05/confine-untrusted-users-including-your-children-with-selinux/</link><pubDate>Fri, 05 Jul 2013 18:50:43 +0000</pubDate><guid>https://major.io/2013/07/05/confine-untrusted-users-including-your-children-with-selinux/</guid><description>The confined user support in SELinux is handy for ensuring that users aren&amp;rsquo;t able to do something that they shouldn&amp;rsquo;t. It seems more effective and easier to use than most of the other methods I&amp;rsquo;ve seen before. Thanks to Dan for reminding me about this during his SELinux in the Enterprise talk from this year&amp;rsquo;s Red Hat Summit.
There are five main SELinux user types (and a handy chart in the Fedora documentation):</description><content type="html"><![CDATA[<p><a href="http://major.io/wp-content/uploads/2011/09/selinux-penguin-125.png"><!-- raw HTML omitted --></a>The <a href="http://danwalsh.livejournal.com/10461.html?thread=88029">confined user support in SELinux</a> is handy for ensuring that users aren&rsquo;t able to do something that they shouldn&rsquo;t. It seems more effective and easier to use than most of the other methods I&rsquo;ve seen before. Thanks to Dan for reminding me about this during his <a href="http://rhsummit.files.wordpress.com/2013/06/summitselinuxenterprise.pdf">SELinux in the Enterprise</a> talk from this year&rsquo;s Red Hat Summit.</p>
<p>There are five main SELinux user types (and a <a href="https://docs.fedoraproject.org/en-US/Fedora/12/html/Security-Enhanced_Linux/sect-Security-Enhanced_Linux-Targeted_Policy-Confined_and_Unconfined_Users.html">handy chart</a> in the Fedora documentation):</p>
<ul>
<li><strong>guest_u:</strong> - no X windows, no sudo, and no networking</li>
<li><strong>xguest_u:</strong> - same as guest_u, but X is allowed and connectivity is allowed to web ports only (handy for kiosks)</li>
<li><strong>user_u:</strong> - same as xguest_u, but networking isn&rsquo;t restricted</li>
<li><strong>staff_u:</strong> - same as user_u, but sudo is allowed (su isn&rsquo;t allowed)</li>
<li><strong>unconfined_u:</strong> - full access (this is the default)</li>
</ul>
<p>One interesting thing to note is that all users are allowed to execute binary applications within their home directories by default. This can be switch off via some booleans (which I&rsquo;ll demonstrate in a moment).</p>
<p>Let&rsquo;s kick off a demonstration to show the power of these restrictions. First off, let&rsquo;s get a list of the default configuration:</p>
<pre><code># semanage login -l

Login Name           SELinux User         MLS/MCS Range        Service

__default__          unconfined_u         s0-s0:c0.c1023       *
root                 unconfined_u         s0-s0:c0.c1023       *
system_u             system_u             s0-s0:c0.c1023       *
</code></pre><p>By default, all new users come with no restrictions (as shown by unconfined_u). I&rsquo;ll create a new user called selinuxtest and set a password. If I ssh to the server as the selinuxtest user, I see that I&rsquo;m unconfined:</p>
<pre><code>$ id -Z
unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c1023
</code></pre><p>That&rsquo;s what we expected. Let&rsquo;s apply the strongest restrictions to this user and apply guest_u:</p>
<pre><code># semanage login -a -s guest_u selinuxtest
</code></pre><p>I&rsquo;ll start a new ssh session as selinuxtest and try out some commands that I&rsquo;d normally expect to work on a Linux server:</p>
<pre><code>$ ping google.com
ping: icmp open socket: Permission denied
$ curl google.com
curl: (7) Failed to connect to 74.125.225.129: Permission denied
$ sudo su -
sudo: unable to change to sudoers gid: Operation not permitted
$  ./hello
Hello world
$ file hello
hello: ELF 64-bit LSB executable, x86-64, version 1 (SYSV), dynamically linked (uses shared libs), for GNU/Linux 2.6.32, BuildID[sha1]=0x5ffb25a7171c3338d6c76147cccc666ddc752dde, not stripped
</code></pre><p>The networking and sudo restrictions applied as we expected. However, I was able to compile a small &ldquo;Hello World&rdquo; binary in C and run it. That could become a problem for some servers. Let&rsquo;s adjust a boolean that will restrict this activity:</p>
<pre><code># getsebool -a | grep exec_content
auditadm_exec_content --&gt; on
guest_exec_content --&gt; on
secadm_exec_content --&gt; on
staff_exec_content --&gt; on
sysadm_exec_content --&gt; on
user_exec_content --&gt; on
xguest_exec_content --&gt; on
# setsebool guest_exec_content off
</code></pre><p>Now I try running the binary again as my selinuxtest user:</p>
<pre><code>$ ./hello
-bash: ./hello: Permission denied
</code></pre><p>I can&rsquo;t execute binary content in my home directory or in /tmp any longer after adjusting the boolean. Let&rsquo;s switch selinuxtest to xguest_u:</p>
<pre><code># semanage login -a -s xguest_u selinuxtest
</code></pre><p>And now I&rsquo;ll re-test as the selinuxtest user:</p>
<pre><code>$ curl -si google.com | head -1
HTTP/1.1 301 Moved Permanently
$ ping google.com
ping: icmp open socket: Permission denied
</code></pre><p>I have full web connectivity but I can&rsquo;t do anything else on the network. Now for a switch to user_u:</p>
<pre><code># semanage login -a -s user_u selinuxtest
</code></pre><p>And testing user_u with selinuxtest reveals:</p>
<pre><code>$ ping -c 1 google.com
PING google.com (74.125.225.134) 56(84) bytes of data.
64 bytes from ord08s09-in-f6.1e100.net (74.125.225.134): icmp_seq=1 ttl=57 time=29.3 ms

--- google.com ping statistics ---
1 packets transmitted, 1 received, 0% packet loss, time 0ms
rtt min/avg/max/mdev = 29.332/29.332/29.332/0.000 ms
$ curl -si google.com | head -n1
HTTP/1.1 301 Moved Permanently
$ sudo su -
sudo: PERM_SUDOERS: setresuid(-1, 1, -1): Operation not permitted
</code></pre><p>Networking is wide open but I still don&rsquo;t have sudo. Let&rsquo;s try staff_u:</p>
<pre><code># semanage login -a -s staff_u selinuxtest
</code></pre><p>Testing staff_u with selinuxtest gives me the expected results:</p>
<pre><code>$ sudo su -
[sudo] password for selinuxtest:
</code></pre><p>I didn&rsquo;t add selinuxtest to sudoers, so this command would fail. However, I&rsquo;m actually allowed to execute it now.</p>
<p>These restrictions could be very helpful when dealing with users that you don&rsquo;t fully trust on your system. You could use these restrictions to add a kiosk user to a Linux machine and allow family members or coworkers to surf the web using your device. In addition, you could use the restrictions as an extra layer of protection on heavily shared servers to prevent users from consuming resources or generating malicious traffic.</p>
]]></content></item><item><title>My interview on the Dave and Gunnar Show</title><link>https://major.io/2013/06/19/my-interview-on-the-dave-and-gunnar-show/</link><pubDate>Wed, 19 Jun 2013 12:04:06 +0000</pubDate><guid>https://major.io/2013/06/19/my-interview-on-the-dave-and-gunnar-show/</guid><description>David Egts and Gunnar Hellekson were kind enough to invite me to participate in their Dave and Gunnar Show podcast during the 2013 Red Hat Summit. We talked about Red Hat Enterprise Linux and my takeaways from the Summit.
The podcast episode [32MB] is about 34 minutes long and my interview starts after the 21 minute mark. I&amp;rsquo;m always nervous when I do these things so you&amp;rsquo;re certainly not allowed to poke fun at my accent.</description><content type="html"><![CDATA[<p><a href="https://twitter.com/davidegts">David Egts</a> and <a href="https://twitter.com/ghelleks">Gunnar Hellekson</a> were kind enough to invite me to participate in their <a href="http://dgshow.org/">Dave and Gunnar Show podcast</a> during the <a href="http://www.redhat.com/summit/">2013 Red Hat Summit</a>. We talked about Red Hat Enterprise Linux and my takeaways from the Summit.</p>
<p>The <a href="http://major.io/wp-content/uploads/2013/06/david-and-gunnar-ep16.mp3">podcast episode</a> [32MB] is about 34 minutes long and my interview starts after the 21 minute mark. I&rsquo;m always nervous when I do these things so you&rsquo;re certainly not allowed to poke fun at my accent. ;)</p>
<p><img src="/wp-content/uploads/2013/06/dgshow16-major.jpg" alt="group_photo"> Gunnar Hellekson (left) and David Egts (right)</p>
]]></content></item><item><title>Red Hat Summit 2013 Recap</title><link>https://major.io/2013/06/14/red-hat-summit-2013-recap/</link><pubDate>Sat, 15 Jun 2013 00:25:50 +0000</pubDate><guid>https://major.io/2013/06/14/red-hat-summit-2013-recap/</guid><description>The 2013 Red Hat Summit was my second one and I enjoyed it more than last year. Quite a few people asked for a recap and some takeaways from the Summit and that&amp;rsquo;s what I hope to do in this post.
Keynotes
It&amp;rsquo;s quite apparent that Red Hat is taking a more assertive - and sometimes aggressive - stance against closed source, overpriced solutions that prevent consumers from getting things done.</description><content type="html"><![CDATA[<p><img src="/wp-content/uploads/2013/06/IMG_20130611_173516.jpg" alt="1"></p>
<p>The 2013 Red Hat Summit was my second one and I enjoyed it more than last year. Quite a few people asked for a recap and some takeaways from the Summit and that&rsquo;s what I hope to do in this post.</p>
<p><strong>Keynotes</strong></p>
<p>It&rsquo;s quite apparent that Red Hat is taking a more assertive - and sometimes aggressive - stance against closed source, overpriced solutions that prevent consumers from getting things done. Jim Whitehurst had a slide that showed &ldquo;Open or Die&rdquo; with a live plant on the left and a dead one on the right (see the photo in the post just below this paragraph). You could hear the gasp in the audience from some of the less technical crowd. Red Hat is making a big push to deliver on Openstack and to modernize their RHEL and RHEV platforms. Paul Cormier detailed some of the upcoming offerings and the overall strategy seems to be a double-down on virtualization via Openstack and further enhancement of Enterprise Linux.</p>
<p><img src="/wp-content/uploads/2013/06/IMG_20130611_180616.jpg" alt="2"></p>
<p>Of the vendor keynotes, the Intel keynote from Dirk Hohndel was superb. He seemed a bit nervous at first and we were quickly losing interest but he brought us back in with some good anecdotes. Dirk went into detail about how a company with a ton of intellectual property could also embrace open source. Surprisingly, the speech really moved me and there were no slides involved; it was just Dirk talking.</p>
<p>You can watch the keynotes on <a href="http://www.redhat.com/summit/2013/gallery/">Red Hat&rsquo;s Summit site</a>. If you only watch one of them, watch <a href="http://videos.cdn.redhat.com/2013-summit-keynotes-hohndel.mp4">Dirk Hohndel&rsquo;s talk</a> (direct link to MP4).</p>
<p><strong>Sessions</strong></p>
<p><img src="/wp-content/uploads/2013/06/IMG_20130613_140551.jpg" alt="5"></p>
<p>The most memorable was <a href="http://www.redhat.com/summit/sessions/index.html#232">Jon Masters' demonstration of the 64-bit ARM platform</a> (AArch64). Although there was no bicycle or spandex involved this year (he apologized for the lack of both), it was amazing to see some firsts. It was the first time AArch64 has been demonstrated in public and the first time Gluster ran on 64-bit ARM. He had a 2U rackmount chassis and the fans were extremely loud. Jon commented that the chips are &ldquo;rarer than gold&rdquo; and that he wasn&rsquo;t going to chance turning the fans off. The server performed quite well during the demonstration and certainly outperformed what I&rsquo;d expect from an ARM system.</p>
<p>Dan Walsh led two informative sessions that I enjoyed. The first was a <a href="http://www.redhat.com/summit/sessions/index.html#418">session on Linux containers</a>. LXC confused me quite a bit before the talk but Dan and the product manager went through how containers work step by step. They gave real world use cases and made comparisons to the more prevalent virtualization methods, like KVM. As you might expect, Dan sprinkled in some useful security tips to make containers more safe to use.</p>
<p>Another of Dan&rsquo;s talks was about <a href="http://www.redhat.com/summit/sessions/index.html#67">how to use SELinux in a large enterprise</a>. He started it off with a brief explanation of SELinux and made us all stand up and say the words on his first slide (&ldquo;SELinux is a labeling system&rdquo;). He offered some tips on how to manage SELinux on multiple machines with Puppet and Ansible. In addition, he showed how custom policies could be easily exported and then passed around as RPM&rsquo;s or within configuration management systems. We also saw how to send auditd logs to remote systems for aggregation and alerting. You can certainly manage SELinux on many machines simply by treating the policies and configuration just like you treat any other service&rsquo;s configuration files.</p>
<p>Even after the Pub Crawl on Thursday night, the <a href="http://www.redhat.com/summit/sessions/index.html#499">Friday morning presentation about systemd</a> was packed with attendees. The presenters went through SysV&rsquo;s shortcomings and what systemd can deliver. It will replace init in RHEL 7. Adding systemd reduces the complexity of managing services and allows you to automate many of the things that are annoying to do manually (like cgroups). Its default method of handling cgroups allows CPU share to be carved up <em>per service</em> rather than per process. That means that if httpd has ten workers and MySQL is running two processes, each <em>service</em> will receive a 50% share of the total CPU (rather than httpd getting a lot extra since it has multiple processes).</p>
<p>The RHEL 7 talks were extremely informative and I was writing until my hand almost fell off. I probably missed a lot of the new features so it might be a good idea to wait for the slides to be published. If you&rsquo;re eager to use RHEL 7 as a desktop, you&rsquo;ll see GNOME&rsquo;s classic mode on the desktop (and it looks great).</p>
<p><strong>After-hours</strong></p>
<p>As usual, the Red Hat Certified Professionals reception at McGreevy&rsquo;s was a great networking opportunity. I met other Linux users from around the world and enjoyed some pretty decent beer and food. I stayed after the reception and received a detailed lesson about how hockey works. The Bruins pushed through three OT&rsquo;s but eventually lost.</p>
<p>The rain ruined Thursday night&rsquo;s plans but the Red Hat marketing folks put together a great alternative in less than 24 hours. We ended up at <a href="http://royaleboston.com/">Royale</a> and were treated to a ton of food and drinks. Some musicians set up late in the evening and we were all wondering what type of music they&rsquo;d play. It was a group called <a href="http://www.alteregobooking.com/">Alter Ego</a> from Montreal and they really rocked the place. They&rsquo;re famous for &ldquo;60 costume changes in 90 minutes&rdquo; and they took us through oldies, disco, and contemporary music. They hit every single music genre I could think of (except country) and everyone was amazed that they entertained us for 90 minutes without a single break. If you get the chance to see this group in person, don&rsquo;t miss it.</p>
<p><img src="/wp-content/uploads/2013/06/PANO_20130613_213602.jpg" alt="12"></p>
<p><strong>Wrap-up</strong></p>
<p>The Red Hat Summits continue to be a good opportunity to learn, network, and experiment. The ratio of attendees seems to be tilting more toward the non-technical side, and this is a problem that the organizers will definitely need to improve. There were several technical sessions packed wall to wall with plenty of non-technical people playing on their phones or checking email on their laptops. It&rsquo;s a tough problem to fix and many conferences have the same issue.</p>
<p>Next year&rsquo;s summit will be in San Francisco in April. I hope to see you there!</p>
]]></content></item><item><title>Supermicro X9SCI/X9SCA server does a shutdown rather than a reboot</title><link>https://major.io/2013/06/03/supermicro-x9scix9sca-server-does-a-shutdown-rather-than-a-reboot/</link><pubDate>Mon, 03 Jun 2013 14:45:34 +0000</pubDate><guid>https://major.io/2013/06/03/supermicro-x9scix9sca-server-does-a-shutdown-rather-than-a-reboot/</guid><description>Most of my websites run on a pair of Supermicro servers that I purchased from Silicon Mechanics (and I can&amp;rsquo;t say enough good things about them and their servers). One problem that kept cropping up was that the servers would become unresponsive during a reboot. If I issued the reboot command in Linux, the machine would begin the reboot process, power off, and remain powered off.
Needless to say, this is highly annoying.</description><content type="html"><![CDATA[<p>Most of my websites run on a pair of Supermicro servers that I purchased from <a href="http://www.siliconmechanics.com/">Silicon Mechanics</a> (and I can&rsquo;t say enough good things about them and their servers). One problem that kept cropping up was that the servers would become unresponsive during a reboot. If I issued the <code>reboot</code> command in Linux, the machine would begin the reboot process, power off, and remain powered off.</p>
<p>Needless to say, this is highly annoying.</p>
<p>The only way to bring the machine back was to use ipmitool on my other server or access the IPMI/iKVM interface on the downed server. I tested Fedora 15 through 19 and confirmed the issue in each OS. Finally, I installed CentOS 6 and the problem disappeared. The servers would reboot and come back online as expected.</p>
<p>Fast forward to this evening. I discovered a <a href="http://ubuntuforums.org/showthread.php?t=2024096">helpful forum thread</a> where users were discussing a similar problem on a X9SCA-F Supermicro board. The fix was to blacklist a kernel module like this:</p>
<pre><code> /etc/modprobe.d/blacklist.conf
</code></pre><p>I tried to <code>rmmod mei</code> and reboot, but the machine stayed powered off again. When I powered it back on with the module blacklisted from the start, I found that I could reboot normally and the server would boot up again. The module is from Intel:</p>
<pre><code># modinfo mei | grep desc
description:    Intel(R) Management Engine Interface
</code></pre><p>The Intel Management Engine is a BIOS extension that enables Intel Active Management Technology (AMT). Intel <a href="http://software.intel.com/sites/default/files/m/2/3/8/9/c/17992-intel_amt_overview.pdf">has a PDF</a> that gives an overview of AMT:</p>
<blockquote>
<p>Intel® Active Management Technology (Intel® AMT) is a capability embedded in Intel-based platforms that enhances the ability of IT organizations to manage enterprise computing facilities. Intel AMT operates independently of the platform processor and operating system. Remote platform management applications can access Intel AMT securely, even when the platform is turned off, as long as the platform is connected to line power and to a network. Independent software vendors (ISVs) can build applications that take advantage of the features of Intel AMT using the application programming interface (API).</p>
</blockquote>
<p>That&rsquo;s a mouthful.</p>
<p>It essentially allows you to manage large amounts of hardware and keep an inventory. You can also pull event logs from the machine even if it&rsquo;s powered off. Applications running within the OS on the server can give data to the AMT interface that allows administrators to retrieve the data without needing access to the OS.</p>
<p>The blacklisted module hasn&rsquo;t affected the server negatively (as far as I can tell).</p>
]]></content></item><item><title>Installing the Xen hypervisor on Fedora 19</title><link>https://major.io/2013/06/02/installing-the-xen-hypervisor-on-fedora-19/</link><pubDate>Mon, 03 Jun 2013 04:27:43 +0000</pubDate><guid>https://major.io/2013/06/02/installing-the-xen-hypervisor-on-fedora-19/</guid><description>It&amp;rsquo;s been a little while since I last posted about installing Xen on Fedora, so I figured that Fedora 19&amp;rsquo;s beta release was as good a time as any to write a new post. To get started, you&amp;rsquo;ll need to get Fedora 19 installed on your favorite hardware (or virtual machine).
Install the Xen hypervisor and tools. Also, ensure that both of the necessary daemons are running on each boot:</description><content type="html"><![CDATA[<p>It&rsquo;s been a little while <a href="/2011/08/05/xen-4-1-on-fedora-15-with-linux-3-0/">since I last posted about installing Xen on Fedora</a>, so I figured that Fedora 19&rsquo;s beta release was as good a time as any to write a new post. To get started, you&rsquo;ll need to get Fedora 19 installed on your favorite hardware (or virtual machine).</p>
<p>Install the Xen hypervisor and tools. Also, ensure that both of the necessary daemons are running on each boot:</p>
<pre><code>yum -y install xen xen-hypervisor xen-libs xen-runtime
chkconfig xend on
chkconfig xendomains on
</code></pre><p>You&rsquo;ll notice that I didn&rsquo;t start the daemons quite yet. We will need the xen hypervisor running before they will be of any use.</p>
<p>Now, let&rsquo;s configure GRUB2. I wrote a <a href="/2012/07/16/boot-the-xen-hypervisor-by-default-in-fedora-17-with-grub-2/">quick post about these steps</a> last year. The Xen kernel entry should already be configured (by grubby), but it&rsquo;s not the default. Fixing that is a quick process:</p>
<pre><code># grep ^menuentry /boot/grub2/grub.cfg | cut -d &quot;'&quot; -f2
Fedora, with Linux 3.9.4-300.fc19.x86_64
Fedora, with Linux 0-rescue-4ea51ecfff4f4e64a5ec903c495ee5b6
Fedora, with Xen hypervisor
# grub2-set-default 'Fedora, with Xen hypervisor'
# grub2-editenv list
saved_entry=Fedora, with Xen hypervisor
</code></pre><p>At this point, you&rsquo;re ready to reboot. After the reboot, verify that Xen is running:</p>
<pre><code># xm dmesg | head
 __  __            _  _    ____    ____    ____    __      _  ___
 \ \/ /___ _ __   | || |  |___ \  |___ \  | ___|  / _| ___/ |/ _ \
  \  // _ \ '_ \  | || |_   __) |   __) |_|___ \ | |_ / __| | (_) |
  /  \  __/ | | | |__   _| / __/ _ / __/|__|__) ||  _| (__| |\__, |
 /_/\_\___|_| |_|    |_|(_)_____(_)_____| |____(_)_|  \___|_|  /_/

(XEN) Xen version 4.2.2 (mockbuild@phx2.fedoraproject.org) (gcc (GCC) 4.8.0 20130412 (Red Hat 4.8.0-2)) Fri May 17 19:39:53 UTC 2013
(XEN) Latest ChangeSet: unavailable
(XEN) Bootloader: GRUB 2.00
(XEN) Command line: placeholder
</code></pre><p>If you&rsquo;re adventurous on the command line, you&rsquo;re done here. However, I enjoy using virt-manager for quick access to virtual machines and I also like all of the scripting and remote administration capabilities that libvirt delivers. Let&rsquo;s get the tools and daemons installed and running:</p>
<pre><code>yum -y install virt-manager dejavu* xorg-x11-xauth
yum -y install libvirt-daemon-driver-network libvirt-daemon-driver-storage libvirt-daemon-xen
chkconfig libvirtd on
service libvirtd start
</code></pre><p>You&rsquo;re now ready to use virt-manager to manage your virtual machines. Simply ssh to your hypervisor with X forwarding enabled (<code>ssh -X hypervisor.mydomain.com</code>) and run <code>virt-manager</code>. You won&rsquo;t have a virtual network or bridge to use for virtual machines quite yet. You have two options: NAT your VM&rsquo;s or configure a network bridge. I prefer the bridge but you may require something different in your environment.</p>
<p>For the NAT option (the easiest for beginners):</p>
<pre><code>yum -y install libvirt-daemon-config-network libvirt-daemon-config-nwfilter
service libvirtd restart
</code></pre><p>For the network-bridge option, you&rsquo;ll need to adjust your network scripts to create a bridge and add your primary network interface to the bridge. That&rsquo;s a bit outside the scope of this post, but the <a href="http://fedoraproject.org/wiki/Networking/Bridging">Fedora Wiki</a> and <a href="http://www.howtoforge.com/virtualization-with-kvm-on-a-fedora-17-server">HowtoForge</a> (ignore the KVM parts of their guide).</p>
<p><strong>You now have a working Xen installation on Fedora 19!</strong></p>
<p><!-- raw HTML omitted -->FOR THOSE WHO EMBRACE SECURITY:<!-- raw HTML omitted --></p>
<p>If you run SELinux in Enforcing mode, there&rsquo;s still a lingering issue where SELinux prevents python (running under xend) from talking to block devices (like logical volumes). I <a href="https://bugzilla.redhat.com/show_bug.cgi?id=839287">opened a bug</a> about a similar problem before but I need to open another one for the block device issue. If you&rsquo;re itching for a workaround, you can force SELinux into permissive mode for the xend_t context only:</p>
<pre><code>yum -y install selinux-policy-devel
semanage permissive -a xend_t
</code></pre><p>That&rsquo;s not the best option for now, but it&rsquo;s certainly better than <code>setenforce 0</code>. ;)</p>
]]></content></item><item><title>Presentation: Demystifying SELinux</title><link>https://major.io/2013/05/28/presentation-demystifying-selinux/</link><pubDate>Wed, 29 May 2013 01:01:09 +0000</pubDate><guid>https://major.io/2013/05/28/presentation-demystifying-selinux/</guid><description>While rolling through my RSS feeds, I found a great presentation by David Quigley titled &amp;ldquo;Demystifying SELinux&amp;rdquo;. He makes come good comparisons between discretionary/mandatory access controls and dives into what makes SELinux useful. Basic troubleshooting commands are covered within the presentation as well.
You can find the presentation over on Speaker Deck. I&amp;rsquo;ve also mirrored a PDF copy here on the site.
UPDATE: If you&amp;rsquo;re going to OSCON 2013 this year, it appears that David will be presenting this topic during the event.</description><content type="html"><![CDATA[<p>While rolling through my RSS feeds, I found a great presentation by David Quigley titled &ldquo;Demystifying SELinux&rdquo;. He makes come good comparisons between discretionary/mandatory access controls and dives into what makes SELinux useful. Basic troubleshooting commands are covered within the presentation as well.</p>
<p>You can find the presentation over on <a href="https://speakerdeck.com/dpquigl/demystifying-selinux">Speaker Deck</a>. I&rsquo;ve also mirrored a <a href="http://major.io/wp-content/uploads/2013/05/demystifying_selinux.pdf">PDF copy</a> here on the site.</p>
<p><strong>UPDATE:</strong> If you&rsquo;re going to <a href="http://www.oscon.com/oscon2013">OSCON 2013</a> this year, it appears that <a href="http://www.oscon.com/oscon2013/public/schedule/speaker/151833">David</a> will be <a href="http://www.oscon.com/oscon2013/public/schedule/detail/29473">presenting this topic</a> during the event.</p>
]]></content></item><item><title>Migrate KVM virtual machines from CentOS 6 to Fedora 18 without the luxury of shared storage</title><link>https://major.io/2013/05/22/migrate-kvm-virtual-machines-from-centos-6-to-fedora-18-without-the-luxury-of-shared-storage/</link><pubDate>Wed, 22 May 2013 15:15:36 +0000</pubDate><guid>https://major.io/2013/05/22/migrate-kvm-virtual-machines-from-centos-6-to-fedora-18-without-the-luxury-of-shared-storage/</guid><description>I&amp;rsquo;ve converted one of my KVM hypervisors from CentOS 6 to Fedora 18 and now comes the task of migrating my virtual machines off of my single remaining CentOS 6 hypervisor. This is definitely on a budget, so there&amp;rsquo;s no shared storage to make this process easier.
Here&amp;rsquo;s how I did it:
Migrate the logical volume
My first VM to migrate is my Fedora development VM where I build and test new packages.</description><content type="html"><![CDATA[<p>I&rsquo;ve converted one of my KVM hypervisors from CentOS 6 to Fedora 18 and now comes the task of migrating my virtual machines off of my single remaining CentOS 6 hypervisor. This is definitely on a budget, so there&rsquo;s no shared storage to make this process easier.</p>
<p>Here&rsquo;s how I did it:</p>
<p><strong>Migrate the logical volume</strong></p>
<p>My first VM to migrate is my Fedora development VM where I build and test new packages. I have a 10G logical volume on the old node:</p>
<pre><code>[root@helium ~]# lvs /dev/mapper/vg_helium-fedora--dev
  LV         VG        Attr     LSize  Pool Origin Data%  Move Log Copy%  Convert
  fedora-dev vg_helium -wi-a--- 10.00g
</code></pre><p>I made a 10G logical volume on the new hypervisor:</p>
<pre><code>[root@hydrogen ~]# lvcreate -n fedora-dev -L10G vg_hydrogen
  Logical volume &quot;fedora-dev&quot; created
</code></pre><p>After getting ssh keys set up between both hypervisors and installing <a href="http://linux.die.net/man/1/pv"><code>pv</code></a> (to track progress), I started the storage migration over ssh:</p>
<pre><code>dd if=/dev/mapper/vg_helium-fedora--dev | pv | ssh hydrogen dd of=/dev/mapper/vg_hydrogen-fedora--dev
</code></pre><p>Luckily it was only a 10GB logical volume so it transferred over in a few minutes.</p>
<p><strong>Dump and adjust the source VM&rsquo;s XML</strong></p>
<p>On the source server, I dumped the VM configuration to an XML file and copied it to the new host:</p>
<pre><code>virsh dumpxml fedora-dev &gt; fedora-dev.xml
scp fedora-dev.xml hydrogen:
</code></pre><p>Before importing the XML file on the new host, there are some adjustments that need to be made. First off was an adjustment of the storage volume since the new host had the same logical volume name but a different volume group (the source line):</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-xml" data-lang="xml">  <span style="color:#268bd2">&lt;disk</span> type=<span style="color:#2aa198">&#39;block&#39;</span> device=<span style="color:#2aa198">&#39;disk&#39;</span><span style="color:#268bd2">&gt;</span>
  <span style="color:#268bd2">&lt;driver</span> name=<span style="color:#2aa198">&#39;qemu&#39;</span> type=<span style="color:#2aa198">&#39;raw&#39;</span> cache=<span style="color:#2aa198">&#39;none&#39;</span> io=<span style="color:#2aa198">&#39;native&#39;</span><span style="color:#268bd2">&gt;&lt;/driver&gt;</span>
  <span style="color:#268bd2">&lt;source</span> dev=<span style="color:#2aa198">&#39;/dev/vg_hydrogen/fedora-dev&#39;</span><span style="color:#268bd2">/&gt;</span>
  <span style="color:#268bd2">&lt;target</span> dev=<span style="color:#2aa198">&#39;vda&#39;</span> bus=<span style="color:#2aa198">&#39;virtio&#39;</span><span style="color:#268bd2">&gt;&lt;/target&gt;</span>


<span style="color:#268bd2">&lt;address</span> type=<span style="color:#2aa198">&#39;pci&#39;</span> domain=<span style="color:#2aa198">&#39;0x0000&#39;</span> bus=<span style="color:#2aa198">&#39;0x00&#39;</span> slot=<span style="color:#2aa198">&#39;0x05&#39;</span> function=<span style="color:#2aa198">&#39;0x0&#39;</span><span style="color:#268bd2">&gt;</span>

<span style="color:#268bd2">&lt;/address&gt;</span>
<span style="color:#268bd2">&lt;/disk&gt;</span>
</code></pre></div><p>Also, there&rsquo;s a mismatch with the machine type (not architecture) between CentOS 6 and Fedora 18. I dumped the XML from a VM running on the Fedora 18 hypervisor and compared the machine type to my old CentOS VM&rsquo;s XML (the XML from the CentOS VM is on top):</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-diff" data-lang="diff"><span style="color:#dc322f">-    &lt;type arch=&#39;x86_64&#39; machine=&#39;rhel6.3.0&#39;&gt;hvm&lt;/type&gt;
</span><span style="color:#dc322f"></span><span style="color:#719e07">+    &lt;type arch=&#39;x86_64&#39; machine=&#39;pc-1.2&#39;&gt;hvm&lt;/type&gt;
</span></code></pre></div><p>I replaced <code>rhel6.3.0</code> with <code>pc-1.2</code>. <em>If you forget this step, your VM won&rsquo;t start.</em> You&rsquo;ll get some errors about a mismatched machine type before the VM boots.</p>
<p>There&rsquo;s one last fix: the path to the <code>qemu-kvm</code> emulator:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-diff" data-lang="diff"><span style="color:#dc322f">-    &lt;emulator&gt;/usr/libexec/qemu-kvm&lt;/emulator&gt;
</span><span style="color:#dc322f"></span><span style="color:#719e07">+    &lt;emulator&gt;/usr/bin/qemu-kvm&lt;/emulator&gt;
</span></code></pre></div><p>Replace <code>/usr/libexec/qemu-kvm</code> with <code>/usr/bin/qemu-kvm</code> and save your XML file.</p>
<p><strong>Import the VM configuration and launch the VM</strong></p>
<p>Importing the VM on the Fedora 18 hypervisor was easy:</p>
<pre><code>virsh define fedora-dev.xml
</code></pre><p>That causes the configuration to load into libvirt and it should appear in <code>virt-manager</code> or <code>virsh list</code> by this point. If not, double check your previous steps and look for error messages in your logs. That doesn&rsquo;t actually start the virtual machine, so I started it on the command line:</p>
<pre><code>virsh start fedora-dev
</code></pre><p>Within a few moments, the VM was up and responding to pings.</p>
<p>It&rsquo;s a good idea to hop into <code>virt-manager</code> and verify that the VM configuration is what you expect. Some configuration options don&rsquo;t line up terribly well between CentOS 6 and Fedora 18. You might need to adjust a few to match the performance you expect to see.</p>
]]></content></item><item><title>Handling terminal color escape sequences in less</title><link>https://major.io/2013/05/21/handling-terminal-color-escape-sequences-in-less/</link><pubDate>Wed, 22 May 2013 02:33:00 +0000</pubDate><guid>https://major.io/2013/05/21/handling-terminal-color-escape-sequences-in-less/</guid><description>This post is a quick one but I wanted to share it since I taught it to someone new today. When you have bash output with colors, less doesn&amp;rsquo;t handle the color codes properly by default:
$ colordiff chunk/functions.php chunk-old/functions.php | less ESC[0;32m22a23,27ESC[0;0m ESC[0;34m&amp;gt; * Load up our functions for grabbing content from postsESC[0;0m ESC[0;34m&amp;gt; */ESC[0;0m ESC[0;34m&amp;gt; require( get_template_directory() . '/content-grabbers.php' );ESC[0;0m ESC[0;34m&amp;gt; ESC[0;0m Toss in the -R flag and you&amp;rsquo;ll be able to see the colors properly (no colors to see here, but use your imagination):</description><content type="html"><![CDATA[<p>This post is a quick one but I wanted to share it since I taught it to someone new today. When you have bash output with colors, <code>less</code> doesn&rsquo;t handle the color codes properly by default:</p>
<pre><code>$ colordiff chunk/functions.php chunk-old/functions.php | less
ESC[0;32m22a23,27ESC[0;0m
ESC[0;34m&gt;       * Load up our functions for grabbing content from postsESC[0;0m
ESC[0;34m&gt;       */ESC[0;0m
ESC[0;34m&gt;      require( get_template_directory() . '/content-grabbers.php' );ESC[0;0m
ESC[0;34m&gt; ESC[0;0m
</code></pre><p>Toss in the <code>-R</code> flag and you&rsquo;ll be able to see the colors properly (no colors to see here, but use your imagination):</p>
<pre><code>$ colordiff chunk/functions.php chunk-old/functions.php | less -R
22a23,27
&gt;        * Load up our functions for grabbing content from posts
&gt;        */
&gt;       require( get_template_directory() . '/content-grabbers.php' );
&gt;
&gt;       /**
</code></pre><p>The <a href="http://linux.die.net/man/1/less">man page</a> for <code>less</code> explains the feature in greater detail:</p>
<pre><code>-R or --RAW-CONTROL-CHARS
       Like -r, but only ANSI &quot;color&quot; escape sequences are output in &quot;raw&quot; form.  Unlike -r, the screen appear-
       ance is maintained correctly in most cases.  ANSI &quot;color&quot; escape sequences are sequences of the form:
            ESC [ ... m
       where the &quot;...&quot; is zero or more color specification characters For  the  purpose  of  keeping  track  of
       screen  appearance,  ANSI  color escape sequences are assumed to not move the cursor.  You can make less
       think that characters other than &quot;m&quot; can end ANSI color escape  sequences  by  setting  the  environment
       variable  LESSANSIENDCHARS to the list of characters which can end a color escape sequence.  And you can
       make less think that characters other than the standard ones may appear between the ESC  and  the  m  by
       setting the environment variable LESSANSIMIDCHARS to the list of characters which can appear.
</code></pre>]]></content></item><item><title>Changing your ssh server’s port from the default: Is it worth it?</title><link>https://major.io/2013/05/14/changing-your-ssh-servers-port-from-the-default-is-it-worth-it/</link><pubDate>Wed, 15 May 2013 04:43:41 +0000</pubDate><guid>https://major.io/2013/05/14/changing-your-ssh-servers-port-from-the-default-is-it-worth-it/</guid><description>Changing my ssh port from the default port (22) has been one of my standard processes for quite some time when I build new servers or virtual machines. However, I see arguments crop up regularly about it (like this reddit thread or this other one).
Before I go any further, let&amp;rsquo;s settle the &amp;ldquo;security through obscurity&amp;rdquo; argument. (This could probably turn into its own post but I&amp;rsquo;ll be brief for now.</description><content type="html"><![CDATA[<p>Changing my ssh port from the default port (22) has been one of my standard processes for quite some time when I build new servers or virtual machines. However, I see arguments crop up regularly about it (like <a href="http://redd.it/1ebe0d">this reddit thread</a> or <a href="http://redd.it/fnz1h">this other one</a>).</p>
<p>Before I go any further, let&rsquo;s settle the &ldquo;security through obscurity&rdquo; argument. <em>(This could probably turn into its own post but I&rsquo;ll be brief for now.)</em> Security should always be applied in layers. This provides multiple levels of protection from initial attacks, like information gathering attempts or casual threats against known vulnerabilities. In addition, these layers of security should be applied <strong>within</strong> the environment so that breaking into one server after getting a pivot point in the environment should be just as difficult (if not more difficult) than the original attack that created the pivot point. If &ldquo;security through obscurity&rdquo; tactics make up <em>one layer</em> of a <em>multi-layered solution</em>, I&rsquo;d encourage you to obscure your environment as long as it doesn&rsquo;t <a href="http://security.blogoverflow.com/2012/08/confidentiality-integrity-availability-the-three-components-of-the-cia-triad/">affect your availability</a>.</p>
<p>The key takeaway is:</p>
<blockquote>
<p>Security through obscurity is effective if it&rsquo;s one layer in a multi-layer security solution</p>
</blockquote>
<p>Let&rsquo;s get back to the original purpose of the post.</p>
<p><strong>The biggest benefit to changing the port is to avoid being seen by casual scans.</strong> The vast majority of people hunting for any open ssh servers will look for port 22. Some will try the usual variants, like 222 and 2222, but those are few and far between. I ran an experiment with a virtual machine exposed to the internet which had sshd listening on port 22. The server stayed online for one week and then I changed the ssh port to 222. <strong>The number of attacks dropped by 98%.</strong> Even though this is solely empirical evidence, it&rsquo;s clear that moving off the standard ssh port reduces your server&rsquo;s profile.</p>
<p>If it&rsquo;s more difficult to scan for your ssh server, your chances of being attacked with an ssh server exploit are reduced. A determined attacker can still find the port if they know your server&rsquo;s IP address via another means (perhaps via a website you host) and they can launch attacks once they find it. Paranoid server administrators might want to check into <a href="https://wiki.archlinux.org/index.php/Port_Knocking">port knocking</a> to reduce that probability even further.</p>
<p>Remembering the non-standard ssh port can be annoying, but if you have a standard set of workstations that you use for access your servers, just utilize your <code>~/.ssh/config</code> file to specify certain ports for certain servers. For example:</p>
<pre><code>Host *.mycompany.com
  Port 4321

Host nonstandard.mypersonalstuff.com
  Port 2345

Host *.mypersonalstuff.com
  Port 5432
</code></pre><p>If you run into SELinux problems with a non-standard ssh port, there are <a href="/2011/09/15/receive-e-mail-reports-for-selinux-avc-denials/">plenty of guides on this topic.</a>. The <code>setroubleshoot-server</code> package helps out with this as well.</p>
<pre><code># semanage port -a -t ssh_port_t -p tcp 4321
# semanage port -l | grep ssh
ssh_port_t                     tcp      4321,22
</code></pre><p>Here is my list of ssh lockdown practices when I build a new server:</p>
<ul>
<li>Update the ssh server package and ensure that automatic updates are configured</li>
<li>Enable SELinux and allow a non-standard ssh port</li>
<li>Add my ssh public key to the server</li>
<li>Disable password logins for ssh</li>
<li>Adjust my <code>AllowUsers</code> setting in sshd_config to only allow my user</li>
<li>Disable root logins</li>
<li>For servers with sensitive data, I install <a href="http://www.fail2ban.org/">fail2ban</a></li>
</ul>
]]></content></item><item><title>Automate CentOS 6 deployments with CIS Security Benchmarks already applied</title><link>https://major.io/2013/04/26/automate-centos-6-deployments-with-cis-security-benchmarks-already-applied/</link><pubDate>Fri, 26 Apr 2013 14:15:24 +0000</pubDate><guid>https://major.io/2013/04/26/automate-centos-6-deployments-with-cis-security-benchmarks-already-applied/</guid><description>A coworker heard me grumbling about Linux system administration standards and recommended that I review the CIS Security Benchmarks. After downloading the Red Hat Enterprise Linux 6 security benchmark PDF, I quickly started to see the value of the document. Some of the standards were the installation defaults, some were often forgotten settings, and some were completely brand new to me.
Automating the standards can be a little treacherous simply due to the number of things to adjust and check.</description><content type="html"><![CDATA[<p>A coworker heard me grumbling about Linux system administration standards and recommended that I review the <a href="http://benchmarks.cisecurity.org/">CIS Security Benchmarks</a>. After downloading the Red Hat Enterprise Linux 6 security benchmark PDF, I quickly started to see the value of the document. Some of the standards were the installation defaults, some were often forgotten settings, and some were completely brand new to me.</p>
<p>Automating the standards can be a little treacherous simply due to the number of things to adjust and check. I&rsquo;ve created a kickstart for CentOS 6 and tossed it on Github:</p>
<ul>
<li><a href="https://github.com/rackerhacker/securekickstarts">https://github.com/rackerhacker/securekickstarts</a></li>
</ul>
<p>Be sure to read the disclaimers in the <a href="https://github.com/rackerhacker/securekickstarts/blob/master/README.md">README</a> before getting started. Also, keep in mind that the kickstarts are in no way approved by or affiliated with the <a href="https://www.cisecurity.org/">Center for Internet Security</a> in any way. This is just something I&rsquo;m offering up to the community in the hope that it helps someone.</p>
]]></content></item><item><title>Limit access to the su command</title><link>https://major.io/2013/04/25/limit-access-to-the-su-command/</link><pubDate>Fri, 26 Apr 2013 04:05:46 +0000</pubDate><guid>https://major.io/2013/04/25/limit-access-to-the-su-command/</guid><description>The wheel group exists for a critical purpose and Wikipedia has a concise definition:
In computing, the term wheel refers to a user account with a wheel bit, a system setting that provides additional special system privileges that empower a user to execute restricted commands that ordinary user accounts cannot access. The term is derived from the slang phrase big wheel, referring to a person with great power or influence.</description><content type="html"><![CDATA[<p>The wheel group exists for a critical purpose and Wikipedia has a <a href="http://en.wikipedia.org/wiki/Wheel_(Unix_term)">concise definition</a>:</p>
<blockquote>
<p>In computing, the term wheel refers to a user account with a wheel bit, a system setting that provides additional special system privileges that empower a user to execute restricted commands that ordinary user accounts cannot access. The term is derived from the slang phrase big wheel, referring to a person with great power or influence.</p>
</blockquote>
<p>On Red Hat systems (including Fedora), the default <code>sudo</code> configuration allows users in the wheel group to use sudo while all others are restricted from using it in <code>/etc/sudoers</code>:</p>
<pre><code>## Allows people in group wheel to run all commands
%wheel        ALL=(ALL)       ALL
</code></pre><p>However, the <code>su</code> command can be used by all users by default (which is something I often forget). Fixing it is easy once you take a look at <code>/etc/pam.d/su</code>:</p>
<pre><code># Uncomment the following line to require a user to be in the &quot;wheel&quot; group.
#auth		required	pam_wheel.so use_uid
</code></pre><p>Uncomment the line and access to <code>su</code> will only be available for users in the wheel group.</p>
]]></content></item><item><title>Reprint: Stop Disabling SELinux!</title><link>https://major.io/2013/04/19/reprint-stop-disabling-selinux/</link><pubDate>Fri, 19 Apr 2013 05:52:23 +0000</pubDate><guid>https://major.io/2013/04/19/reprint-stop-disabling-selinux/</guid><description>This article appeared in SC Magazine and I&amp;rsquo;ve posted it here as well. For those of you who were left wanting more from my previous SELinux post, this should help. If it doesn&amp;rsquo;t help, leave a comment. ;)
The push to cloud transforms the way we apply information security principles to systems and applications. Perimeters of the past, secured heavily with traditional network devices in the outermost ring, lose effectiveness day by day.</description><content type="html"><![CDATA[<p><em>This article appeared in <a href="http://www.scmagazine.com.au/News/340475,stop-disabling-selinux.aspx">SC Magazine</a> and I&rsquo;ve posted it here as well. For those of you who were left wanting more from my <a href="/2013/04/15/seriously-stop-disabling-selinux/">previous SELinux post</a>, this should help. If it doesn&rsquo;t help, leave a comment. ;)</em></p>
<hr>
<p>The push to cloud transforms the way we apply information security principles to systems and applications. Perimeters of the past, secured heavily with traditional network devices in the outermost ring, lose effectiveness day by day. Shifting the focus to &ldquo;defense in depth&rdquo; brings the perimeter down to the individual cloud instances running your application. Security-Enhanced Linux, or SELinux, forms an effective part of that perimeter.</p>
<p>SELinux operates in the realm of mandatory access control, or MAC. The design of MAC involves placing constraints on what a user (a <em>subject</em>) can do to a particular object (a <em>target</em>) on the system. In contrast, discretionary access control, or DAC, allows a user with certain access to use discretion to limit or allow access to certain files, directories, or devices. You can set any file system permissions that you want but SELinux can override them with ease at the operating system level.</p>
<p>Consider a typical server running a web application. An attacker compromises the web application and executes malicious code via the web server daemon itself. SELinux has default policies that prevent the daemon from initiating communication on the network. That limits the attacker’s options to attack other services or servers.</p>
<p>In addition, SELinux sets policies on which files and directories the web server can access, regardless of any file system permissions. This protection limits the attacker’s access to other sensitive parts of the file system even if the administrator set the files to be readable to the world.</p>
<p>This is where SELinux shines. Oddly enough, this is the point where many system administrators actually <em>disable SELinux</em> on their systems.</p>
<p>Troubleshooting these events, called AVC denials, without some helpful tools is challenging and frustrating. Each denial flows into to your audit log as a cryptic message. Most administrators will check the usual suspects, like firewall rules and file system permissions. As frustration builds, they disable SELinux and notice that their application begins working as expected. SELinux remains disabled and hundreds of helpful policies lie dormant solely because one policy caused a problem.</p>
<p>Disabling SELinux without investigation frustrated me to the point where I started a site at <a href="http://stopdisablingselinux.com">stopdisablingselinux.com</a>. The site is a snarky response to Linux administrators who reach for the disable switch as soon as SELinux gets in their way.</p>
<p>All jokes aside, here are some helpful tips to use SELinux effectively:</p>
<p><strong>Use the <em>setroubleshoot</em> helpers to understand denials</strong></p>
<p>Working through denials is easy with the <em>setroubleshoot-server</em> package. When a denial occurs, you still receive a cryptic log message in your audit logs. However, you also receive a message via syslog that is very easy to read. Your server can email you these messages as well. The message contains guidance about adjusting SELinux booleans, setting contexts, or generating new SELinux policies to work around a really unusual problem. When I say guidance, I mean that the tools give you commands to copy and paste to adjust your policies, booleans and contexts.</p>
<p><strong>Review SELinux booleans for quick adjustments</strong></p>
<p>Although the myriad of SELinux user-space tools isn’t within the scope of this article, <em>getsebool</em> and <em>togglesebool</em> deserve a mention. Frequently adjusted policies are controlled by booleans that are toggled on and off with <em>togglesebool</em>. Start with <em>getsebool –a</em> for a full list of booleans and then use <em>togglesebool</em> to enable or disable the policy.</p>
<p><strong>Quickly restore file or directory contexts</strong></p>
<p>Shuffling files or directories around a server can cause SELinux denials due to contexts not matching their original values. This happens to me frequently if I move a configuration file from one system to another. Correcting the context problem involves one of two simple commands. The <em>restorecon</em> command applies the default contexts specific to the file or directory. If you have a file in the directory with the correct context, use <em>chcon</em> to fix the context on the wrong file by giving it the path to the file with the correct context.</p>
<p>Here are some additional links with helpful SELinux documentation:</p>
<ul>
<li><a href="http://selinuxproject.org/page/Main_Page">SELinux Project Wiki</a></li>
<li><a href="https://access.redhat.com/site/documentation/en-US/Red_Hat_Enterprise_Linux/6/html/Security-Enhanced_Linux/">Red Hat Enterprise Linux 6 SELinux Guide</a></li>
<li><a href="http://danwalsh.livejournal.com/">Dan Walsh&rsquo;s Blog</a></li>
</ul>
]]></content></item><item><title>Seriously, stop disabling SELinux</title><link>https://major.io/2013/04/15/seriously-stop-disabling-selinux/</link><pubDate>Tue, 16 Apr 2013 04:40:10 +0000</pubDate><guid>https://major.io/2013/04/15/seriously-stop-disabling-selinux/</guid><description>After many discussions with fellow Linux users, I&amp;rsquo;ve come to realize that most seem to disable SELinux rather than understand why it&amp;rsquo;s denying access. In an effort to turn the tide, I&amp;rsquo;ve created a new site as a public service to SELinux cowards everywhere: stopdisablingselinux.com.
Here are some relatively useful SELinux posts from the blog:
Getting started with SELinux Receive email reports for SELinux AVC denials Edit: The goal of the post was to poke some fun at system administrators who disable SELinux immediately without learning how it works or why they&amp;rsquo;re seeing certain operations being denied.</description><content type="html"><![CDATA[<p>After many discussions with fellow Linux users, I&rsquo;ve come to realize that most seem to disable SELinux rather than understand why it&rsquo;s denying access. In an effort to turn the tide, I&rsquo;ve created a new site as a public service to SELinux cowards everywhere: <a href="http://stopdisablingselinux.com/">stopdisablingselinux.com</a>.</p>
<p>Here are some relatively useful SELinux posts from the blog:</p>
<ul>
<li><a href="/2012/01/25/getting-started-with-selinux/">Getting started with SELinux</a></li>
<li><a href="/2011/09/15/receive-e-mail-reports-for-selinux-avc-denials/">Receive email reports for SELinux AVC denials</a></li>
</ul>
<hr>
<p><strong>Edit:</strong> The goal of the post was to poke some fun at system administrators who disable SELinux immediately without learning how it works or why they&rsquo;re seeing certain operations being denied. Obviously, if your particular workload or demands don&rsquo;t allow for the use of SELinux, then I&rsquo;m going to be the last person to encourage you to use it. Many system administrators have found that it doesn&rsquo;t provide a good ratio of work required to benefit gained, which I totally understand.</p>
]]></content></item><item><title>Remove sensitive information from email headers with postfix</title><link>https://major.io/2013/04/14/remove-sensitive-information-from-email-headers-with-postfix/</link><pubDate>Mon, 15 Apr 2013 02:59:34 +0000</pubDate><guid>https://major.io/2013/04/14/remove-sensitive-information-from-email-headers-with-postfix/</guid><description>I&amp;rsquo;m in the process of moving back to a postfix/dovecot setup for hosting my own mail and I wanted a way to remove the more sensitive email headers that are normally generated when I send mail. My goal is to hide the originating IP address of my mail as well as my mail client type and version.
To get started, make a small file with regular expressions in /etc/postfix/header_checks:
/^Received:.*with ESMTPSA/ IGNORE /^X-Originating-IP:/ IGNORE /^X-Mailer:/ IGNORE /^Mime-Version:/ IGNORE The &amp;ldquo;ESMTPSA&amp;rdquo; match works for me because I only send email via port 465.</description><content type="html"><![CDATA[<p>I&rsquo;m in the process of moving back to a postfix/dovecot setup for hosting my own mail and I wanted a way to remove the more sensitive email headers that are normally generated when I send mail. My goal is to hide the originating IP address of my mail as well as my mail client type and version.</p>
<p>To get started, make a small file with regular expressions in <code>/etc/postfix/header_checks</code>:</p>
<pre><code>/^Received:.*with ESMTPSA/              IGNORE
/^X-Originating-IP:/    IGNORE
/^X-Mailer:/            IGNORE
/^Mime-Version:/        IGNORE
</code></pre><p>The &ldquo;ESMTPSA&rdquo; match works for me because I only send email via port 465. I don&rsquo;t allow SASL authentication via port 25. You may need to adjust the regular expression if you accept SASL authentication via smtp.</p>
<p>Now, add the following two lines to your <code>/etc/postfix/main.cf</code>:</p>
<pre><code>mime_header_checks = regexp:/etc/postfix/header_checks
header_checks = regexp:/etc/postfix/header_checks
</code></pre><p>Rebuild the hash table and reload the postfix configuration:</p>
<pre><code>postmap /etc/postfix/header_checks
postfix reload
</code></pre><p>Now, send a test email. View the headers and you should see the original received header (with your client IP address) removed, along with details about your mail client.</p>
]]></content></item><item><title>drbd 8.4.2 for Fedora 17</title><link>https://major.io/2013/03/31/drbd-8-4-2-for-fedora-17/</link><pubDate>Sun, 31 Mar 2013 17:35:29 +0000</pubDate><guid>https://major.io/2013/03/31/drbd-8-4-2-for-fedora-17/</guid><description>Fedora 17 DRBD users should see version 8.4.2 of the DRBD client tools make it into stable repositories soon. This fixes a bug caused when the kernel version was bumped to 3.8 and the kernel module no longer matched the tools. It&amp;rsquo;s the same problem that recently cropped up on Fedora 18.</description><content type="html"><![CDATA[<p><img src="/wp-content/uploads/2012/01/fedorainfinity.png" alt="1"></p>
<p>Fedora 17 <a href="http://en.wikipedia.org/wiki/DRBD">DRBD</a> users should see version 8.4.2 of the DRBD client tools make it into stable repositories soon. This <a href="https://bugzilla.redhat.com/show_bug.cgi?id=924821">fixes a bug</a> caused when the kernel version was bumped to 3.8 and the kernel module no longer matched the tools. It&rsquo;s the same problem that <a href="/2013/03/15/drbd-8-4-2-for-fedora-18/">recently cropped up on Fedora 18</a>.</p>
<p><!-- raw HTML omitted --></p>
]]></content></item><item><title>virt-manager won’t release the mouse when using ssh forwarding from OS X</title><link>https://major.io/2013/03/20/virt-manager-wont-release-the-mouse-when-using-ssh-forwarding-from-os-x/</link><pubDate>Wed, 20 Mar 2013 05:26:56 +0000</pubDate><guid>https://major.io/2013/03/20/virt-manager-wont-release-the-mouse-when-using-ssh-forwarding-from-os-x/</guid><description>The latest versions of virt-manager don&amp;rsquo;t release the mouse pointer when you&amp;rsquo;re doing X forwarding to a machine running OS X. This can lead to a rather frustrating user experience since your mouse pointer is totally stuck in the window. Although this didn&amp;rsquo;t affect me with CentOS 6 hosts, Fedora 18 hosts were a problem.
There&amp;rsquo;s a relatively elegant fix from btm.geek that solved it for me. On your Mac, exit X11/Xquartz and create an ~/.</description><content type="html"><![CDATA[<p>The latest versions of <a href="http://virt-manager.org/">virt-manager</a> don&rsquo;t release the mouse pointer when you&rsquo;re doing X forwarding to a machine running OS X. This can lead to a rather frustrating user experience since your mouse pointer is totally stuck in the window. Although this didn&rsquo;t affect me with CentOS 6 hosts, Fedora 18 hosts were a problem.</p>
<p>There&rsquo;s a <a href="http://blog.loftninjas.org/2010/11/17/virt-manager-keymaps-on-os-x/">relatively elegant fix from btm.geek</a> that solved it for me. On your Mac, exit X11/Xquartz and create an <code>~/.Xmodmap</code> file containing this:</p>
<pre><code>clear Mod1
keycode 66 = Alt_L
keycode 69 = Alt_R
add Mod1 = Alt_L
add Mod1 = Alt_R
</code></pre><p>Start X11/Xquartz once more and virt-manager should release your mouse pointer if you hold the left control key and left option at the same time.</p>
]]></content></item><item><title>Late night virtualization frustration with kvm</title><link>https://major.io/2013/03/20/late-night-virtualization-frustration-with-kvm/</link><pubDate>Wed, 20 Mar 2013 05:07:21 +0000</pubDate><guid>https://major.io/2013/03/20/late-night-virtualization-frustration-with-kvm/</guid><description>I dragged out an old Aopen MP57-D tonight that was just sitting in the closet and decided to load up kvm on Fedora 18. I soon found myself staring at a very brief error message upon bootup:
kvm: disabled by bios After a reboot, the BIOS screen was up and I saw that Virtualization and VT-d were both enabled. Trusted execution (TXT) was disabled, so I enabled it for kicks and rebooted.</description><content type="html"><![CDATA[<p>I dragged out an old <a href="http://global.aopen.com/products_detail.aspx?Auno=3047">Aopen MP57-D</a> tonight that was just sitting in the closet and decided to load up kvm on Fedora 18. I soon found myself staring at a very brief error message upon bootup:</p>
<pre><code>kvm: disabled by bios
</code></pre><p>After a reboot, the BIOS screen was up and I saw that Virtualization and VT-d were both enabled. Trusted execution (TXT) was disabled, so I enabled it for kicks and rebooted. Now I had two errors:</p>
<pre><code>kvm: disable TXT in the BIOS or activate TXT before enabling KVM
kvm: disabled by bios
</code></pre><p>Time for another trip to the BIOS. I disabled TXT, rebooted, and I was <em>back to the same error where I first started</em>. A quick check of <code>/proc/cpuinfo</code> showed that I had the right processor extensions. Even the output of <code>lshw</code> showed that I should be ready to go. Some digging in Google led me to a <a href="http://reidablog.blogspot.com/2008/06/with-correct-bios-settings-enabled-on.html">blog post for a fix on Dell Optiplex hardware</a>.</p>
<p>The fix was to do this:</p>
<ol>
<li>Within the BIOS, <strong>disable</strong> virtualization, VT-d, and TXT</li>
<li>Save the BIOS configuration, reboot, and <strong>pull power to the computer at grub</strong></li>
<li>Within the BIOS, <strong>enable</strong> virtualization and VT-d but leave TXT disabled</li>
<li>Save the BIOS configuration, reboot, and <strong>pull power to the computer at grub</strong></li>
<li>Boot up the computer normally</li>
</ol>
<p>Although it seems a bit archaic, this actually fixed my problem and set me on my way.</p>
]]></content></item><item><title>Survive the Google Reader exodus with Tiny Tiny RSS</title><link>https://major.io/2013/03/17/survive-the-google-reader-exodus-with-tiny-tiny-rss/</link><pubDate>Sun, 17 Mar 2013 21:27:38 +0000</pubDate><guid>https://major.io/2013/03/17/survive-the-google-reader-exodus-with-tiny-tiny-rss/</guid><description>It&amp;rsquo;s no secret that Google Reader is a popular way to keep up with your RSS feeds, but it&amp;rsquo;s getting shelved later this year. Most folks suggested Feedly as a replacement but I found the UI quite clunky in a browser and on Android devices.
Then someone suggested Tiny Tiny RSS. I couldn&amp;rsquo;t learn more about it on the day Google Reader&amp;rsquo;s shutdown was announced because the site was slammed. In a nutshell, Tiny Tiny RSS is a well-written web UI for managing feeds and a handy API for using it with mobile applications.</description><content type="html"><![CDATA[<p><img src="/wp-content/uploads/2013/03/tinytinyrss.png" alt="1"></p>
<p>It&rsquo;s no secret that <a href="http://en.wikipedia.org/wiki/Google_Reader">Google Reader</a> is a popular way to keep up with your RSS feeds, but it&rsquo;s <a href="http://www.newyorker.com/online/blogs/books/2013/03/farewell-dear-reader.html">getting shelved later this year</a>. Most folks <a href="http://news.cnet.com/8301-1023_3-57574777-93/feedly-adds-500k-new-users-on-google-decision-to-kill-reader/">suggested Feedly as a replacement</a> but I found the UI quite clunky in a browser and on Android devices.</p>
<p>Then someone suggested <a href="http://tt-rss.org/redmine/projects/tt-rss/wiki">Tiny Tiny RSS</a>. I couldn&rsquo;t learn more about it on the day Google Reader&rsquo;s shutdown was announced because the site was slammed. In a nutshell, Tiny Tiny RSS is a well-written web UI for managing feeds and a handy API for using it with mobile applications. The backend code is written in PHP and it supports MySQL and Postgres.</p>
<p>There&rsquo;s also an <a href="https://play.google.com/store/apps/details?id=org.fox.ttrss&amp;hl=en">Android application</a> that gives you a seven day trial once you install it. The <a href="https://play.google.com/store/apps/details?id=org.fox.ttrss.key">pro key costs $1.99</a>.</p>
<p>The installation took me a few minutes and then I was off to the races. I&rsquo;d recommend implementing SSL for accessing your installation (unless you like passing credentials in the clear) and enable keepalive connections in Apache. The UI in the application drags down a ton of javascript as it works and enabling keepalives will keep your page load times low.</p>
<p>If you want to get your Google Reader feeds moved over in bulk, just export them from Google Reader:</p>
<ol>
<li>Click the settings cog at the top right of Google Reader and choose Reader Settings</li>
<li>Choose Import/Export from the menu</li>
<li>Press Export, head over to Google Takeout and download your zip file</li>
</ol>
<p>Unzip the file and find the .xml file. Open up a browser, access Tiny Tiny RSS and do this:</p>
<ol>
<li>Click Actions &gt; Preferences</li>
<li>Click the Feeds tab</li>
<li>Click the OPML button at the bottom</li>
<li>Import the xml file that was in the zip file from Google</li>
</ol>
<p>From there, just <a href="http://tt-rss.org/redmine/projects/tt-rss/wiki/UpdatingFeeds">choose a method for updating feeds</a> and you should be all set!</p>
]]></content></item><item><title>New icanhaz features: reverse DNS and traceroutes</title><link>https://major.io/2013/03/16/new-icanhaz-features-reverse-dns-and-traceroutes/</link><pubDate>Sun, 17 Mar 2013 03:13:53 +0000</pubDate><guid>https://major.io/2013/03/16/new-icanhaz-features-reverse-dns-and-traceroutes/</guid><description>After adding some upgrades for icanhazip.com, I wanted to go a bit further. Adding reverse DNS (PTR) lookups and traceroutes seemed like a decent idea!
Want to beta test some new features on icanhazptr.com and icanhaztrace.com? Give them a try!
Getting your reverse DNS entry is easy:
$ curl -4 icanhazptr.com ord.icanhazip.com $ curl -6 icanhazptr.com ord.icanhazip.com Traceroutes are straightforward as well:
$ curl -4 icanhaztrace.com traceroute to 166.78.118.193 (166.78.118.193), 30 hops max, 60 byte packets 1 212.</description><content type="html"><![CDATA[<p>After <a href="/2013/02/23/more-upgrades-for-icanhazip-com/">adding some upgrades</a> for <a href="http://icanhazip.com">icanhazip.com</a>, I wanted to go a bit further. Adding reverse DNS (PTR) lookups and traceroutes seemed like a decent idea!</p>
<p>Want to beta test some new features on <a href="http://icanhazptr.com">icanhazptr.com</a> and <a href="http://icanhaztrace.com">icanhaztrace.com</a>? Give them a try!</p>
<p>Getting your reverse DNS entry is easy:</p>
<pre><code>$ curl -4 icanhazptr.com
ord.icanhazip.com
$ curl -6 icanhazptr.com
ord.icanhazip.com
</code></pre><p>Traceroutes are straightforward as well:</p>
<pre><code>$ curl -4 icanhaztrace.com
traceroute to 166.78.118.193 (166.78.118.193), 30 hops max, 60 byte packets
 1  212.111.33.229  20.031 ms
 2  212.111.33.233  1.011 ms
 3  149.11.30.61  107.976 ms
...
$ curl -6 icanhaztrace.com
traceroute to 2001:4801:7818:6:abc5:ba2c:ff10:275f (2001:4801:7818:6:abc5:ba2c:ff10:275f), 30 hops max, 80 byte packets
 1  2a01:7e00:ffff:0:8a43:e1ff:fea3:fa7f  2.183 ms
 2  2001:4d78:fe01:2:1:3:b90:1  1.330 ms
 3  2001:978:2:45::d:1  8.388 ms
...
</code></pre><p>While this sits in beta, here are some things to keep in mind:</p>
<ul>
<li>If a PTR record doesn&rsquo;t exist for your IP address, your IP address will be returned</li>
<li>Failing traceroutes will cause your IP address to be returned</li>
<li>A PTR record will be chosen at random if multiple PTR records are returned</li>
<li>PTR lookups for traceroutes are currently disabled</li>
</ul>
<p>Let me know if you find any bugs.</p>
]]></content></item><item><title>drbd 8.4.2 for Fedora 18</title><link>https://major.io/2013/03/15/drbd-8-4-2-for-fedora-18/</link><pubDate>Fri, 15 Mar 2013 12:28:33 +0000</pubDate><guid>https://major.io/2013/03/15/drbd-8-4-2-for-fedora-18/</guid><description>If you use DRBD on Fedora 18, there&amp;rsquo;s a new client tools package on its way to the stable repositories. The kernel module for DRBD was bumped to 8.4.2 and the client tools have been updated to match it.
This fixes a bug that was submitted earlier this month.</description><content type="html"><![CDATA[<p><img src="/wp-content/uploads/2012/01/fedorainfinity.png" alt="1"></p>
<p>If you use <a href="http://en.wikipedia.org/wiki/DRBD">DRBD</a> on Fedora 18, there&rsquo;s a <a href="https://admin.fedoraproject.org/updates/FEDORA-2013-3540/drbd-8.4.2-1.fc18">new client tools package</a> on its way to the stable repositories. The kernel module for DRBD was bumped to 8.4.2 and the client tools have been updated to match it.</p>
<p>This <a href="https://bugzilla.redhat.com/show_bug.cgi?id=917649">fixes a bug</a> that was submitted earlier this month.</p>
<p><!-- raw HTML omitted --></p>
]]></content></item><item><title>Thoughts on RSA Conference 2013</title><link>https://major.io/2013/03/11/thoughts-on-rsa-conference-2013/</link><pubDate>Mon, 11 Mar 2013 19:23:32 +0000</pubDate><guid>https://major.io/2013/03/11/thoughts-on-rsa-conference-2013/</guid><description>This year was my first opportunity to attend the RSA Conference and I learned an unbelievable amount inside and outside the sessions. Here&amp;rsquo;s are my takeaways from the conference:
Be flexible and raise awareness
BYOD was a hot topic at this year&amp;rsquo;s conference and I was fortunate enough to sit in with a Peer2Peer session with 24 other attendees. One security team member from a large company talked about how they reduced their stress level and increased their effectiveness by focusing on securing the data rather than trying to secure every single device on their network.</description><content type="html"><![CDATA[<p><img src="/wp-content/uploads/2013/03/5482.04-moscone-center_2.jpg" alt="1"></p>
<p>This year was my first opportunity to attend the RSA Conference and I learned an unbelievable amount inside and outside the sessions. Here&rsquo;s are my takeaways from the conference:</p>
<p><strong>Be flexible and raise awareness</strong></p>
<p>BYOD was a hot topic at this year&rsquo;s conference and I was fortunate enough to sit in with a Peer2Peer session with 24 other attendees. One security team member from a large company talked about how they reduced their stress level and increased their effectiveness by focusing on <em>securing the data</em> rather than trying to <em>secure every single device</em> on their network. It seems trivial at first, but after additional thought, it really makes sense. Allowing every single device ever made onto your network might not be an option, but there are many actions we can take to make it more difficult for non-trusted machines to access sensitive company data.</p>
<p>Security awareness was talked about often. No matter how much a company spends on security products, a single user clicking on a phishing email can open the door for attackers. It&rsquo;s critical to make security awareness real by making it personal. When users think about more of their actions before taking them, the overall security of the business increases. One of the speakers made a good point that the job of a corporate security team in 2013 is to keep the business secure while allowing employees to soar and do what they do best. The days of blocking access to everything are over.</p>
<p><strong>Maintain and constantly re-evaluate focus</strong></p>
<p>Securing your entire company isn&rsquo;t possible so put your focus on the things that matter most. Wrap security around the most important data you have and shore up security in areas where you are threatened most often. A presenter noted that everyone has legacy baggage in their companies but the stronger companies think about the baggage they leave behind before they create it.</p>
<p><strong>Follow your users</strong></p>
<p>The whole idea of encouraging collaborative security between corporate security teams and the business seemed to surprise attendees the most. One of the talks pushed security departments to learn about what users within the company are doing and how their needs are evolving. This allows security teams to shift focus, modernize, and provide useful, secure alternatives for employees.</p>
<p><strong>Bring outliers into corporate security</strong></p>
<p>The most moving talk I attended was from <a href="http://en.wikipedia.org/wiki/Winn_Schwartau">Winn Schwartau</a> titled <em><a href="https://ae.rsaconference.com/US13/connect/sessionDetail.ww?SESSION_ID=1582">Solving the Cyber Security Hiring Crisis – Hiring the Un-Hireable</a></em>. He had a no holds barred talk about the &ldquo;hiring crisis&rdquo; in information security because we&rsquo;re looking for the wrong types of people. Winn claimed that we&rsquo;re looking for clean cut people for corporate security while we should be considering a larger applicant group. His critical point was that deception should be one of the few reasons (other than lack of skills) for not hiring someone and he offered up several questions to ask to look for deceptive behavior. Questions like &ldquo;How many times have you hacked illegally?&rdquo; and &ldquo;Do you illegally download music or movies?&rdquo; worked well in his experience.</p>
<p>He ended with a quote that I must emphasize:</p>
<blockquote>
<p>If it&rsquo;s important, you&rsquo;ll find a way. If not, you&rsquo;ll find an excuse.</p>
</blockquote>
<p><strong>Summary</strong></p>
<p>Overall, the conference was well worth the trip. The delegate pass price was quite steep but there were tons of conference organizers and security guards who were happy to help attendees. There was rarely a time where sessions where scheduled and none of the available sessions interested me. It was an awesome experience to see Vint Cerf in person and I&rsquo;d recommend taking the time to listen to him talk if you ever have the opportunity.</p>
<p>As a side note, I noticed that security awareness among conference attendees was extremely poor. I&rsquo;ll save that for another post.</p>
]]></content></item><item><title>Controlling sensitive company data means losing some control of it</title><link>https://major.io/2013/03/03/controlling-sensitive-company-data-means-losing-some-control-of-it/</link><pubDate>Sun, 03 Mar 2013 17:18:13 +0000</pubDate><guid>https://major.io/2013/03/03/controlling-sensitive-company-data-means-losing-some-control-of-it/</guid><description>This year&amp;rsquo;s RSA Conference was full of very useful content but the most useful session for me was a peer to peer discussion regarding BYOD on mobile devices. The session had room for about 25 people and many companies were represented. Some companies were huge, household names, while others were very small.
The discussion started around how to authenticate and manage mobile devices, but it soon ended up covering the handling of data on personal and company-issued devices.</description><content type="html"><![CDATA[<p>This year&rsquo;s <a href="http://www.rsaconference.com/events/2013/usa/index.htm">RSA Conference</a> was full of very useful content but the most useful session for me was a peer to peer discussion regarding BYOD on mobile devices. The session had room for about 25 people and many companies were represented. Some companies were huge, household names, while others were very small.</p>
<p>The discussion started around how to authenticate and manage mobile devices, but it soon ended up covering the handling of data on personal and company-issued devices. A corporate security leader for a large company said the healthiest shift for them was when they stopped focusing on the devices themselves and moved their focus to the data they wanted to protect. They found that they could lock down all the devices in the world, but their employees would mishandle the data no matter what actions they took to protect the endpoint.</p>
<p>That led me to start a ruckus on Twitter:</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>Which I soon followed with this:</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>The responses started piling up in a hurry. <em>(To see the verbatim responses for yourself, click the date on one the embedded tweets above.)</em> Here&rsquo;s a quick summary of the suggested ways to attack the problem from the tweets I received:</p>
<ul>
<li><strong>Education &amp; awareness</strong> - Ensure that users not only understand where they should keep confidential data but also ensure they understand how to classify the data they&rsquo;re handling.</li>
<li><strong>Provide alternatives</strong> - If users like the functionality of a particular product, try to purchase an enterprise version of the product or re-create the product internally. Users will be more likely to use the approved version of the product and the company will have a bit more control over the data.</li>
<li><strong>Top-down policies &amp; enforcement</strong> - Make policies that define where data can and cannot go and follow that up with enforcement and accountability.</li>
<li><strong>Deny access</strong> - Set firewall or DLP policies to disallow access to certain products while on the corporate network. This doesn&rsquo;t cover situations where employees are off the corporate network.</li>
</ul>
<p>Many people suggested a blend between educating, providing alternatives, and enforcement. This is a real change for corporate IT and security departments that would normally opt for denying access to unapproved applications entirely. This quickly turns into a game of cat-and-mouse in which there are no clear winners.</p>
<p>Take an example like Evernote. If I was blocked from accessing it at work, I could VPN into another location and send Evernote over the VPN. If VPN access was blocked, I could start an ssh proxy and send the Evernote traffic through it. If ssh was blocked, I could remotely access another system via RDP or VNC where Evernote was installed and use it there. The truly frustrated user might invest in a 3G/4G device and use that in the office instead. That&rsquo;s even worse for the security department since none of their traffic would be passing through the corporate network.</p>
<p>Here are my suggestions for protecting data at a modern company:</p>
<ol>
<li><strong>Listen to your users</strong> - Find out why users like a particular third party application and why they don&rsquo;t like the current tools provided by the company. Learn about the types of data they&rsquo;re storing on that third party application.</li>
<li><strong>Regain some control of your data through alternatives</strong> - If your users prefer a particular application, try to purchase an enterprise or self-hosted version of the application. Your users will be pleased since they get the functionality they expect and the security teams can gain a little more control over the data stored in the application.</li>
<li><strong>Make a solid data classification policy</strong> - Creating an easy to use data classification policy is the first step to securing your data through awareness. Employees need to identify the sensitivity of the data they&rsquo;re handling before they can know what they can and can&rsquo;t do with it. Make the data classifications easy to identify and ensure that users have an escalation point they can use when they have questions or they need to release sensitive data.</li>
<li><strong>Create enforcement policies</strong> - If a user deliberately disobeys corporate policy, this where the rubber meets the road. Ensure that the policy is fair to users of various technical levels within the company and vet it thoroughly with your legal and HR departments. These enforcement policies may be required by various compliance programs, so check to see if they&rsquo;re on paper but not enforced.</li>
<li><strong>Educate users about sensitive data</strong> - Humanize your data classification policy and help users understand how to identify and handle sensitive data. Remind employees about the importance of company data and what can happen if it was misplaced or stolen. There will be a <em>significant</em> amount of questions coming from this process so be sure that you&rsquo;re ready to tackle them. If you do this right, you&rsquo;ll get employees policing themselves and their peers.</li>
<li><strong>Rinse and repeat</strong> - Regularly check in with users to verify that the internal applications are meeting their needs. Go through the awareness work on a regular basis. When policies become dormant or ineffective, revise them to meet the current needs.</li>
</ol>
<p>This problem isn&rsquo;t going away anytime soon and it&rsquo;s rapidly evolving. Your corporate security department must evolve with it. A coworker of mine hit the nail on the head with this:</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>The best thing about this approach is that it scales better and is more effective than denying access. It takes a significant amount of work up front for a corporate security department, but it pays off in the end. Employees soon call out other employees for poor security hygiene and they become informal delegates of the corporate security team. Security can go viral in your organization just like the usage of third party tools.</p>
<p><strong>The key to success is driving security innovation within your company that equals or outpaces the innovation coming from third party applications.</strong></p>
<p>New tools and services may appear on a daily basis, but if your employees know what belongs there and what doesn&rsquo;t, they&rsquo;ll do your work for you.</p>
]]></content></item><item><title>More upgrades for icanhazip.com</title><link>https://major.io/2013/02/23/more-upgrades-for-icanhazip-com/</link><pubDate>Sat, 23 Feb 2013 20:15:24 +0000</pubDate><guid>https://major.io/2013/02/23/more-upgrades-for-icanhazip-com/</guid><description>The feature requests for icanhazip.com finally pushed me over the edge and I&amp;rsquo;ve made some significant changes. Here we go:
Get around proxies on port 81
Quite a few people had issues with local proxies that filtered traffic on port 80 and delivered the wrong results for their external IP address. You can now reach the site on port 81.
Get your external IP address over HTTPS
Some users reported that defensive network infrastructure mangled all of their web traffic to the site, so I&amp;rsquo;ve enabled SSL listeners for icanhazip.</description><content type="html"><![CDATA[<p>The feature requests for icanhazip.com finally pushed me over the edge and I&rsquo;ve made some significant changes. Here we go:</p>
<p><strong>Get around proxies on port 81</strong></p>
<p>Quite a few people had issues with local proxies that filtered traffic on port 80 and delivered the wrong results for their external IP address. You can now <a href="http://icanhazip.com:81/">reach the site on port 81</a>.</p>
<p><strong>Get your external IP address over HTTPS</strong></p>
<p>Some users reported that defensive network infrastructure mangled all of their web traffic to the site, so I&rsquo;ve <a href="https://icanhazip.com/">enabled SSL listeners</a> for icanhazip.com. Bear in mind that the SSL certificate is only valid for icanhazip.com and not the other subdomains (like ipv4.icanhazip.com). If you are using applications like curl to access subdomains, you&rsquo;ll need to use the <code>-k</code> argument, like this:</p>
<pre><code>$ curl https://icanhazip.com/
74.125.225.224
$ curl https://ipv4.icanhazip.com/
curl: (51) SSL peer certificate or SSH remote key was not OK
$ curl -k https://ipv4.icanhazip.com/
74.125.225.224
</code></pre><p><strong>Local icanhazip.com servers</strong></p>
<p>The site now exists in Dallas-Fort Worth (US), Chicago (US), and Maidenhead (UK). There are many new DNS records available to use:</p>
<ul>
<li>Random location: icanhazip.com, ipv4.icanhazip.com, ipv6.icanhazip.com</li>
<li>DFW: dfw.icanhazip.com, ipv4.dfw.icanhazip.com, ipv6.dfw.icanhazip.com</li>
<li>ORD: ord.icanhazip.com, ipv4.ord.icanhazip.com, ipv6.ord.icanhazip.com</li>
<li>UK: uk.icanhazip.com, ipv4.uk.icanhazip.com, ipv6.uk.icanhazip.com</li>
</ul>
<p>One of the HTTP response headers should confirm which node you&rsquo;re querying:</p>
<pre><code>$ curl -si icanhazip.com | grep NODE
X-ICANHAZNODE: ord.icanhazip.com
</code></pre><p><strong>Let me know what you think!</strong></p>
<p>If you have new ideas for features, let me know. Also, be sure to tell me if something&rsquo;s not working properly for you.</p>
]]></content></item><item><title>Six years of rackerhacker.com</title><link>https://major.io/2013/02/22/six-years-of-rackerhacker-com/</link><pubDate>Fri, 22 Feb 2013 18:10:20 +0000</pubDate><guid>https://major.io/2013/02/22/six-years-of-rackerhacker-com/</guid><description>It&amp;rsquo;s that time of year again:
$ whois rackerhacker.com | grep date Creation date: 14 Feb 2007 00:22:00 Expiration date: 14 Feb 2017 00:22:00 The blog is now six years old! Thanks to everyone who has followed it, commented, and suggested new topics for posts.
You may have noticed that I&amp;rsquo;m adding content a bit slower than I usually do, and that&amp;rsquo;s probably due to two big factors: we have a newborn at the house and my work at Rackspace has totally changed.</description><content type="html"><![CDATA[<p>It&rsquo;s that time of year again:</p>
<pre><code>$ whois rackerhacker.com | grep date
Creation date: 14 Feb 2007 00:22:00
Expiration date: 14 Feb 2017 00:22:00
</code></pre><p><strong>The blog is now six years old!</strong> Thanks to everyone who has followed it, commented, and suggested new topics for posts.</p>
<p>You may have noticed that I&rsquo;m adding content a bit slower than I usually do, and that&rsquo;s probably due to two big factors: we have a newborn at the house and <a href="/2012/11/26/reaching-a-new-milestone-and-making-some-big-changes/">my work at Rackspace has totally changed</a>. I&rsquo;ve received quite a few ideas for new posts from readers and I&rsquo;ve come up with some of my own. Thanks to some help from a friend, I should have some more posts on the <a href="http://www.rackspace.com/blog/">Rackspace Blog</a> as well this year.</p>
]]></content></item><item><title>Quick access to OpenPGP tasks with GPGTools in OS X</title><link>https://major.io/2013/02/08/quick-access-to-openpgp-tasks-with-gpgtools-in-os-x/</link><pubDate>Fri, 08 Feb 2013 19:05:30 +0000</pubDate><guid>https://major.io/2013/02/08/quick-access-to-openpgp-tasks-with-gpgtools-in-os-x/</guid><description>I&amp;rsquo;ve been a big fan of the GPGTools suite for Mac for quite a while but I discovered some neat features when right-clicking on a file in Finder today. It&amp;rsquo;s a bit disappointing that I didn&amp;rsquo;t find these sooner!
Encrypting files is simple: just click OpenPGP: Encrypt File and a window will pop asking you which key you&amp;rsquo;d like to use for encryption. You also have the option of encrypting it with a password.</description><content type="html"><![CDATA[<p><img src="/wp-content/uploads/2013/02/gpgtoolscontextmenu.jpg" alt="1"></p>
<p>I&rsquo;ve been a big fan of the <a href="https://www.gpgtools.org/">GPGTools suite for Mac</a> for quite a while but I discovered some neat features when right-clicking on a file in Finder today. It&rsquo;s a bit disappointing that I didn&rsquo;t find these sooner!</p>
<p>Encrypting files is simple: just click <strong>OpenPGP: Encrypt File</strong> and a window will pop asking you which key you&rsquo;d like to use for encryption. You also have the option of encrypting it with a password. Decrypting, signing, and validating files is easy and extremely fast. In addition, you&rsquo;ll get Growl notifications upon success or failure.</p>
<p>GPGTools also integrates with Mail.app to allow for seamless signing, encrypting, decrypting and verification of email content. There&rsquo;s a preview version available that integrates quite well with Mountain Lion&rsquo;s Mail.app, but you can only acquire it <a href="https://www.gpgtools.org/donate.html#donate-paypal">via donation</a>.</p>
]]></content></item><item><title>What my toddler taught me about information security</title><link>https://major.io/2013/01/13/what-my-toddler-taught-me-about-information-security/</link><pubDate>Sun, 13 Jan 2013 17:15:47 +0000</pubDate><guid>https://major.io/2013/01/13/what-my-toddler-taught-me-about-information-security/</guid><description>My new role has caused me to look at information security in a different way. It&amp;rsquo;s always been a hobby for me but I enjoy the challenge of making it my focus each day.
Many companies seem to make a natural progression in security as they grow larger, bring on larger accounts, or find themselves subject to regulation or compliance requirements. That gradual process is usually more straightforward than the reactive process brought on by a security breach and it ends up delivering better overall results for the company.</description><content type="html"><![CDATA[<p>My <a href="/2012/11/26/reaching-a-new-milestone-and-making-some-big-changes/">new role</a> has caused me to look at information security in a different way. It&rsquo;s always been a hobby for me but I enjoy the challenge of making it my focus each day.</p>
<p>Many companies seem to make a natural progression in security as they grow larger, bring on larger accounts, or find themselves subject to regulation or compliance requirements. That gradual process is usually more straightforward than the reactive process brought on by a security breach and it ends up delivering better overall results for the company.</p>
<p>This reactive process seems oddly similar to the way my son has learned to eat. Confused? Keep reading.</p>
<p><strong>Entirely oblivious</strong></p>
<p>This is how my son first got started. He was so busy trying to figure out how to eat that he had no idea how much of a mess he was making. Eventually, someone would either step in all of the dropped food or spilled juice and it would be all over the kitchen.</p>
<p>If you replace the food and juice with information at a small company, you can see how the same would apply. Many startups and small businesses are focused so heavily on building a product or brand that they forget about the importance of securing the data they are generating and collecting. Everything from trade secrets to sensitive customer data is at risk of being lost. Basic security measures are taken and there&rsquo;s usually no way to know if a breach has occurred and how deep the breach has gone.</p>
<p><strong>Purely reactionary</strong></p>
<p>Eventually my son realized that making a mess wasn&rsquo;t a good thing and he started to react whenever he ended up with a lap full of spaghetti. He would notice the problem and cry for someone else to come and help. I&rsquo;d clean him up and he was back to normal again. The food would end up in his lap again, he would cry, and I&rsquo;d be back to clean it up.</p>
<p>Companies find themselves in this situation when they&rsquo;ve been hit with a breach previously and a new issue has appeared. Their security stance has only changed a little and they&rsquo;re able to determine that something has happened after it has happened. Companies in this stage may consider creating a team focused on security issues or they may look to outside contractors or consultants for help. Much of the focus now shifts to answering &ldquo;how do we prevent this from happening again?&rdquo;</p>
<p><strong>Partially proactive</strong></p>
<p>As my son became more skillful at working with a fork and a spoon, he was able to be more focused on eating and he made fewer messes. They may have occurred less frequently but when they did occur, his clothes still needed to be washed and he was still quite upset. He knew what to watch out for and he knew which foods were going to present a particular challenge. It was obvious that he was putting in much more effort to eat spaghetti than he would with something simple like crackers.</p>
<p>This stage in a company&rsquo;s development usually involves a dedicated or semi-dedicated security team that is beginning to understand the threats and risks involved with the company&rsquo;s operation. They&rsquo;re putting focus in certain higher-risk areas but there&rsquo;s still not a lot of proactive work being done to limit the damage from security breaches. For example, a company might institute stricter firewall rules and OS patching for their most important servers but they might not have any security within their internal network. This would allow an attacker free reign over the environment if they can take over one of the servers.</p>
<p><strong>Passionately proactive</strong></p>
<p>When my son eats, he does quite a few things to ensure success. First off, he sits down and asks for his chair to be pushed in before he eats. He wants a paper towel close by in case something bad happens. With certain foods, he knows the chance of making a mess is higher and he tries to put less of it on his fork. He&rsquo;s determined to not let food get in his lap, and when it does, he wants to ensure that his clothes stay as clean as possible.</p>
<p>Companies that reach this stage have now realized the risks involved in the operation of their business and they&rsquo;ve determined how to reduce the impact of a breach. They&rsquo;re consciously aware that they&rsquo;re a target and they are taking an offensive security stance. These companies often test their own security measures to make sure that they&rsquo;re effective against the most frequently seen threats. Their security posture isn&rsquo;t perfect, but they are able to react more efficiently (and with less chaos) when a serious issue presents itself.</p>
<p><strong>So let&rsquo;s summarize…</strong></p>
<p>Some readers may think this post is way too generalized. However, the generalization is the point I&rsquo;m trying to make. Creating a security mindset within a company is generally the easy part; applying it is where things get tough. The concept of information security is actually quite simple: ensure that information is readily available to people who should be able to access it and ensure it&rsquo;s not available for people who shouldn&rsquo;t. If you&rsquo;re starting a small business or you&rsquo;re working for one right now, build your products and your infrastructure with security in mind. Your other option is to retrofit it later, but you&rsquo;ll surely make a mess.</p>
]]></content></item><item><title>Fixing the Lenovo X1 Carbon’s washed out display</title><link>https://major.io/2013/01/08/fixing-the-lenovo-x1-carbons-washed-out-display/</link><pubDate>Tue, 08 Jan 2013 16:30:54 +0000</pubDate><guid>https://major.io/2013/01/08/fixing-the-lenovo-x1-carbons-washed-out-display/</guid><description>Although the X1 Carbon has a much better looking display than the T430s, it still looked a bit washed out when I compared it to other monitors right next to it. The entire display had a weak blue tint and it was difficult to use for extended periods, especially at maximum brightness.
A quick Google search took me to a LaunchPad entry about a better ICC profile for the X1 Carbon.</description><content type="html"><![CDATA[<p>Although the X1 Carbon has a <a href="/2012/10/21/lenovo-thinkpad-t430s-review/">much better looking display than the T430s</a>, it still looked a bit washed out when I compared it to other monitors right next to it. The entire display had a weak blue tint and it was difficult to use for extended periods, especially at maximum brightness.</p>
<p>A quick Google search took me to a <a href="https://answers.launchpad.net/ubuntu-certification/+question/177299">LaunchPad entry</a> about a <a href="http://www.notebookcheck.net/Review-Lenovo-ThinkPad-X1-Subnotebook.55370.0.html">better ICC profile for the X1 Carbon</a>. After applying the ICC file via GNOME Control Center&rsquo;s Color panel, the display looks fantastic.</p>
<p>Feel free to download a copy of the color profile and try it for yourself:</p>
<ul>
<li><a href="http://www.notebookcheck.net/uploads/tx_nbc2/Lenovo_ThinkPad_X1_1366x768_glare_LP133WH2-TLM5.icc">Original Link</a></li>
</ul>
]]></content></item><item><title>Handy settings for the touchpad/clickpad in the Lenovo X1 Carbon</title><link>https://major.io/2012/12/28/handy-settings-for-the-touchpadclickpad-in-the-lenovo-x1-carbon/</link><pubDate>Fri, 28 Dec 2012 16:15:42 +0000</pubDate><guid>https://major.io/2012/12/28/handy-settings-for-the-touchpadclickpad-in-the-lenovo-x1-carbon/</guid><description>UPDATE:I&amp;rsquo;ve found a better configuration via another X1 Carbon user and there&amp;rsquo;s a new post with all the details.
The Lenovo X1 Carbon comes with a pretty useful clickpad just below the keyboard, but the default synaptics settings in X from a Fedora 17 installation aren&amp;rsquo;t the best for this particular laptop. I found some tips about managing clickpads in a Github Gist about the Samsung Series 9 and I adjusted the values for the X1.</description><content type="html"><![CDATA[<p><em><!-- raw HTML omitted -->UPDATE:<!-- raw HTML omitted --> I&rsquo;ve found a better configuration via another X1 Carbon user and there&rsquo;s a <a href="/2013/08/24/get-a-rock-solid-linux-touchpad-configuration-for-the-lenovo-x1-carbon/">new post with all the details</a>.</em></p>
<hr>
<p>The Lenovo X1 Carbon comes with a pretty useful clickpad just below the keyboard, but the default synaptics settings in X from a Fedora 17 installation aren&rsquo;t the best for this particular laptop. I found some tips about managing clickpads in a <a href="https://gist.github.com/2382480">Github Gist about the Samsung Series 9</a> and I adjusted the values for the X1. To get my configuration, just create <code>/etc/X11/xorg.conf.d/10-synaptics.conf</code> and toss this data in there:</p>
<pre><code>Section &quot;InputClass&quot;
  Identifier &quot;touchpad catchall&quot;
  Driver &quot;synaptics&quot;
  MatchIsTouchpad &quot;on&quot;
  MatchDevicePath &quot;/dev/input/event*&quot;
  Option &quot;TapButton1&quot; &quot;1&quot;
  Option &quot;TapButton2&quot; &quot;3&quot;
  Option &quot;TapButton3&quot; &quot;2&quot;
  Option &quot;VertTwoFingerScroll&quot; &quot;on&quot;
  Option &quot;HorizTwoFingerScroll&quot; &quot;on&quot;
  Option &quot;HorizHysteresis&quot; &quot;50&quot;
  Option &quot;VertHysteresis&quot; &quot;50&quot;
  Option &quot;PalmDetect&quot;    &quot;1&quot;
  Option &quot;PalmMinWidth&quot;  &quot;5&quot;
  Option &quot;PalmMinZ&quot;      &quot;40&quot;
EndSection
</code></pre><p>There are a few important settings here to note:</p>
<ul>
<li><strong>TapButtonX</strong> – this sets up the single, double and triple taps to match up to left, right and middle mouse clicks respectively</li>
<li><strong>Vert/HorizHysteresis</strong> – reduces movement during and between taps</li>
<li><strong>Palm</strong>* – enables palm detection while you&rsquo;re typing with some reasonable settings</li>
</ul>
<p>You will need to restart X (or reboot) to apply these settings from the configuration file. If you want to test the settings before restarting, you can apply individual adjustments with <code>synclient</code> without any restarts:</p>
<pre><code>synclient &quot;HorizHysteresis=50&quot;
</code></pre>]]></content></item><item><title>Launch applications quickly with dmenu in XFCE</title><link>https://major.io/2012/12/27/launch-applications-quickly-with-dmenu-in-xfce/</link><pubDate>Thu, 27 Dec 2012 21:09:43 +0000</pubDate><guid>https://major.io/2012/12/27/launch-applications-quickly-with-dmenu-in-xfce/</guid><description>Ever since I saw QuickSilver for the first time, I&amp;rsquo;ve been hooked on quick application launchers. I&amp;rsquo;ve struggled to find a barebones, auto-completing application launcher in Linux for quite some time. My search has ended with dmenu.
I stumbled upon dmenu after trying out the i3 tiling window manager and I was hooked almost immediately. It&amp;rsquo;s extremely fast, unobtrusive, and the auto-completion is really intuitive. Another added bonus is that there is no daemon or window manager hook required for the launcher to operate.</description><content type="html"><![CDATA[<p>Ever since I saw <a href="http://en.wikipedia.org/wiki/Quicksilver_(software)">QuickSilver</a> for the first time, I&rsquo;ve been hooked on quick application launchers. I&rsquo;ve struggled to find a barebones, auto-completing application launcher in Linux for quite some time. My search has ended with <a href="http://tools.suckless.org/dmenu/">dmenu</a>.</p>
<p>I stumbled upon dmenu after trying out the <a href="http://i3wm.org/">i3 tiling window manager</a> and I was hooked almost immediately. It&rsquo;s extremely fast, unobtrusive, and the auto-completion is really intuitive. Another added bonus is that there is no daemon or window manager hook required for the launcher to operate.</p>
<p>Installing dmenu on Fedora is as easy as:</p>
<pre><code>yum install dmenu
</code></pre><p>XFCE is my desktop environment of choice and the dmenu integration is pretty simple:</p>
<ul>
<li><strong>Applications Menu</strong> &gt; <strong>Settings</strong> &gt; <strong>Keyboard</strong></li>
<li>Click the <strong>Application Shortcuts</strong> tab</li>
<li>Click <strong>Add</strong></li>
<li>In the <strong>Command</strong> box, enter <code>/usr/bin/dmenu</code> and press <strong>OK</strong></li>
<li>On the next screen, enter a key combination to launch dmenu (I use LCTRL-SPACE)</li>
<li>Click <strong>OK</strong></li>
</ul>
<p>From now on, you can press your key combination and start typing the name of any executable application in your path for dmenu to run. If you launch dmenu accidentally, just press ESC to close it.</p>
]]></content></item><item><title>Reaching a new milestone and making some big changes</title><link>https://major.io/2012/11/26/reaching-a-new-milestone-and-making-some-big-changes/</link><pubDate>Mon, 26 Nov 2012 15:00:04 +0000</pubDate><guid>https://major.io/2012/11/26/reaching-a-new-milestone-and-making-some-big-changes/</guid><description>This is my 500th post on this blog! I&amp;rsquo;m really grateful for the constant comments, questions, and even the complaints (really!) that I receive about the topics discussed here throughout the years. The sole reason I keep this blog going is for the readers and I hope you&amp;rsquo;re able to get value out of it over time. (If you don&amp;rsquo;t, be sure to let me know so I can make some changes.</description><content type="html"><![CDATA[<p><strong>This is my 500th post on this blog!</strong> I&rsquo;m really grateful for the constant comments, questions, and even the complaints (really!) that I receive about the topics discussed here throughout the years. The sole reason I keep this blog going is for the readers and I hope you&rsquo;re able to get value out of it over time. (If you don&rsquo;t, be sure to let me know so I can make some changes.)</p>
<p>With that said, the best segue I can find for the rest of this post is this quote that I first heard when I was a kid:</p>
<blockquote>
<p>&ldquo;I alone cannot change the world, but I can cast a stone across the waters to create many ripples.&rdquo; - Mother Teresa</p>
</blockquote>
<p>I&rsquo;ll reach my six year Rackspace anniversary in December and I&rsquo;ve spent just shy of the last four years working on Rackspace&rsquo;s cloud virtualization products. It started shortly after the Slicehost acquisition and I was on teams that helped to expand Slicehost, created Cloud Servers, and most recently, launched a new Cloud Servers infrastructure powered by OpenStack. Being able to participate in these big changes and work alongside some of the best technical folks (and friends) has been an amazing experience and I&rsquo;m extremely lucky to be a small part of what we&rsquo;ve made.</p>
<p>Walking away from that challenging work and those amazing people isn&rsquo;t easy. However, I&rsquo;m going to give it a try.</p>
<p>I&rsquo;ve accepted a position as Rackspace&rsquo;s Chief Security Architect and I&rsquo;m transitioning into that position over the next few weeks. There are quite a few familiar faces in this part of the business at Rackspace and I have a strong team of knowledgeable security architects to lead. It certainly won&rsquo;t be an easy road to travel but I&rsquo;m glad to have the opportunity to make a difference along with my team. Also, my team is expanding and <a href="http://jobs.rackspace.com/job/San-Antonio-Senior-Security-Architect-US-Job-TX-78201/2223051/">we&rsquo;re in need of some talented people</a>!</p>
<p>Some of the topics on this blog might change a little but please don&rsquo;t worry: I&rsquo;m still a Linux nerd at heart.</p>
<p>The other big change is that <a href="http://lists.fedoraproject.org/pipermail/devel-announce/2012-November/000993.html">I&rsquo;ve been appointed</a> to the <a href="http://fedoraproject.org/wiki/Board">Fedora Board</a>. I&rsquo;ve been a long time Fedora user (since Core 2 in 2004) and I&rsquo;m eager to continue some of the great work that has been done in the past. I&rsquo;m also a new Fedora Ambassador and I&rsquo;ll be glad to help you get started or get more out of Fedora if you need a hand.</p>
<p>If you follow OpenStack closely and you enjoy using <a href="http://rackerhacker.github.com/supernova/">supernova</a>, I&rsquo;ll still be maintaining that project since I still use OpenStack clouds regularly.</p>
]]></content></item><item><title>Relocating a python virtual environment</title><link>https://major.io/2012/11/25/relocating-a-python-virtual-environment/</link><pubDate>Sun, 25 Nov 2012 21:27:47 +0000</pubDate><guid>https://major.io/2012/11/25/relocating-a-python-virtual-environment/</guid><description>Python&amp;rsquo;s virtual environment capability is extremely handy for situations where you don&amp;rsquo;t want the required modules for a particular python project to get mixed up with your system-wide installed modules. If you work on large python projects (like OpenStack), you&amp;rsquo;ll find that the applications may require certain versions of python modules to operate properly. If these versions differ from the system-wide python modules you already have installed, you might get unexpected results when you try to run the unit tests.</description><content type="html"><![CDATA[<p>Python&rsquo;s <a href="http://pypi.python.org/pypi/virtualenv">virtual environment capability</a> is extremely handy for situations where you don&rsquo;t want the required modules for a particular python project to get mixed up with your system-wide installed modules. If you work on large python projects (like <a href="http://openstack.org/">OpenStack</a>), you&rsquo;ll find that the applications may require certain versions of python modules to operate properly. If these versions differ from the system-wide python modules you already have installed, you might get unexpected results when you try to run the unit tests.</p>
<p>If you build a virtual environment and inspect the files found within the <em>bin</em> directory of the virtual environment, you&rsquo;ll find that the first line in the executable scripts is set to use the python version specific to that virtual environment. Here&rsquo;s an example from a virtual environment containing the OpenStack glance project:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#586e75">#!/home/major/glance/.venv/bin/python</span>
<span style="color:#586e75"># EASY-INSTALL-SCRIPT: &#39;glance==2013.1&#39;,&#39;glance-api&#39;</span>
__requires__ <span style="color:#719e07">=</span> <span style="color:#2aa198">&#39;glance==2013.1&#39;</span>
<span style="color:#719e07">import</span> pkg_resources
pkg_resources<span style="color:#719e07">.</span>run_script(<span style="color:#2aa198">&#39;glance==2013.1&#39;</span>, <span style="color:#2aa198">&#39;glance-api&#39;</span>)
</code></pre></div><p>However, what if I wanted to take this virtual environment and place it somewhere else on the server where multiple people could use it? The path in the first line of the scripts in <em>bin</em> will surely break.</p>
<p>The first option is to make the virtual environment relocatable. This can produce unexpected results for some software projects, so be sure to test it out before trying to use it in a production environment.</p>
<pre><code>$ virtualenv --relocatable .venv
</code></pre><p>A quick check of the same python file now shows this:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#586e75">#!/usr/bin/env python2.6</span>

<span style="color:#719e07">import</span> os; activate_this<span style="color:#719e07">=</span>os<span style="color:#719e07">.</span>path<span style="color:#719e07">.</span>join(os<span style="color:#719e07">.</span>path<span style="color:#719e07">.</span>dirname(os<span style="color:#719e07">.</span>path<span style="color:#719e07">.</span>realpath(__file__)), <span style="color:#2aa198">&#39;activate_this.py&#39;</span>); <span style="color:#b58900">execfile</span>(activate_this, <span style="color:#b58900">dict</span>(__file__<span style="color:#719e07">=</span>activate_this)); <span style="color:#719e07">del</span> os, activate_this

<span style="color:#586e75"># EASY-INSTALL-SCRIPT: &#39;glance==2013.1&#39;,&#39;glance-api&#39;</span>
</code></pre></div><p>This allows for the path to the activate_this.py script to be determined at runtime and allows you to move your virtual environment wherever you like.</p>
<p>In situations where one script within <em>bin</em> would import another script within <em>bin</em>, things can get a little dicey. These are edge cases, of course, but you can get a similar effect by adjusting the path in the first line of each file within <em>bin</em> to the new location of the virtual environment. If you move the virtual environment again, be sure to alter the paths again with <code>sed</code>.</p>
]]></content></item><item><title>Fixing finicky Bluetooth on the Samsung Galaxy S III</title><link>https://major.io/2012/11/20/fixing-finicky-bluetooth-on-the-samsung-galaxy-s-iii/</link><pubDate>Tue, 20 Nov 2012 13:47:51 +0000</pubDate><guid>https://major.io/2012/11/20/fixing-finicky-bluetooth-on-the-samsung-galaxy-s-iii/</guid><description>The biggest gripe I have about my Android phone is that the Bluetooth connectivity is very finicky with my car. Sometimes the phone and car won&amp;rsquo;t connect automatically when I start my car and there are other times where the initial connection is fine but then the car loses the connection to the phone while I&amp;rsquo;m driving. The problem crops up in multiple cars and the biggest suspect I&amp;rsquo;ve found so far is the Galaxy S III&amp;rsquo;s use of Bluetooth Low Energy (BLE).</description><content type="html"><![CDATA[<p>The biggest gripe I have about my Android phone is that the Bluetooth connectivity is very finicky with my car. Sometimes the phone and car won&rsquo;t connect automatically when I start my car and there are other times where the initial connection is fine but then the car loses the connection to the phone while I&rsquo;m driving. The problem crops up in multiple cars and the biggest suspect I&rsquo;ve found so far is the Galaxy S III&rsquo;s use of <a href="http://en.wikipedia.org/wiki/Bluetooth_low_energy">Bluetooth Low Energy (BLE)</a>.</p>
<p>I stumbled upon an application in the Google Play Store called <a href="https://play.google.com/store/apps/details?id=org.floodping.BluetoothKeepalive&amp;hl=en">Bluetooth Keepalive</a> and decided to spend $1.50 to see if it could fix my problem. The application itself is quite simple:</p>
<p><img src="/wp-content/uploads/2012/11/bluetooth_keepalive.jpg" alt="3"></p>
<p>I configured it to start at boot and run as a background service via the configuration menu. After two days of using the application, I haven&rsquo;t had any weird Bluetooth issues in the car. My phone connects as soon as I start my car and it stays connected throughout my trip. There were some situations where my phone used to think it was connected to my car even when I was miles away and those problems are gone as well. Battery life seems to be unaffected by the change.</p>
<p>I&rsquo;m currently running CyanogenMod 10 Nightly w/Android 4.1.2 on an AT&amp;T Galaxy S III (SGH-I747). Your mileage might vary on other ROM&rsquo;s and models.</p>
]]></content></item><item><title>Log Android events remotely to a syslog server</title><link>https://major.io/2012/11/04/log-android-events-remotely-to-a-syslog-server/</link><pubDate>Sun, 04 Nov 2012 20:47:39 +0000</pubDate><guid>https://major.io/2012/11/04/log-android-events-remotely-to-a-syslog-server/</guid><description>I&amp;rsquo;m still quite pleased with my Samsung Galaxy SIII but there are some finicky Bluetooth issues with my car that I simply can&amp;rsquo;t figure out. After discovering logcat, I wondered if there was a way to get logs sent from an Android device to a remote syslog server. It&amp;rsquo;s certainly possible and it actually works quite well.
My phone is currently rooted with CyanogenMod 10 installed. Some of these steps will require rooting your device.</description><content type="html"><![CDATA[<p>I&rsquo;m still quite pleased with my Samsung Galaxy SIII but there are some finicky Bluetooth issues with my car that I simply can&rsquo;t figure out. <a href="https://twitter.com/rackerhacker/status/261292543965274113">After discovering logcat</a>, I wondered if there was a way to get logs sent from an Android device to a remote syslog server. It&rsquo;s certainly possible and it actually works quite well.</p>
<p>My phone is currently rooted with CyanogenMod 10 installed. Some of these steps <strong>will require rooting your device.</strong> Be sure to fully understand the implications of gaining root access on your particular device <strong>before</strong> trying it.</p>
<p>Get started by installing <a href="https://play.google.com/store/apps/details?id=com.keramidas.TitaniumBackup&amp;hl=en">Titanium Backup</a> and <a href="https://play.google.com/store/apps/details?id=sk.madzik.android.logcatudp">Logcat to UDP</a>. Once they&rsquo;re installed, you&rsquo;ll need to enable USB debugging by accessing <strong>Settings &gt; Developer Options</strong>:</p>
<p><img src="/wp-content/uploads/2012/11/2012-11-04-14.31.59.jpg" alt="4"></p>
<p>Now, run Titanium Backup and click the <strong>Backup/Restore</strong> tab at the top. Find the “Logcat to UDP 0.5” application and hold your finger on it for a few seconds. Press <strong>Convert to system app</strong> and wait for that to complete:</p>
<p><img src="/wp-content/uploads/2012/11/2012-11-04-14.36.18.jpg" alt="5"></p>
<p>Now, run the <strong>Logcat to UDP</strong> application and configure it. Put in a server IP address for the remote syslog server and choose a remote port where your syslog server is listening. Be sure to check the <strong>Filter log messages</strong> box and put in a reasonable set of things to watch. My standard filter is:</p>
<pre><code>Sensors:S dalvikvm:S MP-Decision:S overlay:S RichInputConnection:S *:V
</code></pre><p>That filter says that I don&rsquo;t want to see data from the Sensors process (and some other chatty daemons) but I want verbose logs from everything else. The full details on logcat filters can be found in <a href="http://developer.android.com/tools/debugging/debugging-log.html#filteringOutput">Google&rsquo;s Android Developer Documentation</a>.</p>
<p>When all that is done, you can begin receiving syslog data pretty quickly on a CentOS or Fedora server. For CentOS, you only need to make a small adjustment to /etc/rsyslog.conf to begin receiving logs:</p>
<pre><code># Provides UDP syslog reception
$ModLoad imudp
$UDPServerRun 514
</code></pre><p>The standard port is 514, but be sure to change it to match your configuration in the Logcat to UDP application on your phone. Restart rsyslog and you should be able to see logs flowing in from your Android device:</p>
<pre><code># /etc/init.d/rsyslog restart
Shutting down system logger:                               [  OK  ]
Starting system logger:                                    [  OK  ]
# tail /var/log/messages
Nov  4 20:44:04 home.local Iridium: E/ThermalDaemon(  264): ACTION: CPU - Setting CPU[0] to 1512000
Nov  4 20:44:04 home.local Iridium: E/ThermalDaemon(  264): ACTION: CPU - Setting CPU[1] to 1512000
Nov  4 20:44:04 home.local Iridium: E/ThermalDaemon(  264): Fusion mitigation failed - QMI registration incomplete
Nov  4 20:44:07 home.local Iridium: I/ActivityManager(  624): START {act=android.intent.action.MAIN cat=[android.intent.category.HOME] flg=0x10200000 cmp=com.cyanogenmod.trebuchet/.Launcher u=0} from pid 624
Nov  4 20:44:07 home.local Iridium: I/ActivityManager(  624): START {act=android.intent.action.MAIN cat=[android.intent.category.HOME] flg=0x10200000 cmp=com.cyanogenmod.trebuchet/.Launcher u=0} from pid 624
Nov  4 20:44:07 home.local Iridium: I/ActivityManager(  624): START {act=android.intent.action.MAIN cat=[android.intent.category.HOME] flg=0x10200000 cmp=com.cyanogenmod.trebuchet/.Launcher u=0} from pid 624
Nov  4 20:44:08 home.local Iridium: I/ActivityManager(  624): START {act=android.intent.action.MAIN cat=[android.intent.category.HOME] flg=0x10200000 cmp=com.cyanogenmod.trebuchet/.Launcher u=0} from pid 624
Nov  4 20:44:09 home.local Iridium: E/ThermalDaemon(  264): Sensor 'tsens_tz_sensor0' - alarm raised 1 at 57.0 degC
Nov  4 20:44:09 home.local Iridium: E/ThermalDaemon(  264): ACTION: CPU - Setting CPU[0] to 1134000
Nov  4 20:44:09 home.local Iridium: E/ThermalDaemon(  264): ACTION: CPU - Setting CPU[1] to 1134000
</code></pre><p>If you&rsquo;re not seeing logs on your remote server, be sure to check the remote server&rsquo;s firewall since the default rules on a CentOS or Fedora server will block syslog traffic. If you want to generate logs quickly for testing in CyanogenMod, just repeatedly press the home button. A log line from the trebuchet launcher should appear each time.</p>
]]></content></item><item><title>Using git clean to remove subdirectories containing git repositories</title><link>https://major.io/2012/10/24/using-git-clean-to-remove-subdirectories-containing-git-repositories/</link><pubDate>Wed, 24 Oct 2012 20:44:59 +0000</pubDate><guid>https://major.io/2012/10/24/using-git-clean-to-remove-subdirectories-containing-git-repositories/</guid><description>I had a peculiar situation today where I cloned a repository into a directory which was inside another repository. Here&amp;rsquo;s what I was doing:
$ git clone git://gitserver/repo1.git repo1 $ cd repo1 $ git clone git://gitserver/repo2.git repo2 $ git clean -fxd Removing repo2/ $ ls -d repo2 repo2 The second repository existed even after a git clean -fxd. I stumbled upon a GitHub page within the capistrano project that explained the problem - an extra -f was required:</description><content type="html"><![CDATA[<p>I had a peculiar situation today where I cloned a repository into a directory which was inside another repository. Here&rsquo;s what I was doing:</p>
<pre><code>$ git clone git://gitserver/repo1.git repo1
$ cd repo1
$ git clone git://gitserver/repo2.git repo2
$ git clean -fxd
Removing repo2/
$ ls -d repo2
repo2
</code></pre><p>The second repository existed even after a <code>git clean -fxd</code>. I <a href="https://github.com/capistrano/capistrano/issues/135">stumbled upon a GitHub page</a> within the capistrano project that explained the problem - an extra <code>-f</code> was required:</p>
<pre><code>$ git clean -ffxd
Removing repo2/
$ ls -d repo2
ls: cannot access repo2: No such file or directory
</code></pre>]]></content></item><item><title>Lenovo ThinkPad T430s review</title><link>https://major.io/2012/10/21/lenovo-thinkpad-t430s-review/</link><pubDate>Sun, 21 Oct 2012 21:15:44 +0000</pubDate><guid>https://major.io/2012/10/21/lenovo-thinkpad-t430s-review/</guid><description>This post covers the second half of my experience moving back to a Linux desktop but I figured it was a good opportunity to focus on the ThinkPad T430s itself as well as the Lenovo ordering experience. If you follow me on Twitter, you know about my service experience. I&amp;rsquo;ll save that for the end of this post.
This post is a little on the long side, so here&amp;rsquo;s a TL;DR for you if you&amp;rsquo;re in a big hurry:</description><content type="html"><![CDATA[<p>This post covers the second half of my <a href="/2012/10/12/going-back-to-linux-as-a-desktop/">experience moving back to a Linux desktop</a> but I figured it was a good opportunity to focus on the <a href="http://www.lenovo.com/products/us/laptop/thinkpad/t-series/t430s/">ThinkPad T430s</a> itself as well as the Lenovo ordering experience. If you follow me on Twitter, you know about my service experience. I&rsquo;ll save that for the end of this post.</p>
<p>This post is a little on the long side, so here&rsquo;s a TL;DR for you if you&rsquo;re in a big hurry:</p>
<ul>
<li><strong>Good:</strong> build quality, port quantity/location, input devices, battery life, quiet operation</li>
<li><strong>Bad:</strong> LCD display is very washed out and has a blue tint, poor sales support from Lenovo</li>
<li><strong>Suggestions:</strong> Don&rsquo;t buy via Lenovo, try GovConnection and get faster delivery with better service</li>
</ul>
<p><strong>The Laptop</strong></p>
<p>If you asked me for a one-sentence description of the T430s, I&rsquo;d have to say it&rsquo;s a well-built, lightweight laptop with a good keyboard and a less than mediocre screen.</p>
<p>The island-style keyboard was very easy for me to use coming from a MacBook Pro with chiclet keys. The spacing between the keys and the size of the keys themselves were good. I kept pushing the Function key when I meant to push Control, but that can be quickly swapped in the BIOS to make things easier.</p>
<p>Even coming from the MacBook&rsquo;s amazing trackpad, the <strong>trackpad on the ThinkPad was superb</strong>. It tracked gestures and taps extremely well without much configuration in Linux or Windows. It&rsquo;s light years ahead of the Samsung Series 9&rsquo;s trackpad and it&rsquo;s marginally better than the latest Dell laptops. The &ldquo;nipple&rdquo; controller wedged in the keyboard was easy to use and the extra set of mouse buttons below the keyboard (but above the trackpad) were convenient.</p>
<p>I really like having the hardware WiFi on/off switch on the front right side of the laptop for situations where I want to ensure my laptop doesn&rsquo;t start searching for access points before I can be sure it&rsquo;s connecting to the right one. The USB ports were well placed and the &ldquo;always on&rdquo; port on the back is handy for charging phones and tablets when the laptop is powered off (you can disable that in the BIOS if you prefer). The fingerprint reader hasn&rsquo;t been tested since there aren&rsquo;t any open source drivers available for it in Linux.</p>
<p>It&rsquo;s apparent that <strong>build quality is above average</strong> with this laptop. It&rsquo;s certainly not terribly attractive (when compared to a Mac), but for a solid business laptop, it&rsquo;s ahead of the curve. The screen hinges are tight and they don&rsquo;t flex even when typing on a wobbly surface. This really helps when you&rsquo;re using the webcam with the laptop resting on your legs. The ThinkLight above the webcam is a little quirky (this is my first ThinkPad) but it is really useful in lowlight situations.</p>
<p>Now, about that screen. I ordered my laptop with the 1600×900 HD+ screen (best one available). <strong>The color representation is downright terrible.</strong> Almost everything is washed out with a blue tint. If you open a web page with a mostly white background, the text is readable but it hurts my eyes to read it. You can almost see gaps between the pixels on the screen at regular intervals and it gets really distracting when you&rsquo;re editing photos. Even after applying different monitor profiles in Linux and Windows, I&rsquo;ve found the screen to be frustrating to use. The panel on mine is a Samsung panel and I&rsquo;d expect a better performing screen from them.</p>
<p>Outside of the screen itself, the <strong>video performance of the Intel HD4000 is impressive</strong>. Onboard GPU&rsquo;s have really come a long way. I hooked up a second monitor via the DisplayPort and found that the graphics performance was still extremely good. You can play games like Civilization V on this laptop with onboard graphics pretty easily.</p>
<p>All in all, I really do like the ThinkPad. If the screen doesn&rsquo;t bother you, the remainder of this laptop is very convenient and powerful. However, for my use, I need something that performs well for business work as well as creative work. <strong>I&rsquo;ve yet to find something better than the MacBooks for this kind of workload.</strong></p>
<p><strong>The Service</strong></p>
<p>I generally try to start out with something positive when I try to review something, but the only positive thing I can say about Lenovo&rsquo;s ordering experience is that it&rsquo;s consistent. <strong>Consistently bad.</strong> Here&rsquo;s a timeline of my first order:</p>
<ul>
<li>Sep 3 - Ordered laptop. Sales page said laptop would ship around Sep 12.</li>
<li>Sep 4 - Order confirmed. Ship date pushed to Sep 27.</li>
<li>Sep 8 - Order status showed &ldquo;Released to Manufacturing&rdquo;.</li>
<li>Sep 10 - Order status page showed laptop shipped Sep 9 and would be delivered Sep 16.</li>
<li>Sep 12 - Received an email saying a part had delayed my shipment.</li>
<li>Sep 17 - Order status page shows laptop shipped Sep 14 and would be delivered Sep 21. I emailed Lenovo for more detail.</li>
<li>Sep 18 - Lenovo representative replies saying the graphics card was the constrained part. Due to ship in 3-4 weeks or less. I emailed back asking about canceling the order. My email was never acknowledged.</li>
<li>Sep 19 - I emailed Lenovo stating that I wanted my order cancelled immediately. My email was never acknowledged. I received an automated email stating that my order was delayed and would ship within 30 days. I called Lenovo&rsquo;s support line to cancel and waited on hold for almost two hours. Gave up.</li>
<li>Sep 20 - I called Lenovo again and my call was answered after 90 minutes on hold. The representative tried to talk me out of canceling several times and then finally canceled it. My cancellation was only a &ldquo;request&rdquo;, not a guarantee. I was referred to sales and they wanted me to order a different laptop - I declined.</li>
<li>Sep 21 - My order was confirmed cancelled.</li>
</ul>
<p>The salesperson suggested ordering a different laptop without the NVIDIA Optimus graphics card, so I did that on September 21. The order page showed a ship date of September 27, so I was quite pleased. As soon as I paid for the order, the ship date immediately slid out to October 16. Needless to say, I felt like I&rsquo;d been bait-and-switched once again.</p>
<p>It&rsquo;s important to note that through both orders, Lenovo&rsquo;s public-facing order status page worked while the internal order status page accessed via my account page showed timeouts. The internal order status page hasn&rsquo;t worked before, during or after shipping at all after multiple attempts. I&rsquo;ve notified them more than once about it so they could make repairs.</p>
<p>Someone on Twitter suggested trying <a href="http://govconnection.com">GovConnection</a> to order a ThinkPad since they keep models in stock with fast shipping. I ordered one on a Sunday and it shipped on a Monday. The back panel behind the screen was badly damaged and they cross-shipped me a replacement the next day. Their service has been superb and they provided timely updates for my order.</p>
<p>In the end, Lenovo did actually ship my second laptop (without the NVIDIA Optimus card) and it did arrive slightly ahead of schedule. I&rsquo;ll be sending it back to them since I already received a ThinkPad from GovConnection.</p>
<p>Some of you might be saying that I should expect some delays when a laptop is built to order. I&rsquo;m generally fine with that and I&rsquo;ve had minor delays from Dell and Apple in the past with previous orders. The big difference is that the other companies warned me about the delays prior to purchase and also warned me about the parts I added that might cause delays to my order. That bit of forward thinking allowed me to decide whether a certain part was important to me or if I was able to wait for the product to arrive. When it comes down to communication, Lenovo has a lot to learn.</p>
]]></content></item><item><title>Proud to be a part of OpenStack at Rackspace</title><link>https://major.io/2012/10/17/proud-to-be-a-part-of-openstack-at-rackspace/</link><pubDate>Thu, 18 Oct 2012 03:34:37 +0000</pubDate><guid>https://major.io/2012/10/17/proud-to-be-a-part-of-openstack-at-rackspace/</guid><description>Troy Toman delivered a great keynote this morning about OpenStack and how Rackspace uses it:
I&amp;rsquo;m extremely glad to be a tiny part of the OpenStack story and I&amp;rsquo;m proud to see where it&amp;rsquo;s going today. My goals are to make OpenStack environments easier to deploy, maintain and administer. Along with other Rackers, we&amp;rsquo;re making this happen each day and we plan to share our successes and hardships with the community.</description><content type="html"><![CDATA[<p>Troy Toman delivered a great keynote this morning about OpenStack and how Rackspace uses it:</p>
<p><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --></p>
<p>I&rsquo;m extremely glad to be a tiny part of the OpenStack story and I&rsquo;m proud to see where it&rsquo;s going today. My goals are to make OpenStack environments easier to deploy, maintain and administer. Along with other Rackers, we&rsquo;re making this happen each day and we plan to share our successes and hardships with the community.</p>
<p>If you&rsquo;re interested in working on OpenStack at Rackspace, feel free to reach out to me or <a href="http://rackertalent.com/">learn more about our open positions</a>.</p>
]]></content></item><item><title>Going back to Linux as a desktop</title><link>https://major.io/2012/10/12/going-back-to-linux-as-a-desktop/</link><pubDate>Fri, 12 Oct 2012 13:43:01 +0000</pubDate><guid>https://major.io/2012/10/12/going-back-to-linux-as-a-desktop/</guid><description>Although I&amp;rsquo;ve been exclusively using a Mac for everything but servers since about 2008, I found myself considering a move back to Linux on the desktop after seeing how some people were using it at LinuxCon. My conversion from the iPhone to Android was rocky for a very brief period and now I can&amp;rsquo;t think of a reason to ever go back. I approached Linux in the same way and ordered a new ThinkPad shortly after returning from the conference.</description><content type="html"><![CDATA[<p>Although I&rsquo;ve been exclusively using a Mac for everything but servers since
about 2008, I found myself considering a move back to Linux on the desktop
after seeing how some people were using it at LinuxCon. My <a href="/2012/09/06/one-week-with-android/">conversion from
the iPhone to Android</a> was rocky for a very brief period and now I can&rsquo;t
think of a reason to ever go back. I approached Linux in the same way and
ordered a new ThinkPad shortly after returning from the conference.</p>
<p>The ThinkPad ordering experience was one of the worst retail experiences I&rsquo;ve
had so far but that&rsquo;s a separate discussion for a separate post (that&rsquo;s on the
way soon). This post is focused only on my experience getting back into a
Linux desktop for the first time in four years.</p>
<p><strong>The Good</strong></p>
<p>Linux hardware support has come a really long way over the past few years. All
of my hardware was recognized and configured in Fedora 17 without any action
on my part. The fingerprint reader has some proprietary firmware that couldn&rsquo;t
be automatically loaded (which didn&rsquo;t bother me much). Getting network
connectivity via ethernet, WiFi and even a 4G USB stick was surprisingly
simple. Battery life was longer in Linux than in Windows and I was glad to see
that the power management features were working well and already configured
how I&rsquo;d like them to be.</p>
<p>I knew I wasn&rsquo;t a fan of GNOME 3 already, so I loaded up KDE and XFCE. Both
worked extremely well with great performance. Desktop effects were really
responsive and I never saw flickering, crashes, or artifacts. Those were a lot
more frequent previously. I eventually settled into the i3 window manager and
got into a keyboard-based workflow with tiled windows. It was shocking to see
how much time I could save with a tiled window manager when I wasn&rsquo;t pushing
and resizing windows every time I opened a new app or a new window.</p>
<p>The raw X performance has improved drastically. In i3, I rarely found myself
waiting for anything to render and any changes to my desktop were speedy. Font
smoothing and rendering has also come a long way. OS X still leads this
category for me, but I was glad to see some serious advancements in Linux in
this area.</p>
<p>One of the biggest worries I had about Linux was email. I need Exchange
connectivity with good calendaring support for work and I didn&rsquo;t enjoy using
Evolution in the past. My choice this time around was Thunderbird with the
Lightning plugin for calendaring and Enigmail for GPG signing and encryption.
I stacked that on the davmail gateway for Exchange connectivity. This worked
<em>surprisingly</em> well. The performance could have been a bit better, but as far
as functionality is concerned, everything I tried worked. Creating meeting
invitations, responding to meeting invitations, handling email, and searching
the GAL was a relatively smooth experience.</p>
<p>Chrome was easy to install and very stable. Using Skype with video was a
breeze and I was on TeamSpeak calls with my team at work within a few minutes.
I had a slew of terminals to choose from and I settled on <a href="https://terminator-gtk3.readthedocs.io/en/latest/">terminator</a>
since I could have tiled terminals in my tiled window manager (which I&rsquo;m sure
Xzibit would approve).</p>
<p>Virtualization was simple and I was a few package installs away from running
KVM. Xen also worked well via virt-manager and the performance was excellent.
I installed VMWare Workstation since it was my favorite before, but it caused
stack traces in the kernel and I eventually had to remove it.</p>
<p><strong>The Bad</strong></p>
<p>I have yet to find a Twitter client for Linux that I enjoy using. It took me
forever to find even one application which used Twitter&rsquo;s streaming API (which
was released almost three years ago). Many of the applications were either
difficult to use, had confusing UI&rsquo;s, or wasted so much screen real estate
that they became a nuisance. Text-based clients looked good at first glance
but then I became frustrated with the inability to quickly see conversations
or see what a particular reply was referring to in my timeline. My current Mac
client is <a href="https://sites.google.com/site/yorufukurou/home-en">Yorufukurou</a>.</p>
<p>Music management was another sore spot. Some applications, like audacious, fit
the bill perfectly for basic internet radio streaming and playing small
albums. If I tried to look for an application to replace iTunes (library
management, internet radio, podcasts, and sync with a mobile device), I ended
up with Songbird, amarok and rhythmbox. Songbird was fair but lacked a lot of
features that I was eager to get. At first, amarok and rhythmbox looked like
winners but managing a library with them was taking much more time than I was
willing to invest.</p>
<p>The ThinkPad screen is very washed out by default but I found quite a few
forum posts talking about applying ICM profiles to correct it. Quite a few
people made that adjustment in Windows with some very good results. I tried to
do the same in Fedora 17 but struggled after working through several different
methods. Fedora 18 is going to have gnome-color-manager from the start and it
will probably make that process a little easier. Getting a DisplayLink adapter
working in Fedora 17 was problematic but I&rsquo;ve read that native support for
configuring these devices is coming soon.</p>
<p><strong>Conclusion</strong></p>
<p>Linux on the desktop has really improved a substantial amount but I&rsquo;m leaning
back towards the Mac. Although a portion of that decision centers around the
Mac hardware, the majority of the decision hinges on my workflow and the
quality of the applications available for my specific needs. I found myself
much less distracted in Linux mainly because it was more difficult for me to
interact with my coworkers and friends than it was on the Mac.</p>
<p>For a fair fight, I may try to get Linux going on my MacBook to ensure I&rsquo;m
comparing apples to apples. I&rsquo;ll save that for later this year when Fedora 18
is released.</p>
<p>Keep in mind that everyone has a unique workflow and mine may be much
different than yours. I&rsquo;m eager to read your comments and I welcome any
feedback you have.</p>
]]></content></item><item><title>Automatic package updates in CentOS 6</title><link>https://major.io/2012/09/21/automatic-package-updates-in-centos-6/</link><pubDate>Fri, 21 Sep 2012 13:21:01 +0000</pubDate><guid>https://major.io/2012/09/21/automatic-package-updates-in-centos-6/</guid><description>Automating package updates in CentOS 6 is a quick process and it ensures that your system receives the latest available security patches, bugfixes and enhancements. Although it&amp;rsquo;s easy and available right from yum on a normal CentOS 6 system, I still find that many people aren&amp;rsquo;t aware of it.
Before you enable automatic updates, you&amp;rsquo;ll want to ensure that you&amp;rsquo;re excluding certain packages which may be integral to your system. You can either make a list of those packages now or configure the automated updates so that you&amp;rsquo;re emailed a report of what needs to be installed rather than having those packages installed automatically.</description><content type="html"><![CDATA[<p>Automating package updates in CentOS 6 is a quick process and it ensures that your system receives the latest available security patches, bugfixes and enhancements. Although it&rsquo;s easy and available right from yum on a normal CentOS 6 system, I still find that many people aren&rsquo;t aware of it.</p>
<p>Before you enable automatic updates, you&rsquo;ll want to ensure that you&rsquo;re excluding certain packages which may be integral to your system. You can either make a list of those packages now or configure the automated updates so that you&rsquo;re emailed a report of what needs to be installed rather than having those packages installed automatically.</p>
<p>To get started, install yum-cron:</p>
<pre><code>yum -y install yum-cron
</code></pre><p>By default, it&rsquo;s configured to download all of the available updates and apply them immediately after downloading. Reports will be emailed to the root user on the system. To change these settings, just open <code>/etc/sysconfig/yum-cron</code> in your favorite text editor and adjust these lines:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ini" data-lang="ini"><span style="color:#586e75"># Default - check for updates, download, and apply</span>
CHECK_ONLY<span style="color:#719e07">=</span><span style="color:#2aa198">no</span>
DOWNLOAD_ONLY<span style="color:#719e07">=</span><span style="color:#2aa198">no</span>

<span style="color:#586e75"># Download the updates and email a report</span>
CHECK_ONLY<span style="color:#719e07">=</span><span style="color:#2aa198">no</span>
DOWNLOAD_ONLY<span style="color:#719e07">=</span><span style="color:#2aa198">yes</span>

<span style="color:#586e75"># Don&#39;t download the updates, just email a report</span>
CHECK_ONLY<span style="color:#719e07">=</span><span style="color:#2aa198">yes</span>
DOWNLOAD_ONLY<span style="color:#719e07">=</span><span style="color:#2aa198">no</span>
</code></pre></div><p>As mentioned earlier, if you want to exclude certain packages from these updates, just edit your <code>/etc/yum.conf</code> and add:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ini" data-lang="ini">exclude<span style="color:#719e07">=</span><span style="color:#2aa198">kernel* mysql*</span>
</code></pre></div><p>The cron jobs from the <code>yum-cron</code> package are active immediately after installing the package and there&rsquo;s no extra configuration necessary. The job will be run when your normal daily cron jobs are set to run.</p>
]]></content></item><item><title>One week with Android</title><link>https://major.io/2012/09/06/one-week-with-android/</link><pubDate>Fri, 07 Sep 2012 03:53:42 +0000</pubDate><guid>https://major.io/2012/09/06/one-week-with-android/</guid><description>After getting Android-envy at LinuxCon, I decided to push myself out of my comfort zone and ditch my iPhone 4 for a Samsung Galaxy S III. It surprised a lot of people I know since I&amp;rsquo;ve been a big iPhone fan since the original model was released in 2007. I&amp;rsquo;ve carried the original iPhone, the 3GS, and then the 4. There have been good times and bad times, but the devices have served me pretty well overall.</description><content type="html"><![CDATA[<p>After getting Android-envy at <a href="http://events.linuxfoundation.org/events/linuxcon">LinuxCon</a>, I decided to push myself out of my comfort zone and ditch my iPhone 4 for a <a href="http://www.samsung.com/global/galaxys3/">Samsung Galaxy S III</a>. It surprised a lot of people I know since I&rsquo;ve been a big iPhone fan since the original model was released in 2007. I&rsquo;ve carried the original iPhone, the 3GS, and then the 4. There have been good times and bad times, but the devices have served me pretty well overall.</p>
<p><strong>The Good Stuff</strong></p>
<p><img src="/wp-content/uploads/2012/09/41621v6-max-250x250.jpg" alt="3"></p>
<p>One of my coworkers summed up Android devices pretty succinctly: &ldquo;This will be the first phone that feels like <em>your</em> phone.&rdquo; That&rsquo;s what I like about it the most. I have so much more control over what my phone does and when it does it. It seems like there&rsquo;s a checkbox or option list for almost every possible setting on the phone. Everything feels customizable (to a reasonable point). Even trivial things like configuring home screens and adjusting Wi-Fi settings seem to be more user-friendly.</p>
<p>The raw performance of the S3 handset is impressive. All of the menus are responsive and I rarely find myself waiting on the phone to do something. 4G LTE is extremely fast (but it does chow down on your battery) and it&rsquo;s hard to tell when I&rsquo;m on Wi-Fi and when I&rsquo;m not. Photo adjustments are instantaneous and moving around in Chrome is snappy.</p>
<p>Another big benefit is that applications can harness the power of the Linux system under the hood (although some may require getting root access on your phone). Using rsync, ssh, FTP, and samba makes transferring data and managing the device much easier. It also allows you to set up automated backups to remote locations or to another SD card in your phone.</p>
<p><strong>The Not-So-Good Stuff</strong></p>
<p>If you&rsquo;ve ever used a Mac along with Apple&rsquo;s music devices, you know that the integration is tight and well planned. Moving over to Android has been really rough for me and the ways that I manage music. I gave <a href="https://play.google.com/store/apps/details?id=com.doubleTwist.androidPlayer&amp;feature=nav_result">DoubleTwist</a> and <a href="https://play.google.com/store/apps/details?id=com.doubleTwist.androidPlayerProKey">AirSync</a> a try but then I found that all of my music was being transcoded on the fly from AAC to another format. Syncing music took forever, quality was reduced, and the DoubleTwist music player on the phone was difficult to use. I downloaded <a href="https://play.google.com/store/apps/details?id=com.songbirdnest.mediaplayer">SongBird</a> and then tried to use <a href="https://play.google.com/store/apps/details?id=com.google.android.music">Google Play Music</a> but both felt inefficient and confusing.</p>
<p>Eventually, I found <a href="https://play.google.com/store/apps/details?id=berserker.android.apps.sshdroid">SSHDroid</a> and started transferring music via ssh. That worked out well but then I couldn&rsquo;t find any of the music I uploaded on my phone. A friend recommended <a href="https://play.google.com/store/apps/details?id=com.bero.sdrescan">SDRescan</a> since it forces the device to scan itself for any new media files. My current work flow involves uploading the music via ssh, rescanning for media files, and then listening to the new files with Apollo (from <a href="http://www.cyanogenmod.com/">CyanogenMod</a>, more on that later).</p>
<p>Battery life on the S3 is well below what I expected but it sounds like it might be more the shortfall of the device rather than the software. The screen is large and it&rsquo;s very bright even on the lowest settings. The battery settings panel on the phone regularly shows the screen as the largest consumer of energy on the phone. I did make some adjustments, like allowing Wi-Fi to switch off when the phone is asleep, which has helped with battery life. Disabling push email or IMAP IDLE has helped but it&rsquo;s prevented me from getting some of the functionality I want.</p>
<p>Finally, the pre-installed Samsung software was absolutely terrible. There were background processes running that were eating the battery and the interface was hard to use. I&rsquo;m not sure what their target audience is, but it made coming over from the iPhone pretty difficult.</p>
<p><img src="/wp-content/uploads/2012/09/cm7_logo.png" alt="11"></p>
<p><strong>To Flash or Not To Flash</strong></p>
<p>Voiding the warranty and flashing the phone had me pretty nervous, but then again, I had quite a few coworkers who were experienced in the process and they had rarely experienced problems. Luckily, <a href="http://wiki.cyanogenmod.com/wiki/Samsung_Galaxy_S_III_(AT%26T):_Full_Update_Guide">there is a great wiki page</a> that walks you through the process. It&rsquo;s a bit technical but I found it reasonably straightforward to follow. One of the nightly builds caused some problems with the GPS functionality on the phone but that was corrected in a day or two with another nightly build.</p>
<p>Upgrading to new nightly ROMs is unbelievably simple. You can download them manually to your phone and then reboot into recovery mode to flash the phone or you can load up an application on the phone itself which will download the ROM images and install the new image after a quick reboot with one key press. Don&rsquo;t forget to make backups just in case something goes wrong, though.</p>
<p><!-- raw HTML omitted --></p>
<p><strong>My Application List</strong></p>
<p>Here are my favorite applications so far:</p>
<ul>
<li><a href="https://play.google.com/store/apps/details?id=com.onepassword.passwordmanager">1Password Reader</a></li>
<li><a href="https://play.google.com/store/apps/details?id=org.connectbot">ConnectBot</a></li>
<li><a href="https://play.google.com/store/apps/details?id=com.estrongs.android.pop">ES File Explorer</a></li>
<li><a href="https://play.google.com/store/apps/details?id=com.google.android.apps.authenticator2">Google Authenticator</a></li>
<li><a href="https://play.google.com/store/apps/details?id=com.chartcross.gpstest">GPS Test</a></li>
<li><a href="https://play.google.com/store/apps/details?id=com.fsck.k9">K-9 Mail</a></li>
<li><a href="https://play.google.com/store/apps/details?id=com.usk.app.notifymyandroid">Notify My Android</a></li>
<li><a href="https://play.google.com/store/apps/details?id=com.fitnesskeeper.runkeeper.pro">RunKeeper</a></li>
<li><a href="https://play.google.com/store/apps/details?id=com.bero.sdrescan">SDRescan</a></li>
<li><a href="https://play.google.com/store/apps/details?id=berserker.android.apps.sshdroid">SSHDroid</a></li>
<li><a href="https://play.google.com/store/apps/details?id=com.keramidas.TitaniumBackup">Titanium Backup</a></li>
<li><a href="https://play.google.com/store/apps/details?id=com.nitrodesk.droid20.nitroid">TouchDown</a></li>
<li><a href="https://play.google.com/store/apps/details?id=com.farproc.wifi.analyzer">Wifi Analyzer</a></li>
</ul>
<p><strong>More Changes</strong></p>
<p>I&rsquo;m waiting on my new ThinkPad T430s to ship and I&rsquo;m told that Android phones are a bit easier to use within Linux than they are on a Mac. Not having the integrated USB support on the Mac is pretty frustrating. I&rsquo;ll probably amend this post or write another one once I&rsquo;m running Linux on my laptop and using my Android with it regularly.</p>
]]></content></item><item><title>See you at LinuxCon North America 2012!</title><link>https://major.io/2012/08/15/see-you-at-linuxcon-north-america-2012/</link><pubDate>Wed, 15 Aug 2012 14:02:43 +0000</pubDate><guid>https://major.io/2012/08/15/see-you-at-linuxcon-north-america-2012/</guid><description>I was sitting at my desk yesterday when I saw a tweet from @LinuxFoundation:
I&amp;rsquo;ve always wanted to attend LinuxCon, so I fired back with a reply: &amp;ldquo;@linuxfoundation Fedora! #linuxcon #cloudopen&amp;rdquo;. This popped into my Twitter timeline yesterday evening:
Wow! This sure was a surprise since I rarely ever win a contest of any sort.
I&amp;rsquo;d like to give a big thanks to the Linux Foundation for making this possible and to Rackspace for letting me travel on such short notice.</description><content type="html"><![CDATA[<p>I was sitting at my desk yesterday when I <a href="http://twitter.com/linuxfoundation/status/235403911996006400">saw a tweet</a> from <a href="http://twitter.com/linuxfoundation">@LinuxFoundation</a>:</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>I&rsquo;ve always wanted to attend <a href="http://events.linuxfoundation.org/events/linuxcon/">LinuxCon</a>, so I fired back with a reply: &ldquo;@linuxfoundation Fedora! #linuxcon #cloudopen&rdquo;. <a href="http://twitter.com/linuxfoundation/status/235525007328026624">This popped into my Twitter timeline</a> yesterday evening:</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p><strong>Wow!</strong> This sure was a surprise since I rarely ever win a contest of any sort.</p>
<p>I&rsquo;d like to give a big thanks to the Linux Foundation for making this possible and to Rackspace for letting me travel on such short notice. If you don&rsquo;t already give the Linux Foundation some support, <a href="http://www.linuxfoundation.org/about/join">be sure to do so</a>.</p>
]]></content></item><item><title>Monitoring and protecting your reputation online</title><link>https://major.io/2012/08/06/monitoring-and-protecting-your-reputation-online/</link><pubDate>Mon, 06 Aug 2012 14:00:50 +0000</pubDate><guid>https://major.io/2012/08/06/monitoring-and-protecting-your-reputation-online/</guid><description>After a recent issue I had with some users in the Puppy Linux forums, I thought it might be prudent to write a post about how to monitor and protect your reputation online. This guide is mainly geared towards technical people who maintain some type of public presence. That should include folks who talk at conferences, contribute to high profile open source projects, or those who utilize social media to connect with other users and contributors.</description><content type="html"><![CDATA[<p>After a <a href="/2012/08/04/privacy-and-icanhazip-com/">recent issue I had with some users in the Puppy Linux forums</a>, I thought it might be prudent to write a post about how to monitor and protect your reputation online. This guide is mainly geared towards technical people who maintain some type of public presence. That should include folks who talk at conferences, contribute to high profile open source projects, or those who utilize social media to connect with other users and contributors.</p>
<p>The first part is monitoring. A monitoring solution should ideally be inexpensive, have a low lag time between a new mention and a notification, and it should be able to search a lot of resources.</p>
<p>For me, it made sense to use <a href="http://www.google.com/alerts">Google Alerts</a>. I have as-it-happens searches in place for several things:</p>
<ul>
<li>my full name</li>
<li>frequently used handles/usernames on various communication mediums (like IRC, twitter, etc)</li>
<li>the URL&rsquo;s of web sites I maintain</li>
<li>new links to web sites I maintain</li>
</ul>
<p>Google Alerts allow me to get notifications very quickly about new blog posts, forum posts, or other websites which mention something I find to be sensitive. The signal to noise ratio for my searches is quite good but it has taken some time to hone the queries down and reduce the useless notifications.</p>
<p>If you frequent certain IRC channels, you ought to consider setting up an IRC bouncer if the server administrators allow it. You&rsquo;ll have the benefit of getting all of the logs from the channel even when you&rsquo;re not actively at your computer and you may be able to spot things that need attention.</p>
<p>Protecting your reputation is multi-faceted and immensely critical. The same communication mediums that you depend upon to spread your message and meet other people can be used against you in an instant. How many times have you seen hacked Twitter and Facebook accounts and then wondered: &ldquo;I never would have thought someone would have targeted that person. I also figured that they would have protected their account a little more aggressively.&rdquo;</p>
<p>I&rsquo;ve seen people with giant piles of alphabet soup (certifications) after their name (including CSO, CISSP, Security+) have their Twitter accounts hacked and I&rsquo;ve had to tell them about it. It can happen to anyone but it&rsquo;s up to you to make it extremely difficult for it to happen. Here are some tips which apply specifically to Twitter but could be loosely applied to almost anything you use daily:</p>
<ul>
<li>use very strong passwords along with a solid password manager</li>
<li>regularly audit the applications which have access to your account (via OAuth, API&rsquo;s, etc)</li>
<li>for critical accounts, force yourself to change the password regularly</li>
</ul>
<p>If you don&rsquo;t get anything from this post, please understand this. <strong>The most critical piece of your personal infrastructure to protect is your email account.</strong> Think about it - where do your password resets go? Where do your domain name renewal notifications go? It&rsquo;s the crux of your personal security. Even if you have a 100-character password with upper/lower-case letters, numbers, symbols and unicode characters, you&rsquo;re totally unprotected when an attacker forces a password reset email and finds that your email account password is &ldquo;p455w0rd&rdquo;.</p>
<p>For those providers that offer two-factor authentication, you really should consider using it. The pain of two-factor auth may be annoying at first, but imagine the pain when you find your bank account emptied, credit card filled, iPhone/iPad/laptop wiped and your personal identification information stolen.</p>
<p>I&rsquo;ll wrap up this post by talking about what I mentioned at the start of this post: responding to someone who has dragged your name through the mud on false information. Respond promptly and succinctly. Let them know who you are (with proof via links or other means), that their statements are false, and then provide proof and redirection. You certainly don&rsquo;t want to be overly agressive and condescending, but you don&rsquo;t want to be passive about it either. Be assertive and protect what&rsquo;s yours.</p>
<p>My grade school journalism teacher summed it up pretty well (I&rsquo;ll paraphrase):</p>
<blockquote>
<p>Your credibility and reputation are the two best things you&rsquo;ve got. Money and fame will come and go but you&rsquo;ll always land on your feet if you keep your credibility. Your greatest asset is something that nobody else will help you protect.</p>
</blockquote>
]]></content></item><item><title>Privacy and icanhazip.com</title><link>https://major.io/2012/08/04/privacy-and-icanhazip-com/</link><pubDate>Sun, 05 Aug 2012 04:26:50 +0000</pubDate><guid>https://major.io/2012/08/04/privacy-and-icanhazip-com/</guid><description>A forum thread cropped in my inbox today from the Puppy Linux forums titled &amp;ldquo;Major Hayden???&amp;rdquo;. After a little digging, I found a giant thread talking about icanhazip.com.
In these various threads, I was accused of:
being an expert on Linux (totally false!) running a site on behalf of the CIA being a direct descendent of General Michael Hayden (former director of CIA &amp;amp; NSA) recording MAC ID&amp;rsquo;s (which I assume means MAC addresses) on icanhazip.</description><content type="html"><![CDATA[<p>A forum thread cropped in my inbox today from the <a href="http://puppylinux.org/">Puppy Linux</a> forums titled &ldquo;<a href="http://www.murga-linux.com/puppy/viewtopic.php?t=80081">Major Hayden???</a>&rdquo;. After a little digging, I <a href="http://www.murga-linux.com/puppy/viewtopic.php?t=66968">found a giant thread</a> talking about <a href="http://icanhazip.com">icanhazip.com</a>.</p>
<p>In these various threads, I was accused of:</p>
<ul>
<li>being an expert on Linux (totally false!)</li>
<li>running a site on behalf of the <a href="http://en.wikipedia.org/wiki/Central_Intelligence_Agency">CIA</a></li>
<li>being a direct descendent of <a href="http://en.wikipedia.org/wiki/Michael_Hayden_(general)">General Michael Hayden</a> (former director of CIA &amp; <a href="http://en.wikipedia.org/wiki/Nsa">NSA</a>)</li>
<li>recording MAC ID&rsquo;s (which I assume means MAC addresses) on icanhazip.com</li>
<li>recording connection data for the <a href="http://en.wikipedia.org/wiki/Fbi">FBI</a> for US citizens inside the USA</li>
<li>writing spyware</li>
</ul>
<p>Let&rsquo;s get a few things straight.</p>
<p><strong>Relation to General Hayden, the CIA, the FBI or the NSA</strong></p>
<p>Baseless. Hayden may not be terribly common (<a href="http://www.namestatistics.com/search.php?name=hayden&amp;type=last">one site claims</a> 35,000 people with my last name) but I&rsquo;m not related to General Hayden in any way that I&rsquo;m aware of. In addition, I&rsquo;ve never had any affiliation with or done any paid work for the CIA, FBI or NSA. Have I been asked to assist with an investigation or two in the past? Yes. However, these were often pursuant to my work as a systems administrator and you&rsquo;ll find that this is relatively common among people who work for hosting companies.</p>
<p><strong>Running a site on behalf of the US government</strong></p>
<p>Wrong again. I started the site for my own personal use since I was fed up with all of the other providers who were jamming ads into their pages or asking for money before you could use their site with scripts. After all, the CIA doesn&rsquo;t need icanhazip.com <a href="http://www.theonion.com/video/cias-facebook-program-dramatically-cut-agencys-cos,19753/">since they run Facebook already</a>.</p>
<p><strong>Recording personal data on icanhazip.com</strong></p>
<p>The logging on icanhazip.com is standard for any nginx server. Yes, my logs have remote IP addresses, referrer details (if your application provides them) as well as user agents. I take great care to safeguard this data and only release very broad statistics if they seem interesting. For example, I might make a post about <a href="http://www.gnu.org/software/wget/">wget</a> being the most popular application used to access icanhazip.com. Or, I might say something about receiving the majority of my requests from Europe. I&rsquo;ll never release any information personally about anyone using the site and I aggressively rotate the logs once I gather some basic broad stats.</p>
<p>Also, to the <a href="http://www.murga-linux.com/puppy/viewtopic.php?p=547747#547747">Puppy Linux user who incensed me the most</a>, you really need to learn how a network operates. Excluding <a href="http://en.wikipedia.org/wiki/IPv6_address#Stateless_address_autoconfiguration">IPv6&rsquo;s SLAAC and EUI-64</a>, there&rsquo;s no way for me to get your MAC address (not called a MAC ID) or even a portion of your MAC address after your request has passed through several routers to make it to icanhazip.com. I also don&rsquo;t write or distribute spyware. I return a plain text string containing your IP address (and a few HTTP headers) for each request. There is no binary data or redirection ever returned.</p>
<p><strong>Being an expert on Linux</strong></p>
<p>Familiar with Linux? Yes. An expert? Hardly.</p>
<p>As I learned on the ambulance as an EMT, the moment you think you&rsquo;re an expert and that you&rsquo;ve seen everything is the moment before you get hit by a bus (figuratively).</p>
]]></content></item><item><title>Building vpnc with openssl support via MacPorts on Mac OS X</title><link>https://major.io/2012/07/31/building-vpnc-with-openssl-support-via-macports-on-mac-os-x/</link><pubDate>Wed, 01 Aug 2012 04:16:09 +0000</pubDate><guid>https://major.io/2012/07/31/building-vpnc-with-openssl-support-via-macports-on-mac-os-x/</guid><description>If you install vpnc via MacPorts on OS X, you&amp;rsquo;ll find that you have no openssl support after it&amp;rsquo;s built:
$ sudo port install vpnc ---&amp;gt; Computing dependencies for vpnc ---&amp;gt; Cleaning vpnc ---&amp;gt; Scanning binaries for linking errors: 100.0% ---&amp;gt; No broken files found. $ sudo vpnc vpnc was built without openssl: Can't do hybrid or cert mode. This will cause some problems if you&amp;rsquo;re trying to use VPN with a Cisco VPN concentrator which uses SSL VPN technology.</description><content type="html"><![CDATA[<p>If you install <a href="http://www.unix-ag.uni-kl.de/~massar/vpnc/">vpnc</a> via <a href="http://www.macports.org/">MacPorts</a> on OS X, you&rsquo;ll find that you have no openssl support after it&rsquo;s built:</p>
<pre><code>$ sudo port install vpnc
---&gt;  Computing dependencies for vpnc
---&gt;  Cleaning vpnc
---&gt;  Scanning binaries for linking errors: 100.0%
---&gt;  No broken files found.
$ sudo vpnc
vpnc was built without openssl: Can't do hybrid or cert mode.
</code></pre><p>This will cause some problems if you&rsquo;re trying to use VPN with a Cisco VPN concentrator which uses SSL VPN technology. The fix is an easy one. You&rsquo;ll find a variant within the portfile itself:</p>
<pre><code>$ sudo port edit --editor cat vpnc | tail -7
variant             hybrid_cert description &quot;Enable the support for hybrid and cert modes in vpnc&quot; {
    depends_lib-append port:openssl
    build.args-append  &quot;OPENSSL_GPL_VIOLATION=-DOPENSSL_GPL_VIOLATION OPENSSLLIBS=-lcrypto&quot;
}
livecheck.type  regex
livecheck.url   ${homepage}
livecheck.regex &quot;${name}-(\\d+(?:\\.\\d+)*)${extract.suffix}&quot;
</code></pre><p>Simply specify that you want the <strong>hybrid_cert</strong> variant on the command line when you install vpnc and you should be all set:</p>
<pre><code>$ sudo port install vpnc +hybrid_cert
---&gt;  Computing dependencies for vpnc
---&gt;  Deactivating vpnc @0.5.3_0
---&gt;  Cleaning vpnc
---&gt;  Activating vpnc @0.5.3_0+hybrid_cert
---&gt;  Cleaning vpnc
---&gt;  Scanning binaries for linking errors: 100.0%
---&gt;  No broken files found.
$ sudo vpnc
unknown host `&lt;gateway&gt;'
&lt;/gateway&gt;
</code></pre>]]></content></item><item><title>DNS Service Review: Luadns</title><link>https://major.io/2012/07/22/dns-service-review-luadns/</link><pubDate>Sun, 22 Jul 2012 20:31:16 +0000</pubDate><guid>https://major.io/2012/07/22/dns-service-review-luadns/</guid><description>Vitalie Cherpec contacted me back in May about his new hosted DNS offering, Luadns. I gave it a try and I offered to write a review about the service.
DISCLAIMER: I don&amp;rsquo;t write many reviews on this blog, but I want to make sure a few things are clear. Vitalie was kind enough to set up an account for me to test with which would have normally cost me $9/month. However, he didn&amp;rsquo;t give me any compensation of any kind for the review itself and there was nothing done for me outside of what a customer would receive at a paid service level at Luadns.</description><content type="html"><![CDATA[<p>Vitalie Cherpec contacted me back in May about his new hosted DNS offering, <a href="http://www.luadns.com/">Luadns</a>. I gave it a try and I offered to write a review about the service.</p>
<p><a href="/wp-content/uploads/2012/07/luadns.png"><!-- raw HTML omitted --></a><strong>DISCLAIMER:</strong> I don&rsquo;t write many reviews on this blog, but I want to make sure a few things are clear. Vitalie was kind enough to set up an account for me to test with which would have normally cost me $9/month. However, he didn&rsquo;t give me any compensation of any kind for the review itself and there was nothing done for me outside of what a customer would receive at a paid service level at Luadns. <strong>In other words, this is an honest review and I haven&rsquo;t been paid for a favorable (or unfavorable) response.</strong></p>
<p>At first glance, Luadns looks like many of the other hosted DNS services out there. Their DNS servers run <a href="http://en.wikipedia.org/wiki/Djbdns">tinydns</a> and there are globally distributed DNS servers in Germany (Hetzner), California (Linode), New Jersey (Linode), Netherlands (LeaseWeb), and Japan (KDDI). The latency to the two US locations were reasonable from my home in San Antonio (on Time Warner Cable, usually under 70ms) but the overseas servers had reasonable latency except for the server in Germany. I was regularly seeing round trip times of over 300ms to that server.</p>
<p>What makes Luadns unique is <a href="http://www.luadns.com/how.html">how you update</a> your DNS records. You can put your DNS zone files into a git repository in <a href="http://github.com">GitHub</a> or <a href="https://bitbucket.org/">BitBucket</a> and then set up a post-commit hook to nudge Luadns when you make an update. This process gives you a good audit trail of when DNS changes were made, who changed them, and what was changed.</p>
<p>As soon as you push your changes, Luadns is notified and they can go about updating the DNS records on their servers around the world. You also get the option to do manual updates if your business processes require a thorough review of DNS changes prior to their public release. You&rsquo;ll receive an email confirmation each time Luadns is nudged with changes to your zone files.</p>
<p>In my experience, I saw pretty reasonable delays for updates. Here are the times I measured for DNS changes to propagate to all five Luadns servers:</p>
<ul>
<li>Updates to an existing zone: 15-25 seconds (regardless of the amount of updates)</li>
<li>Adding a totally new zone: 30-45 seconds</li>
<li>Deleting a zone: 5-6 minutes (see following paragraph)</li>
</ul>
<p>I contacted Vitalie about the long delay in deleting entire zones from Luadns and he made some adjustments to the domain deletion priority. After his change, deletions were processed in under 20 seconds every time I tried it.</p>
<p>All of my testing was done with basic BIND zone files but Luadns allows you to <a href="http://www.luadns.com/help.html#toc_9">write your zones in Lua</a> if you prefer. That allows you to do some pretty slick automation with templates and you won&rsquo;t have to be quite so repetitive as you normally would with BIND zone files.</p>
<p><strong>Summary</strong></p>
<p>Luadns provides a nice twist on the available DNS hosting solutions available today. Committing zone changes into a git repository allows for some great auditing and opens the door for pull requests that get a look from another team member before the DNS changes are released. The GitHub and Bitbucket integration is well done and the post-commit hooks seemed to work every time I tried them. The delays for zone updates are very reasonable and the pricing seems fair. I operate 48 domains and my bill each month would probably be $19 for the base plan. I&rsquo;d easily go over the 4M queries/month so I&rsquo;d expect to be paying extra.</p>
<p>I&rsquo;d like to see Luadns improve by getting a more reliable European location that Hetzner since I can&rsquo;t get good round trip times from various locations that I&rsquo;ve tried. Anycasted DNS servers would be a big plus, but that&rsquo;s a tough thing for a small company to do. I&rsquo;d also like to see other development languages available other than Lua (python and ruby, perhaps).</p>
<p>Overall, I&rsquo;d recommend Luadns for DNS hosting due to the convenience provided by GitHub/Bitbucket and the audit trail provided by both. Vitalie was easy to work with and he was quick to respond to any inquiry I sent. There&rsquo;s a free pricing tier - why not <a href="http://www.luadns.com/pricing.html">give it a try</a>?</p>
]]></content></item><item><title>Connecting to OpenStack Swift with Cyberduck using swauth</title><link>https://major.io/2012/07/18/connecting-to-openstack-swift-with-cyberduck-using-swauth/</link><pubDate>Wed, 18 Jul 2012 20:21:54 +0000</pubDate><guid>https://major.io/2012/07/18/connecting-to-openstack-swift-with-cyberduck-using-swauth/</guid><description>Connecting to an OpenStack Swift instance that is using swauth for authentication is quite easy within Cyberduck on the Mac. Open Terminal.app and run this command:
defaults write ch.sudo.cyberduck cf.authentication.context /auth/v1.0 Keep in mind that this changes the authentication URI for all OpenStack swift connections made by Cyberduck. If this isn&amp;rsquo;t what you want, you can easily set it back to the default by running:
defaults write ch.sudo.cyberduck cf.authentication.context /v1.0 There&amp;rsquo;s more information about these settings in Cyberduck&amp;rsquo;s Swift Howto.</description><content type="html"><![CDATA[<p>Connecting to an <a href="http://docs.openstack.org/developer/swift/">OpenStack Swift</a> instance that is using <a href="http://gholt.github.com/swauth/dev/">swauth</a> for authentication is quite easy within <a href="http://cyberduck.ch/">Cyberduck</a> on the Mac. Open Terminal.app and run this command:</p>
<pre><code>defaults write ch.sudo.cyberduck cf.authentication.context /auth/v1.0
</code></pre><p>Keep in mind that this changes the authentication URI for <strong>all</strong> OpenStack swift connections made by Cyberduck. If this isn&rsquo;t what you want, you can easily set it back to the default by running:</p>
<pre><code>defaults write ch.sudo.cyberduck cf.authentication.context /v1.0
</code></pre><p>There&rsquo;s more information about these settings in <a href="http://trac.cyberduck.ch/wiki/help/en/howto/openstack">Cyberduck&rsquo;s Swift Howto</a>.</p>
]]></content></item><item><title>Boot the Xen hypervisor by default in Fedora 17 with GRUB 2</title><link>https://major.io/2012/07/16/boot-the-xen-hypervisor-by-default-in-fedora-17-with-grub-2/</link><pubDate>Mon, 16 Jul 2012 15:00:44 +0000</pubDate><guid>https://major.io/2012/07/16/boot-the-xen-hypervisor-by-default-in-fedora-17-with-grub-2/</guid><description>Although GRUB 2 does give us some nice benefits, changing its configuration can be a bit of a challenge if you&amp;rsquo;re used to working with the original GRUB for many, many years. I&amp;rsquo;ve recently installed some Fedora 17 systems with Xen and I&amp;rsquo;ve had to go back to the documentation to change the default GRUB 2 boot option. Hopefully this post will save you some time.
A good place to start reading is on Fedora&amp;rsquo;s own page about GRUB 2 and the helpful commands provided to manage its configuration.</description><content type="html"><![CDATA[<p>Although GRUB 2 does give us some nice benefits, changing its configuration can be a bit of a challenge if you&rsquo;re used to working with the original GRUB for many, many years. I&rsquo;ve recently installed some Fedora 17 systems with Xen and I&rsquo;ve had to go back to the documentation to change the default GRUB 2 boot option. Hopefully this post will save you some time.</p>
<p>A good place to start reading is on Fedora&rsquo;s own page about <a href="http://fedoraproject.org/wiki/GRUB_2">GRUB 2</a> and the helpful commands provided to manage its configuration.</p>
<p>I&rsquo;ll assume you&rsquo;ve installed the <strong>xen</strong> packages already and those packages have configured a (non-default) menu entry in your GRUB 2 configuration. Start by getting a list of your grub menu entry options (without the submenu options):</p>
<pre><code>[root@remotebox ~]# grep ^menuentry /boot/grub2/grub.cfg | cut -d &quot;'&quot; -f2
Fedora
Fedora, with Xen hypervisor
</code></pre><p>We obviously wan&rsquo;t the second one to be our default option. Let&rsquo;s adjust the GRUB 2 settings and then check our work:</p>
<pre><code>[root@remotebox ~]# grub2-set-default 'Fedora, with Xen hypervisor'
[root@remotebox ~]# grub2-editenv list
saved_entry=Fedora, with Xen hypervisor
</code></pre><p>The configuration file hasn&rsquo;t been written yet! I prefer to disable the graphical framebuffer and I like to see all of the kernel boot messages each time I reboot. Some of those messages can be handy if you have failing hardware or a bad configuration somewhere in your boot process. Open up <strong>/etc/sysconfig/grub</strong> in your favorite text editor and remove <strong>rhgb quiet</strong> from the line that starts with <strong>GRUB_CMDLINE_LINUX</strong>.</p>
<p>Write your new GRUB 2 configuration file:</p>
<pre><code>[root@remotebox ~]# grub2-mkconfig -o /boot/grub2/grub.cfg
</code></pre><p>Reboot your server. Once it&rsquo;s back, check to see if you loaded the right boot option. Even without any Xen daemons running, you should be able to check for the presence of the hypervisor:</p>
<pre><code>[root@i7tiny ~]# dmesg | grep -i &quot;xen version&quot;
[    0.000000] Xen version: 4.1.2 (preserve-AD)
</code></pre>]]></content></item><item><title>Mounting an LVM snapshot containing partitions</title><link>https://major.io/2012/07/15/mounting-an-lvm-snapshot-containing-partitions/</link><pubDate>Sun, 15 Jul 2012 20:11:38 +0000</pubDate><guid>https://major.io/2012/07/15/mounting-an-lvm-snapshot-containing-partitions/</guid><description>LVM snapshots can be really handy when you&amp;rsquo;re trying to take a backup of a running virtual machine. However, mounting the snapshot can be tricky if the logical volume is partitioned.
I have a virtual machine running zoneminder on one of my servers at home and I needed to take a backup of the instance with rdiff-backup. I made a snapshot of the logical volume and attempted to mount it:</description><content type="html"><![CDATA[<p>LVM snapshots can be really handy when you&rsquo;re trying to take a backup of a running virtual machine. However, mounting the snapshot can be tricky if the logical volume is partitioned.</p>
<p>I have a virtual machine running <a href="http://www.zoneminder.com/">zoneminder</a> on one of my servers at home and I needed to take a backup of the instance with <a href="http://www.nongnu.org/rdiff-backup/">rdiff-backup</a>. I made a snapshot of the logical volume and attempted to mount it:</p>
<pre><code>[root@i7tiny ~]# lvcreate -s -n snap -L 5G /dev/vg_i7tiny/vm_zoneminder
  Logical volume &quot;snap&quot; created
[root@i7tiny ~]# mount /dev/vg_i7tiny/snap /mnt/snap/
mount: wrong fs type, bad option, bad superblock on /dev/mapper/vg_i7tiny-snap,
       missing codepage or helper program, or other error
       In some cases useful info is found in syslog - try
       dmesg | tail or so
</code></pre><p>Oops. The logical volume has partitions. We will need to mount the volume with an offset so that we can get the right partition. Figuring out the offset can be done fairly easily with fdisk:</p>
<pre><code>[root@i7tiny ~]# fdisk -l /dev/vg_i7tiny/vm_zoneminder

Disk /dev/vg_i7tiny/vm_zoneminder: 53.7 GB, 53687091200 bytes
255 heads, 63 sectors/track, 6527 cylinders, total 104857600 sectors
Units = sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disk identifier: 0x0007a1d5

                       Device Boot      Start         End      Blocks   Id  System
/dev/vg_i7tiny/vm_zoneminder1   *        2048     1026047      512000   83  Linux
/dev/vg_i7tiny/vm_zoneminder2         1026048   102825983    50899968   83  Linux
/dev/vg_i7tiny/vm_zoneminder3       102825984   104857599     1015808   82  Linux swap / Solaris
</code></pre><p>It looks like we have a small boot partition, a big root partition and a swap volume. We want to mount the second volume to copy files from the root filesystem. There are two critical pieces of information here that we need:</p>
<ul>
<li>the <strong>sector</strong> where the partition starts (the <em>Start</em> column from fdisk)</li>
<li>the <strong>number of bytes per sector</strong> (512 in this case - see the third line of the fdisk output)</li>
</ul>
<p>Let&rsquo;s calculate how many bytes we need to skip when we mount the partition and then mount it:</p>
<pre><code>[root@i7tiny ~]# echo &quot;512 * 1026048&quot; | bc
525336576
[root@i7tiny ~]# mount -o offset=525336576 /dev/mapper/vg_i7tiny-snap /mnt/snap/
[root@i7tiny ~]# ls /mnt/snap/
bin  boot  dev  etc  home  lib  lib64  lost+found  media  mnt  opt  proc  root  run  sbin  srv  sys  tmp  usr  var
</code></pre><p>The root filesystem from the virtual machine is now mounted and we can copy some files from it. Don&rsquo;t forget to clean up when you&rsquo;re finished:</p>
<pre><code>[root@i7tiny ~]# umount /mnt/snap/
[root@i7tiny ~]# lvremove -f /dev/vg_i7tiny/snap
  Logical volume &quot;snap&quot; successfully removed
</code></pre><p>If you need to do this with file-backed virtual machine storage or with a flat file you made with dd/dd_rescue, <a href="/2010/12/14/mounting-a-raw-partition-file-made-with-dd-or-dd_rescue-in-linux/">read my post from 2010</a> about tackling that similar problem.</p>
]]></content></item><item><title>X forwarding over ssh woes: DISPLAY is not set</title><link>https://major.io/2012/07/14/x-forwarding-over-ssh-woes-display-is-not-set/</link><pubDate>Sat, 14 Jul 2012 19:56:09 +0000</pubDate><guid>https://major.io/2012/07/14/x-forwarding-over-ssh-woes-display-is-not-set/</guid><description>This problem came up in conversation earlier this week and I realized that I&amp;rsquo;d never written a post about it. Has this ever happened to you before?
$ ssh -YC remotebox [major@remotebox ~]$ xterm xterm: Xt error: Can't open display: xterm: DISPLAY is not set I&amp;rsquo;ve scratched my head on this error message when the remote server is a minimally-installed CentOS, Fedora, or Red Hat system. It turns out that the xorg-x11-xauth package wasn&amp;rsquo;t installed with the minimal package set and I didn&amp;rsquo;t have any authentication credentials ready to hand off to the X server on the remote machine.</description><content type="html"><![CDATA[<p>This problem came up in conversation earlier this week and I realized that I&rsquo;d never written a post about it. Has this ever happened to you before?</p>
<pre><code>$ ssh -YC remotebox
[major@remotebox ~]$ xterm
xterm: Xt error: Can't open display:
xterm: DISPLAY is not set
</code></pre><p>I&rsquo;ve scratched my head on this error message when the remote server is a minimally-installed CentOS, Fedora, or Red Hat system. It turns out that the <strong>xorg-x11-xauth</strong> package wasn&rsquo;t installed with the minimal package set and I didn&rsquo;t have any <a href="http://www.x.org/wiki/Development/Documentation/Security">authentication credentials ready</a> to hand off to the X server on the remote machine.</p>
<p>Luckily, the fix is a quick one:</p>
<pre><code>[root@remotebox ~]# yum -y install xorg-x11-xauth
</code></pre><p>Close the ssh connection to your remote server and give it another try:</p>
<pre><code>$ ssh -YC remotebox
[major@remotebox ~]$ xterm
</code></pre><p>You should now have an xterm from the remote machine on your local computer.</p>
<p>The source of the problem is that you don&rsquo;t have a MIT-MAGIC-COOKIE on the remote system. The <a href="http://www.x.org/archive/X11R6.8.1/doc/Xsecurity.7.html">Xsecurity</a> man page explains it fairly well:</p>
<blockquote>
<p>MIT-MAGIC-COOKIE-1</p>
</blockquote>
<blockquote>
<p>When using MIT-MAGIC-COOKIE-1, the client sends a 128 bit &ldquo;cookie&rdquo; along with the connection setup information. If the cookie presented by the client matches one that the X server has, the connection is allowed access. The cookie is chosen so that it is hard to guess; xdm generates such cookies automatically when this form of access control is used. The user&rsquo;s copy of the cookie is usually stored in the .Xauthority file in the home directory, although the environment variable XAUTHORITY can be used to specify an alternate location. Xdm automatically passes a cookie to the server for each new login session, and stores the cookie in the user file at login.</p>
</blockquote>
<p>Your home directory on the remote server should have a small file called <strong>.Xauthority</strong> with the magic cookie in binary:</p>
<pre><code>[major@remotebox ~]$ ls -al ~/.Xauthority
-rw-------. 1 major major 61 Jul 14 19:28 /home/major/.Xauthority
[major@remotebox ~]$ file ~/.Xauthority
/home/major/.Xauthority: data
</code></pre>]]></content></item><item><title>SELinux, Xen, and block devices in Fedora 17</title><link>https://major.io/2012/07/10/selinux-xen-and-block-devices-in-fedora-17/</link><pubDate>Tue, 10 Jul 2012 05:05:33 +0000</pubDate><guid>https://major.io/2012/07/10/selinux-xen-and-block-devices-in-fedora-17/</guid><description>If you try to run Xen without libvirt on Fedora 17 with SELinux in enforcing mode, you&amp;rsquo;ll be butting heads with SELinux in no time. You&amp;rsquo;ll probably be staring at something like this:
# xm create -c fedora17 Using config file &amp;quot;/etc/xen/fedora17&amp;quot;. Error: Disk isn't accessible If you have setroubleshoot and setroubleshoot-server installed, you should have a friendly message in /var/log/messages telling you the source of the problem:
setroubleshoot: SELinux is preventing /usr/bin/python2.</description><content type="html"><![CDATA[<p>If you try to run Xen without libvirt on Fedora 17 with SELinux in enforcing mode, you&rsquo;ll be butting heads with SELinux in no time. You&rsquo;ll probably be staring at something like this:</p>
<pre><code># xm create -c fedora17
Using config file &quot;/etc/xen/fedora17&quot;.
Error: Disk isn't accessible
</code></pre><p>If you have <code>setroubleshoot</code> and <code>setroubleshoot-server</code> installed, you should have a friendly message in /var/log/messages telling you the source of the problem:</p>
<pre><code>setroubleshoot: SELinux is preventing /usr/bin/python2.7 from read access on the blk_file dm-1.
For complete SELinux messages. run sealert -l 4d890105-d9a4-4b3e-a674-ba7e952942dc
</code></pre><p>The Xen daemon (the python process mentioned in the SELinux denial) is running with a context type of <code>xend_t</code> but the block device I&rsquo;m trying to use for the VM has <code>fixed_disk_device_t</code>:</p>
<pre><code># ps axZ | grep xend
system_u:system_r:xend_t:s0       953 ?        SLl    0:40 /usr/bin/python /usr/sbin/xend
# ls -alZ /dev/dm-1
brw-rw----. root disk system_u:object_r:fixed_disk_device_t:s0 /dev/dm-1
</code></pre><p>SELinux isn&rsquo;t going to allow this to work. However, even if we fix this, SELinux will balk about three additional issues and we&rsquo;ll need to adjust the contexts on every new fixed block device we make. To get over the hump, change the context type on your block device to <code>xen_image_t</code> and re-run the <code>xm create</code>:</p>
<pre><code># chcon -t xen_image_t /dev/dm-1
# ls -alZ /dev/dm-1
brw-rw----. root disk system_u:object_r:xen_image_t:s0 /dev/dm-1
# xm create -c fedora17
Using config file &quot;/etc/xen/fedora17&quot;.
Error: out of pty devices
</code></pre><p>You&rsquo;ll find three new denials in /var/log/messages:</p>
<pre><code>setroubleshoot: SELinux is preventing /usr/bin/python2.7 from read access on the file group.
For complete SELinux messages. run sealert -l b1392df4-dda4-4b82-914c-1e20c62fc898
setroubleshoot: SELinux is preventing /usr/bin/python2.7 from setattr access on the chr_file 1.
For complete SELinux messages. run sealert -l 3e09edc3-aeb7-49f5-96e1-d8148afda48f
setroubleshoot: SELinux is preventing /usr/bin/python2.7 from execute access on the file pt_chown.
For complete SELinux messages. run sealert -l 86395f09-5f33-4f66-8d02-519b61e54139
</code></pre><p>As much as it pains me to suggest it, you can create a custom module to allow all four of these operations by xend:</p>
<pre><code># grep xend /var/log/audit/audit.log | audit2allow -M custom_xen
WARNING: Policy would be downgraded from version 27 to 26.
******************** IMPORTANT ***********************
To make this policy package active, execute:

semodule -i custom_xen.pp

# semodule -i custom_xen.pp
</code></pre><p>You should now be able to start your VM without any complaints from SELinux. I&rsquo;ll reiterate that this isn&rsquo;t ideal, but it&rsquo;s the best balance of security and convenience that I&rsquo;ve found so far.</p>
]]></content></item><item><title>Red Hat Summit 2012: Thursday</title><link>https://major.io/2012/06/28/red-hat-summit-2012-thursday/</link><pubDate>Fri, 29 Jun 2012 04:54:41 +0000</pubDate><guid>https://major.io/2012/06/28/red-hat-summit-2012-thursday/</guid><description>Thursday has felt like the busiest, most jam-packed day of the week. The morning started off with three keynotes from HP, Intel, and Red Hat&amp;rsquo;s CTO, Brian Stevens.
HP&amp;rsquo;s message centered around converged cloud and that customers don&amp;rsquo;t need an all or nothing solution. They can pull the best pieces from every type of hosting to do what&amp;rsquo;s best for their business. The presentation from Intel was extremely heavy on the marketing side and didn&amp;rsquo;t have much to do with Red Hat.</description><content type="html"><![CDATA[<p>Thursday has felt like the busiest, most jam-packed day of the week. The morning started off with three keynotes from HP, Intel, and Red Hat&rsquo;s CTO, Brian Stevens.</p>
<p>HP&rsquo;s message centered around converged cloud and that customers don&rsquo;t need an all or nothing solution. They can pull the best pieces from every type of hosting to do what&rsquo;s best for their business. The presentation from Intel was extremely heavy on the marketing side and didn&rsquo;t have much to do with Red Hat. Pauline Nist talked about how chip fabs operate, the heavy costs involved, and how their processors have changed over time. It felt a bit more like a sales pitch than anything else. She wrapped up with a pretty interesting time lapse video of the construction of a fab building and how the chips are made within the fab.</p>
<p><img src="/wp-content/uploads/2012/06/Photo-Jun-28-10-04-07.jpg" alt="1"></p>
<p><a href="https://www.redhat.com/about/company/management/bios/management-team-brian-stevens-bio">Brian Stevens</a> talked a lot about keeping things in the open, reducing vendor lock-in, and pushing for innovation from multiple sources. He talked a lot about <a href="http://openstack.org/">OpenStack</a> and how it allows people to deliver a consistent user experience on a very open platform.</p>
<p>There was even a video with <a href="http://rackspace.com/">Rackspace&rsquo;s</a> own <a href="https://twitter.com/jimcurry/">Jim Curry</a> talking about the unique challenges for building a cloud orchestration layer and how OpenStack solves quite a few of those challenges.</p>
<p>The sessions broke out after that and I made a beeline for <a href="http://danwalsh.livejournal.com/">Dan Walsh&rsquo;s</a> session entitled &ldquo;Multi-tenancy Virtualization Challenges and Solutions.&rdquo; I first ran into Dan at the FUDCon in Tempe and his session there about SELinux changed my mind about it for good. He covered the basics about the dangers of multi-tenant clouds and then went through multiple examples of how to mitigate the risks. Some of the technologies mentioned included <a href="http://danwalsh.livejournal.com/44090.html">sVirt</a>, SELinux policies, and <a href="https://fedoraproject.org/wiki/Features/Syscall_Filters">libseccomp</a>.</p>
<p>The discussion about libseccomp caught my attention because the idea is genius. We know that SELinux itself is quite solid. However, what if a user finds some other flaw in the kernel which allows them to circumvent the SELinux layer altogether? The seccomp additions to linux 3.5 allow you specify which syscalls a particular process is allowed to make. Dan gave an example of QEMU when running KVM. If QEMU only needs 20-30 syscalls to get its job done, why allow it to run every syscall available in the x86_64 and x86 instruction sets? With seccomp, you can specify which syscalls are allowed and what you want to have happen when a syscall is requested that isn&rsquo;t allowed. Even if someone found a good kernel flaw, they might get blocked from doing anything malicious if the process isn&rsquo;t allowed to make a syscall necessary to tear up the system.</p>
<p>I met up with the Beefy Miracle himself just before lunch for a photo op:</p>
<p><img src="/wp-content/uploads/2012/06/Photo-Jun-28-10-42-07.jpg" alt="beefy_miracle"> Major and the Beefy Miracle</p>
<p>Another good session was entitled &ldquo;Using an Open Source Framework to Catch the Bad Guy&rdquo; and it covered <a href="http://people.redhat.com/sgrubb/audit/">auditd</a> in more detail than I imagined was even possible. <a href="http://summitblog.redhat.com/2012/03/12/st-laurent-norman-mark/">Mark St. Laurent</a> talked about how auditd&rsquo;s standard configuration will work for most users but that the government requires some pretty hefty adjustments. I wasn&rsquo;t aware that you can actually tell auditd to halt the server if it runs out of room to write the audit log to the disk. The US Government requires this to be enabled on many machines.</p>
<p>The last session I attended was Sanjay Rao&rsquo;s &ldquo;Tuning Red Hat Systems for Databases&rdquo; and it was tremendous. He talked about the differences between OLTP/DSS workloads and how all kinds of settings affect their performance. He covered everything from power management to disk elevators to NUMA. There were what seemed like a million slides and all of them contained some really good information. I really wish he could have cracked the presentation into two sessions to allow for some more discussion around details. If you can find the slides from this session, be sure to look through them.</p>
<p><img src="/wp-content/uploads/2012/06/Photo-Jun-28-20-02-29.jpg" alt="11"></p>
<p>As the day ended, a line of buses pulled up in front of the convention center and we were whisked away to Fenway Park for a private party sponsored by IBM. One of my childhood dreams was to travel to Fenway Park (and Wrigley Field) and I can say I&rsquo;m halfway done! The park was amazing and it was truly an experience to walk up next to the Green Monster and look up to see how tall it really is.</p>
<p>The food was great and there was plenty of Sam Adams on tap. I ran into quite a few Red Hat folks and eventually found Thomas Cameron so I could thank him for the great SELinux session he ran on Wednesday. I&rsquo;m going to take his slides back and share them at work to inspire some confidence around managing systems with SELinux enabled. A few twitter friends came up and we had some great conversations about cloud computing, OpenStack, and good beer. For anyone who attends class or works at Harvard, be sure to sync up with Philip Durbin. He&rsquo;s a smart guy and he was a lot of fun to talk to.</p>
<p>Friday&rsquo;s the last day and then the trip home begins. I&rsquo;ll try to write up a wrap-up post tomorrow from the airport.</p>
]]></content></item><item><title>Red Hat Summit 2012: Wednesday</title><link>https://major.io/2012/06/28/red-hat-summit-2012-wednesday/</link><pubDate>Thu, 28 Jun 2012 06:03:51 +0000</pubDate><guid>https://major.io/2012/06/28/red-hat-summit-2012-wednesday/</guid><description>Wednesday was action-packed with dramatic keynotes and great sessions. The morning was kicked off by Paul Cormier and he talked about some new products coming from Red Hat. Much of the product releases were centered around cloud offerings (like Openshift) and his talk was mainly aimed at CIO&amp;rsquo;s and decision makers. There wasn&amp;rsquo;t a lot of technical detail within his talk but it was refreshing to hear a Linux vendor talk about their products as being revolutionary steps in pulling away from vendor lock-in and proprietary solutions.</description><content type="html"><![CDATA[<p>Wednesday was action-packed with dramatic keynotes and great sessions. The morning was kicked off by <a href="http://www.redhat.com/about/company/management/bios/management-team-paul-cormier-bio">Paul Cormier</a> and he talked about some new products coming from Red Hat. Much of the product releases were centered around cloud offerings (like <a href="https://openshift.redhat.com/app/">Openshift</a>) and his talk was mainly aimed at CIO&rsquo;s and decision makers. There wasn&rsquo;t a lot of technical detail within his talk but it was refreshing to hear a Linux vendor talk about their products as being revolutionary steps in pulling away from vendor lock-in and proprietary solutions.</p>
<p>Paul was followed by Irfan Khan who talked about the value of very low latency information exchange and processing. He drove home the point that the biggest value we can gain from information in the current age is related to our ability to gather and interpret the information in as close to real time as possible. I expected a speech from a SAP employee to be relatively dry but I was pleasantly surprised to find that he made a lot of good points. Irfan emphasized that big data providers need to find a way to fit into their customers' landscape without causing too much disruption while also providing some real benefits.</p>
<p>My first session was <a href="http://pl.atyp.us/">Jeff Darcy&rsquo;s</a> discussion about Red Hat&rsquo;s storage offering and what GlusterFS advancements were on the horizon. His talk was standing room only and he covered a lot of highly technical points about GlusterFS. I&rsquo;m getting the feeling that GlusterFS is gaining more momentum and that we&rsquo;ll be seeing more features around consistency and performance very soon.</p>
<p>As a fan of SELinux, I made sure that I was in <a href="http://people.redhat.com/tcameron/">Thomas Cameron&rsquo;s</a> &ldquo;SELinux for Mere Mortals&rdquo; class. Although I feel relatively confident that I can solve SELinux problems when I find them, Thomas covered a lot of easier solutions that I hadn&rsquo;t previously considered. His explanation of the basics of SELinux are a must read for any system administrator working on a Red Hat system. I managed to <a href="http://people.redhat.com/tcameron/Summit11/selinux/cameron_w_530_selinux_for_mere_mortals.pdf">find his slides from last year</a> but he said the new slides should be up by Friday.</p>
<p>I attended another good class about managing network resources with Red Hat. Although the slides were a little wordy, the content was extremely good. The speaker talked about receiving a 40Gb/sec ethernet card from Mellanox and how he bumped the performance from 8Gb/sec to 37Gb/sec by adjusting CPU pinning (for NUMA) as well as some kernel configuration around TCP buffers. It was an eye-opening discussion and it was a good session for people who are trying to find bottlenecks in their hardware.</p>
<p>The afternoon was spend mingling with GlusterFS developers and users as well as the people working the Fedora booth. I managed to pick up some Fedora stickers but I&rsquo;ve yet to get my picture taken with the life-size Beefy Miracle hot dog. That&rsquo;s my goal for tomorrow.</p>
<p>The night wrapped up with the Red Hat Certified Professionals Party at McGreevy&rsquo;s across from the Hynes Convention Center. I ran into a bunch of fellow RHCA&rsquo;s and RHCE&rsquo;s who read my blog and I was glad some of the posts were able to help them along their way to becoming certified. Congratulations to the folks who passed the early rounds of the new JBoss exams! Being the first ones through that process certainly can&rsquo;t be easy.</p>
<p>For anyone who is working towards their RHCA, <a href="/2012/02/13/looking-back-at-the-long-road-to-becoming-a-red-hat-certified-architect/">be sure to read my post about my experience with it</a>. It&rsquo;s a long haul, but the knowledge you&rsquo;ll gain will be worth it.</p>
]]></content></item><item><title>First day of the Red Hat Summit</title><link>https://major.io/2012/06/26/first-day-of-the-red-hat-summit/</link><pubDate>Wed, 27 Jun 2012 02:49:46 +0000</pubDate><guid>https://major.io/2012/06/26/first-day-of-the-red-hat-summit/</guid><description>The Red Hat Summit 2012 kicked off with an evening keynote by Jim Whitehurst explaining the changes in business value over time from the raw materials to the industry that profited from standardization.
He made some excellent points and summaries about what cloud is and isn&amp;rsquo;t. The root of his keynote was around what cloud can do for companies and not so much about what cloud really is. His key point was that the value didn&amp;rsquo;t reside in the nuts and bolts that hold a cloud together.</description><content type="html"><![CDATA[<p><a href="/wp-content/uploads/2012/06/Photo-Jun-26-17-37-57.jpg"><!-- raw HTML omitted --></a></p>
<p>The Red Hat Summit 2012 kicked off with an evening keynote by <a href="https://en.wikipedia.org/wiki/Jim_Whitehurst">Jim Whitehurst</a> explaining the changes in business value over time from the raw materials to the industry that profited from standardization.</p>
<p>He made some excellent points and summaries about what cloud is and isn&rsquo;t. The root of his keynote was around what cloud can do for companies and not so much about what cloud really is. His key point was that the value didn&rsquo;t reside in the nuts and bolts that hold a cloud together. The innovation that can be driven by the cloud itself is where the real value is. Don&rsquo;t think about the guts of what makes it work - think about what you can build on top of it. Think about cost savings, ease of provisioning, and flexibility. There were lots of comparisons to the standardization of machinery components (thanks to the <a href="https://en.wikipedia.org/wiki/Lathe">lathe</a>) and how we wouldn&rsquo;t have as much of the innovation we have today without open standards in technology.</p>
<p>Jim was followed by a SVP from IBM who had an interesting (albeit quite dry and PowerPoint-heavy) presentation about the mobile workforce and how contributions to open source efforts are driving a lot of the innovation today (think <a href="https://hadoop.apache.org/">Hadoop</a> and <a href="http://openstack.org/">OpenStack</a>). He talked about the data generated and consumed by mobile devices and how that data is changing our business values. Technology is no longer a burden but instead a critical tool for putting a company ahead of its competitors.</p>
<p>We <a href="http://www.pourhouseboston.com/">stopped in at a bar</a> after the keynotes and chatted with some locals about Boston and what makes it unique. I think we can safely say we&rsquo;re addicted to the local beers here in town and it&rsquo;s not going to be easy to leave at the week&rsquo;s end.</p>
<p>If you&rsquo;re interested in photos from the keynotes, I took a few and uploaded them to <a href="http://www.flickr.com/photos/texas1emt/sets/72157630307114342/">my Flickr photostream</a>.</p>
]]></content></item><item><title>Off to the 2012 Red Hat Summit</title><link>https://major.io/2012/06/26/off-to-the-2012-red-hat-summit/</link><pubDate>Tue, 26 Jun 2012 10:11:33 +0000</pubDate><guid>https://major.io/2012/06/26/off-to-the-2012-red-hat-summit/</guid><description>I&amp;rsquo;m on my way to my first Red Hat Summit and I&amp;rsquo;m really eager to learn some new things, meet new people, and share my experiences with others. This is my first time attending the summit and my first time in Boston.
If you&amp;rsquo;re not attending, but you&amp;rsquo;d like to follow along with some of the keynotes, you can register online to live stream those events to your desktop. I&amp;rsquo;ll try to share some of the best parts of the keynotes and the sessions here on the blog if TL;DR is a little more your style.</description><content type="html"><![CDATA[<p><a href="/wp-content/uploads/2012/06/redhatsummit2012.jpg"><!-- raw HTML omitted --></a>I&rsquo;m on my way to my first <a href="http://www.redhat.com/summit/">Red Hat Summit</a> and I&rsquo;m really eager to learn some new things, meet new people, and share my experiences with others. This is my first time attending the summit and my first time in Boston.</p>
<p>If you&rsquo;re not attending, but you&rsquo;d like to follow along with some of the keynotes, you can <a href="http://bit.ly/MMik5G">register online</a> to live stream those events to your desktop. I&rsquo;ll try to share some of the best parts of the keynotes and the sessions here on the blog if TL;DR is a little more your style.</p>
<p>As some of you probably know, I can&rsquo;t stay away from Twitter for very long. I&rsquo;ll probably be <a href="http://twitter.com/rackerhacker/">posting interesting stuff there</a> during the summit.</p>
<p><!-- raw HTML omitted --></p>
]]></content></item><item><title>Great guide for using traceroute and understanding its results</title><link>https://major.io/2012/06/13/guide-to-using-and-understanding-traceroute/</link><pubDate>Wed, 13 Jun 2012 12:40:47 +0000</pubDate><guid>https://major.io/2012/06/13/guide-to-using-and-understanding-traceroute/</guid><description>Anyone who has been a system administrator for even a short length of time has probably used traceroute at least once. Although the results often seem simple and straightforward, Richard Steenbergen pointed out in a NANOG presentation [PDF] that many people misinterpret the results and chase down the wrong issues.
Richard makes some great points about where latency comes from and when people often make the wrong assumptions regarding the source and location of the latency.</description><content type="html"><![CDATA[<p>Anyone who has been a system administrator for even a short length of time has probably used <a href="http://en.wikipedia.org/wiki/Traceroute">traceroute</a> at least once. Although the results often seem simple and straightforward, <a href="http://www.linkedin.com/in/rsteenbergen">Richard Steenbergen</a> pointed out in a <a href="http://www.nanog.org/meetings/nanog47/presentations/Sunday/RAS_Traceroute_N47_Sun.pdf">NANOG presentation</a> [PDF] that many people misinterpret the results and chase down the wrong issues.</p>
<p>Richard makes some great points about where latency comes from and when people often make the wrong assumptions regarding the source and location of the latency. For example, it&rsquo;s important to keep in mind that many routers de-prioritize ICMP packets sent directly to them and although you may think a particular hop has a ton of latency, it may just be caused by the router prioritizing the handling of other packets before yours. In addition, different routers measure latency with varying precision (4ms for Cisco).</p>
<p>He also covers tricky routing paths that you might not consider without intimate knowledge of the remote network configuration. Technologies like MPLS can hide parts of the network path from view and those hidden devices could be causing network problems for your traffic.</p>
<p>I sent Richard an email to thank him for assembling this guide and he linked me to a <a href="http://cluepon.net/ras/traceroute.pdf">tablet-handy, book-like version</a>. Both versions have some great information for system and network administrators.</p>
<p>I&rsquo;ve mirrored the PDF&rsquo;s here just in case the links above stop working:</p>
<ul>
<li><a href="/wp-content/uploads/2012/06/RAS_Traceroute_NANOG_slides.pdf">A Practical Guide to (Correctly) Troubleshooting with Traceroute (NANOG presentation slides)</a></li>
<li><a href="/wp-content/uploads/2012/06/RAS_Traceroute_Book_Format.pdf">Traceroute (Book format)</a></li>
</ul>
]]></content></item><item><title>What installing a ceiling fan can teach you about administering servers</title><link>https://major.io/2012/06/11/what-installing-a-ceiling-fan-can-teach-you-about-administering-servers/</link><pubDate>Mon, 11 Jun 2012 16:00:57 +0000</pubDate><guid>https://major.io/2012/06/11/what-installing-a-ceiling-fan-can-teach-you-about-administering-servers/</guid><description>The feedback from my last lengthy post (Lessons learned in the ambulance pay dividends in the datacenter) about analogies between EMS and server administration was mostly positive, so I decided to do it again!
Our ceiling fan in our living room died the night before we had all of our floors replaced and I knew that a portion of my weekend would be lost trying to replace it. The motor was totally dead but at least the lights still worked.</description><content type="html"><![CDATA[<p>The feedback from my last lengthy post (<em><a href="/2012/05/31/lessons-learned-in-the-ambulance-pay-dividends-in-the-datacenter/">Lessons learned in the ambulance pay dividends in the datacenter</a></em>) about analogies between EMS and server administration was mostly positive, so I decided to do it again!</p>
<p>Our ceiling fan in our living room died the night before we had all of our floors replaced and I knew that a portion of my weekend would be lost trying to replace it. The motor was totally dead but at least the lights still worked. However, in Texas, if the motor isn&rsquo;t running, no air is moving and the fan is worthless. Replacing the fan wouldn&rsquo;t be an easy task: our living room is 14 feet tall (that&rsquo;s about 4.3 meters) and our replacement fan was a pretty heavy one. Add in an almost-two-year-old running around the living room during the process and it gets a little tougher.</p>
<p>I took the old fan off pretty easily and was immediately stumped about the new one. The instructions had a method of installation that wasn&rsquo;t compatible with the outlet box in the ceiling and I didn&rsquo;t have the right bolts and washers for the job. A quick trip to Lowe&rsquo;s solved that and I was back in the game. The motor was soon hung, the wiring was connected, and I tested the wall switch. The motor didn&rsquo;t move.</p>
<p>At this point, I figured that the light assembly was required for the motor to run. I screwed everything in, connected the light assembly, and still no movement. I thought that the light switch had possibly gone bad since my wife saw the lights flicker last week when she turned off the fan. Another trip to Lowe&rsquo;s yielded a new switch. Installed that - still no movement. Double-checked the breaker. Re-did the wiring in the fan. Tried the switch again. No movement.</p>
<p>The confusion soon started. The fan was new, the motor was new, the switch was new, and the wiring was verified. I called my stepfather in the hopes that he could think of something I couldn&rsquo;t but he said I&rsquo;d thought of everything. He came over with a voltage tester and verified that the switch had power and so did the fan. He re-did the wiring and tried again. Still no movement.</p>
<p>He tilted his head for a second, then looked down at me:</p>
<blockquote>
<p>You did try pulling the chain for the fan, right? Usually the factory sets them up so that the lights and fan are off when you hang it. You know, for safety.</p>
</blockquote>
<p>After a quick tug of the chain the motor was flying. I felt like an idiot and he had a good chuckle at my expense.</p>
<p><strong>What&rsquo;s the point?</strong></p>
<p>We all do things like this when we administer servers. I touched on this back in <a href="/2010/01/03/a-new-year-system-administrator-inspiration/">January 2010</a> and it&rsquo;s probably important enough to mention again. <strong>Go for the simplest solutions first.</strong> They&rsquo;re not only easier and faster to verify, but you&rsquo;ll be guaranteed to forget about them if you dive right into the more complicated stuff at first. Also, bear in mind that the same set of instructions won&rsquo;t fit all scenarios and situations. Trust your instincts when you know they&rsquo;re right.</p>
<p>Sometimes situations crop up where you really need a second set of eyes. We&rsquo;re all eager to find the solution ourselves and avoid bothering others, but when you find yourself flailing for a solution, the best remedy may be to share your troubles with someone you trust.</p>
]]></content></item><item><title>Keep tabs on OpenStack development with OpenStack Watch on Twitter</title><link>https://major.io/2012/06/08/keep-tabs-on-openstack-development-with-openstack-watch-on-twitter/</link><pubDate>Fri, 08 Jun 2012 12:19:26 +0000</pubDate><guid>https://major.io/2012/06/08/keep-tabs-on-openstack-development-with-openstack-watch-on-twitter/</guid><description>It&amp;rsquo;s no secret that I&amp;rsquo;m a fan of Twitter and OpenStack. I found myself needing a better way to follow the rapid pace of OpenStack development and I figured that a Twitter bot would be a pretty good method for staying up to date.
I&amp;rsquo;d like to invite you to check out @openstackwatch.
First things first, it&amp;rsquo;s a completely unofficial project that I worked on during my spare time and it&amp;rsquo;s not affiliated with OpenStack in any way.</description><content type="html"><![CDATA[<p>It&rsquo;s no secret that I&rsquo;m a fan of <a href="http://twitter.com/">Twitter</a> and <a href="http://openstack.org/">OpenStack</a>. I found myself needing a better way to follow the rapid pace of OpenStack development and I figured that a Twitter bot would be a pretty good method for staying up to date.</p>
<p>I&rsquo;d like to invite you to check out <a href="http://twitter.com/openstackwatch">@openstackwatch</a>.</p>
<p>First things first, it&rsquo;s a completely unofficial project that I worked on during my spare time and it&rsquo;s not affiliated with OpenStack in any way. If it breaks, it&rsquo;s most likely my fault.</p>
<p>The bot watches for ticket status changes in <a href="http://review.openstack.org/">OpenStack&rsquo;s Gerrit server</a> and makes a tweet about the change within a few minutes. Every tweet contains the commit&rsquo;s project, owner, status, and a brief summary of the change. In addition, you&rsquo;ll get a link directly to the review page on the Gerrit server. Here&rsquo;s an example:</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>If you&rsquo;re not a fan of Twitter, there&rsquo;s a link to the RSS feed in the bio section, or you can just add this URL to your RSS feed reader:</p>
<ul>
<li><a href="http://api.twitter.com/1/statuses/user_timeline.rss?screen_name=openstackwatch">http://api.twitter.com/1/statuses/user_timeline.rss?screen_name=openstackwatch</a></li>
</ul>
<p>If you can come up with any ideas for improvements, please <a href="http://twitter.com/rackerhacker">let me know</a>!</p>
]]></content></item><item><title>supernova: Manage multiple OpenStack nova environments with ease</title><link>https://major.io/2012/06/05/supernova-manage-multiple-openstack-nova-environments-with-ease/</link><pubDate>Tue, 05 Jun 2012 13:12:17 +0000</pubDate><guid>https://major.io/2012/06/05/supernova-manage-multiple-openstack-nova-environments-with-ease/</guid><description>While working on multiple nova (OpenStack Compute) environments at Rackspace, I found myself thrashing between multiple terminal windows where I had exported environment variables for python-novaclient to use. I ended up requesting some image and instance deletions in a terminal window only to find that I&amp;rsquo;d done the deletions in the wrong nova environment. Once I realized what I&amp;rsquo;d done (and after a small bit of cursing), I knew there had to be a better way to work with multiple environments.</description><content type="html"><![CDATA[<p><a href="http://major.io/wp-content/uploads/2012/06/firstworldproblems-multiplenovaenvironments.jpg"><!-- raw HTML omitted --></a>While working on multiple nova (<a href="http://openstack.org/projects/compute/">OpenStack Compute</a>) environments at Rackspace, I found myself thrashing between multiple terminal windows where I had exported environment variables for <a href="https://github.com/openstack/python-novaclient">python-novaclient</a> to use. I ended up requesting some image and instance deletions in a terminal window only to find that I&rsquo;d done the deletions in the wrong nova environment. Once I realized what I&rsquo;d done (and after a small bit of cursing), I knew there had to be a better way to work with multiple environments.</p>
<p>That&rsquo;s the purpose behind a small python project of mine: <a href="http://major.github.com/supernova/">supernova</a>.</p>
<p>Using supernova gives you a nice set of benefits:</p>
<ul>
<li>switch between environments quickly</li>
<li>no worrying about which environment variables are exported in which terminal</li>
<li>novarc files are a thing of the past</li>
<li>share your simple configuration file skeleton with your teams</li>
<li>credentials can be stored in your OS keyring/keychain</li>
<li>add novaclient debugging to particular requests without touching configuration files</li>
</ul>
<p>Installation is very straightforward:</p>
<pre><code>git clone git://github.com/major/supernova.git
cd supernova
python setup.py install
</code></pre><p>All of the <a href="http://major.github.com/supernova/">configuration instructions and usage examples are over in GitHub</a>. As with any of the code I write, if you find a problem or spot an idea for an improvement, submit an issue or pull request. I try to jump on those as soon as I can.</p>
]]></content></item><item><title>Fedora 17 XVA ready to import into XenServer</title><link>https://major.io/2012/06/03/fedora-17-xva-ready-to-import-into-xenserver/</link><pubDate>Mon, 04 Jun 2012 02:25:45 +0000</pubDate><guid>https://major.io/2012/06/03/fedora-17-xva-ready-to-import-into-xenserver/</guid><description>After I wrote a post about my kickstart update for Fedora 17, I asked if anyone wanted a XVA export of a working Fedora 17 instance. Without further ado, here&amp;rsquo;s the bzip2-compressed XVA file ready to be decompressed and imported into XenServer:
F17.xva.bz2 (221MB) The kickstart used to generate the XVA can be found on GitHub. To import this virtual machine export, use XenCenter or ssh to your XenServer instance and run:</description><content type="html"><![CDATA[<p><a href="/wp-content/uploads/2012/06/xen_logo_small.png"><!-- raw HTML omitted --></a>After I <a href="/2012/05/30/fedora-17-released-xenserver-kickstarts-updated/">wrote a post</a> about my kickstart update for Fedora 17, I asked if anyone wanted a XVA export of a working Fedora 17 instance. Without further ado, here&rsquo;s the bzip2-compressed XVA file ready to be decompressed and imported into XenServer:</p>
<ul>
<li><a href="http://c3364925.r25.cf0.rackcdn.com/F17.xva.bz2">F17.xva.bz2</a> (221MB)</li>
</ul>
<p>The kickstart used to generate the XVA can be <a href="https://github.com/rackerhacker/kickstarts/blob/master/fedora17-minimal-xenserver6.ks">found on GitHub</a>. To import this virtual machine export, use XenCenter or ssh to your XenServer instance and run:</p>
<pre><code>xe vm-import filename=F17.xva
</code></pre><p>The VM should try to get its network configuration via DHCP and you can log in as <strong>root</strong> with the password <strong>qwerty</strong>. It should go without saying, but you ought to change that password at your earliest opportunity. (It&rsquo;s #20 on the <a href="http://www.nytimes.com/2010/01/21/technology/21password.html">New York Times' list of simplest passwords</a>.)</p>
]]></content></item><item><title>Lessons learned in the ambulance pay dividends in the datacenter</title><link>https://major.io/2012/05/31/lessons-learned-in-the-ambulance-pay-dividends-in-the-datacenter/</link><pubDate>Thu, 31 May 2012 15:00:47 +0000</pubDate><guid>https://major.io/2012/05/31/lessons-learned-in-the-ambulance-pay-dividends-in-the-datacenter/</guid><description>While cleaning up a room at home in preparation for some new flooring, I found my original documents from when I first became certified as an Emergency Medical Technician (EMT) in Texas. That was way back in May of 2000 and I received it just before I graduated from high school later in the month. After renewing it twice, I decided to let my certification go this year. It expires today and although I&amp;rsquo;m sad to see it go, I know that sometimes you have to let one thing go so that you can excel in something else.</description><content type="html"><![CDATA[<p>While cleaning up a room at home in preparation for some new flooring, I found my original documents from when I first became certified as an Emergency Medical Technician (EMT) in Texas. That was way back in May of 2000 and I received it just before I graduated from high school later in the month. After renewing it twice, I decided to let my certification go this year. It expires today and although I&rsquo;m sad to see it go, I know that sometimes you have to let one thing go so that you can excel in something else.</p>
<p>I <a href="http://twitter.com/#!/rackerhacker/status/207854697434976256">mentioned this</a> yesterday on Twitter and <a href="https://twitter.com/jnewland">Jesse Newland from GitHub</a> came back with a good reply:</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>It began to make more sense the more I thought about it (and once <a href="http://twitter.com/#!/markimbriaco/status/207894151788888067">Mark Imbriaco</a> and <a href="http://twitter.com/#!/jcsalterego/status/207893999716016130">Jerry Chen</a> asked for it as well). Working in Operations in a large server environment has a lot of similarities to working on an ambulance:</p>
<ul>
<li>both involve fixing things (whether it&rsquo;s technology or an illness/injury)</li>
<li>there are plenty of highly stressful situations in both occupations</li>
<li>lots of money is riding on the decisions made at a keyboard or at a stretcher</li>
<li>if you can&rsquo;t work as a team, you can&rsquo;t do either job effectively</li>
<li>there is always room for improvement (and I do mean always)</li>
<li>not having all the facts can lead to perilous situations</li>
</ul>
<p>Without further ado, here are some lessons I learned on the ambulance which have really helped me as a member of an operations team. I&rsquo;ve broken them up into separate chunks (more on that lesson shortly) to make it a little easier to read:</p>
<p><strong>Whatever happens, keep your cool</strong></p>
<p>One of the worst situations you can have on an ambulance is when an EMT or paramedic feels overwhelmed to the point that they can&rsquo;t function. Imagine rolling up with your partner on a multi-car collision with several injured drivers and passengers. It&rsquo;s just the two of you at the scene and you need to start working. You&rsquo;re obviously outnumbered and you won&rsquo;t be able to treat everyone at once. Now, imagine that your partner hasn&rsquo;t seen this type of situation and is actively buckling under the pressure. The quality of care you&rsquo;re trained to deliver and the efficiency at which you can deliver it has now been slashed in half. Even worse, getting your partner back on track might take some work and this may slow you down even more.</p>
<p>The same can be said about working on large incidents affecting your customers. You&rsquo;re probably going to be outnumbered by the amount of servers having a problem and you won&rsquo;t get them back online any sooner if you&rsquo;re beginning to freak out. Just remember, as with servers and as with people (most of the time), they were running fine at one time and they&rsquo;ll be running fine again soon. Your job is to bridge the gap between those times and try to get to the end goal as soon as possible.</p>
<p>You might miss some things or not complete certain tasks as well as you&rsquo;d like to. You might slip and make things worse than they were before. One step backward and two steps forward is painful, but it&rsquo;s still progress. Keep your mind clear and focused so that you can use your knowledge, skills, and experience to pave a path out.</p>
<p><strong>Triage, triage, triage</strong></p>
<p>Going back to the multi-car collision scenario, you&rsquo;re well aware that you won&rsquo;t be able to take care of everyone at once. This is where skillful triaging is key. Find the people who are in the most dire situations and treat them first. Although it seems counterproductive, you may have to pass over the people who are hurt so badly that they have little chance of survival. Spending additional time with those people may cause patients with treatable conditions to deteriorate further unnecessarily. It may sound callous, but I&rsquo;d rather have a few people with serious injuries get treated than lose all of them while I&rsquo;m treating someone who is essentially near death.</p>
<p>Lots of this can be carried over into maintaining servers. When a big problem occurs, you can spend all of your time wrestling with servers that are beyond repair only to watch the remainder of your environment crash around you. Find ways to stop the bleeding first and then figure out some solid fixes.</p>
<p>For example, if your database cluster gets out of sync, think of the things you can do to reduce the amount of bad data coming in. Could you have your load balancer send traffic elsewhere? Could you disable your application until the database problem is solved? If you lose sight of what&rsquo;s causing you immediate pain, you may spend all day trying to fix the broken database cluster only to find that you have many multitudes more data to sort out due to your application running throughout the whole process.</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p><strong>Learn from your mistakes and don&rsquo;t dwell on them</strong></p>
<p>Medical mistakes can range anywhere from unnoticeable to career-endingly serious. One missed tidbit of a patient&rsquo;s medical history, one small math error when administering drugs, or one slip of the hand can make a bad situation much worse. I&rsquo;ve made mistakes on the ambulance and I&rsquo;ve been very fortunate that almost all of them were very small and inconsequential. If I made one that went unnoticed, I made an effort to notify my supervisor and whoever would be taking over care of my patient. For the mistakes I didn&rsquo;t even notice on my own, my partners would often be quick to point out the error.</p>
<p>Getting called out on a mistake (even if you call yourself out on it) hurts. Funnel the frustration from it into a plan to fix it. Do some reading to understand the right solution. Learn mnemonics to remember in stressful situations. Make notes for yourself. Practice. Those small steps will reduce your mistakes through increasing your confidence.</p>
<p>Although most Ops engineers should survive big incidents with their lives intact, mistakes are still made and they can be costly. Mistakes can turn into a positive learning experience for everyone on the team. There&rsquo;s a <a href="http://codeascraft.etsy.com/2012/05/22/blameless-postmortems/">great post</a> on Etsy&rsquo;s &ldquo;Code as Craft&rdquo; blog about this topic.</p>
<p>John Allspaw wrote:</p>
<blockquote>
<p>A funny thing happens when engineers make mistakes and feel safe when giving details about it: they are not only willing to be held accountable, they are also enthusiastic in helping the rest of the company avoid the same error in the future. They are, after all, the most expert in their own error.</p>
</blockquote>
<p>The only true mistake is the one which is made but never learned from. Accept it, learn from it, teach others to avoid it and move forward.</p>
<p><strong>Get all the facts to avoid assumptions</strong></p>
<p>My mother (an Engish teacher) always told me to put the most important things at the beginning and and the end when I write. If there&rsquo;s anything more important than keeping your cool under duress, it&rsquo;s that you should have as many facts as you can before you get started.</p>
<p>On the ambulance, you&rsquo;re always looking for the very small clues to ensure that your patient is getting the proper treatment. You may walk up to a patient with slurred speech who can&rsquo;t walk straight. You may think he&rsquo;s drunk until you see a small bottle of insulin and a blood glucose meter. Wait, did his blood sugar bottom out? Did he take his insulin at the wrong time? Did he take the wrong amount? Missing that small bit of information may lead you to put your &ldquo;drunk&rdquo; patient onto a stretcher without the proper treatment only to find that you&rsquo;re dealing with a diabetic coma as you get to the hospital. That incorrect assumption could have turned a serious situation into a possibly fatal one.</p>
<p>Responding to incidents with servers is much the same. Skipping over a server with data corruption or not realizing that a change was made (and documented) earlier in the day could lead to serious damage. Forgetting to check log files, streams of exceptions, or reports from customers can lead to bad assumptions which could extend your downtime or cause the loss of data.</p>
<ul>
<li>
<ul>
<li>*In summary, here&rsquo;s my internal runbook from when I was working full time as an EMT:<!-- raw HTML omitted --></li>
</ul>
<ol>
<li>Stop the bleeding</li>
<li>Find the root cause of the problem</li>
<li>Make a plan (or plans) to fix it</li>
<li>Vet out your best plan with your partner if it seems risky</li>
<li>Execute the plan</li>
<li>Monitor the results</li>
<li>Review the plan&rsquo;s success or failure with a trusted expert</li>
</ol>
</li>
</ul>
<p>When I&rsquo;m fighting outages at work, I reach back into this runbook and try my best to follow the steps. It helps me keep my cool, reduce mistakes, and proceed with better plans. I&rsquo;d be curious to hear your feedback about how this runbook could work for your Operations team or if you have ideas for edits.</p>
]]></content></item><item><title>Fedora 17 released, XenServer kickstarts updated</title><link>https://major.io/2012/05/30/fedora-17-released-xenserver-kickstarts-updated/</link><pubDate>Wed, 30 May 2012 20:50:42 +0000</pubDate><guid>https://major.io/2012/05/30/fedora-17-released-xenserver-kickstarts-updated/</guid><description>Fedora 17 was released yesterday and you can download it now!
In addition, I made some edits to my kickstarts repository on GitHub to match up with the final release of Fedora 17. The kickstart took less than ten minutes to complete even with a remote repository configured for RPM packages.
Fedora &amp;amp; XenServer users: Would posting an actual XVA file for download make it easier for you to get started?</description><content type="html"><![CDATA[<p><a href="/wp-content/uploads/2012/05/its-a-beefy-miracle.png"><!-- raw HTML omitted --></a></p>
<p>Fedora 17 was released yesterday and you can <a href="http://fedoraproject.org/en/get-fedora-options">download it now</a>!</p>
<p>In addition, I made some edits to my <a href="https://github.com/rackerhacker/kickstarts">kickstarts repository on GitHub</a> to match up with the final release of Fedora 17. The kickstart took less than ten minutes to complete even with a remote repository configured for RPM packages.</p>
<p><strong>Fedora &amp; XenServer users:</strong> Would posting an actual XVA file for download make it easier for you to get started? I&rsquo;ve considered doing this from time to time since you can easily import the VM directly into XenServer as a template via the command line or XenCenter. Let me know if you&rsquo;d find this useful and I should be able to get it done this week.</p>
]]></content></item><item><title>Fedora 17 is a go!</title><link>https://major.io/2012/05/25/fedora-17-is-a-go/</link><pubDate>Fri, 25 May 2012 12:54:13 +0000</pubDate><guid>https://major.io/2012/05/25/fedora-17-is-a-go/</guid><description>I was glad to see the Fedora 17 Final is declared GOLD! announcement on the Fedora devel-announce list. If all goes well, we should see the formal release on May 29.
As some of you might have noticed, I&amp;rsquo;m a fan of Xen. If you want to get Fedora 17 rolling in XenServer right now, just try my minimal kickstart. I&amp;rsquo;ll try to get a XVA/OVF-formatted image online in the next few days so you can hit the ground running if you&amp;rsquo;re in a hurry.</description><content type="html"><![CDATA[<p><img src="/wp-content/uploads/2012/01/fedorainfinity.png" alt="1"></p>
<p>I was glad to see the <em><a href="http://lists.fedoraproject.org/pipermail/devel-announce/2012-May/000933.html">Fedora 17 Final is declared GOLD!</a></em> announcement on the Fedora devel-announce list. If all goes well, we should see the <a href="http://fedoraproject.org/wiki/Releases/17/Schedule">formal release on May 29</a>.</p>
<p>As some of you might have noticed, I&rsquo;m a fan of Xen. If you want to get Fedora 17 rolling in XenServer right now, just try my <a href="https://github.com/rackerhacker/kickstarts">minimal kickstart</a>. I&rsquo;ll try to get a XVA/OVF-formatted image online in the next few days so you can hit the ground running if you&rsquo;re in a hurry.</p>
<p>Check out the new <a href="http://fedoraproject.org/wiki/Releases/17/FeatureList#Fedora_17_Accepted_Features">Fedora 17 features and improvements</a> and prepare yourself for the <a href="http://beefymiracle.org/">Beefy Miracle</a>.</p>
]]></content></item><item><title>Lesser-known but extremely handy Linux tools</title><link>https://major.io/2012/05/11/lesser-known-but-extremely-handy-linux-tools/</link><pubDate>Fri, 11 May 2012 21:28:58 +0000</pubDate><guid>https://major.io/2012/05/11/lesser-known-but-extremely-handy-linux-tools/</guid><description>Kristóf Kovács has a fantastic post about some lesser-known Linux tools that can really come in handy in different situations.
If you haven&amp;rsquo;t tried dstat (I hadn&amp;rsquo;t until I saw Kristóf&amp;rsquo;s post), this is a great one to try. You can keep a running tally on various server metrics including load average, network transfer, and disk operations.
Here is some sample output:
----total-cpu-usage---- ---paging-- ---load-avg--- ------memory-usage----- -net/total- ---procs--- --io/total- ---system-- ----tcp-sockets---- usr sys idl wai hiq siq| in out | 1m 5m 15m | used buff cach free| recv send|run blk new| read writ| int csw |lis act syn tim clo 0 0 100 0 0 0| 0 0 |0.</description><content type="html"><![CDATA[<p>Kristóf Kovács <a href="http://kkovacs.eu/cool-but-obscure-unix-tools">has a fantastic post</a> about some lesser-known Linux tools that can really come in handy in different situations.</p>
<p>If you haven&rsquo;t tried <code>dstat</code> (I hadn&rsquo;t until I saw Kristóf&rsquo;s post), this is a great one to try. You can keep a running tally on various server metrics including load average, network transfer, and disk operations.</p>
<p>Here is some sample output:</p>
<pre><code>----total-cpu-usage---- ---paging-- ---load-avg--- ------memory-usage----- -net/total- ---procs--- --io/total- ---system-- ----tcp-sockets----
usr sys idl wai hiq siq|  in   out | 1m   5m  15m | used  buff  cach  free| recv  send|run blk new| read  writ| int   csw |lis act syn tim clo
  0   0 100   0   0   0|   0     0 |0.07 0.25 0.25| 866M  249M  537M  387M|1314B  180B|  0   0   0|   0     0 |  70    80 | 13   7   0   0   5
  0   0 100   0   0   0|   0     0 |0.07 0.25 0.25| 866M  249M  537M  387M|1779B 1004B|  0   0   0|   0     0 |  84    78 | 13   7   0   0   5
  0   0 100   0   0   0|   0     0 |0.07 0.25 0.25| 866M  249M  537M  387M| 904B  362B|1.0   0 1.0|   0     0 |  75    86 | 13   9   0   0   5
  0   0 100   0   0   0|   0     0 |0.07 0.25 0.25| 866M  249M  537M  386M|2203B 1559B|  0   0   0|   0     0 | 180   127 | 13   7   0   0   5
  0   0 100   0   0   0|   0     0 |0.07 0.25 0.25| 866M  249M  537M  386M| 260B  130B|  0   0   0|   0     0 |  53    66 | 13   7   0   0   5
  0   0 100   0   0   0|   0     0 |0.07 0.25 0.25| 866M  249M  537M  387M|  52B  114B|  0   0   0|   0     0 |  54    77 | 13   7   0   0   5
  0   0 100   0   0   0|   0     0 |0.07 0.25 0.25| 866M  249M  537M  387M|2271B  872B|  0   0   0|   0     0 |  94    79 | 13   7   0   0   5
  0   0 100   0   0   0|   0     0 |0.07 0.25 0.25| 866M  249M  537M  387M|  52B  130B|  0   0   0|   0     0 |  54    74 | 13   7   0   0   5
  0   0 100   0   0   0|   0     0 |0.07 0.25 0.25| 866M  249M  537M  387M|1126B 1254B|  0   0   0|   0  24.0 |  80    87 | 13   7   0   0   5
  0   0 100   0   0   0|   0     0 |0.07 0.25 0.25| 866M  249M  537M  387M|1030B  130B|  0   0   0|   0     0 |  88    82 | 13   7   0   0   5
  0   0 100   0   0   0|   0     0 |0.06 0.24 0.25| 866M  249M  537M  387M| 578B  114B|  0   0   0|   0     0 |  53    64 | 13   7   0   0   5
  0   0 100   0   0   0|   0     0 |0.06 0.24 0.25| 866M  249M  537M  387M|1597B  890B|  0   0   0|   0     0 |  85    79 | 13   7   0   0   5
  0   0 100   0   0   0|   0     0 |0.06 0.24 0.25| 866M  249M  537M  387M| 552B  114B|  0   0   0|   0     0 |  63    77 | 13   7   0   0   5
  0   0 100   0   0   0|   0     0 |0.06 0.24 0.25| 866M  249M  537M  387M|1624B 1254B|  0   0   0|   0     0 |  81    75 | 13   7   0   0   5
  0   0 100   0   0   0|   0     0 |0.06 0.24 0.25| 866M  249M  537M  387M| 478B  114B|  0   0   0|   0     0 |  67    73 | 13   7   0   0   5
  0   0 100   0   0   0|   0     0 |0.06 0.24 0.25| 866M  249M  537M  387M| 418B  114B|  0   0   0|   0     0 |  59    74 | 13   7   0   0   5
  0   0 100   0   0   0|   0     0 |0.06 0.24 0.25| 866M  249M  537M  387M|1265B  874B|  0   0   0|   0     0 |  82    73 | 13   7   0   0   5
  0   0 100   0   0   0|   0     0 |0.06 0.24 0.25| 866M  249M  537M  387M| 758B  114B|  0   0   0|   0     0 |  60    80 | 13   7   0   0   5
  0   0 100   0   0   0|   0     0 |0.06 0.24 0.25| 866M  249M  537M  387M|1236B 1255B|  0   0   0|   0  4.00 |  93    79 | 13   7   0   0   5
  0   0 100   0   0   0|   0     0 |0.06 0.24 0.25| 866M  249M  537M  387M|  52B  130B|  0   0   0|   0     0 |  71    70 | 13   7   0   0   5
  0   0 100   0   0   0|   0     0 |0.05 0.23 0.25| 866M  249M  537M  387M| 214B  114B|  0   0   0|   0     0 |  55    73 | 13   7   0   0   5
  0   0 100   0   0   0|   0     0 |0.05 0.23 0.25| 866M  249M  537M  387M|1201B  890B|  0   0   0|   0     0 |  80    80 | 13   7   0   0   5
  0   0 100   0   0   0|   0     0 |0.05 0.23 0.25| 866M  249M  537M  387M| 108B  114B|  0   0   0|   0     0 |  53    66 | 13   7   0   0   5
  0   0 100   0   0   0|   0     0 |0.05 0.23 0.25| 866M  249M  537M  387M|1344B 1254B|  0   0   0|   0  10.0 | 119    85 | 13   7   0   0   5
  0   0 100   0   0   0|   0     0 |0.05 0.23 0.25| 866M  249M  537M  387M| 172B  130B|  0   0   0|   0  8.00 |  80    82 | 13   7   0   0   5
</code></pre><p>Learn more about <code>dstat</code> on <a href="http://dag.wieers.com/home-made/dstat/">Dag Wieërs' site</a>.</p>
]]></content></item><item><title>Performance and redundancy boost for icanhazip.com</title><link>https://major.io/2012/04/18/performance-and-redundancy-boost-for-icanhazip-com/</link><pubDate>Wed, 18 Apr 2012 23:30:06 +0000</pubDate><guid>https://major.io/2012/04/18/performance-and-redundancy-boost-for-icanhazip-com/</guid><description>It&amp;rsquo;s been a few years since I started a little project to operate a service to return your IPv4 and IPv6 address. Although there are a bunch of other sites that offer this service as well, I&amp;rsquo;ve been amazed by the gradually increasing traffic to icanhazip.com.
Here&amp;rsquo;s a sample of the latest statistics:
Hits per day: 1.8 million (about 21 hits/second) Unique IP addresses per day: 25,555 Hits per day from IPv6 addresses: 1,069 (a little sad) Bandwidth used per day: ~ 400MB The site is now running on multiple Cloud Servers at Rackspace behind a load balancer cluster.</description><content type="html"><![CDATA[<p>It&rsquo;s been a few years since I started <a href="/2009/07/31/get-the-public-facing-ip-for-any-server-with-icanhazip-com/">a little project</a> to operate a service to return your IPv4 and IPv6 address. Although there are a bunch of other sites that offer this service as well, I&rsquo;ve been amazed by the gradually increasing traffic to <a href="http://icanhazip.com/">icanhazip.com</a>.</p>
<p>Here&rsquo;s a sample of the latest statistics:</p>
<ul>
<li>Hits per day: <strong>1.8 million</strong> (about 21 hits/second)</li>
<li>Unique IP addresses per day: <strong>25,555</strong></li>
<li>Hits per day from IPv6 addresses: <strong>1,069</strong> (a little sad)</li>
<li>Bandwidth used per day: <strong>~ 400MB</strong></li>
</ul>
<p>The site is now running on multiple <a href="http://www.rackspace.com/cloud/cloud_hosting_products/servers/">Cloud Servers</a> at <a href="http://www.rackspace.com/cloud/">Rackspace</a> behind a <a href="http://www.rackspace.com/cloud/cloud_hosting_products/loadbalancers/">load balancer cluster</a>. In addition, the DNS records are hosted with Rackspace&rsquo;s <a href="http://www.rackspace.com/cloud/cloud_hosting_products/dns/">Cloud DNS</a> service.</p>
<p>This should allow the site to reply more quickly and reliably. If you have suggestions for other improvements, let me know!</p>
]]></content></item><item><title>Getting a Technical Job at Rackspace</title><link>https://major.io/2012/04/09/getting-a-technical-job-at-rackspace/</link><pubDate>Mon, 09 Apr 2012 14:00:56 +0000</pubDate><guid>https://major.io/2012/04/09/getting-a-technical-job-at-rackspace/</guid><description>You&amp;rsquo;ve probably noticed that the blog has slowed down a bit recently. Part of the slowdown is due to an uptick in work required to get OpenStack Nova and its related software up and running at Rackspace for Cloud Servers and another part of it is a severe case of writer&amp;rsquo;s block. I threw out some questions on Twitter about the topics people would like to see covered in some new posts and a commonly requested topic was employment at Rackspace.</description><content type="html"><![CDATA[<p>You&rsquo;ve probably noticed that the blog has slowed down a bit recently. Part of the slowdown is due to an uptick in work required to get <a href="http://www.openstack.org/">OpenStack</a> <a href="http://nova.openstack.org/">Nova</a> and its related software up and running at <a href="http://rackspace.com/">Rackspace</a> for <a href="http://www.rackspace.com/cloud/cloud_hosting_products/servers/">Cloud Servers</a> and another part of it is a severe case of writer&rsquo;s block. I threw out some questions on Twitter about the topics people would like to see covered in some new posts and a commonly requested topic was employment at Rackspace.</p>
<p><img src="/wp-content/uploads/2012/04/boromir_rackspace_job.jpg" alt="5"></p>
<p><em>First things first, getting a job at Rackspace isn&rsquo;t easy.</em> We don&rsquo;t <em>intentionally</em> make the process difficult. It&rsquo;s just that the work we do is unique and demanding.</p>
<p>We work in a fast-paced, extremely dynamic team-centric environment. While some people in the company work in extremely small teams or sometimes all by themselves, that&rsquo;s pretty few and far between. We look for people who can survive and flourish in this atmosphere and we look for people who can do it all while working as a team. Even with all of this hustle and bustle, we still remember why we&rsquo;re doing it: <strong>the pursuit of Fanatical Support for our customers</strong>.</p>
<p>Another thing to keep in mind is that there&rsquo;s no true secret for making it through the application process. There&rsquo;s no magic combination of skills or &ldquo;silver bullet&rdquo; that will scoot you through. Every candidate is reviewed individually for each position. There have been several times at the end of an interview where we&rsquo;ve gotten together and said: &ldquo;Wow, this candidate is solid, but they&rsquo;re just not right for this position. Let&rsquo;s find the right spot and see if there&rsquo;s a spot open.&rdquo; We look for the right candidate for the right position at the right time.</p>
<p>One of the best ways to get ahead in the screening or interview process is to do a little homework about Rackspace and the products we offer. Much of this is covered in a <a href="/2011/05/02/do-your-homework-before-a-technical-interview/">post I wrote in 2011</a>. You&rsquo;ll go into the interviews with more confidence and it will be much more obvious that you&rsquo;re really interested in the position.</p>
<p>Don&rsquo;t be discouraged if the process takes a little longer than you expected. When I was hired in 2006, I went through two phone pre-screens and then three back-to-back interviews in person. Things have changed a little since then and I&rsquo;ve heard of some candidates receiving two to three pre-screens via telephone and then one or two interviews in person. The additional screening and interviews may be due to Rackers trying to find the right fit for a particular applicant. As I said previously, we look for the right fit for each applicant. We may consider you for a different position than you applied for if we feel like your skill set or personality fits that role better.</p>
<p>A very common question is what to wear to a Rackspace interview. It&rsquo;s confusing to know exactly what&rsquo;s expected since we have Rackers in the building wearing everything from suits to flip-flops. This is where you really have to go with your gut. Interviewing for a customer-facing sales position while wearing a hoodie and shorts is probably going to bring a suboptimal result. Keep in mind that there&rsquo;s really nothing negative about overdressing (but keep your tuxedo in the closet, seriously). I wore a shirt and tie for my interviews in 2006 but my tie got caught in the car door and was shredded. After a lot of cursing, I took off the tie and decided to wing it with my dress shirt. Nobody ever said a word about it.</p>
<p>Remember to be flexible during the interviews. You might be asked to draw a solution on a whiteboard or think through a really complicated situation. Roll with it and keep your confidence up. When you don&rsquo;t know something, admit it, but then talk about how you&rsquo;d research an answer.</p>
<p>There&rsquo;s one last thing to keep in mind and it&rsquo;s really critical. If you&rsquo;re ever asked about how you would solve a problem or how you solved a problem in the past, <strong>don&rsquo;t divulge any information which is confidential or proprietary to your current company</strong>. Just tell the interviewers that you&rsquo;ve solved the solution in the past but you&rsquo;ll need to keep things vague to maintain confidentiality. We will definitely understand and we will encourage you to maintain that confidentiality.</p>
<p>Leave your comments if you have any! I&rsquo;ll be glad to answer any questions you have.</p>
]]></content></item><item><title>Why technical people should blog (but don’t)</title><link>https://major.io/2012/03/30/why-technical-people-should-blog-but-dont/</link><pubDate>Fri, 30 Mar 2012 14:30:45 +0000</pubDate><guid>https://major.io/2012/03/30/why-technical-people-should-blog-but-dont/</guid><description>I originally wrote this post for the Rackspace Blogbut I decided to post it here in case some of my readers might have missed it. Please feel free to leave your comments at the end of the post. Sometimes people talk to me about posts I&amp;rsquo;ve written on my blog, or posts they wish I would write. At some point during the discussion, I&amp;rsquo;ll almost always ask the person why they don&amp;rsquo;t start up their own blog or contribute to someone else&amp;rsquo;s.</description><content type="html"><![CDATA[<p><!-- raw HTML omitted -->I originally wrote this post for the <!-- raw HTML omitted -->Rackspace Blog<!-- raw HTML omitted --> but I decided to post it here in case some of my readers might have missed it. Please feel free to leave your comments at the end of the post.<!-- raw HTML omitted --></p>
<hr>
<p>Sometimes people talk to me about posts I&rsquo;ve written on my blog, or posts they wish I would write. At some point during the discussion, I&rsquo;ll almost always ask the person why they don&rsquo;t start up their own blog or contribute to someone else&rsquo;s. Very few people actually seem interested when I probe them about writing posts on technical topics.<!-- raw HTML omitted --></p>
<p>My mother was always the one who told me (and her students) that everyone has a story. She said that writing could be therapeutic in ways you probably won&rsquo;t consider until you&rsquo;ve written something that someone else enjoys. Just as software developers exist to write software for their users, writers exist to write stories for their readers. There&rsquo;s nothing that says technical people can&rsquo;t become excellent writers who inspire others to learn and share their knowledge with others.</p>
<p>The goal of this post is to encourage technical people to enjoy writing, write efficiently and feel comfortable doing it. I&rsquo;ll roll through some of the most common responses I&rsquo;ve received about why technical people don&rsquo;t blog about what they know.</p>
<blockquote>
<p>I don&rsquo;t think I&rsquo;m really an expert on anything. I&rsquo;m not an authority on any topic I can think of.</p>
</blockquote>
<p>I&rsquo;m leading off with this response because it&rsquo;s the most critical to refute. If you don&rsquo;t take away anything else from this post, let it be this: you don&rsquo;t need to be an expert on a topic to write about it.</p>
<p>You can find examples of this by rolling through some of the posts on my blog. I&rsquo;d consider myself to be an expert on one, maybe two topics, but I&rsquo;ve written over 450 posts in the span of just over five years. I certainly didn&rsquo;t write all of those about the one or two topics I know best.</p>
<p>Write about what you know and don&rsquo;t be afraid to do a little research to become an authority on something. A great example of this was my post, entitled &ldquo;<a href="/2012/02/02/kerberos-for-haters/">Kerberos for haters</a>.&rdquo; I had almost no expertise in Kerberos. In fact, I couldn&rsquo;t even configure it properly for my RHCA exam! However, I did a ton of research and began to understand how most of the pieces fit together. Many other people were just as confused and I decided to pack all of the knowledge I had about Kerberos into a blog post. Positive and negative feedback rolled in and it was obvious that my post taught some readers, inspired some others and angered a few.</p>
<p>What a great way to lead into the next response:</p>
<blockquote>
<p>What if I say something that isn&rsquo;t correct? I&rsquo;ll look like an idiot in front of the whole internet!</p>
</blockquote>
<p>Been there, done that. Every writer makes errors and comes up with bad assumptions at least once. Readers will call you out on your mistakes (some do it delicately while others don&rsquo;t) and it&rsquo;s your duty to correct your post or correct the reader. I&rsquo;ve written posts with errors, and I&rsquo;ve gotten a little lazy on my fact-checking from time to time. As my middle school journalism teacher always reminded me, the most important part of a mistake is what you do to clean it up and learn from it.</p>
<p>In short: you&rsquo;ll make mistakes. As long as you&rsquo;ve done your due diligence to minimize them and respond to them promptly, your readers should forgive you.</p>
<p>Speaking of errors:</p>
<blockquote>
<p>I&rsquo;m great at a command prompt but my spelling and grammar are awful. I write terribly.</p>
</blockquote>
<p>This is easily fixed. If you&rsquo;re one of those folks who live the do-it-yourself type of lifestyle, pick up a copy of <a href="http://en.wikipedia.org/wiki/The_Elements_of_Style"><em>The Elements of Style</em></a> by Strunk &amp; White. There are free PDF versions online or you can borrow one from your nearest journalist. No matter the situation you&rsquo;re in, this book has details about where punctuation should and shouldn&rsquo;t be, how to structure sentences and paragraphs, and how to properly cite your sources (really vital for research posts).</p>
<p>Hauling around a copy of an ultra-dry reference book may not be your thing. If that&rsquo;s the case, find someone you know who has a knack for writing. You can usually find helpful folks in marketing or corporate communications in most big companies who will take your post and return it covered in red ink ready for corrections (thanks, Garrett!). I&rsquo;ve even <a href="http://fiverr.com/categories/all/tags/proofreading/order/latest/pages/1">spotted some folks on Fiverr</a> who will do this for as low as $5.</p>
<p>I&rsquo;ll wrap up with the second most common response:</p>
<blockquote>
<p>I don&rsquo;t know who I&rsquo;m writing for? What if I write about something simple and the really technical folks think I&rsquo;m a noob? What if I write something crazy complex and it goes over most people&rsquo;s heads?</p>
</blockquote>
<p>I&rsquo;ve done both of these. Most Linux system administrators worth their salt know how to add and remove iptables rules, and they&rsquo;d consider it to be pretty trivial work. Would it surprise you to know that out of over 450 posts, my post about <a href="/2007/02/09/delete-single-iptables-rules/">deleting a single iptables rule</a> is in the top five most accessed posts per month? I receive just over 11 percent of my monthly hits to this post. People are either learning from it or they can&rsquo;t remember how to delete the rule and they want to use the post as a quick reference. Either way, the post is valuable to many people even if I think it&rsquo;s the simplest topic possible.</p>
<p>On the flip side, I went nuts and wrote up a <a href="/redundant-cloud-hosting-configuration-guide/">complete how-to</a> for a redundant cloud hosting configuration complete with LVS, glusterfs, MySQL on DRBD, memcached, haproxy and ldirectord. I thought it would be valuable knowledge to a few folks but that it might sail over the heads of most of my readers. Again, I was wrong. The post is constantly in the top 10 most visited posts on the blog and I&rsquo;ve probably received more feedback via comments, email and IRC about that post than any other. Once again, a post I thought would be mostly useless turned into a real conversation starter.</p>
<p><strong>Let&rsquo;s conclude and wrap up.</strong> Keep these things in mind if you feel discouraged about writing:</p>
<ul>
<li>Write about what interests you whether you&rsquo;re an expert on it or not</li>
<li>Don&rsquo;t be afraid to fail</li>
<li>Be responsive to your readers</li>
<li>Even if you think nobody will read your post, write it</li>
<li>Always ensure your voice shines through in your writing — this is what makes it special and appealing</li>
</ul>
]]></content></item><item><title>mysql-json-bridge: a simple JSON API for MySQL</title><link>https://major.io/2012/03/28/mysql-json-bridge-a-simple-json-api-for-mysql/</link><pubDate>Thu, 29 Mar 2012 02:34:53 +0000</pubDate><guid>https://major.io/2012/03/28/mysql-json-bridge-a-simple-json-api-for-mysql/</guid><description>My quest to get better at Python led me to create a new project on GitHub. It&amp;rsquo;s called mysql-json-bridge and it&amp;rsquo;s ready for you to use.
Why do we need a JSON API for MySQL?
The real need sprang from a situation I was facing daily at Rackspace. We have a lot of production and pre-production environments which are in flux but we need a way to query data from various MySQL servers for multiple purposes.</description><content type="html"><![CDATA[<p>My quest to get better at <a href="http://python.org">Python</a> led me to create a new project on GitHub. It&rsquo;s called <a href="https://github.com/rackerhacker/mysql-json-bridge">mysql-json-bridge</a> and it&rsquo;s ready for you to use.</p>
<p><strong>Why do we need a JSON API for MySQL?</strong></p>
<p>The real need sprang from a situation I was facing daily at <a href="http://rackspace.com/">Rackspace</a>. We have a lot of production and pre-production environments which are in flux but we need a way to query data from various MySQL servers for multiple purposes. Some folks need data in ruby or python scripts while others need to drag in data with .NET and Java. Wrestling with the various adapters and all of the user privileges on disparate database servers behind different firewalls on different networks was less than enjoyable.</p>
<p>That&rsquo;s where this bridge comes in.</p>
<p>The bridge essentially gives anyone the ability to talk to multiple database servers across different environments by talking to a single endpoint with easily configurable security and encryption. As long as the remote user can make an HTTP POST and parse some JSON, they can query data from multiple MySQL endpoints.</p>
<p><strong>How does it work?</strong></p>
<p>It all starts with a simple HTTP POST. I&rsquo;ve become a big fan of the Python <a href="http://python-requests.org">requests</a> module. If you&rsquo;re using it, this is all you need to submit a query:</p>
<pre><code>import requests
payload = {'sql': 'SELECT * FROM some_tables WHERE some_column=some_value'}
url = &quot;http://localhost:5000/my_environment/my_database&quot;
r = requests.post(url, data=payload)
print r.text
</code></pre><p>The bridge takes your query and feeds it into the corresponding MySQL server. When the results come back, they&rsquo;re converted to JSON and returned via the same HTTP connection.</p>
<p><strong>What technology does it use?</strong></p>
<p><a href="http://flask.pocoo.org/">Flask</a> does the heavy lifting for the HTTP requests and <a href="https://github.com/facebook/tornado/blob/master/tornado/database.py">Facebook&rsquo;s Tornado database class</a> wraps the <a href="http://mysql-python.sourceforge.net/">MySQLdb</a> module in something a little more user friendly. Other than those modules, <a href="http://pyyaml.org/">PyYAML</a> and <a href="http://python-requests.org">requests</a> are the only other modules not provided by the standard Python libraries.</p>
<p><strong>Is it fast?</strong></p>
<p>Yes. I haven&rsquo;t done any detailed benchmarks on it yet, but the overhead is quite low even with a lot of concurrency. The biggest slowdowns come from network latency between you and the bridge or between the bridge and the database server. Keep in mind that gigantic result sets will take a longer time to transfer across the network and get transformed into JSON.</p>
<p><strong>I found a bug. I have an idea for an improvement. You&rsquo;re terrible at Python.</strong></p>
<p>All feedback (and every pull request) is welcome. I&rsquo;m still getting the hang of Python (hey, I&rsquo;ve only been writing in it seriously for a few weeks!) and I&rsquo;m always eager to learn a new or better way to accomplish something. Feel free to create an issue in GitHub or submit a pull request with a patch.</p>
]]></content></item><item><title>Compare commits between two git branches</title><link>https://major.io/2012/03/15/compare-commits-between-two-git-branches/</link><pubDate>Thu, 15 Mar 2012 15:00:24 +0000</pubDate><guid>https://major.io/2012/03/15/compare-commits-between-two-git-branches/</guid><description>I found myself stuck in a particularly nasty situation a few weeks ago where I had two git branches with some commits that were mixed up. Some commits destined for a branch called development ended up in master. To make matters worse, development was rebased on top of master and the history was obviously mangled.
My goal was to find out which commits existed in development but didn&amp;rsquo;t exist anywhere in master.</description><content type="html"><![CDATA[<p>I found myself stuck in a particularly nasty situation a few weeks ago where I had two git branches with some commits that were mixed up. Some commits destined for a branch called development ended up in master. To make matters worse, development was rebased on top of master and the history was obviously mangled.</p>
<p>My goal was to find out which commits existed in development but didn&rsquo;t exist anywhere in master. From there, I needed to find out which commits existed in master that didn&rsquo;t exist in development. That would give me all of the commits that needed to be in the development branch.</p>
<p>I constructed this awful looking bash mess to figure out which commits were in development but not in master:</p>
<p>I had a list of commits that existed in development but not in master:</p>
<pre><code>965cf71 Trollface
acda854 Some patch 2
bf1f3e2 Some patch 1
db1980c Packaging
</code></pre><p>From there, I could swap <code>MASTER</code> and <code>DEV</code> to figure out which commits existed in master but not in development. Only a couple of commits showed up and these were the ones which were committed and pushed to master inadvertently. After a couple of careful cherry picks and reversions, my branches were back to normal.</p>
]]></content></item><item><title>New Fedora and EPEL package: httpry</title><link>https://major.io/2012/03/14/new-fedora-and-epel-package-httpry/</link><pubDate>Wed, 14 Mar 2012 14:00:29 +0000</pubDate><guid>https://major.io/2012/03/14/new-fedora-and-epel-package-httpry/</guid><description>A fellow Racker showed me httpry about five years ago and I&amp;rsquo;ve had in my toolbox as a handy way to watch HTTP traffic. I&amp;rsquo;d used some crazy tcpdump arguments and some bash one-liners to pull out the information I needed but I never could get the live look that I really wanted.
Here&amp;rsquo;s an example of what httpry&amp;rsquo;s output looks like on a busy site like icanhazip.com:
GET icanhazip.</description><content type="html"><![CDATA[<p>A fellow Racker showed me <a href="http://dumpsterventures.com/jason/httpry/">httpry</a> about five years ago and I&rsquo;ve had in my toolbox as a handy way to watch HTTP traffic. I&rsquo;d used some crazy tcpdump arguments and some bash one-liners to pull out the information I needed but I never could get the live look that I really wanted.</p>
<p>Here&rsquo;s an example of what httpry&rsquo;s output looks like on a busy site like icanhazip.com:</p>
<pre><code> GET	icanhazip.com	/	HTTP/1.1	-	-
2012-03-13 23:29:39 192.x.x.x	186.x.x.x &lt; -	-	-	HTTP/1.1	200	OK
2012-03-13 23:29:39 187.x.x.x	192.x.x.x &gt; GET	icanhazip.com	/	HTTP/1.0	-	-
2012-03-13 23:29:39 192.x.x.x	187.x.x.x &lt; -	-	-	HTTP/1.0	200	OK
2012-03-13 23:29:39 188.x.x.x	192.x.x.x &gt; GET	icanhazip.com	/	HTTP/1.1	-	-
2012-03-13 23:29:39 192.x.x.x	188.x.x.x &lt; -	-	-	HTTP/1.1	200	OK
2012-03-13 23:29:39 189.x.x.x	192.x.x.x &gt; GET	icanhazip.com	/	HTTP/1.1	-	-
2012-03-13 23:29:39 192.x.x.x	189.x.x.x &lt; -	-	-	HTTP/1.1	200	OK
</code></pre><p>You can watch the requests come in and the responses go out in real time. It even allows for BPF-style packet filters which allow you to narrow down the source and/or destination IP addresses and ports you want to watch. You can run it as a foreground process or as a daemon depending on your needs.</p>
<p>It&rsquo;s now available as a <a href="https://admin.fedoraproject.org/updates/httpry">RPM package</a> for Fedora 15, 16, 17 (and rawhide) as well as EPEL 6 (for RHEL/CentOS/SL 6).</p>
]]></content></item><item><title>Installing XenServer 6.0.2 on an AOpen MP57</title><link>https://major.io/2012/03/12/installing-xenserver-6-0-2-on-an-aopen-mp57/</link><pubDate>Mon, 12 Mar 2012 17:00:56 +0000</pubDate><guid>https://major.io/2012/03/12/installing-xenserver-6-0-2-on-an-aopen-mp57/</guid><description>Getting XenServer installed on some unusual platforms takes a bit of work and the AOpen MP57 is a challenging platform for a XenServer 6.0.2 installation.
My MP57 box came with the i57QMx-vP motherboard. If yours came with something else, this post may or may not work for you.
You&amp;rsquo;ll need the XenServer 6 installation ISO burned to a CD to get started. Boot the CD in your MP57 and wait for the initial boot screen to appear.</description><content type="html"><![CDATA[<p><a href="/wp-content/uploads/2012/03/BBM-APN-MP57D.jpg"><!-- raw HTML omitted --></a>Getting XenServer installed on some unusual platforms takes a bit of work and the <a href="http://global.aopen.com/products_detail.aspx?Auno=3047">AOpen MP57</a> is a challenging platform for a XenServer 6.0.2 installation.</p>
<p>My MP57 box came with the i57QMx-vP motherboard. If yours came with something else, this post may or may not work for you.</p>
<p>You&rsquo;ll need the <a href="https://www.citrix.com/lang/English/lp/lp_1688615.asp">XenServer 6 installation ISO</a> burned to a CD to get started. Boot the CD in your MP57 and wait for the initial boot screen to appear. Type <strong>safe</strong> at the prompt and press enter. Go through the normal installation steps and reboot.</p>
<p>After the reboot, you&rsquo;ll notice that there&rsquo;s no video output for dom0. Hop on another nearby computer and ssh to your XenServer installation using the root user and the password that you set during the installation process. Open up <code>/boot/extlinux.conf</code> in your favorite text editor and make sure the <code>label xe</code> section looks like this:</p>
<pre><code>label xe
  # XenServer
  kernel mboot.c32
  append /boot/xen.gz mem=1024G dom0_max_vcpus=4 dom0_mem=752M lowmem_emergency_pool=1M crashkernel=64M@32M acpi=off console=vga --- /boot/vmlinuz-2.6-xen root=LABEL=root-aouozuoo ro xencons=hvc console=hvc0 console=tty0 vga=785 --- /boot/initrd-2.6-xen.img
</code></pre><p>The <code>console=vga</code> adjustment ensures that the dom0 console is piped to the vga output and <code>acpi=off</code> fixes the lockup that will occur when the vga output is sent to your display. I also removed <code>splash</code> and <code>quiet</code> from the kernel line so that I could see all of the boot messages in detail.</p>
]]></content></item><item><title>The StartupBus stops at Rackspace</title><link>https://major.io/2012/03/11/the-startupbus-stops-at-rackspace/</link><pubDate>Sun, 11 Mar 2012 17:00:29 +0000</pubDate><guid>https://major.io/2012/03/11/the-startupbus-stops-at-rackspace/</guid><description>I helped welcome the folks on the StartupBus from SXSW as they arrived at Rackspace&amp;rsquo;s headquarters last Friday. Here&amp;rsquo;s a video sampling of the day&amp;rsquo;s events:
Want to participate in quirky stuff like this while serving customers and working on the latest technology? Become a Racker.</description><content type="html"><![CDATA[<p>I helped welcome the folks on the StartupBus from SXSW as they arrived at Rackspace&rsquo;s headquarters last Friday. Here&rsquo;s a video sampling of the day&rsquo;s events:</p>
<p><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --></p>
<p>Want to participate in quirky stuff like this while serving customers and working on the latest technology? <a href="http://jobs.rackspace.com/">Become a Racker</a>.</p>
]]></content></item><item><title>Handy hints for using dtrace on the Mac</title><link>https://major.io/2012/03/10/handy-hints-for-using-dtrace-on-the-mac/</link><pubDate>Sat, 10 Mar 2012 18:49:59 +0000</pubDate><guid>https://major.io/2012/03/10/handy-hints-for-using-dtrace-on-the-mac/</guid><description>I&amp;rsquo;m a big fan of Linux tools which allow you to monitor things in great detail. Some of my favorites are strace, the systemtap tools, and sysstat. Finding tools similar to these on a Mac is a little more difficult.
There&amp;rsquo;s a great blog post from Brendan Gregg&amp;rsquo;s blog that covers a lot of detail around dtrace and its related tools:
http://dtrace.org/blogs/brendan/2011/10/10/top-10-dtrace-scripts-for-mac-os-x/ One of the handier tools is iosnoop.</description><content type="html"><![CDATA[<p>I&rsquo;m a big fan of Linux tools which allow you to monitor things in great detail. Some of my favorites are strace, the <a href="http://sourceware.org/systemtap/">systemtap</a> tools, and sysstat. Finding tools similar to these on a Mac is a little more difficult.</p>
<p>There&rsquo;s a great blog post from Brendan Gregg&rsquo;s blog that covers a lot of detail around dtrace and its related tools:</p>
<ul>
<li><a href="http://dtrace.org/blogs/brendan/2011/10/10/top-10-dtrace-scripts-for-mac-os-x/">http://dtrace.org/blogs/brendan/2011/10/10/top-10-dtrace-scripts-for-mac-os-x/</a></li>
</ul>
<p>One of the handier tools is <code>iosnoop</code>. It gives you a much easier to read (and easier to generate) view of the disk I/O on your Mac. If you remember, I talked about how to do this in Linux in the <a href="/2010/12/07/tap-into-your-linux-system-with-systemtap/">systemtap</a> post as well as the post about <a href="/2008/03/11/hunting-down-elusive-sources-of-iowait/">finding elusive sources of iowait</a>. This could give you a lot of handy information if you&rsquo;re staring at beachballs regularly while your disk drive churns.</p>
]]></content></item><item><title>Preparing for Red Hat Exams</title><link>https://major.io/2012/02/28/preparing-for-red-hat-exams/</link><pubDate>Tue, 28 Feb 2012 21:35:28 +0000</pubDate><guid>https://major.io/2012/02/28/preparing-for-red-hat-exams/</guid><description>I originally wrote this post for the Rackspace Blogbut I&amp;rsquo;ve posted it here just in case anyone following my blog&amp;rsquo;s feed finds it useful. Feel free to share your feedback!Getting yourself ready for any type of examination is usually a stressful experience that involves procrastination and some late nights leading up to the test. Every time I take one, I always say to myself, “I’m really going to get ahead of this next time and study early.</description><content type="html"><![CDATA[<p><!-- raw HTML omitted -->I originally wrote this post for the <!-- raw HTML omitted -->Rackspace Blog<!-- raw HTML omitted --> but I&rsquo;ve posted it here just in case anyone following my blog&rsquo;s feed finds it useful. Feel free to share your feedback!<!-- raw HTML omitted --></p>
<p>Getting yourself ready for any type of examination is usually a stressful experience that involves procrastination and some late nights leading up to the test. Every time I take one, I always say to myself, “I’m really going to get ahead of this next time and study early. This last minute stuff is terrible.” But I always forget all of this as the next exam rolls around.</p>
<p>Quick note: As you read through the remainder of the post, you may wonder why some of it is a bit vague. Every Red Hat test taker is under a NDA to prevent disclosure of test information that may reduce the security of the exam itself. Penalties start with losing credit for the exams previously taken and they can escalate up to legal action. I hope you’ll understand why I’m not able to go into details about certain portions of the Red Hat examinations.</p>
<p>I’ve taken seven Red Hat exams already: two for the RHCE and five for the RHCA. These tests certainly aren’t easy, but there are some good guidelines and tips you can use to make your studying efforts less stressful and more productive. Without further ado, here are my recommendations for prospective Red Hat examinees:</p>
<h4 id="build-a-flexible-study-environment">Build a flexible study environment</h4>
<p>This is critical. You’ll need some spare servers or some available virtual machines to practice the objectives on each exam. However, don’t feel like you need to spend the money on a Red Hat subscription to get your studying done. Most of the test objectives on the majority of exams can be completed with very similar Linux distributions, like Scientific Linux or CentOS. Look for a version of the distribution that is closest to what you’ll be tested on at exam time. Your study environment should meet some basic criteria:</p>
<ul>
<li>You should be able to quickly build and tear down servers or virtual machines</li>
<li>Keep the latency to your environment low to avoid getting frustrated</li>
<li>Use applications like VirtualBox, VMWare Fusion/Workstation to practice on your own computer</li>
<li>Consider using VMs from cloud providers if you’re under a time crunch</li>
</ul>
<p>Some exams may require some bare-metal access to the server itself (especially <a href="https://www.redhat.com/courses/ex442_red_hat_enterprise_system_monitoring_and_performance_tuning_expertise_exam/">EX442</a>), so keep that in mind when you’re looking for a good practice environment. You may need some specific network or storage setups for some exams (as with <a href="https://www.redhat.com/courses/ex436_red_hat_enterprise_clustering_and_storage_management_expertise_exam/">EX436</a>). If you’re not sure what you need, be sure to ask your instructor or someone else you know who has taken the exam already.</p>
<h4 id="prioritize-doing-over-reading">Prioritize doing over reading</h4>
<p>The Red Hat exams are all hands-on, practical exams. You won’t find any essays or multiple-choice questions in these exams. Although the materials from Red Hat are full of good information, reading this information can only get you so far. You need to practice setting up the services on your own to be fully prepared for the test. If you’re not pressed for time, reading through the book can give you some details about the lab sequences, which you might miss by solely reading through labs themselves.</p>
<h4 id="research-the-why-not-the-what-to-remember">Research the why, not the what, to remember</h4>
<p>This is especially important for the RHCA exam track. You may find that there is a ton of material to cover for the exam and that it’s difficult to remember each command to bring a certain service online or to repair a problem. Instead of thinking through the problem as “first, I do this, then I do this”, try to understand why each step is important in the first place.</p>
<p>Here’s a good example. I’ll be the first one to admit that Kerberos drives me crazy. I’ve even <a href="http://rackerhacker.com/2012/02/02/kerberos-for-haters/">written posts</a> about it. The commands seemed really archaic, the daemons didn’t make sense, and the lack of readline support in the Kerberos tools made me want to throw my computer out the window (come on, MIT!). I put my class materials aside, went to Google in a browser, and started researching Kerberos.</p>
<p>I read some of MIT’s documentation, ventured over to Wikipedia, and poked at some of the documentation within the Kerberos RPM packages. After a while, I began to realize how it all fit together. “Okay,” I thought to myself, “I need principals in a keytab to do these things, but I need to have a database for the admin stuff first.” Suddenly, the order of things in my head wasn’t just memorized any longer. The process of operations seemed to make logical sense because I fully understood how the pieces of a Kerberos infrastructure fit together.</p>
<p>If you start to get discouraged, take a break and learn more about why you’re doing what you’re doing. Once it becomes second nature, working through the problems on the exam becomes much easier.</p>
<h4 id="lean-on-your-available-resources">Lean on your available resources</h4>
<p>Don’t forget that there are other knowledgeable folks available to talk to when you get bogged down. Lean on other RHCE’s, RHCA’s, or experienced Linux users to get the answers or explanations you need. If you already have a Red Hat certification, head over to the <a href="https://certforums.redhat.com/login.php">Red Hat Certification Forums</a> and meet up with other examinees that are discussing test preparation.</p>
<p>Also, you’ll find some knowledgeable (but sometimes snarky or quirky) people on IRC who are eager to point you in the right direction. Try the #rhel, #centos, or #fedora channels if you’re struggling through the configuration of a certain service. Many Linux users may roll their eyes about it, but Twitter is also a pretty good way to reach out to people who have a lot of Linux experience.</p>
<h4 id="summary">Summary</h4>
<p>Remember to lean on the knowledge of others, get hands-on with the test objectives and do your research when you’re frustrated. The exams from Red Hat are generally difficult and cover a lot of material, but with the right amount of preparation and determination you can pass the exams and get the certifications you want.</p>
]]></content></item><item><title>Five years of rackerhacker.com</title><link>https://major.io/2012/02/14/five-years-of-rackerhacker-com/</link><pubDate>Tue, 14 Feb 2012 14:45:43 +0000</pubDate><guid>https://major.io/2012/02/14/five-years-of-rackerhacker-com/</guid><description>Today marks the fifth year that this blog has existed on the internet. I bought the domain on February 14th, 2007 and tossed together a quick WordPress installation (I can&amp;rsquo;t even remember the version now!) to hold my notes that I was gathering at work.
Rackspace as an entry-level Linux system administrator. The abrupt change from &amp;ldquo;top dog at the startup&amp;rdquo; to &amp;ldquo;wow, I don&amp;rsquo;t know anything about Linux&amp;rdquo; caught me by surprise and I was trying to stuff as much knowledge into my brain as quickly as I could.</description><content type="html"><![CDATA[<p>Today marks the fifth year that this blog has existed on the internet. I bought the domain on February 14th, 2007 and tossed together a quick WordPress installation (I can&rsquo;t even remember the version now!) to hold my notes that I was gathering at work.</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p><a href="http://rackspace.com/">Rackspace</a> as an entry-level Linux system administrator. The abrupt change from &ldquo;top dog at the startup&rdquo; to &ldquo;wow, I don&rsquo;t know anything about Linux&rdquo; caught me by surprise and I was trying to stuff as much knowledge into my brain as quickly as I could. My teammates at Rackspace were eager to show me the ropes of wrangling servers and supporting customers.</p>
<p>As I mentioned already, the blog started out just as a place to stuff my notes from the things I learned at work. I figured that it would be nice to store it in a searchable format but it would also be great if I could link other people to certain posts if they needed more information to fix a problem. It was a way to retain knowledge but yet give it back to the people around me who needed it.</p>
<p>The blog has hit 456 posts (this one is #457) and it&rsquo;s gone from a few page views per day to just over 20,000 per day. Here are the top five most accessed posts (since I&rsquo;ve been keeping stats):</p>
<ol>
<li><a href="/2008/11/02/syncing-an-iphone-with-a-new-mac-without-hassles/">Syncing an iPhone with a new Mac without hassles</a></li>
<li><a href="/2008/01/24/ip_conntrack-table-full-dropping-packet/">ip_conntrack: table full, dropping packet</a></li>
<li><a href="/2007/02/09/delete-single-iptables-rules/">Delete a single iptables rule</a></li>
<li><a href="/2007/01/24/increase-mysql-connection-limit/">Increase MySQL connection limit</a></li>
<li><a href="/2008/06/24/mysql-error-1040-too-many-connections/">MySQL Error 1040: too many connections</a></li>
</ol>
<p>I&rsquo;d like to send out a big thanks to the people who read this blog, add comments (or complaints!), and suggest new topics. You are the reason why I take the time to keep this blog going.</p>
]]></content></item><item><title>Looking back at the long road to becoming a Red Hat Certified Architect</title><link>https://major.io/2012/02/13/looking-back-at-the-long-road-to-becoming-a-red-hat-certified-architect/</link><pubDate>Mon, 13 Feb 2012 15:00:41 +0000</pubDate><guid>https://major.io/2012/02/13/looking-back-at-the-long-road-to-becoming-a-red-hat-certified-architect/</guid><description>The grades came back last Friday and I&amp;rsquo;ve passed the last exam in the requirements to become a Red Hat Certified Architect (RHCA). I was fortunate enough to be part of Rackspace&amp;rsquo;s RHCA pilot program and we took our first exam back at the end of 2010. It&amp;rsquo;s definitely a good feeling to be finished and I&amp;rsquo;m definitely ready to give back some knowledge to the readers of this blog.</description><content type="html"><![CDATA[<p>The grades came back last Friday and I&rsquo;ve passed the last exam in the requirements to become a <a href="http://www.redhat.com/training/certifications/rhca/">Red Hat Certified Architect (RHCA)</a>. I was fortunate enough to be part of Rackspace&rsquo;s RHCA pilot program and we took our first exam back at the end of 2010. It&rsquo;s definitely a good feeling to be finished and I&rsquo;m definitely ready to give back some knowledge to the readers of this blog.</p>
<p><strong>First things first:</strong> there are going to be many part of this post which probably aren&rsquo;t as specific as you&rsquo;d like. A lot of that is due to the NDA that all Red Hat examinees agree to when they take an exam. We aren&rsquo;t allowed to talk about what was on the exam or our experiences during the exam. If we do, penalties range from smaller things like losing certifications all the way up to serious stuff like legal action. It goes without saying that I want to protect the security of the exams, I don&rsquo;t want to lose my certifications, and I don&rsquo;t want to hire a lawyer. Please try to keep this in mind if you yearn for more specifics than I&rsquo;m able to give.</p>
<p><strong>Red Hat Certified Engineer</strong></p>
<p>The <a href="http://www.redhat.com/training/courses/ex200/examobjective">RHCSA</a> and <a href="http://www.redhat.com/training/courses/ex300/examobjective">RHCE</a> exams are the first step on the path to the RHCA. You can&rsquo;t take any of the RHCA prerequisite exams without it. These exams cover a really broad spectrum of material including apache configuration, NFS, iptables and mail services. The two links above will take you to the exam objectives for each exam.</p>
<p>I&rsquo;ve always recommended the RHCE exam for Linux administrators who are trying to sharpen their skills and get to the next level whether they use Red Hat or not. The exam covers a lot of good material that makes a solid foundation for any Linux user without throwing in too many Red Hat-specific knowledge.</p>
<p>The exam (like all Red Hat exams) is fully practical. There are no multiple choice questions or essays. You&rsquo;ll have to meet all of the objectives by logging into a local Red Hat system and making the system do what it needs to do.</p>
<p>Quick tips for the RHCSA/RHCE exams:</p>
<ul>
<li>Keep your eye on the clock. Time can really get away from you if you get stuck in the weeds on a problem that should be relatively straightforward.</li>
<li>Leave time at the end to check your work. When you set up a lot of services, it&rsquo;s inevitable that you might configure a service for one problem that breaks the functionality required by a problem you completed already.</li>
<li>Always reboot before you leave. We all forget to use <code>chkconfig</code> when we&rsquo;re in a hurry.</li>
<li>Practice, practice, practice. There&rsquo;s not one objective on this exam that you can&rsquo;t test in a VM on your own.</li>
</ul>
<p><strong>Red Hat Enterprise System Monitoring and Performance Tuning</strong></p>
<p>Our group at Rackspace started off with <a href="http://www.redhat.com/training/courses/ex442/examobjective">EX442</a> and it was a very difficult way to start off the RHCA track. Take a look at the objectives and you&rsquo;ll see that much of the exam is related to tweaking system performance and then monitoring that performance with graphs and raw data. You&rsquo;ll have to turn a lot of knobs on the kernel and you&rsquo;ll need to know where to store these configurations so they&rsquo;ll be persistent.</p>
<p>In addition, the objective regarding TCP buffers and related settings is a real challenge. You&rsquo;ll have to wrestle with some math that appears to be relatively simple, but can get confusing quickly. Some of the settings can&rsquo;t really be checked to know if your setting is correct. The objectives mention tuning disk scheduling - you don&rsquo;t really have the time or tools to know if your setting is ideal.</p>
<p>Quick tips for EX442:</p>
<ul>
<li>Use the documentation available to you. Install the <code>kernel-doc</code> package while you practice and during the exam.</li>
<li>Be careful with your math. You have a Linux machine in front of you! Don&rsquo;t forget about <code>bc</code>.</li>
<li>Watch your units. Know the difference between a kilobyte (KB) and a kibibyte (KiB).</li>
<li>Make comments in files where you adjust kernel configurations. It will help you keep track of which question the kernel adjustment is meant to satisfy.</li>
</ul>
<p><strong>Red Hat Enterprise Storage Management</strong></p>
<p>I&rsquo;m surprised to say this now, but I actually enjoyed <a href="http://www.redhat.com/training/courses/ex436/examobjective">EX436</a>. I&rsquo;ve always used other clustering tools like heartbeat and pacemaker, but I&rsquo;ve never had the need to use the Red Hat Cluster Suite. Although RHCS definitely has a lot of quirks and rough edges, it&rsquo;s pretty solid once you get familiar with the GUI and command line tools.</p>
<p>You get the opportunity to mess around with some pretty useful technology like iSCSI, GFS, and clustered LVM. These are things that you&rsquo;re probably already using or will be using soon in a large server environment. The web interface for RHCS is quite peculiar and you may find yourself wanting to put your fist through the screen when you&rsquo;re staring down the endless animated GIFs when the cluster is syncing its configuration. Do your best to be patient because you certainly don&rsquo;t want to short circuit the cluster sync.</p>
<p>Quick tips for EX436:</p>
<ul>
<li>Be patient. You&rsquo;ll feel like the RHCS web interface is mocking you when you&rsquo;re pressed for time.</li>
<li>Watch the clock. It&rsquo;s extremely easy to burn a lot of time on this exam if you get stuck on a particular problem.</li>
<li>Double check your entries in the web interface. Make sure you&rsquo;re doing things in the right order and that you&rsquo;ve set up the prerequisites before adding services to the cluster. If you get it wrong, you could put your cluster into a weird state.</li>
<li>Use man pages. If you don&rsquo;t mess with GFS a lot, the man pages will save you in a pinch.</li>
</ul>
<p><strong>Red Hat Enterprise Deployment and Systems Management</strong></p>
<p>If there&rsquo;s one exam where time management is critical, it&rsquo;s <a href="http://www.redhat.com/training/courses/ex401/examobjective">EX401</a>. Importing data into the Satellite Server takes quite a bit of time and there&rsquo;s almost nothing you can do to speed it up. It probably goes without saying, but as with most long-running tasks, you&rsquo;ll want to run it in screen. The last thing you&rsquo;d ever want is to abort the import due to an errant click or CTRL-C (I did it while practicing - it&rsquo;s aggravating).</p>
<p>There are other test objectives which you can either complete or partially complete while you wait for the import to finish.</p>
<p>Also, take the time to really dig into the Satellite Server web interface while your practicing for the exam. Knowing where to find the most common configuration items will really save some time when you&rsquo;re in the exam. You can sometimes get pretty bogged down in the interface so don&rsquo;t forget to use multiple tabs to keep your work organized.</p>
<p>I felt like this exam was the easiest out of the bunch since you could go back and test every single question with good time management. <em>Did I mention how important time management was on this exam already?</em> If I forgot to mention it earlier, be sure to focus on time management for this test.</p>
<p>Quick tips for EX401:</p>
<ul>
<li>Time management will make or break you on this test. Keep an eye on the clock and make sure you&rsquo;ve done absolutely every piece of the exam that you can while you wait for the server to do its work.</li>
<li>Scour the web interface. Keep a mental map in your mind where the big chunks of configuration items are.</li>
<li>Go back and test everything. If you manage your time well, you should have enough time to verify each and every objective on this exam.</li>
</ul>
<p><strong>Red Hat Enterprise Directory Services and Authentication</strong></p>
<p>At first, <a href="http://www.redhat.com/training/courses/ex423/examobjective">EX423</a> looks pretty straightforward. Red Hat&rsquo;s authentication configuration tools make LDAP authentication setup pretty easy. However, this exam comes with a lot of curveballs.</p>
<p>The GUI interface for the Directory Services component is a little frustrating to use. I found that the GUI stopped responding to keyboard input occasionally unless I clicked on another window and came back. If you misconfigure the SSL certificates in the interface, your LDAP server is down for the count. If you don&rsquo;t input the correct data into the setup scripts at the beginning, you might not notice it until much later when it&rsquo;s either too difficult to dig yourself out of the hole or it&rsquo;s too late to start over with a clean configuration.</p>
<p>I didn&rsquo;t feel pressed for time on this exam too much and that was pretty refreshing after taking the EX401 test. It&rsquo;s extremely critical to watch what you type and click on this exam. Some mistakes can be quickly corrected while others may require you to blow away the LDAP server configuration and re-provision the whole thing.</p>
<p>Quick tips for EX423:</p>
<ul>
<li>Always watch what you&rsquo;re typing. A simple mistake can lead to confusion or bigger issues down the road.</li>
<li>Don&rsquo;t ignore the LDIF objectives. As you practice, you&rsquo;ll find that manipulating LDIF files is a little more involved than you expected.</li>
<li>Practice starting over. Throw out your Directory Services configuration and get the experience of what it&rsquo;s like to start over and get back in the game.</li>
</ul>
<p><strong>Red Hat Enterprise Security: Network Services</strong></p>
<p>There&rsquo;s no sugar coating it - <a href="http://www.redhat.com/training/courses/ex333/examobjective">EX333</a> is a beast. It&rsquo;s a six hour exam broken into two three-hour chunks. It covers a ton of material and I refer to it as &ldquo;the RHCE on steroids.&rdquo; You might argue that I thought it was hard since it was the last test and I was ready to be finished, but I really think this exam is a tough one.</p>
<p>Practicing for the Kerberos and DNS objectives was the hardest for me. I just couldn&rsquo;t understand Kerberos, no matter how hard I tried. The realization that I would really have to learn it soon set in. I dug into the Kerberos design documentation on MIT&rsquo;s site, read the summaries on Wikipedia, and scoured the documentation available in the Kerberos RPM packages. Once I understood <em>why</em> Kerberos is set up the way it is and <em>why</em> the security measures are present, everything began to come together. I was able to remember the steps not because I was memorizing them, but because I understood how Kerberos worked.</p>
<p>When you&rsquo;re working through the DNS objectives, keep an eye out for punctuation. I blew through a good 20 minutes in what seemed like the blink of an eye when I forgot a period in my TSIG key configuration while studying. Make sure you use the resources available to you, like <code>system-config-bind</code> and sample configs in <code>/usr/share/doc/bind*/examples/</code>. Get to know commands like <code>dig</code> really well.</p>
<p>If you&rsquo;re overwhelmed by OpenSSL&rsquo;s command line syntax, check out the <code>/etc/pki/tls/misc/CA</code> script. There are some handy comments at the top of the script that explain how to use it. You can also pluck OpenSSL commands right out of the script if you need to run them yourself.</p>
<ul>
<li>Don&rsquo;t just memorize. Do some research to understand how everything fits together.</li>
<li>Manage your time. DNS and Kerberos have lots of small nuances that can become time sinks when done incorrectly.</li>
<li>Use the available documentation and tools. Try practicing without study materials so that you&rsquo;re forced to use the docs and tools available within the server.</li>
</ul>
<p><strong>Ranking the exams</strong></p>
<p>A couple of folks on Twitter asked me to rank the exams from most difficult to least difficult. Keep in mind that these are a little subjective since I was more familiar with some objectives than others for certain tests.</p>
<ul>
<li><strong>EX333 - Enterprise Security: Network Services:</strong> a tubload of material and a very long exam</li>
<li><strong>EX442 - System Monitoring and Performance Tuning:</strong> very difficult to check your work, lots of calculations</li>
<li><strong>EX423 - Directory Services and Authentication:</strong> not a lot of material to cover, but tons of curveballs</li>
<li><strong>EX436 - Storage Management:</strong> the web interface made things much easier, lots of documentation available</li>
<li><strong>EX401 - Deployment and Systems Management:</strong> every objective can be tested, I build RPM&rsquo;s already</li>
</ul>
]]></content></item><item><title>Installing Fedora 16 in XenServer</title><link>https://major.io/2012/02/11/installing-fedora-16-in-xenserver/</link><pubDate>Sun, 12 Feb 2012 03:39:11 +0000</pubDate><guid>https://major.io/2012/02/11/installing-fedora-16-in-xenserver/</guid><description>Getting Fedora 16 working in XenServer isn&amp;rsquo;t the easiest thing to do, but I&amp;rsquo;ve put together a repository on GitHub that should help. The repository contains a kickstart file along with some brief instructions to help with the installation. If you&amp;rsquo;re ready to get started right now, just clone the repository:
git clone git://github.com/rackerhacker/kickstarts.git kickstarts There are some big issues with Fedora 16 which cause problems for installations within XenServer:</description><content type="html"><![CDATA[<p>Getting Fedora 16 working in XenServer isn&rsquo;t the easiest thing to do, but I&rsquo;ve put together a <a href="https://github.com/rackerhacker/kickstarts">repository on GitHub</a> that should help. The repository contains a kickstart file along with some brief instructions to help with the installation. If you&rsquo;re ready to get started right now, just clone the repository:</p>
<pre><code>git clone git://github.com/rackerhacker/kickstarts.git kickstarts
</code></pre><p>There are some big issues with Fedora 16 which cause problems for installations within XenServer:</p>
<ul>
<li>the installer sets up a console on something other than hvc0</li>
<li>anaconda won&rsquo;t start without being in serial mode</li>
<li>anaconda tries to use GPT partitions by default</li>
<li>grub2 is now standard, but it causes problems for older XenServer versions</li>
</ul>
<p>My kickstart works around the grub2 problem by throwing down an old-style grub configuration file and creating the proper symlinks. This config will still be updated when you upgrade kernels (at least in Fedora 16). It also sets up a very simple partitioning schema with one root and one swap partition. A DOS partition table is used in lieu of a GPT partition table.</p>
<p>When you start the installation, be sure to review the <a href="https://github.com/rackerhacker/kickstarts/blob/master/Fedora%2016%20Minimal%20on%20XenServer%206/README.md">README.md</a> in the git repository. It has some special instructions for boot options to meet the requirements of Fedora 16 and the kickstart file.</p>
]]></content></item><item><title>Using OpenSSL’s s_client command with web servers using Server Name Indication (SNI)</title><link>https://major.io/2012/02/07/using-openssls-s_client-command-with-web-servers-using-server-name-indication-sni/</link><pubDate>Tue, 07 Feb 2012 14:07:41 +0000</pubDate><guid>https://major.io/2012/02/07/using-openssls-s_client-command-with-web-servers-using-server-name-indication-sni/</guid><description>One of the handiest tools in the OpenSSL toolbox is s_client. You can quickly view lots of details about the SSL certificates installed on a particular server and diagnose problems. For example, use this command to look at Google&amp;rsquo;s SSL certificates:
openssl s_client -connect encrypted.google.com:443 You&amp;rsquo;ll see the chain of certificates back to the original certificate authority where Google bought its certificate at the top, a copy of their SSL certificate in plain text in the middle, and a bunch of session-related information at the bottom.</description><content type="html"><![CDATA[<p>One of the handiest tools in the OpenSSL toolbox is <code>s_client</code>. You can quickly view lots of details about the SSL certificates installed on a particular server and diagnose problems. For example, use this command to look at Google&rsquo;s SSL certificates:</p>
<pre><code>openssl s_client -connect encrypted.google.com:443
</code></pre><p>You&rsquo;ll see the chain of certificates back to the original certificate authority where Google bought its certificate at the top, a copy of their SSL certificate in plain text in the middle, and a bunch of session-related information at the bottom.</p>
<p>This works really well when a site has one SSL certificate installed per IP address (this used to be a hard requirement). With <a href="http://en.wikipedia.org/wiki/Server_Name_Indication">Server Name Indication</a> (SNI), a web server can have multiple SSL certificates installed on the same IP address. SNI-capable browsers will specify the hostname of the server they&rsquo;re trying to reach during the initial handshake process. This allows the web server to determine the correct SSL certificate to use for the connection.</p>
<p>If you try to connect to rackerhacker.com with <code>s_client</code>, you&rsquo;ll find that you receive the default SSL certificate installed on my server and not the one for this site:</p>
<pre><code>$ openssl s_client -connect rackerhacker.com:443
Certificate chain
 0 s:/C=US/ST=Texas/L=San Antonio/O=MHTX Enterprises/CN=*.mhtx.net
   i:/C=US/O=SecureTrust Corporation/CN=SecureTrust CA
 1 s:/C=US/O=SecureTrust Corporation/CN=SecureTrust CA
   i:/C=US/O=Entrust.net/OU=www.entrust.net/CPS incorp. by ref. (limits liab.)/OU=(c) 1999 Entrust.net Limited/CN=Entrust.net Secure Server Certification Authority
</code></pre><p>Add on the <code>-servername</code> argument and <code>s_client</code> will do the additional SNI negotiation step for you:</p>
<pre><code>$ openssl s_client -connect rackerhacker.com:443 -servername rackerhacker.com
Certificate chain
 0 s:/OU=Domain Control Validated/OU=PositiveSSL/CN=rackerhacker.com
   i:/C=GB/ST=Greater Manchester/L=Salford/O=Comodo CA Limited/CN=PositiveSSL CA
 1 s:/C=GB/ST=Greater Manchester/L=Salford/O=Comodo CA Limited/CN=PositiveSSL CA
   i:/C=US/ST=UT/L=Salt Lake City/O=The USERTRUST Network/OU=http://www.usertrust.com/CN=UTN-USERFirst-Hardware
 2 s:/C=US/ST=UT/L=Salt Lake City/O=The USERTRUST Network/OU=http://www.usertrust.com/CN=UTN-USERFirst-Hardware
   i:/C=SE/O=AddTrust AB/OU=AddTrust External TTP Network/CN=AddTrust External CA Root
 3 s:/C=SE/O=AddTrust AB/OU=AddTrust External TTP Network/CN=AddTrust External CA Root
   i:/C=SE/O=AddTrust AB/OU=AddTrust External TTP Network/CN=AddTrust External CA Root
</code></pre><p>You may be asking yourself this question:</p>
<blockquote>
<p>Why doesn&rsquo;t the web server just use the <code>Host:</code> header that my browser sends already to figure out which SSL certificate to use?</p>
</blockquote>
<p>Keep in mind that the SSL negotiation must occur <strong>prior</strong> to sending the HTTP request through to the remote server. That means that the browser and the server have to do the certificate exchange earlier in the process and the browser wouldn&rsquo;t get the opportunity to specify which site it&rsquo;s trying to reach. SNI fixes that by allowing a <code>Host:</code> header type of exchange during the SSL negotiation process.</p>
]]></content></item><item><title>The Kerberos-hater’s guide to installing Kerberos</title><link>https://major.io/2012/02/05/the-kerberos-haters-guide-to-installing-kerberos/</link><pubDate>Sun, 05 Feb 2012 21:03:52 +0000</pubDate><guid>https://major.io/2012/02/05/the-kerberos-haters-guide-to-installing-kerberos/</guid><description>As promised in my earlier post entitled Kerberos for haters, I&amp;rsquo;ve assembled the simplest possible guide to get Kerberos up an running on two CentOS 5 servers.
Also, I don&amp;rsquo;t really hate Kerberos. It&amp;rsquo;s a bit of an inside joke with my coworkers who are studying for some of the RHCA exams at Rackspace. The additional security provided by Kerberos is quite good but the setup involves a lot of small steps.</description><content type="html"><![CDATA[<p><img src="/wp-content/uploads/2012/02/haters_gonna_hate_elephhant.jpg" alt="1"></p>
<p>As promised in my earlier post entitled <a href="/2012/02/02/kerberos-for-haters/">Kerberos for haters</a>, I&rsquo;ve assembled the simplest possible guide to get Kerberos up an running on two CentOS 5 servers.</p>
<p>Also, I don&rsquo;t really <em>hate</em> Kerberos. It&rsquo;s a bit of an inside joke with my coworkers who are studying for some of the <a href="http://www.redhat.com/training/certifications/rhca/">RHCA</a> exams at Rackspace. The additional security provided by Kerberos is quite good but the setup involves a lot of small steps. If you miss one of the steps or if you get something done out of order, you may have to scrap the whole setup and start over unless you can make sense of the errors in the log files. A lot of my dislikes for Kerberos comes from the number of steps required in the setup process and the difficulty in tracking down issues when they crop up.</p>
<p>To complete this guide, you&rsquo;ll need the following:</p>
<ul>
<li>two CentOS, Red Hat Enterprise Linux or Scientific Linux 5 servers or VM&rsquo;s</li>
<li>some patience</li>
</ul>
<p>Here&rsquo;s how I plan to name my servers:</p>
<ul>
<li><strong>kdc.example.com</strong> – the Kerberos KDC server at 192.168.250.2</li>
<li><strong>client.example.com</strong> – the Kerberos client at 192.168.250.3</li>
</ul>
<p><strong>CRITICAL STEP:</strong> Before getting started, ensure that both systems have their hostnames properly set and both systems have the hostnames and IP addresses of both systems in <code>/etc/hosts</code>. Your server and client must be able to know the IP and hostname of the other system as well as themselves.</p>
<p>First off, we will need <a href="http://en.wikipedia.org/wiki/Network_Information_Service">NIS</a> working to serve up the user information for our client. Install the NIS server components on the KDC server:</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>Set the NIS domain and set a static port for <code>ypserv</code> to make it easier to firewall off. Edit <code>/etc/sysconfig/network</code> on the KDC server:</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>Manually set the NIS domain on the KDC server and add it to <code>/etc/yp.conf</code>:</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>Adjust <code>/var/yp/securenets</code> on the KDC server for additional security:</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>Start the NIS server and generate the NIS maps:</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>I usually like to prepare my iptables rules ahead of time so I ensure that it doesn&rsquo;t derail me later on. Paste this into the KDC&rsquo;s terminal:</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>We need our time in sync for Kerberos to work properly. Install NTP on both nodes, start it, and ensure it comes up at boot time:</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>Now we&rsquo;re ready to set up Kerberos. Start by installing some packages on the KDC:</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>We will need to make some edits to <code>/etc/krb5.conf</code> on the KDC to set up our KDC realm. Ensure that the <code>default_realm</code> is set:</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>The <code>[realms]</code> section should look like this:</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>The <code>[domain_realm]</code> section should look like this:</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>Add <code>validate = true</code> within the <code>pam { }</code> block of the <code>[appdefaults]</code> section:</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>Adjust <code>/var/kerberos/krb5kdc/kdc.conf</code> on the KDC:</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>There&rsquo;s one last configuration file to edit on the KDC! Ensure that <code>/var/kerberos/krb5kdc/kadm5.acl</code> looks like this:</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>We&rsquo;re now ready to make a KDC database to hold our sensitive Kerberos data. Create the database and set a good password which you can remember. This command also stashes your password on the KDC so you don&rsquo;t have to enter it each time you start the KDC:</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>On the KDC, create a principal for the admin user as well as user1 (which we&rsquo;ll create shortly). Also, export the admin details to the kadmind key tab. You&rsquo;ll get some extra output after each one of these commands but I&rsquo;ve snipped it to reduce the length of the post.</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>Let&rsquo;s start the Kerberos KDC and kadmin daemons:</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>Now that the administration work is done, let&rsquo;s create a principal for our KDC server and stick it in it&rsquo;s keytab:</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>Transfer your <code>/etc/krb5.conf</code> from the KDC server to the client. Hop onto the client server, install the Kerberos client package and add some host principals:</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>There aren&rsquo;t any daemons on the client side, so the configuration is pretty much wrapped up there for Kerberos. However, we now need to tell both servers to use Kerberos for auth and your client servers needs to use NIS to get user data.</p>
<ul>
<li>On the KDC:
<ul>
<li>run <code>authconfig-tui</code></li>
<li>choose <strong>Use Kerberos</strong> from the second column</li>
<li>press <strong>Next</strong></li>
<li>don&rsquo;t edit the configuration (authconfig got the data from <code>/etc/krb.conf</code>)</li>
<li>press <strong>OK</strong></li>
</ul>
</li>
<li>On the client:
<ul>
<li>run <code>authconfig-tui</code></li>
<li>choose <strong>Use NIS</strong> and <strong>Use Kerberos</strong></li>
<li>press <strong>Next</strong></li>
<li>enter your NIS domain (EXAMPLE.COM) and NIS server (kdc.example.com or 192.168.250.2)</li>
<li>press <strong>Next</strong></li>
<li>don&rsquo;t edit the Kerberos configuration (authconfig got the data from <code>/etc/krb.conf</code>)</li>
<li>press <strong>OK</strong></li>
</ul>
</li>
</ul>
<p><strong>Got NIS problems?</strong> If the NIS connection stalls on the client, ensure that you have the iptables rules present on the KDC that we added near the beginning of this guide. Also, if you forgot to add <strong>both</strong> hosts to <strong>both</strong> servers' <code>/etc/hosts</code>, go do that now.</p>
<p>Let&rsquo;s make our test user on the KDC. <strong>Don&rsquo;t add this user to the client</strong> — we&rsquo;ll get the user information via NIS and authenticate via Kerberos shortly. We&rsquo;ll also rebuild our NIS maps after adding the user:</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>On the client, see if you can get the password hash for the user1 account via NIS:</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>You can see why NIS isn&rsquo;t a good way to authenticate users. Someone could easily pull the hash for any account and brute force the hash on their own server. Go back to the KDC and lock out the user account:</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>Go back to the client and try to pull the password hash now:</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>On the plus side, the user&rsquo;s password hash is now gone. On the negative side, you&rsquo;ve just prevented this user from logging in locally or via NIS. Don&rsquo;t worry, the user can log in via Kerberos now. Let&rsquo;s prepare a home directory on the client for the user:</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>Note: In a real-world scenario, you&rsquo;d probably want to export this user&rsquo;s home directory via NFS so they didn&rsquo;t get a different home directory on every server.</p>
<p>While you&rsquo;re still on the client, try to log into the client via the user. Use the password that you used when you created the user1 principal on the KDC.</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>List your Kerberos tickets and you should see one for your user principal:</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>Your KDC should have a couple of lines in its <code>/var/log/krb5kdc.log</code> showing the authentication:</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>The first line shows that the client asked for a Authentication Server Request (AS_REQ) and the second line shows that the client then asked for a Ticket Granting Server Request (TGS_REQ). In layman&rsquo;s terms, the client first asked for a ticket-granting ticket (TGT) so it could authenticate to other services. When it actually tried to log in via <code>ssh</code> it asked for a ticket (and received it).</p>
<p><strong>YOU JUST CONFIGURED KERBEROS!</strong></p>
<p>From here, the sky&rsquo;s the limit. Another popular implementation of Kerberos is encrypted NFSv4. You can even go crazy and use <a href="http://wiki.centos.org/HowTos/HttpKerberosAuth">Kerberos with apache</a>.</p>
<p>Let me know if you have any questions about this post or if you spot any errors. With this many steps, there&rsquo;s bound to be a typo or two in this guide. Keep in mind that there are some obvious spots for network-level and service-level security improvements. This guide was intended to give you the basics and it doesn&rsquo;t cover all of the security implications involved with a Kerberos implementation.</p>
]]></content></item><item><title>Get notifications instead of automatic updates in Scientific Linux</title><link>https://major.io/2012/02/04/get-notifications-instead-of-automatic-updates-in-scientific-linux/</link><pubDate>Sat, 04 Feb 2012 19:01:54 +0000</pubDate><guid>https://major.io/2012/02/04/get-notifications-instead-of-automatic-updates-in-scientific-linux/</guid><description>Scientific Linux installations have a package called yum-autoupdate by default and the package contains two files:
# rpm -ql yum-autoupdate /etc/cron.daily/yum-autoupdate /etc/sysconfig/yum-autoupdate The cron job contains the entire script to run automatic updates once a day and the configuration file controls its behavior. However, you can&amp;rsquo;t get the same functionality as Fedora&amp;rsquo;s yum-updatesd package where you can receive notifications for updates rather than automatically updating the packages.
To get those notifications in Scientific Linux, just make two small edits to this portion of /etc/cron.</description><content type="html"><![CDATA[<p>Scientific Linux installations have a package called <code>yum-autoupdate</code> by default and the package contains two files:</p>
<pre><code># rpm -ql yum-autoupdate
/etc/cron.daily/yum-autoupdate
/etc/sysconfig/yum-autoupdate
</code></pre><p>The cron job contains the entire script to run automatic updates once a day and the configuration file controls its behavior. However, you can&rsquo;t get the same functionality as Fedora&rsquo;s <code>yum-updatesd</code> package where you can receive notifications for updates rather than automatically updating the packages.</p>
<p>To get those notifications in Scientific Linux, just make two small edits to this portion of <code>/etc/cron.daily/yum-autoupdate</code>:</p>
<pre><code>173           echo &quot;    Starting Yum with command&quot;
174           echo &quot;     /usr/bin/yum -c $TEMPCONFIGFILE -e 0 -d 1 -y update&quot;
175   fi
176   /usr/bin/yum -c $TEMPCONFIGFILE -e 0 -d 1 -y update &gt; $TEMPFILE 2&gt;&amp;1
177   if [ -s $TEMPFILE ] ; then
</code></pre><p>Adjust the <code>update</code> commands to look like this:</p>
<pre><code>173           echo &quot;    Starting Yum with command&quot;
174           echo &quot;     /usr/bin/yum -c $TEMPCONFIGFILE -e 0 -d 1 -y check-update&quot;
175   fi
176   /usr/bin/yum -c $TEMPCONFIGFILE -e 0 -d 1 -y check-update &gt; $TEMPFILE 2&gt;&amp;1
177   if [ -s $TEMPFILE ] ; then
</code></pre><p>Since you won&rsquo;t be auto-updating with this script any longer, you may want to comment out the <code>EXCLUDE=</code> line in <code>/etc/sysconfig/yum-autoupdate</code> so that you&rsquo;ll receive notifications for all packages with updates. Also, to avoid having your changes updated with a newer <code>yum-autoupdate</code> package later, add the package to your list of excluded packages in <code>/etc/yum.conf</code>.</p>
]]></content></item><item><title>Kerberos for haters</title><link>https://major.io/2012/02/02/kerberos-for-haters/</link><pubDate>Fri, 03 Feb 2012 04:29:32 +0000</pubDate><guid>https://major.io/2012/02/02/kerberos-for-haters/</guid><description>I&amp;rsquo;ll be the first one to admit that Kerberos drives me a little insane. It&amp;rsquo;s a requirement for two of the exams in Red Hat&amp;rsquo;s RHCA certification track and I&amp;rsquo;ve been forced to learn it. It provides some pretty nice security features for large server environments. You get central single sign ons, encrypted authentication, and bidirectional validation. However, getting it configured can be a real pain due to some rather archaic commands and shells.</description><content type="html"><![CDATA[<p>I&rsquo;ll be the first one to admit that Kerberos drives me a little insane. It&rsquo;s a requirement for two of the exams in <a href="http://www.redhat.com/training/certifications/rhca/">Red Hat&rsquo;s RHCA certification track</a> and I&rsquo;ve been forced to learn it. It provides some pretty nice security features for large server environments. You get central single sign ons, encrypted authentication, and bidirectional validation. However, getting it configured can be a real pain due to some rather archaic commands and shells.</p>
<p>Here&rsquo;s Kerberos in a nutshell within a two-server environment: One server is a Kerberos key distribution center (KDC) and the other is a Kerberos client. The KDC has the list of users and their passwords. Consider a situation where a user tries to ssh into the Kerberos client:</p>
<ul>
<li>sshd calls to pam to authenticate the user</li>
<li>pam calls to the KDC for a ticket granting ticket (TGT) to see if the user can authenticate</li>
<li>the KDC replies to the client with a TGT encrypted with the user&rsquo;s password</li>
<li>pam (on the client) tries to decrypt the TGT with the password that the user provided via ssh</li>
<li>if pam can decrypt the TGT, it knows the user is providing the right password</li>
</ul>
<p>Now that the client has a a TGT for that user, it can ask for tickets to access other network services. What if the user who just logged in wants to access another Kerberized service in the environment?</p>
<ul>
<li>client calls the KDC and asks for a ticket to grant access to the other service</li>
<li>KDC replies with two copies of the ticket:
<ul>
<li>one copy is encrypted with the user&rsquo;s current TGT</li>
<li>a second copy is encrypted with the password of the network service the user wants to access</li>
</ul>
</li>
<li>the client can decrypt the ticket which was encrypted with the current TGT since it has the TGT already</li>
<li>client makes an authenticator by taking the decrypted ticket and encrypting it with a timestamp</li>
<li>client passes the authenticator and the second copy of the ticket it received from the KDC</li>
<li>the other network service decrypts the second copy of the ticket and verifies the password</li>
<li>the other network service uses the decrypted ticket to decrypt the authenticator it received from the client</li>
<li>if the timestamp looks good, the other network service allows the user access</li>
</ul>
<p>Okay, that&rsquo;s confusing. Let&rsquo;s take it one step further. Enabling pre-authentication requires that clients send a request containing a timestamp encrypted with the user&rsquo;s password prior to asking for a TGT. Without this requirement, an attacker can ask for a TGT one time and then brute force the TGT offline. Pre-authentication forces the client to send a timestamped request encrypted with the user&rsquo;s password back to the KDC before they can ask for a TGT. This means the attacker is forced to try different passwords when encrypting the timestamp in the hopes that they&rsquo;ll get a TGT to work with eventually. One would hope that you have something configured on the KDC to set off an alarm for multiple failed pre-authentication attempts.</p>
<p>Oh, but we can totally kick it up another notch. What if an attacker is able to give a bad password to a client but they&rsquo;re also able to impersonate the KDC? They could reply to the TGT request (as the KDC) with a TGT encrypted with whichever password they choose and get access to the client system. Enabling mutual authentication stops this attack since it forces the client to ask the KDC for the client&rsquo;s own host principal password (this password is set when the client is configured to talk to the KDC). The attacker shouldn&rsquo;t have any clue what that password is and the attack will be thwarted.</p>
<p>By this point, you&rsquo;re either saying &ldquo;Oh man, I don&rsquo;t ever want to do this.&rdquo; or &ldquo;How do I set up Kerberos?&rdquo;. Stay tuned if you&rsquo;re in the second group. I&rsquo;ll have a dead simple (or as close to dead simple as one can get with Kerberos) how-to on the blog shortly.</p>
<p>In the meantime, here are a few links for extra Kerberos bedtime reading:</p>
<ul>
<li><a href="http://en.wikipedia.org/wiki/Kerberos_(protocol)">Kerberos on Wikipedia</a></li>
<li><a href="http://www.kerberos.org/software/whykerberos.pdf">MIT&rsquo;s &ldquo;Why Kerberos&rdquo;</a> [PDF]</li>
<li><a href="http://learn-networking.com/network-security/how-kerberos-authentication-works">How Kerberos Authentication Works</a></li>
</ul>
]]></content></item><item><title>OpenStack bleeding-edge Python packages are now available</title><link>https://major.io/2012/02/01/openstack-bleeding-edge-python-packages-are-now-available/</link><pubDate>Wed, 01 Feb 2012 15:05:16 +0000</pubDate><guid>https://major.io/2012/02/01/openstack-bleeding-edge-python-packages-are-now-available/</guid><description>I sometimes enjoy living on the edge occasionally and that sometimes means I keep up with OpenStack changes commit by commit. If you&amp;rsquo;re in the same boat as I am, you may save some time by using my repository of bleeding-edge Python packages from the OpenStack projects:
pypi.mhtx.net Python packages are updated moments after the commit is merged into the repositories under OpenStack&amp;rsquo;s github account.
Although the packages will contain the latest code available, rest assured that the code has passed an initial code review (by humans), unit tests, and varying levels of functional or integrated testing.</description><content type="html"><![CDATA[<p>I sometimes enjoy living on the edge occasionally and that sometimes means I keep up with OpenStack changes commit by commit. If you&rsquo;re in the same boat as I am, you may save some time by using my repository of bleeding-edge Python packages from the OpenStack projects:</p>
<ul>
<li><a href="http://pypi.mhtx.net/">pypi.mhtx.net</a></li>
</ul>
<p>Python packages are updated moments after the commit is merged into the repositories under <a href="http://github.com/openstack">OpenStack&rsquo;s github account</a>.</p>
<p>Although the packages will contain the latest code available, rest assured that the code has passed an initial code review (by humans), unit tests, and varying levels of functional or integrated testing. There may still be a bug or two cropping up after that, so be aware of that as you utilize these packages.</p>
<p>The package versions utilize a standard format:</p>
<!-- raw HTML omitted -->
<p>If you need to check the git log up to that particular commit, just run <code>git log</code>:</p>
<!-- raw HTML omitted -->
<p>Instructions for configuring <code>pip</code> or <code>easy_install</code> are provided <a href="http://pypi.mhtx.net/">within the repository</a>.</p>
<p>In addition, the repository is accessible via IPv4 and IPv6.</p>
]]></content></item><item><title>Create a local PyPi repository using only mod_rewrite</title><link>https://major.io/2012/01/31/create-a-local-pypi-repository-using-only-mod_rewrite/</link><pubDate>Wed, 01 Feb 2012 04:02:49 +0000</pubDate><guid>https://major.io/2012/01/31/create-a-local-pypi-repository-using-only-mod_rewrite/</guid><description>Regular users of Python&amp;rsquo;s package tools like pip or easy_install are probably familiar with the PyPi repository. It&amp;rsquo;s a one-stop-shop to learn more about available Python packages and get them installed on your server.
However, certain folks may find the need to host a local PyPi repository for their own packages. You may need it to store Python code which you don&amp;rsquo;t plan to release publicly or you may need to add proprietary patches to upstream Python packages.</description><content type="html"><![CDATA[<p>Regular users of Python&rsquo;s package tools like <a href="http://pypi.python.org/pypi/pip">pip</a> or <a href="http://pypi.python.org/pypi/setuptools">easy_install</a> are probably familiar with the <a href="http://pypi.python.org/pypi">PyPi</a> repository. It&rsquo;s a one-stop-shop to learn more about available Python packages and get them installed on your server.</p>
<p>However, certain folks may find the need to host a local PyPi repository for their own packages. You may need it to store Python code which you don&rsquo;t plan to release publicly or you may need to add proprietary patches to upstream Python packages. Regardless of the reason to have it, a local PyPi repository is relatively easy to configure.</p>
<p>You&rsquo;ll need to start with a base directory for your PyPi repository. For this example, I chose <code>/var/pypi</code>. The directory structure should look something like this:</p>
<pre><code>/var/pypi/simple/[package_name]/[package_tarball]
</code></pre><p>For a package like <code>pip</code>, you&rsquo;d make a structure like this:</p>
<pre><code>/var/pypi/simple/pip/pip-1.0.2.tar.gz
</code></pre><p>Once you have at least one package stored locally, it&rsquo;s time to configure apache. Here&rsquo;s a snippet from the virtual host I configured:</p>
<pre><code>DocumentRoot /var/pypi/
ServerName pypi.example.com

Options +Indexes

RewriteEngine On
RewriteRule ^/robots.txt - [L]
RewriteRule ^/icons/.* - [L]
RewriteRule ^/index\..* - [L]

RewriteCond /var/pypi/$1 !-f
RewriteCond /var/pypi/$1 !-d
RewriteRule ^/(.*)/?$ http://pypi.python.org/$1 [R,L]
</code></pre><p>The last set of rewrite directives check to see if the request refers to an existing file or directory under your document root. If it does, your server will reply with a directory listing or with the actual file to download. If the directory or file doesn&rsquo;t exist, apache will send the client a redirection to the main PyPi site.</p>
<p>Reload your apache configuration to bring in your new changes. Let&rsquo;s try to download the <code>pip</code> tarball from our local server in the example I mentioned above:</p>
<pre><code>$ curl -I http://pypi.example.com/simple/pip/
HTTP/1.1 200 OK

$ curl -I http://pypi.example.com/simple/pip/pip-1.0.2.tar.gz
HTTP/1.1 200 OK
</code></pre><p>I&rsquo;ve obviously snipped a bit of the response above, but you can see that apache is responding with 200&rsquo;s since it has the directories and files that I was trying to retrieve via curl. Let&rsquo;s try to get something we don&rsquo;t have locally, like <code>kombu</code>:</p>
<pre><code>$ curl -I http://pypi.example.com/simple/kombu/
HTTP/1.1 302 Found
Location: http://pypi.python.org/simple/kombu/
</code></pre><p>Our local PyPi repository doesn&rsquo;t have <code>kombu</code> so it will refer our Python tools over to the official PyPi repository to get the listing of available package versions for <code>kombu</code>.</p>
<p>Now we need to tell <code>pip</code> to use our local repository. Edit <code>~/.pip/pip.conf</code> and add:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ini" data-lang="ini"><span style="color:#719e07">[global]</span>
index-url <span style="color:#719e07">=</span> <span style="color:#2aa198">http://pypi.example.com/simple/</span>
</code></pre></div><p>If you&rsquo;d rather use <code>easy_install</code>, edit <code>~/.pydistutils.cfg</code> and add:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ini" data-lang="ini"><span style="color:#719e07">[easy_install]</span>
index_url <span style="color:#719e07">=</span> <span style="color:#2aa198">http://pypi.example.com/simple/</span>
</code></pre></div><p>Once your tools are configured, try installing a package you have locally and try to install one that you know you won&rsquo;t have locally. You can add <code>-v</code> to <code>pip install</code> to watch it retrieve different URL&rsquo;s to get the packages it needs. If you spot any peculiar behavior or unexpected redirections, double-check your mod_rewrite rules in your apache configuration and check the spelling of your directories under your document root.</p>
]]></content></item><item><title>Getting started with SELinux</title><link>https://major.io/2012/01/25/getting-started-with-selinux/</link><pubDate>Thu, 26 Jan 2012 04:28:41 +0000</pubDate><guid>https://major.io/2012/01/25/getting-started-with-selinux/</guid><description>I used to be one of those folks who would install Fedora, CentOS, Scientific Linux, or Red Hat and disable SELinux during the installation. It always seemed like SELinux would get in my way and keep me from getting work done.
Later on, I found that one of my servers (which I&amp;rsquo;d previously secured quite thoroughly) had some rogue processes running that were spawned through httpd. Had I actually been using SELinux in enforcing mode, those processes would have probably never even started.</description><content type="html"><![CDATA[<p>I used to be one of those folks who would install Fedora, CentOS, Scientific Linux, or Red Hat and disable SELinux during the installation. It always seemed like SELinux would get in my way and keep me from getting work done.</p>
<p>Later on, I found that one of my servers (which I&rsquo;d previously secured quite thoroughly) had some rogue processes running that were spawned through httpd. Had I actually been using SELinux in enforcing mode, those processes would have probably never even started.</p>
<p>If you&rsquo;re trying to get started with SELinux but you&rsquo;re not sure how to do it without completely disrupting your server&rsquo;s workflow, these tips should help:</p>
<p><strong>Get some good reporting and monitoring</strong></p>
<p>Two of the most handy SELinux tools are <a href="https://fedorahosted.org/setroubleshoot/">setroubleshoot and setroubleshoot-server</a>. If you&rsquo;re running a server without X, you can use <a href="/2011/09/15/receive-e-mail-reports-for-selinux-avc-denials/">my guide for configuring setroubleshoot-server</a>. You will receive email alerts within seconds of an AVC denial and the emails should contain tips on how to resolve the denial if the original action should be allowed. If the AVC denial caught something you didn&rsquo;t expect, you&rsquo;ll know about the potential security breach almost immediately.</p>
<p><strong>Start out with SELinux in permissive mode</strong></p>
<p>If you&rsquo;re overly concerned about SELinux getting in your way, or if you&rsquo;re enabling SELinux on a server that has been running without SELinux since it was installed, start out with SELinux in permissive mode. To make the change effective immediately, just run:</p>
<pre><code># setenforce 0
# getenforce
Permissive
</code></pre><p>Edit <code>/etc/sysconfig/selinux</code> to make it persistent across reboots:</p>
<pre><code># This file controls the state of SELinux on the system.
# SELINUX= can take one of these three values:
#     enforcing - SELinux security policy is enforced.
#     permissive - SELinux prints warnings instead of enforcing.
#     disabled - No SELinux policy is loaded.
SELINUX=permissive
</code></pre><p><strong>Adjust booleans before adding your own custom modules</strong></p>
<p>There are a lot of booleans you can toggle to get the functionality you need without adding your own custom SELinux modules with <code>audit2allow</code>. If you wanted to see all of the applicable booleans for <code>httpd</code>, just use <code>getsebool</code>:</p>
<pre><code># getsebool -a | grep httpd
httpd_builtin_scripting --&gt; on
httpd_can_check_spam --&gt; off
httpd_can_network_connect --&gt; on
httpd_can_network_connect_cobbler --&gt; off
httpd_can_network_connect_db --&gt; off
httpd_can_network_memcache --&gt; off
httpd_can_network_relay --&gt; on
httpd_can_sendmail --&gt; on
... and so on ...
</code></pre><p>Toggling booleans is easy with <code>togglesebool</code>:</p>
<pre><code># togglesebool httpd_can_network_memcache
httpd_can_network_memcache: active
</code></pre><p>Now <code>httpd</code> can talk to <code>memcache</code>. You can also use <code>setsebool</code> if you want to be specific about your setting (this is good for scripts):</p>
<pre><code># setsebool httpd_can_network_memcache on
</code></pre><p><strong>Tracking your history of AVC denials</strong></p>
<p>All of your AVC denals are logged by <code>auditd</code> in <code>/var/log/audit/audit.log</code> but it&rsquo;s not the easiest file to read and parse. That&rsquo;s where <code>aureport</code> comes in:</p>
<pre><code># aureport --avc | tail -n 5
45. 01/24/2012 04:23:29 postdrop unconfined_u:system_r:httpd_t:s0 4 fifo_file getattr system_u:object_r:postfix_public_t:s0 denied 1061
46. 01/24/2012 04:23:29 postdrop unconfined_u:system_r:httpd_t:s0 2 fifo_file write system_u:object_r:postfix_public_t:s0 denied 1062
47. 01/24/2012 04:23:29 postdrop unconfined_u:system_r:httpd_t:s0 2 fifo_file open system_u:object_r:postfix_public_t:s0 denied 1062
48. 01/24/2012 14:01:58 sendmail unconfined_u:system_r:httpd_t:s0 160 process setrlimit unconfined_u:system_r:httpd_t:s0 denied 1123
49. 01/24/2012 14:01:58 postdrop unconfined_u:system_r:httpd_t:s0 4 dir search system_u:object_r:postfix_public_t:s0 denied 1124
</code></pre><p><strong>Summary</strong></p>
<p>There&rsquo;s no need to be scared of or be annoyed by SELinux in your server environment. While it takes some getting used to (and what new software doesn&rsquo;t?), you&rsquo;ll have an extra layer of security and access restrictions which should let you sleep a little better at night.</p>
]]></content></item><item><title>XenServer 6: Storage repository on software RAID</title><link>https://major.io/2012/01/16/xenserver-6-storage-repository-on-software-raid/</link><pubDate>Mon, 16 Jan 2012 15:00:21 +0000</pubDate><guid>https://major.io/2012/01/16/xenserver-6-storage-repository-on-software-raid/</guid><description>Although Citrix recommends against using software RAID with XenServer due to performance issues, I&amp;rsquo;ve had some pretty awful experiences with hardware RAID cards over the last few years. In addition, the price of software RAID makes it a very desirable solution.
Before you get started, go through the steps to disable GPT. That post also explains an optional adjustment to get a larger root partition (which I would recommend). You cannot complete the steps in this post if your XenServer installation uses GPT.</description><content type="html"><![CDATA[<p>Although Citrix recommends against using software RAID with XenServer due to performance issues, I&rsquo;ve had some pretty awful experiences with hardware RAID cards over the last few years. In addition, the price of software RAID makes it a very desirable solution.</p>
<p><strong>Before you get started,</strong> <a href="http://rackerhacker.com/2012/01/13/xenserver-6-disable-gpt-and-get-a-larger-root-partition/">go through the steps to disable GPT</a>. That post also explains an optional adjustment to get a larger root partition (which I would recommend). <em>You cannot complete the steps in this post if your XenServer installation uses GPT.</em></p>
<p>You should have three partitions on your first disk after the installation:</p>
<pre><code># fdisk -l /dev/sda
-- SNIP --
   Device Boot      Start         End      Blocks   Id  System
/dev/sda1   *           1        2611    20971520   83  Linux
/dev/sda2            2611        5222    20971520   83  Linux
/dev/sda3            5222       19457   114345281   8e  Linux LVM
</code></pre><p>Here&rsquo;s a quick explanation of your partitions:</p>
<ul>
<li><strong>/dev/sda1:</strong> the XenServer root partition</li>
<li><strong>/dev/sda2:</strong> XenServer uses this partition for temporary space during upgrades</li>
<li><strong>/dev/sda3:</strong> your storage repository should be in this logical volume</li>
</ul>
<p>We need to replicate the same partition structure across each of your drives and the software RAID volume will span the across the third partition on each disk. Copying the partition structure from disk to disk is done easily with <code>sfdisk</code>:</p>
<p><!-- raw HTML omitted --><!-- raw HTML omitted -->WHOA THERE! NO TURNING BACK!<!-- raw HTML omitted --> This step is destructive! If your other disks have any data on them, this step will make it (relatively) impossible to retrieve data on those disks again. Back up any data on the other disks in your XenServer machine before running these next commands.<!-- raw HTML omitted --></p>
<pre><code>sfdisk -d /dev/sda | sfdisk --force /dev/sdb
sfdisk -d /dev/sda | sfdisk --force /dev/sdc
sfdisk -d /dev/sda | sfdisk --force /dev/sdd
</code></pre><p>If you have only two disks, stop with <code>/dev/sdb</code> and you&rsquo;ll be making a RAID 1 array. My machine has four disks and I&rsquo;ll be making a RAID 10 array.</p>
<p>We need to destroy the main storage repository, but we need to unplug the physical block device first. Get the storage repository uuid first, then use it to find the corresponding physical block device. Once the physical block device is unplugged, the storage repository can be destroyed:</p>
<pre><code># xe sr-list name-label=Local\ storage | head -1
uuid ( RO)                : 75264965-f981-749e-0f9a-e32856c46361
# xe pbd-list sr-uuid=75264965-f981-749e-0f9a-e32856c46361 | head -1
uuid ( RO)                  : ff7e9656-c27c-1889-7a6d-687a561f0ad0
# xe pbd-unplug uuid=ff7e9656-c27c-1889-7a6d-687a561f0ad0
# xe sr-destroy uuid=75264965-f981-749e-0f9a-e32856c46361
</code></pre><p>All of the LVM data from <code>/dev/sda3</code> should now be gone:</p>
<pre><code># lvdisplay &amp;&amp; vgdisplay &amp;&amp; pvdisplay
#
</code></pre><p>Change the third partition on each physical disk to be a software RAID partition type:</p>
<pre><code>echo -e &quot;t\n3\nfd\nw\n&quot; | fdisk /dev/sda
echo -e &quot;t\n3\nfd\nw\n&quot; | fdisk /dev/sdb
echo -e &quot;t\n3\nfd\nw\n&quot; | fdisk /dev/sdc
echo -e &quot;t\n3\nfd\nw\n&quot; | fdisk /dev/sdd
</code></pre><p>Stop here and reboot your XenServer box to pick up the new partition changes. Once the server comes back from the reboot, start up a software RAID volume with <code>mdadm</code>:</p>
<pre><code>// RAID 1 for two drives
mdadm --create /dev/md0 -l 1 -n 2 /dev/sda3 /dev/sdb3
// RAID 10 for four drives
mdadm --create /dev/md0 -l 10 -n 4 /dev/sda3 /dev/sdb3 /dev/sdc3 /dev/sdd3
</code></pre><p>Check to see that your RAID array is building:</p>
<pre><code># cat /proc/mdstat
Personalities : [raid10]
md0 : active raid10 sdd3[3] sdc3[2] sdb3[1] sda3[0]
      228690432 blocks 64K chunks 2 near-copies [4/4] [UUUU]
      [&gt;....................]  resync =  0.3% (694272/228690432) finish=16.4min speed=231424K/sec
</code></pre><p>Although you don&rsquo;t have to wait for the resync to complete, just be aware that XenServer doesn&rsquo;t do well with a lot of disk I/O within dom0. You may notice unusually slow performance in dom0 until it finishes. Save the array&rsquo;s configuration for reboots:</p>
<pre><code> /etc/mdadm.conf
</code></pre><p>Edit the <code>/etc/mdadm.conf</code> file and append <code>auto=yes</code> to the end of the line (but leave everything on one line):</p>
<pre><code>ARRAY /dev/md0 level=raid10 num-devices=4 metadata=0.90 \
  UUID=2876748c:5117eed5:ce4d62d3:9592bd84 auto=yes
</code></pre><p>Create a new storage repository on the RAID volume with thin provisioning (thanks to <a href="http://www.scriptkiddie.org/blog/2010/06/20/xenserver-5-6-thin-provisioning-with-ext3/">Spherical Chicken</a> for the command):</p>
<pre><code>xe sr-create content-type=user type=ext device-config:device=/dev/md0 shared=false name-label=&quot;Local storage&quot;
</code></pre><p>This command takes some time to complete since it makes logical volumes and then makes an ext3 filesystem for the new storage repository. Bigger RAID arrays will take more time and it&rsquo;s guaranteed to take longer than you&rsquo;d expect if your RAID array is still building. As soon as it completes, you&rsquo;ll be given the uuid of your new storage repository and it should appear within the XenCenter interface.</p>
<p>TIP: If you run into any problems during reboots, open <code>/boot/extlinux.conf</code> and remove <code>splash</code> and <code>quiet</code> from the <code>label xe</code> boot section. This removes the framebuffer during boot-up and it causes a lot more output to be printed to the console. It won&rsquo;t affect the display once your XenServer box has fully booted.</p>
]]></content></item><item><title>XenServer 6: Disable GPT and get a larger root partition</title><link>https://major.io/2012/01/13/xenserver-6-disable-gpt-and-get-a-larger-root-partition/</link><pubDate>Fri, 13 Jan 2012 15:00:10 +0000</pubDate><guid>https://major.io/2012/01/13/xenserver-6-disable-gpt-and-get-a-larger-root-partition/</guid><description>XenServer 6 is a solid virtualization platform, but the installer doesn&amp;rsquo;t give you many options for customized configurations. By default, it installs with a 4GB root partition and uses GUID Partition Tables (GPT). GPT is new in XenServer 6.
I&amp;rsquo;d rather use MBR partition tables and get a larger root partition. If you want to make these adjustments in your XenServer 6 installation, follow these steps after booting into the XenServer 6 install disc:</description><content type="html"><![CDATA[<p><a href="http://www.citrix.com/English/ps2/products/product.asp?contentID=683148&amp;ntref=prod_top">XenServer 6</a> is a solid virtualization platform, but the installer doesn&rsquo;t give you many options for customized configurations. By default, it installs with a 4GB root partition and uses <a href="http://en.wikipedia.org/wiki/GUID_Partition_Table">GUID Partition Tables (GPT)</a>. GPT is new in XenServer 6.</p>
<p>I&rsquo;d rather use <a href="http://en.wikipedia.org/wiki/Master_boot_record">MBR partition tables</a> and get a larger root partition. If you want to make these adjustments in your XenServer 6 installation, follow these steps after booting into the <a href="http://www.citrix.com/lang/English/lp/lp_1688615.asp">XenServer 6 install disc</a>:</p>
<p><a href="/wp-content/uploads/2012/01/01.jpg"><!-- raw HTML omitted --></a></p>
<p>When the installer initially boots, press F2 to access the advanced installation options.</p>
<p><!-- raw HTML omitted --></p>
<p><a href="/wp-content/uploads/2012/01/02.jpg"><!-- raw HTML omitted --></a></p>
<p>Type <code>shell</code> and press enter. The installer should begin booting into a pre-installation shell where you can make your adjustments.</p>
<p><!-- raw HTML omitted --></p>
<p><a href="/wp-content/uploads/2012/01/04.jpg"><!-- raw HTML omitted --></a></p>
<p>Once you&rsquo;ve booted into the pre-installation shell, type <code>vi /opt/xensource/installer/constants.py</code> and press enter.</p>
<p><!-- raw HTML omitted --></p>
<p><a href="/wp-content/uploads/2012/01/05.jpg"><!-- raw HTML omitted --></a></p>
<p>Change <code>GPT_SUPPORT = True</code> to <code>GPT_SUPPORT = False</code> to disable GPT and use MBR partition tables. Adjust the value of <code>root_size</code> from 4096 (the default) to a larger number to get a bigger root partition. The size is specified in MB, so 4096 is 4GB. Save the file and exit <code>vim</code>.</p>
<p><!-- raw HTML omitted --></p>
<p><a href="/wp-content/uploads/2012/01/06.jpg"><!-- raw HTML omitted --></a></p>
<p>Type <code>exit</code> and the installer should start.</p>
<p><!-- raw HTML omitted --></p>
<p>Once the installation is complete, you should have a bigger root partition on a MBT partition table:</p>
<pre><code># df -h /
Filesystem            Size  Used Avail Use% Mounted on
/dev/sda1              20G  1.8G   17G  10% /
# fdisk -l /dev/sda

Disk /dev/sda: 160.0 GB, 160041885696 bytes
255 heads, 63 sectors/track, 19457 cylinders
Units = cylinders of 16065 * 512 = 8225280 bytes

   Device Boot      Start         End      Blocks   Id  System
/dev/sda1   *           1        2611    20971520   83  Linux
/dev/sda2            2611        5222    20971520   83  Linux
/dev/sda3            5222       19457   114345281   8e  Linux LVM
</code></pre>]]></content></item><item><title>Fight SOPA &amp; PIPA</title><link>https://major.io/2012/01/12/fight-sopa-and-pipa/</link><pubDate>Thu, 12 Jan 2012 13:36:38 +0000</pubDate><guid>https://major.io/2012/01/12/fight-sopa-and-pipa/</guid><description>Get informed about what the US government is trying to accomplish with SOPA and PIPA. Get involved and do what you can to prevent it from moving any further.
WordPress &amp;gt; Help Stop SOPA/PIPA Reddit - SOPA Stop American Censorship Wikipedia - Stop Online Piracy Act EFF - Take Action Why Rackspace opposes the Stop Online Piracy Act Mozilla - Protect The Internet You may need a little humor after all of that reading.</description><content type="html"><![CDATA[<p><a href="/wp-content/uploads/2012/01/sopacantspelldns.jpg"><!-- raw HTML omitted --></a></p>
<p>Get informed about what the US government is trying to accomplish with SOPA and PIPA. Get involved and do what you can to prevent it from moving any further.</p>
<ul>
<li><a href="http://wordpress.org/news/2012/01/help-stop-sopa-pipa/">WordPress &gt; Help Stop SOPA/PIPA</a></li>
<li><a href="http://reddit.com/r/sopa">Reddit - SOPA</a></li>
<li><a href="http://americancensorship.org/">Stop American Censorship</a></li>
<li><a href="http://en.wikipedia.org/wiki/Stop_Online_Piracy_Act">Wikipedia - Stop Online Piracy Act</a></li>
<li><a href="https://action.eff.org/o/9042/p/dia/action/public/?action_KEY=8173">EFF - Take Action</a></li>
<li><a href="http://www.rackspace.com/cloud/blog/2011/12/24/why-rackspace-opposes-the-%E2%80%9Cstop-online-piracy-act%E2%80%9D/">Why Rackspace opposes the Stop Online Piracy Act</a></li>
<li><a href="http://www.mozilla.org/sopa/">Mozilla - Protect The Internet</a></li>
</ul>
<p>You may need a little humor after all of that reading. Head on over to <a href="http://knowyourmeme.com/memes/events/protect-ip-act-stop-online-piracy-act">Know Your Meme</a> (warning: NSFW language in certain areas) for a laugh.</p>
]]></content></item><item><title>Native IPv6 connectivity in Mikrotik’s RouterOS</title><link>https://major.io/2012/01/11/native-ipv6-connectivity-in-mikrotiks-routeros/</link><pubDate>Wed, 11 Jan 2012 13:30:07 +0000</pubDate><guid>https://major.io/2012/01/11/native-ipv6-connectivity-in-mikrotiks-routeros/</guid><description>It&amp;rsquo;s no secret that I&amp;rsquo;m a big fan of the Routerboard devices and the RouterOS software from Mikrotik that runs on them. The hardware is solid, the software is stable and feature-rich, and I found a great vendor that ships quickly.
I recently added a RB493G (~ $230 USD) to sit in front of a pair of colocated servers. The majority of the setup routine was the same as with my previous devices except for the IPv6 configuration.</description><content type="html"><![CDATA[<p>It&rsquo;s no secret that I&rsquo;m a big fan of the <a href="http://routerboard.com/">Routerboard</a> devices and the <a href="http://www.mikrotik.com/software.html">RouterOS software from Mikrotik</a> that runs on them. The hardware is solid, the software is stable and feature-rich, and I found a <a href="http://www.roc-noc.com/">great vendor</a> that ships quickly.</p>
<p>I recently added a <a href="http://routerboard.com/RB493G">RB493G</a> (<a href="http://www.roc-noc.com/mikrotik/routerboard/rb493g-complete.html">~ $230 USD</a>) to sit in front of a pair of colocated servers. The majority of the setup routine was the same as with my previous devices except for the IPv6 configuration.</p>
<p>In the past, I&rsquo;ve set up IPv6 tunnels with <a href="http://ipv6.he.net/">Hurricane Electric</a> and it&rsquo;s been mostly a cut-and-paste operation from the sample configuration in their IPv6 tunnel portal. Setting up native IPv6 involved a little more legwork.</p>
<p>If your provider will give you two /64&rsquo;s or an entire /48, getting IPv6 connectivity for your WAN/LAN interfaces is simple. However, if you can only get one /64, you&rsquo;ll have to see if your provider can route it to you via your Mikrotik&rsquo;s <a href="http://en.wikipedia.org/wiki/Link-local_address#IPv6">link local</a> interface (I wouldn&rsquo;t recommend this for many reasons).</p>
<p>I split my Mikrotik into two interfaces: wan and lanbridge. The lanbridge bridge joins all of the LAN ethernet ports (ether2-9 on the RB493G) and the wan interface connects to the upstream switch.</p>
<p>My configuration:</p>
<pre><code>/ipv6 address
add address=2001:DB8:0:1::2/64 advertise=yes disabled=no eui-64=no interface=wan
add address=2001:DB8:0:2::1/64 advertise=yes disabled=no eui-64=no interface=lanbridge
/ipv6 route
add disabled=no distance=1 dst-address=::/0 gateway=2001:DB8:0:1::1 scope=30 \
  target-scope=10
/ipv6 nd
add advertise-dns=no advertise-mac-address=yes disabled=no hop-limit=64 \
  interface=all managed-address-configuration=no mtu=unspecified \
  other-configuration=no ra-delay=3s ra-interval=3m20s-10m ra-lifetime=30m \
  reachable-time=unspecified retransmit-interval=unspecified
/ipv6 nd prefix default
set autonomous=yes preferred-lifetime=1w valid-lifetime=4w2d
</code></pre><p>Explanation:</p>
<pre><code>/ipv6 address
add address=2001:DB8:0:1::2/64 advertise=yes disabled=no eui-64=no interface=wan
add address=2001:DB8:0:2::1/64 advertise=yes disabled=no eui-64=no interface=lanbridge
</code></pre><p>These two lines configure the IPv6 addresses for the firewall&rsquo;s interfaces. My provider&rsquo;s router holds the 2001:DB8:0:1::1/64 address and routes the remainder of that /64 to me via 2001:DB8:0:1::2/64. The second /64 is on the lanbridge interface and my LAN devices take their IP addresses from that block. My provider routes that second /64 to me via the 2001:DB8:0:1::2/64 IP on my wan interface.</p>
<pre><code>/ipv6 route
add disabled=no distance=1 dst-address=::/0 gateway=2001:DB8:0:1::1 scope=30 \
  target-scope=10
</code></pre><p>I&rsquo;ve set a <a href="http://tldp.org/HOWTO/html_single/Linux+IPv6-HOWTO/#AEN1083">gateway</a> for IPv6 traffic so that the Mikrotik knows where to send internet-bound IPv6 traffic (in this case, to my ISP&rsquo;s core router).</p>
<pre><code>/ipv6 nd
add advertise-dns=no advertise-mac-address=yes disabled=no hop-limit=64 \
  interface=lanbridge managed-address-configuration=no mtu=unspecified \
  other-configuration=no ra-delay=3s ra-interval=3m20s-10m ra-lifetime=30m \
  reachable-time=unspecified retransmit-interval=unspecified
/ipv6 nd prefix default
set autonomous=yes preferred-lifetime=1w valid-lifetime=4w2d
</code></pre><p>These last two lines configure the <a href="http://en.wikipedia.org/wiki/Neighbor_Discovery_Protocol">neighbor discovery</a> on my lanbridge interface. This allows my LAN devices to do <a href="http://en.wikipedia.org/wiki/IPv6#Stateless_address_autoconfiguration_.28SLAAC.29">stateless autoconfiguration</a> (which gives them an IPv6 address as well as the gateway).</p>
<p>Want to read up on IPv6?</p>
<ul>
<li><a href="http://tldp.org/HOWTO/html_single/Linux+IPv6-HOWTO/">Linux IPv6 HOWTO</a></li>
<li><a href="http://en.wikipedia.org/wiki/IPv6">IPv6 on Wikipedia</a></li>
<li><a href="http://www.roesen.org/files/ipv6_cheat_sheet.pdf">IPv6 Cheat Sheet</a> [PDF]</li>
<li><a href="http://www.ripe.net/lir-services/resource-management/number-resources/ipv6/ipv6-subnetting-card">IPv6 Subnetting Card</a></li>
</ul>
]]></content></item><item><title>SELinux and .forward files</title><link>https://major.io/2012/01/02/selinux-and-forward-files/</link><pubDate>Mon, 02 Jan 2012 22:44:43 +0000</pubDate><guid>https://major.io/2012/01/02/selinux-and-forward-files/</guid><description>If you want to forward e-mail from root to another user, you can usually place a .forward file in root&amp;rsquo;s home directory and your mail server will take care of the rest:
/root/.forward With SELinux, you&amp;rsquo;ll end up getting an AVC denial each time your mail server tries to read the contents of the .forward file:
type=AVC msg=audit(1325543823.787:7416): avc: denied { open } for pid=9850 comm=&amp;quot;local&amp;quot; name=&amp;quot;.forward&amp;quot; dev=md0 ino=17694734 scontext=system_u:system_r:postfix_local_t:s0 tcontext=unconfined_u:object_r:admin_home_t:s0 tclass=file The reason is that your .</description><content type="html"><![CDATA[<p>If you want to forward e-mail from root to another user, you can usually place a <code>.forward</code> file in root&rsquo;s home directory and your mail server will take care of the rest:</p>
<pre><code> /root/.forward
</code></pre><p>With SELinux, you&rsquo;ll end up getting an AVC denial each time your mail server tries to read the contents of the <code>.forward</code> file:</p>
<pre><code>type=AVC msg=audit(1325543823.787:7416): avc:  denied  { open } for  pid=9850
  comm=&quot;local&quot; name=&quot;.forward&quot; dev=md0 ino=17694734
  scontext=system_u:system_r:postfix_local_t:s0
  tcontext=unconfined_u:object_r:admin_home_t:s0 tclass=file
</code></pre><p>The reason is that your <code>.forward</code> file doesn&rsquo;t have the right SELinux contexts. You can set the correct contest quickly with <code>restorecon</code>:</p>
<pre><code># ls -Z /root/.forward
-rw-r--r--. root root unconfined_u:object_r:admin_home_t:s0 /root/.forward
# restorecon -v /root/.forward
restorecon reset /root/.forward context unconfined_u:object_r:admin_home_t:s0-&gt;system_u:object_r:mail_forward_t:s0
# ls -Z /root/.forward
-rw-r--r--. root root system_u:object_r:mail_home_t:s0 /root/.forward
</code></pre><p>Try to send another e-mail to root and you should see the mail server forward the e-mail properly without any additional AVC denials.</p>
]]></content></item><item><title>Getting online with a CradlePoint PHS-300 and an AT&amp;T USBConnect Mercury</title><link>https://major.io/2011/12/16/getting-online-with-a-cradlepoint-phs-300-and-an-att-usbconnect-mercury/</link><pubDate>Fri, 16 Dec 2011 07:07:08 +0000</pubDate><guid>https://major.io/2011/12/16/getting-online-with-a-cradlepoint-phs-300-and-an-att-usbconnect-mercury/</guid><description>Anyone who has used a 3G ExpressCard or USB stick knows how handy they can be when you need internet access away from home (and away from Wi-Fi). I&amp;rsquo;ve run into some situations recently where I needed to share my 3G connection with more than one device without using internet sharing on my MacBook Pro.
That led me to pick up a CradlePoint PHS-300 (discontinued by the manufacturer, but available from Amazon for about $35).</description><content type="html"><![CDATA[<p>Anyone who has used a 3G ExpressCard or USB stick knows how handy they can be when you need internet access away from home (and away from Wi-Fi). I&rsquo;ve run into some situations recently where I needed to share my 3G connection with more than one device without using internet sharing on my MacBook Pro.</p>
<p><img src="/wp-content/uploads/2011/12/phs300.jpg" alt="1"></p>
<p>That led me to pick up a CradlePoint PHS-300 (discontinued by the manufacturer, but available from <a href="http://www.amazon.com/CradlePoint-PHS300-Personal-Hotspot-Wireless/dp/B001212ELY">Amazon</a> for about $35). It&rsquo;s compatible with my AT&amp;T USBConnect Mercury (a.k.a. Sierra Wireless Compass 885/885U) USB stick.</p>
<p>Configuring the PHS-300 was extremely easy since I could just associate with the wireless network and enter the password printed on the bottom of the unit. However, getting the 3G stick to work was an immense pain. If you&rsquo;re trying to pair up these products, these steps should help:</p>
<ul>
<li>Access the PHS-300&rsquo;s web interface</li>
<li>Click the <strong>Modem</strong> tab</li>
<li>Click <strong>Settings</strong> on the left</li>
<li>Click <strong>Always on</strong> under <strong>Reconnect Mode</strong></li>
<li>Uncheck <strong>Aggressive Modem Reset</strong></li>
<li>Put the following into the <strong>AT Dial Script</strong> text box:</li>
</ul>
<pre><code>ATE0V1&amp;F&amp;D2&amp;C1S0=0
ATDT*99***1#
</code></pre><ul>
<li>Add <code>ISP.CINGULAR</code> to the <strong>Access Point Name (APN)</strong> box</li>
<li>Flip the <strong>Connect Mode</strong> under <strong>Dual WiMAX/3G Settings</strong> to <strong>3G Only</strong></li>
<li>Scroll up and push <strong>Save Settings</strong> and then <strong>Reboot Now</strong></li>
</ul>
<p>Once the PHS-300 reboots, the USB stick may light up, then turn off, and the display on the PHS-300 might show a red light for the 3G card. Wait about 10-15 seconds for the light to turn green. The lights on the 3G stick should be glowing and blinking as well.</p>
<p><strong>So how did I figure this out?</strong></p>
<p>After scouring Google search results, Sierra Wireless FAQ&rsquo;s, CradlePoint&rsquo;s support pages, and trolling through minicom (yes, minicom), I thought I&rsquo;d try connecting with my MacBook Pro using the 3G Watcher application provided by Sierra Wireless. Before connecting, I opened up Console.app and watched the <code>ppp.log</code> file. Sure enough, two lines popped up that were quite relevant to my interests:</p>
<pre><code>Fri Dec 16 00:37:51 2011 : Initializing phone: ATE0V1&amp;F&amp;D2&amp;C1S0=0
Fri Dec 16 00:37:51 2011 : Dialing: ATDT*99***1#
</code></pre><p>I didn&rsquo;t have the exact initialization string in the PHS-300 and that was the cause of the failure the entire time.</p>
<p>If you&rsquo;d like to talk to your USBConnect Mercury stick with minicom, just install minicom from macports (<code>sudo port -v install minicom</code>) and start it up like so:</p>
<pre><code>sudo minicom -D /dev/cu.sierra04
</code></pre><p>For other Sierra Wireless cards and adapters, there&rsquo;s a <a href="http://mycusthelp.net/SIERRAWIRELESS/_cs/AnswerDetail.aspx?aid=7">helpful page</a> on Sierra Wireless' site for Eee PC users.</p>
]]></content></item><item><title>Automatically upgrading to new point releases of Scientific Linux</title><link>https://major.io/2011/11/23/automatically-upgrading-to-new-point-releases-of-scientific-linux/</link><pubDate>Wed, 23 Nov 2011 13:20:12 +0000</pubDate><guid>https://major.io/2011/11/23/automatically-upgrading-to-new-point-releases-of-scientific-linux/</guid><description>When you install Scientific Linux, it will keep you on the same point release that you installed. For example, if you install it from a 6.0 DVD, you&amp;rsquo;ll stay on 6.0 and get security releases for that point release only.
Getting it to behave like Red Hat Enterprise Linux and CentOS is a painless process. Just install the sl6x repository with yum:
yum install yum-conf-sl6x Check to ensure that you&amp;rsquo;re getting updates from the new repository:</description><content type="html"><![CDATA[<p>When you install Scientific Linux, it will keep you on the same point release that you installed. For example, if you install it from a 6.0 DVD, you&rsquo;ll stay on 6.0 and get security releases for that point release only.</p>
<p>Getting it to behave like Red Hat Enterprise Linux and CentOS is a painless process. Just install the <em>sl6x</em> repository with <code>yum</code>:</p>
<pre><code>yum install yum-conf-sl6x
</code></pre><p>Check to ensure that you&rsquo;re getting updates from the new repository:</p>
<pre><code># yum repolist
repo id            repo name                                              status
sl                 Scientific Linux 6.1 - x86_64                          6,251
sl-security        Scientific Linux 6.1 - x86_64 - security updates         548
sl6x               Scientific Linux 6x - x86_64                           6,251
sl6x-security      Scientific Linux 6x - x86_64 - security updates          548
repolist: 13,598
</code></pre>]]></content></item><item><title>DisplayLink USB to DVI issues in OS X Lion</title><link>https://major.io/2011/11/17/displaylink-usb-to-dvi-issues-in-os-x-lion/</link><pubDate>Thu, 17 Nov 2011 13:38:48 +0000</pubDate><guid>https://major.io/2011/11/17/displaylink-usb-to-dvi-issues-in-os-x-lion/</guid><description>I added a DisplayLink USB to DVI adapter to my MacBook Pro a while back and it occasionally has some issues where it won&amp;rsquo;t start the display after connecting the USB cable. My logs in Console.app usually contain something like this:
The IOUSBFamily is having trouble enumerating a USB device that has been plugged in. It will keep retrying. (Port 4 of Hub at 0xfa100000) The IOUSBFamily was not able to enumerate a device.</description><content type="html"><![CDATA[<p>I added a <a href="http://www.displaylink.com/">DisplayLink USB to DVI adapter</a> to my MacBook Pro a while back and it occasionally has some issues where it won&rsquo;t start the display after connecting the USB cable. My logs in Console.app usually contain something like this:</p>
<pre><code>The IOUSBFamily is having trouble enumerating a USB device that has been plugged in.  It will keep retrying.  (Port 4 of Hub at 0xfa100000)
The IOUSBFamily was not able to enumerate a device.
The IOUSBFamily is having trouble enumerating a USB device that has been plugged in.  It will keep retrying.  (Port 4 of Hub at 0xfa100000)
The IOUSBFamily was not able to enumerate a device.
The IOUSBFamily is having trouble enumerating a USB device that has been plugged in.  It will keep retrying.  (Port 4 of Hub at 0xfa100000)
The IOUSBFamily gave up enumerating a USB device after 10 retries.  (Port 4 of Hub at 0xfa100000)
The IOUSBFamily was not able to enumerate a device.
</code></pre><p>The solution is a bit goofy, but here&rsquo;s what you can do:</p>
<ol>
<li>Unplug the adapter from the USB port.</li>
<li>Disconnect the DVI cable from the DisplayLink adapter.</li>
<li>Power off the display you normally use with the adapter.</li>
<li>Connect the USB cable between your computer and the DisplayLink adapter.</li>
<li>Wait for your displays to flash (as if a new display was connected).</li>
<li>The light on your DisplayLink adapter should be on now.</li>
<li>Connect the DVI cable to the DisplayLink adapter.</li>
<li>Wait a few seconds and then power on the display connected to the adapter.</li>
</ol>
<p>If this process doesn&rsquo;t work, try a reboot and repeat the process once Finder finishes starting up.</p>
]]></content></item><item><title>Live upgrade Fedora 15 to Fedora 16 using yum</title><link>https://major.io/2011/11/14/live-upgrading-fedora-15-to-fedora-16-using-yum/</link><pubDate>Tue, 15 Nov 2011 04:37:39 +0000</pubDate><guid>https://major.io/2011/11/14/live-upgrading-fedora-15-to-fedora-16-using-yum/</guid><description>Before we get started, I really ought to drop this here:
This begs the question: When should you use another method to upgrade Fedora? What other methods are there?
You have a few other methods to get the upgrade done:
Toss in a CD or DVD: You can upgrade via the anaconda installer provided on the CD, DVD or netinstall media. My experiences with this method for Fedora (as well as CentOS, Scientific Linux, and Red Hat) haven&amp;rsquo;t been too positive, but your results may vary.</description><content type="html"><![CDATA[<p>Before we get started, I really ought to drop this here:</p>
<!-- raw HTML omitted -->
<p><a href="/wp-content/uploads/2011/11/Logo_fedoralogo.png"><!-- raw HTML omitted --></a>This begs the question: <em>When should you use another method to upgrade Fedora? What other methods are there?</em></p>
<p>You have a few other methods to get the upgrade done:</p>
<ul>
<li><strong>Toss in a CD or DVD:</strong> You can upgrade via the anaconda installer provided on the CD, DVD or netinstall media. My experiences with this method for Fedora (as well as CentOS, Scientific Linux, and Red Hat) haven&rsquo;t been too positive, but your results may vary.</li>
<li><strong>Download the newer release&rsquo;s fedora-release RPM, install it with <code>rpm</code>, and <code>yum upgrade</code>:</strong> This is the really old way of doing things. Don&rsquo;t try this (read the next bullet).</li>
<li><strong>Use <code>yum</code>&rsquo;s distro-sync functionality:</strong> If you can&rsquo;t go the <code>preupgrade</code> route, I&rsquo;d recommend giving this a try. However, leave plenty of time to fix small glitches after it&rsquo;s done (and after your first reboot).</li>
</ul>
<p><strong>Personal anecdote time</strong> <em>(Keep scrolling for the meat and potatoes)</em></p>
<p>I have a dedicated server at <a href="http://joesdatacenter.com/">Joe&rsquo;s Datacenter</a> (love those folks) with IPMI and KVM-over-LAN access. The <code>preupgrade</code> method won&rsquo;t work for me because my <code>/boot</code> partition is on a software RAID volume. There&rsquo;s a <a href="https://bugzilla.redhat.com/show_bug.cgi?id=504826">rat&rsquo;s nest of a Bugzilla ticket</a> over on Red Hat&rsquo;s site about this problem. I&rsquo;m really only left with a live upgrade using <code>yum</code>.</p>
<p><strong>Live <code>yum</code> upgrade process</strong></p>
<p>Before even beginning the upgrade, I double-checked that I&rsquo;d applied all of the available updates for my server. Once that was done, I realized I was one kernel revision behind and I rebooted to ensure I was in the latest Fedora 15 kernel.</p>
<p>A good practice here is to run <code>package-cleanup --orphans</code> (it&rsquo;s in the <code>yum-utils</code> package) to find any packages which don&rsquo;t exist on any Fedora mirrors. In my case, I had two old kernels and a JungleDisk package. I removed the two old kernels (probably wasn&rsquo;t necessary) and left JungleDisk alone (it worked fine after the upgrade). If you have any external repositories, such as Livna or RPMForge, you may want to disable those until the upgrade is done. Should the initial upgrade checks bomb out, try adding as few repositories back in as possible to see if it clears up the problem.</p>
<p>Once you make it this far, just follow the instructions available in Fedora&rsquo;s documentation: <a href="http://fedoraproject.org/wiki/Upgrading_Fedora_using_yum#Fedora_15_-.3E_Fedora_16">Upgrading Fedora using yum</a>. I set SELinux to permissive mode during the upgrade just in case it caused problems.</p>
<p>I&rsquo;d recommend skipping the <code>grub2-install</code> portion since your original grub installation will still be present after the upgrade. If your server has EFI (not BIOS), <strong>don&rsquo;t use</strong> <code>grub2</code> yet. Keep an eye on the previously mentioned documentation page to see if the problems get ironed out between <code>grub2</code> and EFI.</p>
<p><strong>Before you reboot,</strong> be sure to get a list of your active processes and daemons. After your reboot, some old SysVinit scripts will be converted into Systemd service scripts. They might not start automatically and you might need to enable and/or start some services.</p>
<p>New to Systemd? This will be an extremely handy resource: <a href="http://fedoraproject.org/wiki/SysVinit_to_Systemd_Cheatsheet">SysVinit to Systemd Cheatsheet</a>.</p>
<p>I haven&rsquo;t seen too many issues after cleaning up some daemons that didn&rsquo;t start properly. There is a problem between <code>asterisk</code> and SELinux that I haven&rsquo;t nailed down yet but it&rsquo;s not a showstopper.</p>
<p>Good luck during your upgrades. Keep in mind that Fedora 15 could be EOL&rsquo;d as early as May or June 20102 when Fedora 17 is released.</p>
]]></content></item><item><title>Tracing a build through OpenStack Compute (Nova)</title><link>https://major.io/2011/11/07/tracing-a-build-through-openstack-compute-nova/</link><pubDate>Mon, 07 Nov 2011 15:05:42 +0000</pubDate><guid>https://major.io/2011/11/07/tracing-a-build-through-openstack-compute-nova/</guid><description>My work at Rackspace has changed a bit in the last few weeks and I&amp;rsquo;ve shifted from managing a team of engineers to a full technical focus on OpenStack Nova. Although it was difficult to leave my management position, I&amp;rsquo;m happy to get back to my roots and dig into the technical stuff again.
One of the first things I wanted to tackle was understanding how a build request flows through Nova to a XenServer hypervisor.</description><content type="html"><![CDATA[<p><img src="/wp-content/uploads/2011/11/openstack-justheo.png" alt="1"></p>
<p>My work at Rackspace has changed a bit in the last few weeks and I&rsquo;ve shifted from managing a team of engineers to a full technical focus on <a href="http://openstack.org/projects/compute/">OpenStack Nova</a>. Although it was difficult to leave my management position, I&rsquo;m happy to get back to my roots and dig into the technical stuff again.</p>
<p>One of the first things I wanted to tackle was understanding how a build request flows through Nova to a XenServer hypervisor. Following this process through the code is a bit tricky (I&rsquo;m still learning python, so that could explain it). Here are the basic steps:</p>
<ul>
<li>Client requests a build via the API.</li>
<li>The API runs some checks (quotas, auth, etc) and hands the build off to the scheduler.</li>
<li>The scheduler figures out where the instance should go.</li>
<li>The scheduler drops a message in queue specific to one compute node (where the instance will be built).</li>
<li>The API responds to the client and the client is now unblocked and free to do other things.</li>
<li>The compute node updates the database with the instance details and calls to the hypervisor to assemble block devices for the instance.</li>
<li>A message is dropped into the network node&rsquo;s queue (from the compute node) to begin assembling networks for the instance. The compute node blocks and waits while this step completes.</li>
<li>Once the networking details come back (via the queue), the compute node does the remaining adjustments on the hypervisor and starts up the actual instance.</li>
<li>When the instance starts successfully (or fails to do so), the database is updated and a message is dropped onto another message queue as a notification that the build is complete.</li>
</ul>
<p><img src="/wp-content/uploads/2011/11/Tracing-an-Instance-Build-Through-Nova.png" alt="3"></p>
<p>Click on the thumbnail on the right to see the flow chart I created to explain this process.</p>
<p><strong>Please note:</strong> This information should be accurate to the Nova code as of November 1, 2011. There could be some refactoring of these build processes before <a href="https://launchpad.net/nova/essex">Essex</a> is released.</p>
<p><!-- raw HTML omitted --></p>
]]></content></item><item><title>Installing irssi via MacPorts on OS X Lion 10.7.1</title><link>https://major.io/2011/09/30/installing-irssi-via-macports-on-os-x-lion-10-7-1/</link><pubDate>Fri, 30 Sep 2011 13:24:44 +0000</pubDate><guid>https://major.io/2011/09/30/installing-irssi-via-macports-on-os-x-lion-10-7-1/</guid><description>I&amp;rsquo;ve floated back and forth between graphical IRC clients and terminal-based clients for a long time. However, I was sad to see that irssi wouldn&amp;rsquo;t build via MacPorts on OS X Lion. During the build, I saw quite a few errors from the compiler:
-E, -S, -save-temps and -M options are not allowed with multiple -arch flags Sure enough, when I looked at the lines in the output, both x86_64 and i386 were passed to the compiler:</description><content type="html"><![CDATA[<p>I&rsquo;ve floated back and forth between graphical IRC clients and terminal-based clients for a long time. However, I was sad to see that irssi wouldn&rsquo;t build via MacPorts on OS X Lion. During the build, I saw quite a few errors from the compiler:</p>
<pre><code>-E, -S, -save-temps and -M options are not allowed with multiple -arch flags
</code></pre><p>Sure enough, when I looked at the lines in the output, both x86_64 and i386 were passed to the compiler:</p>
<pre><code>... -pipe -O2 -arch x86_64 -arch i386 -fno-common ...
</code></pre><p>I <a href="http://trac.macports.org/ticket/31467">opened a ticket in trac</a> and began looking for a workaround. <a href="http://trac.macports.org/ticket/13004#comment:4">Another trac ticket</a> (from four years ago) on the MacPorts site gave some pointers on how to work around the bug for a previous version.</p>
<p>I changed up the instructions a bit since we&rsquo;re not dealing with the ppc architecture any longer:</p>
<pre><code>sudo port -v clean irssi +perl
sudo port -v configure irssi +perl
cd /opt/local/var/macports/build/_opt_local_var_macports_sources_rsync.macports.org_release_tarballs_ports_irc_irssi/irssi/work/
sudo find . -type f -exec sed -i &quot;&quot; -e &quot;s/-arch i386//g&quot; {} \;
cd
sudo port -v install irssi +perl
</code></pre><p>The build worked!</p>
<pre><code>$ irssi -v
irssi 0.8.15 (20100403 1617)
</code></pre>]]></content></item><item><title>Getting back to using eth0 in Fedora 15</title><link>https://major.io/2011/09/25/getting-back-to-using-eth0-in-fedora-15/</link><pubDate>Sun, 25 Sep 2011 22:08:20 +0000</pubDate><guid>https://major.io/2011/09/25/getting-back-to-using-eth0-in-fedora-15/</guid><description>Fedora 15 was released with some updates to allow for consistent network device names. Once it&amp;rsquo;s installed, you&amp;rsquo;ll end up with network devices that are named something other than eth0, eth1, and so on.
For example, all onboard ethernet adapters are labeled as emX (em1, em2…) and all PCI ethernet adapters are labeled as pXpX (p[slot]p[port], like p7p1 for port 1 on slot 7). Ethernet devices within Xen virtual machines aren&amp;rsquo;t adjusted.</description><content type="html"><![CDATA[<p>Fedora 15 was released with some updates to allow for <a href="http://fedoraproject.org/wiki/Features/ConsistentNetworkDeviceNaming">consistent network device names</a>. Once it&rsquo;s installed, you&rsquo;ll end up with network devices that are named something other than eth0, eth1, and so on.</p>
<p>For example, all onboard ethernet adapters are labeled as emX (em1, em2…) and all PCI ethernet adapters are labeled as pXpX (p[slot]p[port], like p7p1 for port 1 on slot 7). Ethernet devices within Xen virtual machines aren&rsquo;t adjusted.</p>
<p>This may make sense to people who swap out the chassis on servers regularly and they don&rsquo;t want to mess with hard-coding MAC addresses in network configuration files. Also, it should give users predictable names even if a running system&rsquo;s drives are inserted into a newer hardware revision of the same server.</p>
<p>However, I don&rsquo;t like this on my personal dedicated servers and I prefer to revert back to the old way of doing things. Getting back to eth0 is pretty simple and it only requires a few configuration files to be edited followed by a reboot.</p>
<p>First, add <code>biosdevname=0</code> to your <code>grub.conf</code> on the kernel line:</p>
<pre><code>title Fedora (2.6.40.4-5.fc15.x86_64)
	root (hd0,0)
	kernel /boot/vmlinuz-2.6.40.4-5.fc15.x86_64 ro root=/dev/md0 SYSFONT=latarcyrheb-sun16 KEYTABLE=us biosdevname=0 quiet LANG=en_US.UTF-8
	initrd /boot/initramfs-2.6.40.4-5.fc15.x86_64.img
</code></pre><p>Open <code>/etc/udev/rules.d/70-persistent-net.rules</code> in your favorite text editor (create it if it doesn&rsquo;t exist) and add in the following:</p>
<pre><code># Be sure to put your MAC addresses in the fields below
SUBSYSTEM==&quot;net&quot;, ACTION==&quot;add&quot;, DRIVERS==&quot;?*&quot;, ATTR{address}==&quot;00:11:22:33:44:10&quot;, ATTR{dev_id}==&quot;0x0&quot;, ATTR{type}==&quot;1&quot;, KERNEL==&quot;eth*&quot;, NAME=&quot;eth0&quot;
SUBSYSTEM==&quot;net&quot;, ACTION==&quot;add&quot;, DRIVERS==&quot;?*&quot;, ATTR{address}==&quot;00:11:22:33:44:11&quot;, ATTR{dev_id}==&quot;0x0&quot;, ATTR{type}==&quot;1&quot;, KERNEL==&quot;eth*&quot;, NAME=&quot;eth1&quot;
</code></pre><p>Be sure to rename your <code>ifcfg-*</code> files in <code>/etc/sysconfig/network-scripts/</code> to match the device names you&rsquo;ve assigned. Just for good measure, I add in the MAC address in <code>/etc/sysconfig/network-scripts/ifcfg-ethX</code>:</p>
<pre><code>...
HWADDR=00:11:22:33:44:10
...
</code></pre><p>Reboot the server and you should be back to eth0 and eth1 after a reboot.</p>
]]></content></item><item><title>Receive e-mail reports for SELinux AVC denials</title><link>https://major.io/2011/09/15/receive-e-mail-reports-for-selinux-avc-denials/</link><pubDate>Fri, 16 Sep 2011 04:17:04 +0000</pubDate><guid>https://major.io/2011/09/15/receive-e-mail-reports-for-selinux-avc-denials/</guid><description>SELinux isn&amp;rsquo;t a technology that&amp;rsquo;s easy to tackle for newcomers. However, there&amp;rsquo;s been a lot of work to smooth out the rough edges while still keeping a tight grip on what applications and users are allowed to do on a Linux system. One of the biggest efforts has been around setroubleshoot.
The purpose behind setroubleshoot is to let users know when access has been denied, help them resolve it if necessary, and to reduce overall frustration while working through tight security restrictions in the default SELinux policies.</description><content type="html"><![CDATA[<p>SELinux isn&rsquo;t a technology that&rsquo;s easy to tackle for newcomers. However, there&rsquo;s been a lot of work to smooth out the rough edges while still keeping a tight grip on what applications and users are allowed to do on a Linux system. One of the biggest efforts has been around <a href="https://fedorahosted.org/setroubleshoot/wiki/SETroubleShoot%20Overview">setroubleshoot</a>.</p>
<p>The purpose behind setroubleshoot is to let users know when access has been denied, help them resolve it if necessary, and to reduce overall frustration while working through tight security restrictions in the default SELinux policies. The GUI frontend for setroubleshoot is great for users who run Linux desktops or those who run servers with a display attached. Don&rsquo;t worry, you can configure setroubleshoot on remote servers to send alerts elsewhere when a GUI alert isn&rsquo;t an option.</p>
<p>Install a few packages to get started:</p>
<pre><code>yum install setroubleshoot{-server,-plugins,-doc}
</code></pre><p>Open <code>/etc/setroubleshoot/setroubleshoot.conf</code> in your favorite text editor and adjust the <code>[email]</code> section to fit your server:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ini" data-lang="ini">recipients_filepath <span style="color:#719e07">=</span> <span style="color:#2aa198">/var/lib/setroubleshoot/email_alert_recipients</span>
smtp_port <span style="color:#719e07">=</span> <span style="color:#2aa198">25</span>
smtp_host <span style="color:#719e07">=</span> <span style="color:#2aa198">localhost</span>
from_address <span style="color:#719e07">=</span> <span style="color:#2aa198">selinux@myserver.com</span>
subject <span style="color:#719e07">=</span> <span style="color:#2aa198">[MyServer] SELinux AVC Alert</span>
</code></pre></div><p>You could probably see it coming, but you need to put the e-mail addresses for your recipients into <code>/var/lib/setroubleshoot/email_alert_recipients</code>:</p>
<pre><code>echo &quot;selinux@mycompany.com&quot; &gt;&gt; /var/lib/setroubleshoot/email_alert_recipients
</code></pre><p>You&rsquo;ll notice that setroubleshoot doesn&rsquo;t have an init script and it doesn&rsquo;t exist in systemd in Fedora 15. It runs through the <a href="http://en.wikipedia.org/wiki/D-Bus">dbus-daemon</a> and a quick bounce of the messagebus via its init script brings in the necessary components to run setroubleshoot:</p>
<pre><code>service messagebus restart
</code></pre><p>A really easy (and safe) test is to ask sshd to bind to a non-standard port. Simply define an additional port on in your <code>/etc/ssh/sshd_config</code> like this:</p>
<pre><code>Port 22
Port 222
</code></pre><p>When you restart sshd, it will bind to port 22 with success, but it won&rsquo;t be allowed to bind to port 222 (since that&rsquo;s blocked by SELinux as a non-standard port for the <code>ssh_port_t</code> port type). <strong>DON&rsquo;T WORRY!</strong> Your sshd server will still be listening on port 22. If you wait a moment, you&rsquo;ll get an e-mail (perhaps two) that not only notify you of the denial, but they make suggestions for how to fix it:</p>
<pre><code>SELinux is preventing /usr/sbin/sshd from name_bind access on the tcp_socket port 222.

*****  Plugin bind_ports (99.5 confidence) suggests  *************************

If you want to allow /usr/sbin/sshd to bind to network port 222
Then you need to modify the port type.
Do
# semanage port -a -t PORT_TYPE -p tcp 222
   where PORT_TYPE is one of the following: ...
</code></pre><p>For this particular example, the quick fix would be to run:</p>
<pre><code>semanage port -a -t ssh_port_t -p tcp 222
</code></pre><hr>
<p><em>Much of this post&rsquo;s information was gathered from the detailed documentation on <a href="http://fedoraproject.org/wiki/Docs/Drafts/SELinux/SETroubleShoot/UserFAQ">Fedora&rsquo;s setroubleshoot User&rsquo;s FAQ</a> as well as <a href="http://danwalsh.livejournal.com/20931.html">Dan Walsh&rsquo;s setroubleshoot blog post</a>.</em></p>
]]></content></item><item><title>Getting apache, PHP, and memcached working with SELinux</title><link>https://major.io/2011/09/07/getting-apache-php-and-memcached-working-with-selinux/</link><pubDate>Thu, 08 Sep 2011 03:55:00 +0000</pubDate><guid>https://major.io/2011/09/07/getting-apache-php-and-memcached-working-with-selinux/</guid><description>I&amp;rsquo;m using SELinux more often now on my Fedora 15 installations and I came up against a peculiar issue today on a new server. My PHP installation is configured to store its sessions in memcached and I brought over some working configurations from another server. However, each time I accessed a page which tried to initiate a session, the page load would hang for about a minute and I&amp;rsquo;d find this in my apache error logs:</description><content type="html"><![CDATA[<p><img src="/wp-content/uploads/2011/09/selinux-penguin-125.png" alt="1"></p>
<p>I&rsquo;m using SELinux more often now on my Fedora 15 installations and I came up against a peculiar issue today on a new server. My PHP installation is configured to store its sessions in memcached and I brought over some working configurations from another server. However, each time I accessed a page which tried to initiate a session, the page load would hang for about a minute and I&rsquo;d find this in my apache error logs:</p>
<pre><code>[Thu Sep 08 03:23:40 2011] [error] [client 11.22.33.44] PHP Warning:
Unknown: Failed to write session data (memcached). Please verify that
the current setting of session.save_path is correct (127.0.0.1:11211)
in Unknown on line 0
</code></pre><p>I ran through my usual list of checks:</p>
<ul>
<li>netstat showed memcached bound to the correct ports/interfaces</li>
<li>memcached was running and I could reach it via telnet</li>
<li>memcached-tool could connect and pull stats from memcached</li>
<li>double-checked my php.ini</li>
<li>tested memcached connectivity via a PHP and ruby script - they worked</li>
</ul>
<p>Even after all that, I still couldn&rsquo;t figure out what was wrong. I ran strace on memcached while I ran a curl against the page which creates a session and I found something significant - memcached wasn&rsquo;t seeing any connections whatsoever at that time. A quick check of the lo interface with tcpdump showed the same result. Just before I threw a chair, I remembered one thing:</p>
<p><em>SELinux.</em></p>
<p>A quick check for AVC denials showed the problem:</p>
<pre><code># aureport --avc | tail -n 1
4021. 09/08/2011 03:23:38 httpd system_u:system_r:httpd_t:s0 42 tcp_socket name_connect system_u:object_r:memcache_port_t:s0 denied 31536
</code></pre><p>I&rsquo;m far from being a guru on SELinux, so I leaned on audit2allow for help:</p>
<pre><code># grep memcache /var/log/audit/audit.log | audit2allow

#============= httpd_t ==============
#!!!! This avc can be allowed using one of the these booleans:
#     httpd_can_network_relay, httpd_can_network_memcache, httpd_can_network_connect

allow httpd_t memcache_port_t:tcp_socket name_connect;
</code></pre><p>The boolean we&rsquo;re looking for is <code>httpd_can_network_memcache</code>. Flipping the boolean can be done in a snap:</p>
<pre><code># setsebool -P httpd_can_network_memcache 1
# getsebool httpd_can_network_memcache
httpd_can_network_memcache --&gt; on
</code></pre><p>After adjusting the boolean, apache was able to make connections to memcached without a hitch. My page which created sessions loaded quickly and I could see data being stored in memcached. If you want to check the status of all of the apache-related SELinux booleans, just use getsebool:</p>
<pre><code># getsebool -a | grep httpd | grep off$
allow_httpd_anon_write --&gt; off
allow_httpd_mod_auth_ntlm_winbind --&gt; off
allow_httpd_mod_auth_pam --&gt; off
allow_httpd_sys_script_anon_write --&gt; off
httpd_can_check_spam --&gt; off
httpd_can_network_connect_cobbler --&gt; off
httpd_can_network_connect_db --&gt; off
httpd_can_network_relay --&gt; off
httpd_can_sendmail --&gt; off
httpd_dbus_avahi --&gt; off
httpd_enable_ftp_server --&gt; off
httpd_enable_homedirs --&gt; off
httpd_execmem --&gt; off
httpd_read_user_content --&gt; off
httpd_setrlimit --&gt; off
httpd_ssi_exec --&gt; off
httpd_tmp_exec --&gt; off
httpd_unified --&gt; off
httpd_use_cifs --&gt; off
httpd_use_gpg --&gt; off
httpd_use_nfs --&gt; off
</code></pre><p>If you&rsquo;re interested in SELinux, a good way to get your feet wet is to head over to the CentOS Wiki and review their <a href="http://wiki.centos.org/HowTos/SELinux">SELinux Howtos</a></p>
]]></content></item><item><title>You might see me on CNN Money soon</title><link>https://major.io/2011/09/02/you-might-see-me-on-cnn-money-soon/</link><pubDate>Fri, 02 Sep 2011 15:40:42 +0000</pubDate><guid>https://major.io/2011/09/02/you-might-see-me-on-cnn-money-soon/</guid><description>A videographer from CNN Money stopped by the office today to ask about what makes Rackspace a unique place to work. As soon as we got started, everyone started to make as many distractions as they could to crack me up. Very few succeeded. ;-)
Thanks to @pinojo for snapping the photo.</description><content type="html"><![CDATA[<p><a href="/wp-content/uploads/2011/09/cnnmoneyinterview.jpg"><!-- raw HTML omitted --></a>A videographer from CNN Money stopped by the office today to ask about what makes Rackspace a unique place to work. As soon as we got started, everyone started to make as many distractions as they could to crack me up. Very few succeeded. ;-)</p>
<p>Thanks to <a href="http://twitter.com/pinojo">@pinojo</a> for snapping the photo.</p>
<p><!-- raw HTML omitted --></p>
]]></content></item><item><title>How to write e-mails to nerds (that they will actually read)</title><link>https://major.io/2011/08/26/how-to-write-e-mails-to-nerds-that-they-will-actually-read/</link><pubDate>Fri, 26 Aug 2011 13:00:06 +0000</pubDate><guid>https://major.io/2011/08/26/how-to-write-e-mails-to-nerds-that-they-will-actually-read/</guid><description>Standard e-mail etiquette is pretty obvious to most of us and if you&amp;rsquo;re good at it, you&amp;rsquo;ll get your point across more often without stepping on toes or causing unneeded confusion. Simple things like identifying yourself well, avoiding sarcasm and adding context to statements are all extremely beneficial. However, writing e-mails to highly technical developers, system administrators, and engineers is a little trickier. These types of e-mail recipients don&amp;rsquo;t really enjoy handling e-mail (inbound or outbound) and most find that e-mail is just a speed bump which interrupts their productivity.</description><content type="html"><![CDATA[<p>Standard e-mail etiquette is pretty obvious to most of us and if you&rsquo;re good at it, you&rsquo;ll get your point across more often without stepping on toes or causing unneeded confusion. Simple things like identifying yourself well, avoiding sarcasm and adding context to statements are all extremely beneficial. However, writing e-mails to highly technical developers, system administrators, and engineers is a little trickier. These types of e-mail recipients don&rsquo;t really enjoy handling e-mail (inbound or outbound) and most find that e-mail is just a speed bump which interrupts their productivity.</p>
<p>If you&rsquo;re not technical, you might be asking yourself: <em>&ldquo;I need to e-mail technical people and they need to take what I say seriously? How do I do it?&quot;</em> It&rsquo;s not impossible, but the rest of this blog post should help.</p>
<h3 id="brevity-is-key">Brevity is key</h3>
<p>There are some people who thrive on receiving e-mail, sending e-mail, and talking about e-mail that they&rsquo;ve sent or received. Most nerds don&rsquo;t feel this way.</p>
<p>You need to get your point across concisely and succinctly so that your e-mail is seen as less of a distraction. Avoid adding a lot of context where it isn&rsquo;t needed and try to summarize business needs and processes unless details are absolutely critical. If you need to send your e-mail to multiple recipients and some of those recipients need additional details, provide an abstract at the beginning of the e-mail.</p>
<h3 id="learn-the-ways-of-tldr">Learn the ways of TL;DR</h3>
<p>I&rsquo;ve heard quite a few conversations like these around the office:</p>
<blockquote>
<p>Nerd 1: &ldquo;Did you get that e-mail from [name here]?&rdquo;</p>
</blockquote>
<blockquote>
<p>Nerd 2: &ldquo;The six page one with four PDF files attached?&rdquo;</p>
</blockquote>
<blockquote>
<p>Nerd 1: &ldquo;Yeah. That one.&rdquo;</p>
</blockquote>
<blockquote>
<p>Nerd 2: &ldquo;TL;DR dude, seriously. Did you read it?&rdquo;</p>
</blockquote>
<blockquote>
<p>Nerd 1: &ldquo;Nah. I might read it later.&rdquo;</p>
</blockquote>
<p>If someone&rsquo;s ever mentioned &ldquo;TL;DR&rdquo; (too long; didn&rsquo;t read) when your e-mail was mentioned, don&rsquo;t fret. It&rsquo;s a quick fix. Just add a quick summary to the top of your e-mail prefaced with &ldquo;TL;DR&rdquo;. Provide a really brief summary (bulleted lists are a plus) of your e-mail in the section and then start your e-mail right afterwards. Here&rsquo;s an example:</p>
<pre><code>TL;DR
  * next software release deploys Monday
  * two bugs remaining to fix
  * we will get started at 8AM Saturday, yeaaaaah
</code></pre><p><em>Missed the joke? <a href="http://en.wikipedia.org/wiki/Bill_Lumbergh">Head over to Wikipedia</a>.</em></p>
<p>If one of the summary points interests a recipient, they&rsquo;ll scan your e-mail for the pertinent sections. Some recipients may only need to see what&rsquo;s in the summary and they won&rsquo;t bother reading the remainder. Either way, the effectiveness of your e-mail increases by leaps and bounds.</p>
<h3 id="plain-text">Plain text</h3>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p><a href="http://www.mutt.org/">mutt</a>, a text-based console-only e-mail reader. Click the thumbnail at the right and imagine what your e-mails would look like if they&rsquo;re full of images, stylesheets and background images. Better yet, imagine if your entire e-mail was in an image and the e-mail itself had no text.</p>
<p>Here are a few more tips under this category:</p>
<ul>
<li>Don&rsquo;t use Outlook stationery.</li>
<li>Never send e-mails with an image as the e-mail itself.</li>
<li><strong>No Comic Sans at any time. Period.</strong></li>
<li>Avoid graphical e-mail signatures (more on that in a moment).</li>
</ul>
<h3 id="e-mail-signatures">E-mail signatures</h3>
<p>Brevity can definitely be applied to e-mail signatures, too. How many times have you seen e-mails that end like this:</p>
<pre><code>Frank Frankelton MCSE, RHCSA, RHCE, CCNA, RHCA, LPIC-3, Ph.D., M.D., Esq., CMDBA
Systems Adminstrator Extraordinaire, Database Administrator, All-around great guy
Office: 210-555-1212
Mobile: 210-555-1213
Other Mobile: 210-555-1214
Fax: 210-555-1215
VOIP: 210-555-1216
AIM: frankeltonia
Twitter: @frankyfrank
Jabber: frankfurter@frankeltonisinthehouse.com
Big Company, Inc
</code></pre><p>You might think that nobody would ever send out e-mails with a signature like the one above, but I&rsquo;ve seen some that are actually worse. Keep the signature short and only put in the information that people really need to know. Generally, your name and title or department is sufficient for e-mail signatures (unless your local/federal laws require otherwise). Always preface it with a double dash &ldquo;-&rdquo; on a line by itself to signify that the remainder of the e-mail is the signature.</p>
<h3 id="summary">Summary</h3>
<p>Keep it simple, keep it brief, and keep it relevant. While the suggestions above might not apply to every business or every person, following the suggestions will increase the effectiveness of your e-mails and ensure that your voice is heard on the other end.</p>
<p>I&rsquo;m really interested to hear your comments. Are there some suggestions you have that I missed in the post? Did I make some suggestions which didn&rsquo;t make sense or don&rsquo;t apply to you? Let me know!</p>
]]></content></item><item><title>Contest winners from the “Inspire a sysadmin” contest</title><link>https://major.io/2011/08/22/contest-winners-from-the-inspire-a-sysadmin-contest/</link><pubDate>Mon, 22 Aug 2011 12:43:53 +0000</pubDate><guid>https://major.io/2011/08/22/contest-winners-from-the-inspire-a-sysadmin-contest/</guid><description>Before I get started, I&amp;rsquo;d like to give a big thanks to all of the visitors who dropped by and participated in the contest last week. Also, thanks to ThinkGeek for offering to pay for (and double) one of the prizes!
Here are the list of winners:
Grand Prize ($50 at ThinkGeek): Dan Udey Runners-Up ($25 at ThinkGeek): Joe Wright, Susan Price, and Giovanni Tirloni Dan&amp;rsquo;s comment rang true with me since much of a sysadmin&amp;rsquo;s job involves responding to crises regardless of how much planning you put forth:</description><content type="html"><![CDATA[<p>Before I get started, I&rsquo;d like to give a big thanks to all of the visitors who dropped by and <a href="http://rackerhacker.com/2011/08/17/inspire-a-sysadmin-get-a-thinkgeek-gift-certificate/">participated in the contest</a> last week. Also, thanks to <a href="http://thinkgeek.com/">ThinkGeek</a> for offering to pay for (and double) one of the prizes!</p>
<p>Here are the list of winners:</p>
<ul>
<li>Grand Prize ($50 at ThinkGeek): <strong>Dan Udey</strong></li>
<li>Runners-Up ($25 at ThinkGeek): <strong>Joe Wright, Susan Price, and Giovanni Tirloni</strong></li>
</ul>
<p><a href="http://rackerhacker.com/2011/08/17/inspire-a-sysadmin-get-a-thinkgeek-gift-certificate/#comment-23915">Dan&rsquo;s comment rang true</a> with me since much of a sysadmin&rsquo;s job involves responding to crises regardless of how much planning you put forth:</p>
<blockquote>
<p>Keep a cool head. Focus. Work methodically. Figure out what to do and get it done, and people will remember you as the person who performs under pressure. Once you can do that, you&rsquo;re a sysadmin.</p>
</blockquote>
<p><a href="http://rackerhacker.com/2011/08/17/inspire-a-sysadmin-get-a-thinkgeek-gift-certificate/#comment-23911">Joe touched on a critical point</a> about system administration:</p>
<blockquote>
<p>Tell the truth. If you break something, &lsquo;fess up and fix it. If you don&rsquo;t know how to do something, admit it and learn how to do the task. Create your own culture of honesty on the job; others will respect and follow your example.</p>
</blockquote>
<p><a href="http://rackerhacker.com/2011/08/17/inspire-a-sysadmin-get-a-thinkgeek-gift-certificate/#comment-23921">Susan offered some inspiration</a> for system administrators stuck in frustrating situations:</p>
<blockquote>
<p>I know, I know - dumb users, RTFM. Believe me, I&rsquo;ve been there. In fact - one of your strategies should be to establish a trusted community where you can VENT about these issues, and get support for yourself. Ask for answers when you don&rsquo;t know them. Restock on the compassion and patience.</p>
</blockquote>
<p><a href="http://rackerhacker.com/2011/08/17/inspire-a-sysadmin-get-a-thinkgeek-gift-certificate/#comment-23907">Giovanni talked about the basics</a> and what every system administrator should know to get started in a career. We probably take this for granted, but this is critical to keep in mind:</p>
<blockquote>
<p>If you are starting in the system administration area, don&rsquo;t praise yourself only because you (blindly?) fixed an issue or helped that friend with his/her server. Ask yourself: Why what I did fixed the issue? Why that was happening in the first place? And more importantly, how to avoid it for all eternity? You won&rsquo;t but it doesn&rsquo;t hurt to aim high.</p>
</blockquote>
<p><img src="/wp-content/uploads/2011/08/giftcert-preview.png" alt="7"></p>
<p>Even though it isn&rsquo;t a runner-up, <a href="http://rackerhacker.com/2011/08/17/inspire-a-sysadmin-get-a-thinkgeek-gift-certificate/#comment-23919">Paul&rsquo;s comment</a> certainly deserves an honorable mention. His comment is actually a true story (with a slight amount of embellishment, of course) and it serves as a reminder that system administrators and developers must stand up for their beliefs even if it goes against the beliefs of their superiors. If your managers don&rsquo;t value the feedback, it might be a sign that a career change is in order.</p>
<p>Once again, <strong>a big thanks</strong> goes out to everyone who submitted a comment. I&rsquo;ll reach out to the winners today and get the gift certificates sent out to them.</p>
]]></content></item><item><title>Inspire a sysadmin, get a ThinkGeek gift certificate</title><link>https://major.io/2011/08/17/inspire-a-sysadmin-get-a-thinkgeek-gift-certificate/</link><pubDate>Wed, 17 Aug 2011 12:36:52 +0000</pubDate><guid>https://major.io/2011/08/17/inspire-a-sysadmin-get-a-thinkgeek-gift-certificate/</guid><description>UPDATE: THE STAKES ARE RAISED!Check the end of this post for details.
Today is my birthday and I&amp;rsquo;m doing things in reverse - you are getting gifts today. I&amp;rsquo;m giving away four $25 gift certificates to ThinkGeek today (yep, that&amp;rsquo;s $100 out of my pocket) but you&amp;rsquo;ll have to do something to earn them.
I&amp;rsquo;m looking for words of wisdom and guidance from the readers of my blog for system administrators who are just getting started.</description><content type="html"><![CDATA[<p><!-- raw HTML omitted -->UPDATE: THE STAKES ARE RAISED!<!-- raw HTML omitted --> Check the end of this post for details.</p>
<p><a href="/wp-content/uploads/2011/08/giftcert-preview.png"><!-- raw HTML omitted --></a>Today is my birthday and I&rsquo;m doing things in reverse - you are getting gifts today. I&rsquo;m giving away four $25 gift certificates to <a href="http://www.thinkgeek.com/">ThinkGeek</a> today (yep, that&rsquo;s $100 out of my pocket) but you&rsquo;ll have to do something to earn them.</p>
<p>I&rsquo;m looking for words of wisdom and guidance from the readers of my blog for system administrators who are just getting started. I talk to brand new sysadmins and college graduates regularly and they&rsquo;re all hungry for what the seasoned folks in the industry know. They&rsquo;re not specifically on the hunt for hard facts and how-to&rsquo;s; they&rsquo;re looking for guidance on how to gain experience, reduce errors and learn efficiently.</p>
<p>Let&rsquo;s get to the important stuff: <strong>How does this contest work?</strong></p>
<ul>
<li><strong>Write a comment.</strong> Put an inspirational story, anecdote, or random words of wisdom for system administrators who are new to the industry in a comment on this post. Although it doesn&rsquo;t have to be extraordinarily lengthy, try to write more than just a sentence or two.</li>
<li><strong>Give me a way to contact you.</strong> Add something to your comment so I can contact you if you&rsquo;re the winner.</li>
<li><strong>Do it soon.</strong> The contest ends at <a href="http://www.timeanddate.com/worldclock/fixedtime.html?iso=20110817T2359&amp;p1=400">11:59PM CDT tonight</a>.</li>
</ul>
<p>I&rsquo;ll be the judge of the comments and I&rsquo;m going to choose the winners based on the content of the comment. The more <a href="http://dictionary.reference.com/browse/inspiration">inspirational</a> and <a href="http://dictionary.reference.com/browse/profound">profound</a> your comment is, the better chance you have of winning. Any comment written in <a href="http://cache.ohinternet.com/images/f/fa/HAPPYCAT_I_CAN_HAS_CHEEZBURGER.JPG">LOLCats caption style</a> will lose points immediately. ;)</p>
<p><em>One last thing: This contest isn&rsquo;t affiliated with my employer or ThinkGeek. I&rsquo;m doing this on my own. However, I&rsquo;m a big fan of both my employer and ThinkGeek, but that&rsquo;s irrelevant right now.</em></p>
<hr>
<p><strong>UPDATE:</strong> The folks at ThinkGeek decided to not only pay for one of the gift certificates, but they&rsquo;re going to <em>double</em> it. There&rsquo;s now a $50 certificate for the best entry and three more $25 certificates for second, third and fourth best entries. Thanks again to ThinkGeek for offering this up!<!-- raw HTML omitted --></p>
<hr>
<p><strong>UPDATE:</strong> <a href="/2011/08/22/contest-winners-from-the-inspire-a-sysadmin-contest/">The winners have been announced!</a><!-- raw HTML omitted --></p>
]]></content></item><item><title>Highlight IP addresses with a double click in Firefox</title><link>https://major.io/2011/08/16/highlight-ip-addresses-with-a-double-click-in-firefox/</link><pubDate>Tue, 16 Aug 2011 12:46:07 +0000</pubDate><guid>https://major.io/2011/08/16/highlight-ip-addresses-with-a-double-click-in-firefox/</guid><description>My daily work involves working with a large number of servers and one of my frustrations with Firefox is that it&amp;rsquo;s not possible to select an entire IP address with a double click with the default settings. Although it works right out of the box with Safari, you have to make a configuration adjustment in Firefox to get the same behavior.
To change the setting in Firefox, open up a new Firefox tab and go to about:config in the browser.</description><content type="html"><![CDATA[<p>My daily work involves working with a large number of servers and one of my frustrations with Firefox is that it&rsquo;s not possible to select an entire IP address with a double click with the default settings. Although it works right out of the box with Safari, you have to make a configuration adjustment in Firefox to get the same behavior.</p>
<p>To change the setting in Firefox, open up a new Firefox tab and go to <code>about:config</code> in the browser. Paste <code>word_select.stop</code> in the search bar that appears below your tab bar and double click the <code>layout.word_select.stop_at_punctuation</code> line. It should become bold and the value on the end will flip from true to false.</p>
<p>Go back to another tab and <a href="http://icanhazip.com/">open a web page which displays an IP address</a>. Double click on any portion of the IP address and Firefox should highlight the entire address.</p>
]]></content></item><item><title>Xen Summit 2011: My Takeways</title><link>https://major.io/2011/08/08/xen-summit-2011-my-takeways/</link><pubDate>Mon, 08 Aug 2011 12:58:54 +0000</pubDate><guid>https://major.io/2011/08/08/xen-summit-2011-my-takeways/</guid><description>Quite a few people who couldn&amp;rsquo;t make it to Xen Summit 2011 this year asked me to write a post summarizing my takeaways from the event. I&amp;rsquo;m not generally one to back down from peer pressure, so read on if you&amp;rsquo;re interested about the discussions at this year&amp;rsquo;s Summit.
The feeling I had at last year&amp;rsquo;s summit is that Xen was on the verge of losing traction in the market. Very few distributions still had Xen support going forward and much of the discussion was around the lack of dom0 support in upstream Linux kernels.</description><content type="html"><![CDATA[<p><img src="/wp-content/uploads/2011/08/xensummit_na11_small.png" alt="1"></p>
<p>Quite a few people who couldn&rsquo;t make it to <a href="http://xen.org/community/xensummit.html">Xen Summit 2011</a> this year asked me to write a post summarizing my takeaways from the event. I&rsquo;m not generally one to back down from peer pressure, so read on if you&rsquo;re interested about the discussions at this year&rsquo;s Summit.</p>
<p>The feeling I had at last year&rsquo;s summit is that Xen was on the verge of losing traction in the market. Very few distributions still had Xen support going forward and much of the discussion was around the lack of dom0 support in upstream Linux kernels. Distribution vendors were hesitant to drag patches forward into modern kernels and this made it much more difficult to get Xen working for many people.</p>
<p><img src="/wp-content/uploads/2011/08/Photo-Aug-01-5-54-05-PM.jpeg" alt="3"></p>
<p>This year was quite different. The number of attendees was up, the <a href="http://www.citrix.com/tv/#videos/4386">venue was much better</a>, and there was an obvious buzz of energy in the room. As many of the presenters noted, this excitement stemmed from the <a href="http://blog.xen.org/index.php/2011/06/14/linux-3-0-how-did-we-get-initial-domain-dom0-support-there/">upstream dom0 support in Linux 3.0</a>. This inclusion is a huge win and it helps to drive Xen forward since the developers don&rsquo;t have to worry about dragging patches forward. They can focus on improving performance, adding features, and tightening security.</p>
<p>Many of the discussions this year focused on security and performance. Ian Pratt discussed Xen&rsquo;s ability to view memory pages of virtual machines via an API to detect malware running inside the instance. Memory pages could be identified and marked as not executable or applications could be triggered when a VM attempts to touch a particular memory page. Also, the whole VM could be frozen if needed.</p>
<p>There&rsquo;s also a big push to bring code out of the dom0 and push it into utility VM&rsquo;s. Driver domains could manage the network or I/O infrastructure and this would further reduce the amount of privileged code actively running in dom0. There is already very little code required for the Xen hypervisor itself (much much less than the Linux kernel - I&rsquo;m looking at you, <a href="http://en.wikipedia.org/wiki/Kernel-based_Virtual_Machine">KVM</a>) and this reduces the attack surface for potential compromises of the hypervisor. Some projects even aim to restart driver domains multiple times per minute to ensure that any malicious code injected into those virtual machines can&rsquo;t exist for long periods.</p>
<p>Pradeep Vincent from <a href="http://aws.amazon.com/">Amazon</a> talked about how Amazon uses Xen and the pain points they have with its current architecture. Much of his discussion was around scaling problems (and we see many of the same issues at <a href="http://www.rackspace.com">Rackspace</a>). Higher performance could easily be gained by multi-threaded operations in dom0 when attaching block devices and creating virtual network interfaces. He also saw some areas for performance gains in the pvops I/O code.</p>
<p>Quite a few of the talks centered on the ARM architecture and what Xen is able to do on those systems after <a href="http://www.xen.org/products/xen_arm.html">Samsung published their port in 2008</a>. HVM is on the way for ARM and it might even show up in Xen 4.2. Some demos of Xen on mobile phones from Samsung were amazing. They showed how an attacker could compromise the web browser on the phone with a keylogger, but that application was running in a VM. Once the user switched back to the phone&rsquo;s main menu, the keylogger couldn&rsquo;t access the keystrokes any longer. After that, a simple close of the browser killed the VM and destroyed the malicious code.</p>
<p>Xen 4.2 should be available in early 2012 and the feature list is staggering. Improvements to libxenlight, pvops performance (even in HVM), and guest memory sharing should be available with the new release. Nested virtualization (run a hypervisor inside a hypervisor) is also coming in Xen 4.2 and I&rsquo;m sure Xzibit will be a huge fan. This should streamline hypervisor testing, allow for embedded hypervisor options and extend the capabilities of client hypervisors. Remus should be available in 4.2 as well, but it might be marked as experimental. OVMF will be added as a BIOS option for UEFI (along with the standard SeaBIOS) and this should allow for Mac OS X guests. UEFI allows Windows to boot faster since it switches to PV mode sooner and it allows for simpler platform certification for software vendors.</p>
<p><img src="/wp-content/uploads/2011/08/OpenStackLogo_270x279.jpg" alt="10"></p>
<p>Mike McClurg&rsquo;s presentation on <a href="http://www.xen.org/products/cloudxen.html">XCP</a> was pretty important to me since Rackspace is a big consumer of <a href="http://www.citrix.com/English/ps2/products/product.asp?contentID=683148">XenServer</a>. If you&rsquo;re not familiar with XCP, it&rsquo;s basically open-source XenServer which runs on bleeding edge (and sometimes unstable) components. XCP 1.5 and XenServer 6 should be available in November with Xen 4.1 and Linux 2.6.32. GPU passthrough, up to 1TB RAM, and disaster recovery will be available. Another goal for the XCP team is to work closely with OpenStack via Project Olympus. Mike&rsquo;s vision is to have XCP become the configuration of choice for open source clouds. <a href="http://blog.xen.org/index.php/2011/07/22/project-kronos/">Project Kronos</a> was also extremely interesting. It&rsquo;s essentially XCP&rsquo;s XenAPI stack running on Debian and Ubuntu. You&rsquo;d be able to install either OS on a physical server and run XCP&rsquo;s services on it for a fully OSS hypervisor.</p>
<p>Konrad Wilk gave an update on Linux pvops and it appears there is a shift to get Xen working well on a desktop. This includes 3D graphics support, S3/hibernate capabilities and various bug fixes. There&rsquo;s also a push to get PV functionality into HVM and get HVM functionality into PV. Driver/device domains were discussed again in Patrick Kolp&rsquo;s talk and he had plenty of graphs showing performance changes when regularly restarting device domains. The performance dips were almost negligible with 10 second restarts and the security gains were significant.</p>
<p>There were several other great presentations on other topics like <a href="http://www.gluster.org/">GlusterFS</a>, <a href="http://nova.openstack.org/">OpenStack Nova</a>, and <a href="http://cgit.freedesktop.org/~ewalsh/linpicker/">Linpicker</a> (from the NSA!). If these types of things interests you, keep your eyes peeled for Xen Summit 2012 next year. The <a href="http://weatherspark.com/#!dashboard;q=santa+clara,+ca">weather in the bay area</a> is well worth the trip. ;)</p>
]]></content></item><item><title>Xen 4.1 on Fedora 15 with Linux 3.0</title><link>https://major.io/2011/08/05/xen-4-1-on-fedora-15-with-linux-3-0/</link><pubDate>Sat, 06 Aug 2011 04:34:06 +0000</pubDate><guid>https://major.io/2011/08/05/xen-4-1-on-fedora-15-with-linux-3-0/</guid><description>If you haven&amp;rsquo;t noticed already, full Xen dom0 support was added in the Linux 3.0 kernel. This means there&amp;rsquo;s no longer a need to drag patches forward from old kernels and work from special branches and git repositories when building a kernel for dom0.
Something else you might not have noticed is that the Fedora kernel team has quietly slipped Linux 3.0 into Fedora 15&amp;rsquo;s update channels in disguise. Click that link, scroll down, and you&amp;rsquo;ll see “Rebase to 3.</description><content type="html"><![CDATA[<p>If you haven&rsquo;t noticed already, <a href="http://blog.xen.org/index.php/2011/06/02/xen-celebrates-full-dom0-and-domu-support-in-linux-3-0/">full Xen dom0 support</a> was added in the <a href="http://kernelnewbies.org/Linux_3.0">Linux 3.0 kernel</a>. This means there&rsquo;s no longer a need to drag patches forward from old kernels and work from special branches and git repositories when building a kernel for <a href="http://wiki.xensource.com/xenwiki/Dom0">dom0</a>.</p>
<p>Something else you might not have noticed is that the Fedora kernel team has <a href="https://admin.fedoraproject.org/updates/kernel-2.6.40-4.fc15">quietly slipped Linux 3.0</a> into Fedora 15&rsquo;s update channels in disguise. Click that link, scroll down, and you&rsquo;ll see <em>“Rebase to 3.0. Version reports as 2.6.40 for compatibility with older userspace.”</em> Although I&rsquo;m not a fan of calling something what it isn&rsquo;t (2.6.40 doesn&rsquo;t exist on kernel.org), I can understand some of the reasoning behind the choice.</p>
<p>This change makes the Xen installation on Fedora 15 pretty trivial. To get started, update your kernel to the latest if you&rsquo;re not already on Fedora&rsquo;s 2.6.40 kernels:</p>
<!-- raw HTML omitted -->
<p>We need three more packages (quite a few dependencies will roll in with them):</p>
<!-- raw HTML omitted -->
<p>The xen package reels in the hypervisor itself along with libraries and command line tools (like xl and xm). Libvirt gives us easy access to VM management with the <code>virsh</code> command and python-virtinst gives us the handy <code>virt-install</code> command to make OS installations easy.</p>
<p>Once those packages are installed, we need to make some adjustments in your grub configuration. Open <code>/boot/grub/menu.lst</code> in your text editor of choice and add something like this at the bottom:</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>Ensure that the <code>root (hd0,1)</code> is applicable to your system (adjust it if it isn&rsquo;t). Also, check the kernel version to ensure it matches your installed kernel and adjust the <code>root=</code> portion to match your root volume. Flip the <code>default</code> line to a value which will boot your new grub entry and ensure the timeout is set to a reasonable number if you need to temporarily switch back to your original grub entry at boot time. (Hey, we all make mistakes.)</p>
<p>I take one extra precaution and change the <code>UPDATEDEFAULT=yes</code> line to <code>no</code> in <code>/etc/sysconfig/kernel</code>. This ensures that future kernel updates don&rsquo;t trample the entry you&rsquo;ve just made. Keep in mind that you&rsquo;ll need to manually update your grub configuration when you do kernel upgrades later.</p>
<p>Cross your fingers and reboot. If your system doesn&rsquo;t reboot properly, reboot it again and choose your old kernel from the grub menu. Double-check your configuration for fat-fingering and give it another try. If your system boots and pings but you have no output via a monitor, don&rsquo;t fret. There&rsquo;s a <a href="http://marc.info/?l=linux-kernel&amp;m=131169794026271&amp;w=2">patch</a> for the problem which <a href="http://marc.info/?l=linux-kernel&amp;m=131169794026271&amp;w=2">should appear soon</a> in Linux 3.0. The impatient can snag a kernel source RPM, add the patch file, and <a href="http://fedoraproject.org/wiki/Building_a_custom_kernel">build a local kernel</a> (or you can <a href="http://majorhayden.com/RPMS/kernel-3.0.0-1.mhayden.fc16/">download my local build</a> from when I did it).</p>
<p>Log in and verify that you booted into the dom0:</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>Once you&rsquo;re done with that, make sure libvirtd is running:</p>
<!-- raw HTML omitted -->
<p>Try installing a VM:</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>You should have a VM installation underway pretty quickly and it will be visible via port 5905 on the local host. Enjoy the power and freedom of your brand new <a href="http://en.wikipedia.org/wiki/Hypervisor#Classification">type 1 hypervisor</a>.</p>
]]></content></item><item><title>Success with stress</title><link>https://major.io/2011/07/21/success-with-stress/</link><pubDate>Fri, 22 Jul 2011 01:50:34 +0000</pubDate><guid>https://major.io/2011/07/21/success-with-stress/</guid><description>This is a copy of a post I wrote for the Rackspace Talent blog. Much of it probably applies to the job of a system administrator, so I figured it would be a good idea to post it here as well. Let me know what you think!
Although Rackspace has one of the best work environments of any company I’ve worked for, there are plenty of opportunities to become stressed.</description><content type="html"><![CDATA[<p><em>This is a <a href="http://rackertalent.com/rackers/success-with-stress/">copy of a post</a> I wrote for the <a href="http://rackertalent.com/rackers/">Rackspace Talent</a> blog. Much of it probably applies to the job of a system administrator, so I figured it would be a good idea to post it here as well. Let me know what you think!</em></p>
<hr>
<p><a href="/wp-content/uploads/2011/07/BustTheKeyboard.jpg"><!-- raw HTML omitted --></a>Although Rackspace has one of the best work environments of any company I’ve worked for, there are plenty of opportunities to become stressed.<!-- raw HTML omitted --></p>
<p>Stress can come from a variety of sources. Some of the obvious ones involve dealing with outages or tight deadlines, but there are some that aren’t so obvious, such as maintaining the customers’ trust and interpersonal issues.</p>
<p>There’s one thing you must remember: stress doesn’t have to rule your life. I’ve learned (and sometimes stumbled upon) some good techniques to prevent many of the negative effects of stressful situations at work and they’re definitely worth a try.</p>
<p><strong>Know what you’re up against</strong></p>
<p>It’s hard to battle a source of stress if you don’t know why it’s bothering you. Take the problem you’re facing and break it down into pieces. There are going to be some things you can and can’t change. Put the things you can’t change aside and focus on the things you’re able to change. As you tackle the list of things you can change, you might find ways to work around the things you can’t.</p>
<p><strong>Interpersonal issues are easy</strong></p>
<p>Stress that comes from dealing with coworkers may seem insurmountable at times. However, this type of stress is easily fixed and it normally stems from insufficient communication or conflicting goals. There’s an informal policy I’ve had on most of my teams called “Take it to the Racker” and it’s been quite successful. The basic idea is that if you have problems with another Racker, whether it’s something personal or work-related, take the grievance to them directly (in private, of course) and find common ground.</p>
<p>More often than not, this process leads to a good work relationship. It also improves communication drastically in the short term and it generally lasts if the people involved keep up the communication over time. I’ve seen Rackers who are so upset that they refuse to sit next to each other and after this process, they’re eating lunch together and working on the same projects.</p>
<p><strong>Don’t fight your battles alone</strong></p>
<p>Your best resources for fighting stress are all around you. Lean on your manager or your coworkers for help. Remember what your mother always told you: a trouble shared is a trouble halved. Your coworker might have a solution to a particular problem which frees up an hour for you each day and allows you to work on other projects. Your manager might not know that a particular task doesn’t fit your strengths and they might be able to provide you with another project that plays to your strengths.</p>
<p>Is it possible to reduce your stress level to zero at work? I don’t think so. However, you should always have a goal to reduce it when it makes sense.</p>
<p>As always, I’m interested to hear your comments. Which stress-reduction strategies work best for you? What is the source of most of your stress?</p>
]]></content></item><item><title>Keep all old kernels when upgrading via yum</title><link>https://major.io/2011/06/16/keep-all-old-kernels-when-upgrading-via-yum/</link><pubDate>Thu, 16 Jun 2011 12:50:46 +0000</pubDate><guid>https://major.io/2011/06/16/keep-all-old-kernels-when-upgrading-via-yum/</guid><description>Some might call me paranoid, but I get nervous when my package manager automatically removes a kernel. I logged into my Fedora 15 VM this morning and found this:
================================================================================ Package Arch Version Repository Size ================================================================================ Installing: kernel x86_64 2.6.35.13-92.fc14 updates 22 M Removing: kernel x86_64 2.6.35.11-83.fc14 @updates 104 M Transaction Summary ================================================================================ Install 1 Package(s) Remove 1 Package(s) Fedora 15&amp;rsquo;s default behavior is to keep three kernels: the latest one and the two previous versions.</description><content type="html"><![CDATA[<p>Some might call me paranoid, but I get nervous when my package manager automatically removes a kernel. I logged into my Fedora 15 VM this morning and found this:</p>
<pre><code>================================================================================
 Package        Arch           Version                   Repository        Size
================================================================================
Installing:
 kernel         x86_64         2.6.35.13-92.fc14         updates           22 M
Removing:
 kernel         x86_64         2.6.35.11-83.fc14         @updates         104 M

Transaction Summary
================================================================================
Install       1 Package(s)
Remove        1 Package(s)
</code></pre><p>Fedora 15&rsquo;s default behavior is to keep three kernels: the latest one and the two previous versions. However, this behavior may be counter-productive if you compile your own modules, or if you have compatibility issues with subsequent kernel versions.</p>
<p>You can change how yum handles kernel packages with some simple changes to your <code>/etc/yum.conf</code>. The <code>installonly_limit</code> option controls how many old packages are kept:</p>
<blockquote>
<p><strong>installonly_limit</strong> Number of packages listed in installonlypkgs to keep installed at the same time. Setting to 0 disables this feature. Default is &lsquo;0&rsquo;.</p>
</blockquote>
<p>I disabled the functionality altogether by setting <code>installonly_limit</code> to 0:</p>
<pre><code>#installonly_limit=3
installonly_limit=0
</code></pre><p>It&rsquo;s important to keep in mind that you will need to purge these packages from your system yourself now. Kernel packages can occupy a fair amount of disk space, so make a note to go back and clean them up when you no longer need them.</p>
]]></content></item><item><title>Measure traffic flows with Mikrotik’s RouterOS and ntop on Fedora 15</title><link>https://major.io/2011/06/05/measure-traffic-flows-with-mikrotiks-routeros-and-ntop-on-fedora-15/</link><pubDate>Sun, 05 Jun 2011 14:58:26 +0000</pubDate><guid>https://major.io/2011/06/05/measure-traffic-flows-with-mikrotiks-routeros-and-ntop-on-fedora-15/</guid><description>It&amp;rsquo;s no secret that I&amp;rsquo;m a big fan of the RouterBoard network devices paired with Mikrotik&amp;rsquo;s RouterOS. I discovered today that these devices offer Cisco NetFlow-compatible statistics gathering which can be directed to a Linux box running ntop. Mikrotik calls it “traffic flow” and it&amp;rsquo;s much more efficient than setting up a mirrored or spanned port and then using ntop to dump traffic on that interface.
These instructions are for Fedora 15, but they should be pretty similar on most other Linux distributions.</description><content type="html"><![CDATA[<p>It&rsquo;s no secret that I&rsquo;m a big fan of the <a href="http://www.routerboard.com/">RouterBoard</a> network devices paired with <a href="http://www.mikrotik.com/software.html">Mikrotik&rsquo;s RouterOS</a>. I discovered today that these devices offer Cisco NetFlow-compatible statistics gathering which can be directed to a Linux box running <a href="http://www.ntop.org/">ntop</a>. Mikrotik calls it “traffic flow” and it&rsquo;s much more efficient than setting up a mirrored or spanned port and then using ntop to dump traffic on that interface.</p>
<p>These instructions are for Fedora 15, but they should be pretty similar on most other Linux distributions. Install ntop first:</p>
<!-- raw HTML omitted -->
<p>Adjust <code>/etc/ntop.conf</code> so that ntop listens on something other than localhost:</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>I had to comment out the <code>sched_yield()</code> option to get ntop to start:</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>Set an admin password for ntop:</p>
<!-- raw HTML omitted -->
<p>Once you set the password, you may need to press CTRL-C to get back to a prompt in some ntop versions.</p>
<p>Start ntop:</p>
<!-- raw HTML omitted -->
<p>Open a web browser and open <a href="http://example.com">http://example.com</a>:3000 to access the ntop interface. Roll your mouse over the <strong>Plugins</strong> menu, then <strong>NetFlow</strong>, and then click <strong>Activate</strong>. Roll your mouse over the <strong>Plugins</strong> menu again, then <strong>NetFlow</strong>, and then click <strong>Configure</strong>. Click <strong>Add NetFlow Device</strong> and fill in the following:</p>
<ul>
<li>Type “Mikrotik” in the <strong>NetFlow Device</strong> section and click <strong>Set Interface Name</strong>.</li>
<li>Type 2055 in the <strong>Local Collector UDP Port</strong> section and click <strong>Set Port</strong>.</li>
<li>Type in your router&rsquo;s IP/netmask in the <strong>Virtual NetFlow Interface Network Address</strong> section and click <strong>Set Interface Address</strong>.</li>
</ul>
<p>Enabling traffic flow on the Mikrotik can be done with just two configuration lines:</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>Wait about a minute and then try reviewing some of the data in the ntop interface. Depending on the amount of traffic on your network, you might see data in as little as 10-15 seconds.</p>
]]></content></item><item><title>Handy networking cheat sheets from Packet Life</title><link>https://major.io/2011/05/25/handy-networking-cheat-sheets-from-packet-life/</link><pubDate>Wed, 25 May 2011 13:38:45 +0000</pubDate><guid>https://major.io/2011/05/25/handy-networking-cheat-sheets-from-packet-life/</guid><description>If you find yourself forgetting bits and pieces about network topics, Packet Life&amp;rsquo;s cheat sheets should be a handy resource for you. Lots of topics are included, such as VOIP, NAT, MPLS, BGP, and IOS basics. They&amp;rsquo;re all in PDF form and free to download.
Cheat Sheets - Packet Life
Thanks to @speude for mentioning the site on Twitter.</description><content type="html"><![CDATA[<p>If you find yourself forgetting bits and pieces about network topics, Packet Life&rsquo;s cheat sheets should be a handy resource for you. Lots of topics are included, such as VOIP, NAT, MPLS, BGP, and IOS basics. They&rsquo;re all in PDF form and free to download.</p>
<p><a href="http://packetlife.net/library/cheat-sheets/">Cheat Sheets - Packet Life</a></p>
<p><em>Thanks to <a href="http://twitter.com/speude/status/73333508344524800">@speude</a> for mentioning the site on Twitter.</em></p>
]]></content></item><item><title>Do your homework before a technical interview</title><link>https://major.io/2011/05/02/do-your-homework-before-a-technical-interview/</link><pubDate>Tue, 03 May 2011 02:05:05 +0000</pubDate><guid>https://major.io/2011/05/02/do-your-homework-before-a-technical-interview/</guid><description>If you work for a growing company like I do, it&amp;rsquo;s inevitable that you&amp;rsquo;ll have to do your fair share of interviewing. I love it when I leave an interview with a good feeling about the candidate. That &amp;ldquo;wow, they really nailed it&amp;rdquo; feeling is always great to have when you need to fill a critical role. Most often, the successful candidates are the ones who do their homework before they ever walk in our office doors.</description><content type="html"><![CDATA[<p>If you work for a growing company like I do, it&rsquo;s inevitable that you&rsquo;ll have to do your fair share of interviewing. I love it when I leave an interview with a good feeling about the candidate. That &ldquo;wow, they really nailed it&rdquo; feeling is always great to have when you need to fill a critical role. Most often, the successful candidates are the ones who do their homework before they ever walk in our office doors.</p>
<p>What do I mean by &ldquo;do your homework?&rdquo; Here are some bullet points to get you on your way:</p>
<p><strong>Know what the company does.</strong></p>
<p>This one is critical and it should be easy. However, make sure to do thorough research first. For example, if you interviewed at a company like Apple, becoming familiar with their hardware lineup should be a no-brainer. That&rsquo;s their bread and butter. On the other hand, remember that Apple isn&rsquo;t solely a hardware company; they write lots of software, provide online productivity services, and they distribute music, movies, and other entertainment media.</p>
<p>While you&rsquo;re doing this research, try to discover what makes the company unique. Sure, <a href="http://www.apple.com/">Apple</a> sells laptops and desktops (just like a lot of other companies), but what makes their particular products unique? Is there something unique about the way they provide their services? Have they cornered a certain market segment by providing a combination of products and services to that group of consumers? Answering these simple questions may help you tip the scales in the interview process.</p>
<p><strong>Try one or more of the company&rsquo;s products.</strong></p>
<p>The feasibility of trying a company&rsquo;s product before an interview could be debatable. For example, if you wanted to interview at <a href="http://cray.com/">Cray</a>, you probably don&rsquo;t need to drop $2M USD on your own <a href="http://www.cray.com/Products/XE/Systems.aspx">XE6</a> before walking in the door. For companies where the barrier to entry for purchasing a product is much lower, such as cloud computing companies, there&rsquo;s no excuse to not try things out first. Amazon has a <a href="http://aws.amazon.com/free/">free tier</a> and a Rackspace Cloud Server could cost you <a href="http://www.rackspace.com/cloud/cloud_hosting_products/servers/pricing/">as little as $2.50 per week</a>.</p>
<p>It&rsquo;s concerning when I talk to an applicant about a job working with Rackspace&rsquo;s Cloud Servers and they haven&rsquo;t tried out any cloud products from any provider. How can I take a candidate&rsquo;s interest seriously when they haven&rsquo;t shown interest in any portion of my group&rsquo;s market segment?</p>
<p><strong>Know what the company&rsquo;s competitors do.</strong></p>
<p>It&rsquo;s often more impressive to an interviewer to know what a company&rsquo;s competitors are doing and how it compares to what that company is doing in the market. For example, if you can walk into an interview and say &ldquo;I like the way your company makes these widgets, but Company X is able to make them more lightweight, and I value that more than the added customer service your company offers.&rdquo; This shows the interviewer that you&rsquo;re familiar with various products in the segment and you&rsquo;ve used them enough to understand what makes them different.</p>
<p>Some of you might be thinking: &ldquo;Why would I say something like that to the interviewer? They&rsquo;ll think I&rsquo;m being too negative about their product.&rdquo; That&rsquo;s always possible, but you can guard against it by wording everything carefully. Make sure you have a solid reason for the way you feel that is based on something substantial (usability, price, features, etc). I&rsquo;ve had candidates talk for five to ten minutes about why one of our product is inferior to one of our competitors' products and I was very impressed.</p>
<p>One quick gotcha: your interviewer might turn your comments back on you and ask you how you would improve one of the inferior products (I do this regularly). Make sure that you&rsquo;re prepared for that question and consider offering up a suggestion before the question is presented to you.</p>
<p><strong>Can&rsquo;t get the information you need? Ask!</strong></p>
<p>When you reach the end of the interview and the interviewer asks if you have questions, be sure to ask any questions about topics you had trouble researching. Going back to the Cray example, compare what you know about an XE6 to servers you&rsquo;ve used before. You could mention a problem you had with the density of your previous configurations and ask how they overcame that hurdle at Cray. If it&rsquo;s a proprietary trade secret, you might not get an answer, but they&rsquo;ll know that you did some serious research ahead of time. If they can share the answer, they might still be impressed, and you might end up learning something you didn&rsquo;t know prior to the interview.</p>
<p><strong>Conclusion</strong></p>
<p>In summary, doing your homework and learning about the company shows the interviewers that you not only have what it takes to do the work, but that the work interests you as well. I&rsquo;ve interviewed folks in the past who lacked on technical ability but had plenty of desire and drive. More often than not, those people are now Rackers.</p>
]]></content></item><item><title>MySQLTuner mentioned at the O’Reilly MySQL Conference</title><link>https://major.io/2011/04/15/mysqltuner-mentioned-at-the-oreilly-mysql-conference/</link><pubDate>Sat, 16 Apr 2011 01:56:31 +0000</pubDate><guid>https://major.io/2011/04/15/mysqltuner-mentioned-at-the-oreilly-mysql-conference/</guid><description>If you push play, the video should scoot out to about the 14m40s mark where MySQLTuner appears on one of the slides. Thanks to Trent Hornibrook for letting me know!</description><content type="html"><![CDATA[<!-- raw HTML omitted -->
<p>If you push play, the video should scoot out to about the 14m40s mark where MySQLTuner appears on one of the slides. Thanks to <a href="http://twitter.com/#!/trenthornibrook">Trent Hornibrook</a> for letting me know!</p>
]]></content></item><item><title>Lifehacker effect on icanhazip.com</title><link>https://major.io/2011/03/30/lifehacker-effect-on-icanhazip-com/</link><pubDate>Wed, 30 Mar 2011 13:28:55 +0000</pubDate><guid>https://major.io/2011/03/30/lifehacker-effect-on-icanhazip-com/</guid><description>I was surprised to see coverage about icanhazip.com on Lifehacker last Sunday and I was curious to know what effect the story would have on my site&amp;rsquo;s overall traffic. Dave Drager wrote a great summary of what the site offers and how people can use it in their daily work. It&amp;rsquo;s pretty obvious that icanhazip.com really only serves a niche group of internet users, but even I was surprised at the level of interest.</description><content type="html"><![CDATA[<p>I was surprised to see coverage about <a href="http://rackerhacker.com/icanhazip-com-faq/">icanhazip.com</a> on <a href="http://www.lifehacker.com.au/2011/03/find-your-public-ip-anywhere-with-icanhazip-com/">Lifehacker</a> last Sunday and I was curious to know what effect the story would have on my site&rsquo;s overall traffic. <a href="http://www.lifehacker.com.au/author/dave-drager/">Dave Drager</a> wrote a great summary of what the site offers and how people can use it in their daily work. It&rsquo;s pretty obvious that icanhazip.com really only serves a niche group of internet users, but even I was surprised at the level of interest.</p>
<p><img src="/wp-content/uploads/2011/03/icanhazip_lifehacker_traffic.jpg" alt="lifehacker_graph"> icanhazip.com traffic data - March 2011</p>
<p>The graph on the right shows some recent traffic data from March 2011. The Lifehacker story was published around 7AM on March 27th in Australia, so I first started seeing a spike on the 26th (my server&rsquo;s time zone is UTC-5). The yellow bar is a count of the unique visits while the other bars count page views, hits and total bandwidth.</p>
<p>The count of unique visitors certainly increased (by about 10-11x), but the overall hits didn&rsquo;t increase by much. I&rsquo;d imagine that most visitors accessed the site, noticed that it displayed their public IP, and then they went on their way. As I&rsquo;ve said before, this site is easy to re-create and will really only serve a niche segment of internet users.</p>
<p>On most days, I&rsquo;ll receive a very high number of hits from a relatively small number of unique IP addresses. There are quite a few people who check their public-facing IP address every second, but it seems like the majority stick to a more reasonable interval of 5-30 minutes. I&rsquo;ve yet to find the value in checking my public IP address once per second, but there are obviously some folks out there who find it valuable (or they aren&rsquo;t good at implementing sleeps in their scripts).</p>
<p>Here&rsquo;s a bit of trivia about the site for those who are interested:</p>
<ul>
<li>Almost 40% of the traffic to the site is from Eastern European and Asian countries</li>
<li>The average user on the site generates about 45 hits per day</li>
<li>Linux users make up 91% of the traffic on the site (based on user agent strings)</li>
<li>Over 88% of the hits to the site are requests made with curl or wget</li>
<li>Most traffic is received between 4-5PM CDT</li>
<li>Almost 98% of the visitors who reach the site do so via a direct link without a referrer</li>
</ul>
]]></content></item><item><title>How to survive as a technical manager</title><link>https://major.io/2011/03/29/how-to-survive-as-a-technical-manager/</link><pubDate>Tue, 29 Mar 2011 14:25:59 +0000</pubDate><guid>https://major.io/2011/03/29/how-to-survive-as-a-technical-manager/</guid><description>Anyone who says management is easy obviously hasn&amp;rsquo;t done it for very long or they&amp;rsquo;re not doing their job very well. Coordinating the activities and personal development of each person on the team is always a challenge and it introduces an unbelievable number of variables into an already difficult job. However, watching members of the team grow and succeed in their work is tremendously rewarding.
Taking on the job of a technical manager presents its own unique challenges.</description><content type="html"><![CDATA[<p>Anyone who says management is easy obviously hasn&rsquo;t done it for very long or they&rsquo;re not doing their job very well. Coordinating the activities and personal development of each person on the team is always a challenge and it introduces an unbelievable number of variables into an already difficult job. However, watching members of the team grow and succeed in their work is tremendously rewarding.</p>
<p>Taking on the job of a technical manager presents its own unique challenges. It&rsquo;s easy for a technical manager to lose focus and get down in the weeds of daily work. It&rsquo;s also very difficult to let go of the reins and resign to the fact that the direct involvement in technical work will have to be reduced.</p>
<p>These problems resonate with me as I&rsquo;ve recently taken on another technical management role at Rackspace. My previous experience involved managing a team of technicians at various skill levels who were working on customer environments made up of dedicated servers and network equipment. The current position has quite a few differences. I&rsquo;m now managing a small group of highly technical and extremely dedicated Linux engineers and we&rsquo;re responsible for maintaining the systems and networks which run the Cloud Servers product.</p>
<p>One of my goals of this blog is to help others learn things much more easily than I have. Here are some things I&rsquo;ve had to learn the hard way while working as a technical manager:</p>
<p><strong>Get out of the mindset of an individual contributor</strong></p>
<p>When you&rsquo;re a system administrator on a team (or by yourself), you&rsquo;re often judged on your personal job performance. Team interaction is important for some companies (especially at Rackspace), but not for others. Breaking the mindset of being an individual contributor was extremely difficult for me to do.</p>
<p>Managers are judged on the success of the team as a whole. Encouraging your team members to succeed and playing an active role in their personal and professional development is key. Each time you find yourself buried in the weeds of a problem rather than facilitating your team&rsquo;s work on the problem is when your performance as a manager will drop. If you do it more often, you may find that your team members aren&rsquo;t getting the support they need.</p>
<p><strong>Don&rsquo;t be afraid of your team becoming smarter than you</strong></p>
<p>One of the biggest things I&rsquo;ve heard from my team is: &ldquo;Aren&rsquo;t you worried about losing your technical skills when you&rsquo;re a manager?&rdquo; My answer: &ldquo;Of course.&rdquo; Anyone who has technical abilities will always be afraid of watching those abilities wane over time. However, as your team becomes stronger, you should be able to continue learning not through your own work, but through theirs. When your team members see that you&rsquo;re still interested in learning and you&rsquo;re now able to learn from them, they&rsquo;ll become more energized about their own work.</p>
<p>If you find yourself thinking negatively about a potential job candidate because they&rsquo;re smarter than you, step back and think for a moment. Put your own ego aside and consider what&rsquo;s best for you, your team, and your company. Your goal is to build a strong and successful team, not to pad your own ego. If your managers are judging you (as a technical manager) on your technical ability, then you need to solve that problem first.</p>
<p><strong>Inspire instead of direct</strong></p>
<p>Every manager faces the challenge of working with team members who disagree with a particular company policy or with the direction of their particular infrastructure. Keep in mind that your team members are probably not intending to be insubordinate and they might have something useful to contribute.</p>
<p>When you find yourself locking horns with your team members, inspire a discussion about the problem. Break out the disagreement onto a whiteboard and let the team make suggestions for improvements. Even if the entire discussion leads back to the fact that the original problem is inevitable, fostering that feedback loop is critical. You&rsquo;ll learn more about your team while they find ways to express their opinions and feel empowered.</p>
<p>The really tough part is when your team comes up with an alternative plan and you find yourself presenting to your leadership team. Always remember to take it seriously and know that you may need to refine the plan many times over before you find something acceptable for your team and the business.</p>
<p><strong>De-stress by staying on task</strong></p>
<p>If you&rsquo;re anything like me, you need some way to keep tabs on action items coming from meetings, e-mails, phone calls, and walk-ups. I&rsquo;ve heard great things about applications like <a href="http://www.omnigroup.com/products/omnifocus/">OmniFocus</a> and <a href="http://culturedcode.com/things/">Things</a>, but I settled on <a href="http://2doapp.com/">2Do</a>. I really enjoy a strong to-do list which allows me to set priorities, due dates, and write extended notes about a particular task.</p>
<p>The best way to tackle a wall of tasks is to keep them organized into a concise list. Even if it&rsquo;s a small task, get it into your list so it&rsquo;s on your radar and you won&rsquo;t forget it. Work through the simple tasks and the high priority ones first but watch out for tasks with due dates.</p>
<p><strong>Conclusion</strong></p>
<p>All of these processes get easier over time and although your job will surely have challenges and pitfalls, the enjoyment will greatly increase. I feel privileged to lead a team of talented people who work on a complex and ever-expanding product.</p>
<p>Also, I&rsquo;d like to mention that I&rsquo;m not an expert on management! There are probably much better ways to do much of this than I&rsquo;ve outlined in this post. Be sure to share your ideas in the comments section below.</p>
]]></content></item><item><title>Strategies for detecting a compromised Linux server</title><link>https://major.io/2011/03/09/strategies-for-detecting-a-compromised-linux-server/</link><pubDate>Thu, 10 Mar 2011 02:52:16 +0000</pubDate><guid>https://major.io/2011/03/09/strategies-for-detecting-a-compromised-linux-server/</guid><description>There are few things which will rattle systems administrators more than a compromised server. It gives you the same feeling that you would have if someone broke into your house or car, except that it&amp;rsquo;s much more difficult (with a server) to determine how to clean up the compromise and found out how the attacker gained access. In addition, leaving a compromise in place for an extended period can lead to other problems:</description><content type="html"><![CDATA[<p>There are few things which will rattle systems administrators more than a compromised server. It gives you the same feeling that you would have if someone broke into your house or car, except that it&rsquo;s much more difficult (with a server) to determine how to clean up the compromise and found out how the attacker gained access. In addition, leaving a compromise in place for an extended period can lead to other problems:</p>
<ul>
<li>your server could be used to gain access other servers</li>
<li>data could be stolen from your server&rsquo;s databases or storage devices</li>
<li>an attacker could capture data from your server&rsquo;s local network</li>
<li>denial of service attacks could be launched using your server as an active participant</li>
</ul>
<p>The best ways to limit your server&rsquo;s attack surface are pretty obvious: limit network access, keep your OS packages up to date, and regularly audit any code which is accessible externally or internally. As we all know, your server can still become compromised even with all of these preventative measures in place.</p>
<p>Here are some tips which will allow you to rapidly detect a compromise on your servers:</p>
<p><strong>Abnormal network usage patterns and atypical bandwidth consumption</strong></p>
<p>Most sites will have a fairly normal traffic pattern which repeats itself daily. If your traffic graph suddenly has a plateau or spikes drastically during different parts of the day, that could signify that there is something worth reviewing. Also, if your site normally consumes about 2TB of traffic per month and you&rsquo;re at the 1.5TB mark on the fifth day of the month, you might want to examine the server more closely.</p>
<p>On the flip side, look for dips in network traffic as well. This may mean that a compromise is interfering with the operation of a particular daemon, or there may be a rogue daemon listening on a trusted port during certain periods.</p>
<p>Many compromises consist of simple scripts which scan for other servers to infect or participate in large denial of service attacks. The scans may show up as a large amount of packets, but the denial of service attacks will usually consume a large amount of bandwidth. Keeping tabs on network traffic is easily done with open source software like <a href="http://munin-monitoring.org/">munin</a>, <a href="http://www.cacti.net/">cacti</a>, or <a href="http://oss.oetiker.ch/mrtg/">MRTG</a>.</p>
<p><strong>Unusual open ports</strong></p>
<p>If you run a web server on port 80, but <code>netstat -ntlp</code> shows something listening on various ports over 1024, those processes are worth reviewing. Use commands like <code>lsof</code> to probe the system for the files and network ports held open by the processes. You can also check within <code>/proc/[pid]</code> to find the directory where the processes were originally launched.</p>
<p>Watch out for processes started within directories like <code>/dev/shm</code>, <code>/tmp</code> or any directories in which your daemons have write access. You might see that some processes were started in a user&rsquo;s home directory. If that&rsquo;s the case, it might be a good time to reset that user&rsquo;s password or clear out their ssh key. Review the output from <code>last</code> authentication logs to see if there are account logins from peculiar locations. If you know the user lives in the US, but there are logins from various other countries over a short period, you&rsquo;ve got a serious problem.</p>
<p>I&rsquo;ve used applications like <a href="http://www.chkrootkit.org/">chkrootkit</a> and <a href="http://www.rootkit.nl/projects/rootkit_hunter.html">rkhunter</a> in the past, but I still prefer a keen eye and <code>netstat</code> on most occasions.</p>
<p><strong>Command output is unusual</strong></p>
<p>I&rsquo;ve seen compromises in the past where the attacker actually took the time to replace integral applications like <code>ps</code>, <code>top</code> and <code>lsof</code> to hide the evidence of the ongoing compromise. However, a quick peek in <code>/proc</code> revealed that there was a lot more going on.</p>
<p>If you suspect a compromise like this one, you may want to use the functionality provided by <code>rpm</code> to verify the integrity of the packages currently installed. You can quickly hunt for changed files by running <code>rpm -Va | grep ^..5</code>.</p>
<p>Keeping tabs on changing files can be a challenge, but applications like <a href="http://www.tripwire.org/">tripwire</a> and good ol' <a href="http://www.logwatch.org/">logwatch</a> can save you in a pinch.</p>
<p><strong>Summary</strong></p>
<p>We can all agree that the best way to prevent a compromise is to take precautions before putting anything into production. In real life, something will always be forgotten, so detection is a must. It&rsquo;s critical to keep in mind that <em>monitoring a server means more than keeping track on uptime</em>. Keeping tabs on performance anomalies will allow you to find the compromise sooner and that keeps the damage done to a minimum.</p>
]]></content></item><item><title>Dual-primary DRBD with OCFS2</title><link>https://major.io/2011/02/13/dual-primary-drbd-with-ocfs2/</link><pubDate>Mon, 14 Feb 2011 02:12:58 +0000</pubDate><guid>https://major.io/2011/02/13/dual-primary-drbd-with-ocfs2/</guid><description>As promised in one of my previous posts about dual-primary DRBD and OCFS2, I&amp;rsquo;ve compiled a step-by-step guide for Fedora. These instructions should be somewhat close to what you would use on CentOS or Red Hat Enterprise Linux. However, CentOS and Red Hat don&amp;rsquo;t provide some of the packages needed, so you will need to use other software repositories like RPMFusion or EPEL.
In this guide, I&amp;rsquo;ll be using two Fedora 14 instances in the Rackspace Cloud with separate public and private networks.</description><content type="html"><![CDATA[<p>As promised in one of my <a href="/2010/12/02/keep-web-servers-in-sync-with-drbd-and-ocfs2/">previous posts</a> about dual-primary DRBD and OCFS2, I&rsquo;ve compiled a step-by-step guide for Fedora. These instructions should be somewhat close to what you would use on CentOS or Red Hat Enterprise Linux. However, CentOS and Red Hat don&rsquo;t provide some of the packages needed, so you will need to use other software repositories like <a href="http://rpmfusion.org/">RPMFusion</a> or <a href="http://fedoraproject.org/wiki/EPEL">EPEL</a>.</p>
<p>In this guide, I&rsquo;ll be using two Fedora 14 instances in the <a href="http://rackspacecloud.com/">Rackspace Cloud</a> with separate public and private networks. The instances are called server1 and server2 to make things easier to follow.</p>
<p><strong>NOTE: All of the instructions below should be done on both servers unless otherwise specified.</strong></p>
<ul>
<li>
<ul>
<li>*First, we need to set up DRBD with two primary nodes. I&rsquo;ll be using loop files for this setup since I don&rsquo;t have access to raw partitions.<!-- raw HTML omitted --></li>
</ul>
</li>
</ul>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>Put this <a href="/wp-content/uploads/2011/02/loop-for-drbd.txt">loop file initialization init script</a> in /etc/init.d/loop-for-drbd and finish setting it up:</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>Place this DRBD resource file in <code>/etc/drbd.d/r0.res</code>. Be sure to adjust the server names and IP addresses for your servers.</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>The <code>net</code> section is telling DRBD to do the following:</p>
<ul>
<li><em>allow-two-primaries</em> – Generally, DRBD has a primary and a secondary node. In this case, we will allow both nodes to have the filesystem mounted at the same time. <strong>Do this only with a clustered filesystem. If you do this with a non-clustered filesystem like ext2/ext3/ext4 or reiserfs, <em>you will have data corruption</em>. Seriously!</strong></li>
<li><em>after-sb-0pri discard-zero-changes</em> – DRBD detected a split-brain scenario, but none of the nodes think they&rsquo;re a primary. DRBD will take the newest modifications and apply them to the node that didn&rsquo;t have any changes.</li>
<li><em>after-sb-1pri discard-secondary</em> – DRBD detected a split-brain scenario, but one node is the primary and the other is the secondary. In this case, DRBD will decide that the secondary node is the victim and it will sync data from the primary to the secondary automatically.</li>
<li><em>after-sb-2pri disconnect</em> – DRBD detected a split-brain scenario, but it can&rsquo;t figure out which node has the right data. It tries to protect the consistency of both nodes by disconnecting the DRBD volume entirely. You&rsquo;ll have to tell DRBD which node has the valid data in order to reconnect the volume. <strong>Use extreme caution if you find yourself in this scenario.</strong></li>
</ul>
<p>If you&rsquo;d like to read about DRBD split-brain behavior in more detail, <a href="http://www.drbd.org/users-guide/s-configure-split-brain-behavior.html">review the documentation</a>.</p>
<p>I generally turn off the usage reporting functionality in DRBD within <code>/etc/drbd.d/global_common.conf</code>:</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>Now we can create the volume and start DRBD:</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>You may see some errors thrown about having two primaries but neither are up to date. That can be fixed by running the following command on the <strong>primary node only</strong>:</p>
<!-- raw HTML omitted -->
<p>If you run <code>cat /proc/drbd</code> on the secondary node, you should see the DRBD sync running:</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>Before you go any further, wait for the DRBD sync to fully finish. When it completes, it should look like this:</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>Now, <strong>on the secondary node only</strong> make it a primary node as well:</p>
<!-- raw HTML omitted -->
<p>You should see this on the secondary node if you&rsquo;ve done everything properly:</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>We&rsquo;re now ready to move on to configuring OCFS2. Only one package is needed:</p>
<!-- raw HTML omitted -->
<p>Ensure that you have your servers and their private IP addresses in <code>/etc/hosts</code> before proceeding. Create the <code>/etc/ocfs2</code> directory and place the following configuration in <code>/etc/ocfs2/cluster.conf</code> (adjust the server names and IP addresses):</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>Now it&rsquo;s time to configure OCFS2. Run <code>service o2cb configure</code> and follow the prompts. Use the defaults for all of the responses except for two questions:</p>
<ul>
<li>Answer “y” to “Load O2CB driver on boot”</li>
<li>Answer “web” to “Cluster to start on boot”</li>
</ul>
<p>Start OCFS2 and enable it at boot up:</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>Create an OCFS2 partition <strong>on the primary node only</strong>:</p>
<!-- raw HTML omitted -->
<p>Mount the volumes and configure them to automatically mount at boot time. You might be wondering why I do the mounting within <code>/etc/rc.local</code>. I chose to go that route since mounting via fstab was often unreliable for me due to the incorrect ordering of events at boot time. Using rc.local allows the mounts to work properly upon every reboot.</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>At this point, you should be all done. If you want to test OCFS2, copy a file into your /mnt/storage mount on one node and check that it appears on the other node. If you remove it, it should be gone instantly on both nodes. This is a great opportunity to test reboots of both machines to ensure that everything comes up properly at boot time.</p>
]]></content></item><item><title>FUDCon 2011: Day One</title><link>https://major.io/2011/01/30/fudcon-2011-day-one/</link><pubDate>Sun, 30 Jan 2011 07:33:38 +0000</pubDate><guid>https://major.io/2011/01/30/fudcon-2011-day-one/</guid><description>The first day of FUDCon 2011 in Tempe is coming to a close tonight and I&amp;rsquo;m completely exhausted. As promised, I&amp;rsquo;ll try to summarize the day and cover the talks which I attended.
The day started out with Jared Smith&amp;rsquo;s &amp;ldquo;State of Fedora&amp;rdquo; address. The audio has already been posted on the wiki, but the speech was very positive overall. He talked about some of the struggles that have happened in the past and how they&amp;rsquo;ll probably happen again in some form or another.</description><content type="html"><![CDATA[<p>The first day of FUDCon 2011 in Tempe is coming to a close tonight and I&rsquo;m completely exhausted. <a href="/2011/01/29/gearing-up-for-fudcon-2011/">As promised</a>, I&rsquo;ll try to summarize the day and cover the talks which I attended.</p>
<p>The day started out with <a href="http://fedoraproject.org/wiki/User:Jsmith">Jared Smith&rsquo;s</a> &ldquo;State of Fedora&rdquo; address. The audio has already been <a href="http://fedoraproject.org/w/uploads/4/44/State_of_fedora_tempe_2011.ogg">posted on the wiki</a>, but the speech was very positive overall. He talked about some of the struggles that have happened in the past and how they&rsquo;ll probably happen again in some form or another. It was pretty inspirational and you could obviously tell that people in the room were energized by it.</p>
<p>After the address, all of the talks were pitched in <a href="http://en.wikipedia.org/wiki/BarCamp">BarCamp format</a>. It was a very efficient and entertaining way to create a schedule for the conference. Everyone had 15-20 seconds to present their talk and then they had to rush outside to post their topic on the wall. We all had the opportunity to go outside and vote for the talks that sounded interesting. Once the votes were tallied, the schedule was set and the conference was fully underway.</p>
<p>The first talk for me was about <a href="http://twitter.com/marekgoldmann">Marek Goldmann&rsquo;s</a> <a href="http://www.jboss.org/boxgrinder.html">BoxGrinder</a>. <em>(Note: If you Google for BoxGrinder, make sure that you enter it as a single word. You&rsquo;ll get some wild unrelated results if you use two words.)</em> In short, BoxGrinder gives you the ability to have a <a href="http://fedoraproject.org/wiki/Anaconda/Kickstart">kickstart</a>-ish method for automatically building images for virtual machine environments. It&rsquo;s completely <a href="http://community.jboss.org/wiki/BoxGrinderBuildPlugins">plugin-based</a>, so you can have different platform and delivery plugins depending on where your VM needs to be deployed. For example, you could deploy a VM with BoxGrinder that is in a format for VMWare (platform) and is delivered to the target server via SFTP (delivery). The public cloud plugins are only compatible with Amazon&rsquo;s products, but I&rsquo;m eager to change that during one of the upcoming hackfests.</p>
<p>The <a href="http://www.osrg.net/sheepdog/">Sheepdog</a> talk started up right after lunch and although it was interesting, I think it left most people with quite a few questions when it was over. However, I think people are generally apprehensive when anyone tries to do anything innovative with storage. Losing data due to a bug is a big concern and many of the questions went deeper into data safety than performance and functionality.</p>
<p>Next up was <a href="http://fedoraproject.org/wiki/Python_in_Fedora_13">Dave Malcolm&rsquo;s</a> talk about the different implementations of python. This was definitely an eye-opening talk for my coworker and I. Dave covered CPython, Jython, PyPy and various other implementations and compared their advantages and disadvantages. I&rsquo;m still pretty new to Python (I&rsquo;m clutching on to ruby, PHP and perl still), but this talk really had me thinking about which implementations are best for a particular environment or task. It was quite a bit of fun to learn about some of the deep underpinnings of Python and how they differ depending on the specific implementation.</p>
<p><a href="http://pl.atyp.us/">Jeff Darcy&rsquo;s</a> talk about <a href="http://fedoraproject.org/wiki/Features/CloudFS">CloudFS</a> was very intriguing. I&rsquo;ve been a fan of <a href="http://www.gluster.org/">GlusterFS</a> recently, but I eventually moved away due to a lack of enterprise features and degrading performance. Jeff is working to add in encryption and authentication without rewriting the filesystem itself. There are quite a few tricky problems involved in the encryption portion due to partial writes and general security during the handshake process. CloudFS could potentially be a network filesystem which could be shared by multiple tenants with their own individual namespaces and segregated UID&rsquo;s. This could be a big win for providers as they could offer up large amounts of storage in an organized fashion without too many management headaches.</p>
<p>We wrapped up the day of talks with <a href="http://clalance.blogspot.com/">Chris Lalancette&rsquo;s</a> presentation about <a href="http://incubator.apache.org/deltacloud/">Deltacloud</a>. In short, it&rsquo;s a bag of daemons that allow you to manage multiple public or private clouds. Everything from image management to provisioning are included in the project. Questions were raised about whether another application was needed since vendor-specific libraries are abundant and libcloud offers many of the same features in a simpler package.</p>
<p>Tonight&rsquo;s social event was FUDPub at ASU&rsquo;s Memoral Union building. The food and drinks were excellent (thanks to <a href="http://rackspace.com/">Rackspace</a>!) and it was a great opportunity to relax and talk with other Fedora users and developers. We had the opportunity to meet people from around the world while playing round after round of bowling and billiards. The discussions were extremely valuable, but as I said before, it was quite tiring.</p>
<p>I&rsquo;ve compiled the FUDCon photos I&rsquo;ve taken into a <a href="http://www.flickr.com/photos/texas1emt/sets/72157625935659726/">Flickr photo set</a>.</p>
<p>That&rsquo;s the end of today&rsquo;s summary. I&rsquo;ll try to keep this going tomorrow as well. Thanks for reading!</p>
]]></content></item><item><title>Gearing up for FUDCon 2011</title><link>https://major.io/2011/01/29/gearing-up-for-fudcon-2011/</link><pubDate>Sat, 29 Jan 2011 15:01:20 +0000</pubDate><guid>https://major.io/2011/01/29/gearing-up-for-fudcon-2011/</guid><description>FUDCon 2011 in Tempe hasn&amp;rsquo;t even fully started yet, but it&amp;rsquo;s been well worth the trip already. We put quite a few names with faces (or IRC nicks with faces) and discussed our initial forays into Linux when we were young.
From what I was told last night, this is the first conference organized by folks not already working for Red Hat (even though some of them were hired on after planning was underway) and presentations are done in BarCamp format.</description><content type="html"><![CDATA[<p><a href="http://fedoraproject.org/wiki/FUDCon:Tempe_2011">FUDCon 2011</a> in Tempe hasn&rsquo;t even fully started yet, but it&rsquo;s been well worth the trip already. We put quite a few names with faces (or IRC nicks with faces) and discussed our initial forays into Linux when we were young.</p>
<p>From what I was told last night, this is the first conference organized by folks not already working for Red Hat (even though some of them were hired on after planning was underway) and presentations are done in <a href="http://barcamp.org/">BarCamp format</a>. This morning kicks off with the BarCamp pitches themselves and they are supposed to last only 20 seconds each. I&rsquo;m new to this format of conferences but I&rsquo;m eager to see how it works.</p>
<p>Quite a few people on Twitter have asked me if I could toss some summaries of some of the talks onto the blog. I will certainly try my best to do so!</p>
<p>Here&rsquo;s a sampling of the photos I&rsquo;ve taken so far:</p>
<ul>
<li><a href="http://rkrh.kr/1296241117.jpg">iPad being used as a laptop</a></li>
<li><a href="http://rkrh.kr/1296265179.jpg">List of sponsors (hey, it&rsquo;s Rackspace!)</a></li>
<li><a href="http://thesocialnerd.com/best-job-title-ever-written">Ian Weller has a great job title</a></li>
<li><a href="http://thesocialnerd.com/everybody-needs-a-little-loaf-love">Strange &ldquo;Loaf Love&rdquo; truck in the hotel parking lot</a></li>
<li><a href="http://thesocialnerd.com/qr-barcodes-at-fudcon-are-a-great-idea">My conference badge along with a handy QR barcode</a></li>
<li><a href="http://thesocialnerd.com/sunrise-over-tempe-arizona">Sunrise over Tempe</a></li>
</ul>
]]></content></item><item><title>Single boot Linux on an Intel Mac Mini</title><link>https://major.io/2011/01/26/single-boot-linux-on-an-intel-mac-mini/</link><pubDate>Wed, 26 Jan 2011 13:32:53 +0000</pubDate><guid>https://major.io/2011/01/26/single-boot-linux-on-an-intel-mac-mini/</guid><description>After reading the title of this post, you might wonder “Why would someone pay for a Mac Mini and then not use OS X with it?” Well, if you have a somewhat older Mac Mini you want to use as a server with Linux, these instructions will come in handy.
To get started, you&amp;rsquo;ll need a few things:
Mac OS X Install Disc Your favorite Linux distribution&amp;rsquo;s install or live CD/DVD A CD with refit on it First off, boot the Mac into your normal OS X installation first and mute the sound.</description><content type="html"><![CDATA[<p>After reading the title of this post, you might wonder “Why would someone pay for a Mac Mini and then not use OS X with it?” Well, if you have a somewhat older Mac Mini you want to use as a server with Linux, these instructions will come in handy.</p>
<p>To get started, you&rsquo;ll need a few things:</p>
<ul>
<li>Mac OS X Install Disc</li>
<li>Your <a href="http://mirror.rackspace.com/fedora/releases/">favorite</a> Linux distribution&rsquo;s install or live CD/DVD</li>
<li>A CD with <a href="http://refit.sourceforge.net/">refit</a> on it</li>
</ul>
<p>First off, boot the Mac into your normal OS X installation first and mute the sound. This will get rid of the Mac chime on bootup. It&rsquo;s really difficult to get this done properly outside of OS X, so take the time to do it now. Put your Linux CD/DVD in the drive and reboot. While it&rsquo;s rebooting, hold down the Option key (alt key if you&rsquo;re using a PC keyboard) and you&rsquo;ll have the option to boot from the disc when it boots up. The boot screen might say “Windows” for the Linux CD/DVD, but choose it anyway.</p>
<p>When I installed Fedora, I had to switch the hard drive&rsquo;s partition table from GPT to a plain old “msdos” partition table. Hop into a terminal, start <code>parted</code> on your main hard disk and type <code>mklabel msdos</code>. This will instantly erase the hard drive — make sure you&rsquo;re ready for this step. If you&rsquo;re using an anaconda-based installation, you can get to a root shell by pressing CTRL-ALT-F2. When you&rsquo;re done with <code>parted</code> in that terminal, switch back to anaconda with CTRL-ALT-F6.</p>
<p>At this point, you shouldn&rsquo;t have any partitions on your disk and you&rsquo;ll be ready to install your Linux distribution normally. I generally put everything in one giant partition as it makes the “bless” step a little easier later on.</p>
<p>Eject the Linux CD/DVD once the installation is complete and toss in the refit CD that you burned previously. Reboot the Mini again while holding Option (or alt key) and choose the disc again at bootup. When refit appears, choose the second icon from the left in the bottom row and press enter. It might say that your GPT partition is empty — that&rsquo;s okay.</p>
<p>Reboot again, but hold down the Eject key (or F12 on PC keyboards) during boot to eject the refit disc. Pop in the OS X install disc (may need to reboot again to get it to boot) and open a terminal once the install disc fully boots. Once you&rsquo;re in the terminal, run <code>diskutil list</code> to figure out which partition is your boot partition. If you did one giant partition, this should be <code>/dev/disk0s1</code>. Just “bless” the partition to make it valid for booting:</p>
<!-- raw HTML omitted -->
<p>Reboot again while holding Eject (or F12) to get the OS X disc out of the drive. At this point, you should be ready to go for hands-off booting. My Mac Mini went through about 10-20 seconds of wild screen flickering from grey to black to grey to black but then I saw the familiar Fedora framebuffer.</p>
<p>If you intend to run the Mac Mini headless with Linux, you&rsquo;re going to run into a problem. The legacy BIOS used to boot Linux requires a monitor to be attached, but there are <a href="http://soledadpenades.com/2009/02/10/mac-mini-as-a-headless-server/">some workarounds</a>. Also, if you want the Mini to power back on in case of a power failure, just run this at each boot:</p>
<!-- raw HTML omitted -->
<p>Helpful resources:</p>
<p><a href="http://mac.linux.be/content/single-boot-linux-without-delay">http://mac.linux.be/content/single-boot-linux-without-delay</a></p>
<p><a href="http://www.alphatek.info/2009/07/22/natively-run-fedora-11-on-an-intel-mac/">http://www.alphatek.info/2009/07/22/natively-run-fedora-11-on-an-intel-mac/</a></p>
]]></content></item><item><title>icanhazip.com now supports both IPv4 and IPv6 addresses</title><link>https://major.io/2011/01/15/icanhazip-com-now-supports-both-ipv4-and-ipv6-addresses/</link><pubDate>Sat, 15 Jan 2011 14:08:12 +0000</pubDate><guid>https://major.io/2011/01/15/icanhazip-com-now-supports-both-ipv4-and-ipv6-addresses/</guid><description>If you&amp;rsquo;re a fan of icanhazip.com, you&amp;rsquo;ll be excited to know that it now supports IPv4 and IPv6 addresses. The icanhazip.com domain now has A and AAAA records, so you should now be able to reach it on connections that are IPv6 only.
Should you find yourself in a situation in which you need to force it to display your IPv4 or IPv6 address (which is handy if you are running a dual stack), simply access ipv4.</description><content type="html"><![CDATA[<p>If you&rsquo;re a fan of <a href="http://icanhazip.com/">icanhazip.com</a>, you&rsquo;ll be excited to know that it now supports IPv4 and IPv6 addresses. The icanhazip.com domain now has A and AAAA records, so you should now be able to reach it on connections that are IPv6 only.</p>
<p>Should you find yourself in a situation in which you need to force it to display your IPv4 or IPv6 address (which is handy if you are running a dual stack), simply access ipv4.icanhazip.com or ipv6.icanhazip.com. Those sites will always return your IPv4 and IPv6 addresses, respectively.</p>
<p>As always, if you find any bugs or other problems, be sure to let me know!</p>
]]></content></item><item><title>Sending binary e-mail attachments from the command line with mutt</title><link>https://major.io/2011/01/10/sending-binary-e-mail-attachments-from-the-command-line-with-mutt/</link><pubDate>Tue, 11 Jan 2011 01:10:58 +0000</pubDate><guid>https://major.io/2011/01/10/sending-binary-e-mail-attachments-from-the-command-line-with-mutt/</guid><description>E-mailing a binary e-mail attachment from a Linux server has always been difficult for me because I never found a reliable method to get it done. I&amp;rsquo;ve used uuencode to pipe data into mail on various systems but the attachment is often unreadable by many e-mail clients.
Someone finally showed me a simple, fool-proof method to send binary attachments reliably from various Linux systems:
echo &amp;quot;Cheeseburger&amp;quot; | mutt -s &amp;quot;OHAI!&amp;quot; -a lolcat.</description><content type="html"><![CDATA[<p>E-mailing a binary e-mail attachment from a Linux server has always been difficult for me because I never found a reliable method to get it done. I&rsquo;ve used <code>uuencode</code> to pipe data into <code>mail</code> on various systems but the attachment is often unreadable by many e-mail clients.</p>
<p>Someone finally showed me a simple, fool-proof method to send binary attachments reliably from various Linux systems:</p>
<pre><code>echo &quot;Cheeseburger&quot; | mutt -s &quot;OHAI!&quot; -a lolcat.jpg -- recipient@domain.com
</code></pre><p>If you e-mail doesn&rsquo;t arrive, remember to consider the size of the file that you&rsquo;re sending and the restrictions of the receiver&rsquo;s e-mail server. Keep in mind that encoding the binary attachment will cause the size of the e-mail to creep up a bit more (about 1.37x plus a little extra with <a href="http://en.wikipedia.org/wiki/Base64#MIME">Base64</a>).</p>
]]></content></item><item><title>Strategies for storing backups</title><link>https://major.io/2011/01/09/strategies-for-storing-backups/</link><pubDate>Mon, 10 Jan 2011 01:20:44 +0000</pubDate><guid>https://major.io/2011/01/09/strategies-for-storing-backups/</guid><description>Although it&amp;rsquo;s not a glamorous subject for system administrators, backups are necessary for any production environment. Those who run their systems without backups generally learn from their errors in a very painful way. However, the way you store your backups may sometimes prove to be just as vital as the methods you use to backup your data.
For my environments, I follow a strategy like this: I have some backups immediately accessible, others that are accessible very quickly (but not instantly), and others that are offsite and may take a bit more time to access.</description><content type="html"><![CDATA[<p>Although it&rsquo;s not a glamorous subject for system administrators, backups are necessary for any production environment. Those who run their systems without backups generally learn from their errors in a very painful way. However, the way you store your backups may sometimes prove to be just as vital as the methods you use to backup your data.</p>
<p>For my environments, I follow a strategy like this: I have some backups immediately accessible, others that are accessible very quickly (but not instantly), and others that are offsite and may take a bit more time to access.</p>
<p><strong>Immediately accessible backups</strong></p>
<p>One of the easiest way to have an immediately accessible backup is to have multiple machines online running the same versions of code or databases in a high availability group. If you have a node which fails, the remaining nodes should be able to handle the requests immediately. You may not consider this to be a backup under the traditional definition of what a backup should be, but it&rsquo;s functionally similar.</p>
<p><strong>Backups that are accessible quickly</strong></p>
<p>This second level of backups should be stored very close to your environment or within the environment itself. If you have multiple database and web server nodes, you could consider storing your web backups on the database servers and vice versa. For those who run very sensitive applications, this may violate the provisions of different certifications and regulations. A server dedicated to holding backups may be a viable alternative for additional security.</p>
<p><strong>Offsite backup storage</strong></p>
<p>These are the backups that need to be geographically distant from your main environment. Also, you should always consider storing these backups on more than one medium with more than one company.</p>
<p>For example, if your hosting providers offers a storage service, it&rsquo;s fine to store one set of your backups there, but consider storing them with a competitor as well. If you store your backups with your hosting provider in multiple places, you could be caught be a provider issue and lose access to your backups entirely. Hosting with multiple providers will allow you to access at least one copy of your backups even if there are billing or technical issues with a particular provider.</p>
<p>Another thing to keep in mind with offsite backup storage is how long it will take to transfer the backups to your hosting environment in case of an emergency. If your hosting environment is in Texas, but your backups are stored in Australia, you&rsquo;re going to have a longer wait when you transfer your data back.</p>
<p><strong>A specific example</strong></p>
<p>My environments are all in Dallas, Texas and I have a highly available environment with multiple instances. My second layer of backups are stored within the environment as well as in Rackspace&rsquo;s Cloud Files in Dallas. My third layer of backups are stored with Amazon S3 via Jungle Disk and at my home on a RAID array.</p>
<p>While I hope you never need to access your backups under duress, these tips should help to reduce your stress if you need to restore data in a hurry.</p>
]]></content></item><item><title>One RHCA exam down, five to go</title><link>https://major.io/2011/01/06/one-rhca-exam-down-five-to-go/</link><pubDate>Fri, 07 Jan 2011 02:24:49 +0000</pubDate><guid>https://major.io/2011/01/06/one-rhca-exam-down-five-to-go/</guid><description>While I&amp;rsquo;m not the biggest proponent of certifications, I still think you can learn some valuable information while studying for some certification tests. A good example of that is Red Hat&amp;rsquo;s System Monitoring and Performance Tuning Expertise Exam. It&amp;rsquo;s one test out of five in the Red Hat Certified Architect (RHCA) track of exams. I passed the exam this week and it&amp;rsquo;s definitely one of the most difficult certification exams I&amp;rsquo;ve ever taken.</description><content type="html"><![CDATA[<p><a href="/wp-content/uploads/2011/01/Major_Hayden_EX442.jpg"><!-- raw HTML omitted --></a>While I&rsquo;m not the biggest proponent of certifications, I still think you can learn some valuable information while studying for some certification tests. A good example of that is Red Hat&rsquo;s <a href="https://www.redhat.com/courses/ex442_red_hat_enterprise_system_monitoring_and_performance_tuning_expertise_exam/">System Monitoring and Performance Tuning Expertise Exam</a>. It&rsquo;s one test out of five in the <a href="http://www.redhat.com/certification/rhca/">Red Hat Certified Architect (RHCA)</a> track of exams. I passed the exam this week and it&rsquo;s definitely one of the most difficult certification exams I&rsquo;ve ever taken.</p>
<p>Many of the <a href="https://www.redhat.com/courses/rh442_red_hat_enterprise_system_monitoring_and_performance_tuning/details/">topics covered on the EX442 exam</a> are really handy for Linux system administrators. The advanced filesystem configuration, software RAID optimization, and process scheduling knowledge can provide some tangible performance gains on just about any Linux server.</p>
<p>I especially enjoyed learning about <a href="http://sourceware.org/systemtap/">SystemTap</a>. I&rsquo;d heard about it before, but I never really understood what it did and how much power it can give a system administrator on a server. You can <a href="http://sourceware.org/systemtap/examples/keyword-index.html">sample a wide variety of system events</a> quickly with relative ease and you&rsquo;re not bound to the hardware dependencies of <a href="http://oprofile.sourceforge.net/news/">OProfile</a>.</p>
<p>Bear in mind that you&rsquo;ll need a valid <a href="http://www.redhat.com/certification/rhce/">RHCE</a> to get started with the RHCA exams. That&rsquo;s another certification that I&rsquo;d definitely recommend as it helps you learn a wide variety of administration topics that can be applied to almost any Linux distribution available today.</p>
]]></content></item><item><title>Using GNU sort to sort a list by IP address</title><link>https://major.io/2011/01/06/using-gnu-sort-to-sort-a-list-by-ip-address/</link><pubDate>Thu, 06 Jan 2011 13:52:58 +0000</pubDate><guid>https://major.io/2011/01/06/using-gnu-sort-to-sort-a-list-by-ip-address/</guid><description>My daily work requires me to work with a lot of customer data and much of it involves IP address allocations. If you find that you need to sort a list by IP address with GNU sort on a Linux server, just use these handy arguments for sort:
sort -n -t . -k 1,1 -k 2,2 -k 3,3 -k 4,4 somefile.txt For this to work, the file you&amp;rsquo;re sorting needs to have the IP address as the first item on each line.</description><content type="html"><![CDATA[<p>My <a href="http://rackspace.com/">daily work</a> requires me to work with a lot of customer data and much of it involves IP address allocations. If you find that you need to sort a list by IP address with GNU sort on a Linux server, just use these handy arguments for sort:</p>
<pre><code>sort -n -t . -k 1,1 -k 2,2 -k 3,3 -k 4,4 somefile.txt
</code></pre><p><em>For this to work, the file you&rsquo;re sorting needs to have the IP address as the first item on each line.</em></p>
]]></content></item><item><title>Mounting a raw partition file made with dd or dd_rescue in Linux</title><link>https://major.io/2010/12/14/mounting-a-raw-partition-file-made-with-dd-or-dd_rescue-in-linux/</link><pubDate>Wed, 15 Dec 2010 01:07:24 +0000</pubDate><guid>https://major.io/2010/12/14/mounting-a-raw-partition-file-made-with-dd-or-dd_rescue-in-linux/</guid><description>This situation might not affect everyone, but it struck me today and left me scratching my head. Consider a situation where you need to clone one drive to another with dd or when a hard drive is failing badly and you use dd_rescue to salvage whatever data you can.
Let&amp;rsquo;s say you cloned data from a drive using something like this:
# dd if=/dev/sda of=/mnt/nfs/backup/harddrive.img Once that&amp;rsquo;s finished, you should end up with your partition table as well as the grub data from the MBR in your image file.</description><content type="html"><![CDATA[<p>This situation might not affect everyone, but it struck me today and left me scratching my head. Consider a situation where you need to clone one drive to another with <a href="http://en.wikipedia.org/wiki/Dd_(Unix)">dd</a> or when a hard drive is failing badly and you use <a href="http://www.garloff.de/kurt/linux/ddrescue/">dd_rescue</a> to salvage whatever data you can.</p>
<p>Let&rsquo;s say you cloned data from a drive using something like this:</p>
<pre><code># dd if=/dev/sda of=/mnt/nfs/backup/harddrive.img
</code></pre><p>Once that&rsquo;s finished, you should end up with your partition table as well as the grub data from the MBR in your image file. If you run <code>file</code> against the image file you made, you should see something like this:</p>
<pre><code># file harddrive.img
harddrive.img: x86 boot sector; GRand Unified Bootloader, stage1 version 0x3, stage2
address 0x2000, stage2 segment 0x200, GRUB version 0.97; partition 1: ID=0x83,
active, starthead 1, startsector 63, 33640047 sectors, code offset 0x48
</code></pre><p>What if you want to pull some files from this image without writing it out to another disk? Mounting it like a loop file isn&rsquo;t going to work:</p>
<pre><code># mount harddrive /mnt/temp
mount: you must specify the filesystem type
</code></pre><p>The key is to mount the file <a href="http://www.linuxquestions.org/questions/linux-general-1/trouble-mounting-dd-image-file-644362/#post3660310">with an offset specified</a>. In the output from <code>file</code>, there is a particular portion of the output that will help you:</p>
<pre><code>... startsector 63 ...
</code></pre><p>This means that the filesystem itself starts on sector 63. You can also view this with <code>fdisk -l</code>:</p>
<pre><code># fdisk -l harddrive.img
                    Device Boot      Start         End      Blocks   Id  System
harddrive.img                *          63    33640109    16820023+  83  Linux
</code></pre><p>Since we need to scoot 63 sectors ahead, and each sector is 512 bytes long, we need to use an offset of 32,256 bytes. Fire up the mount command and you&rsquo;ll be on your way:</p>
<pre><code># mount -o ro,loop,offset=32256 harddrive.img /mnt/loop
# mount | grep harddrive.img
/root/harddrive.img on /mnt/loop type ext3 (ro,loop=/dev/loop1,offset=32256)
</code></pre><p>If you made this image under duress (due to a failing drive or other emergency), you might have to check and repair the filesystem first. Doing that is easy if you make a loop device:</p>
<pre><code># losetup --offset 32256 /dev/loop2 harddrive.img
# fsck /dev/loop2
</code></pre><p>Once that&rsquo;s complete, you can save some time and mount the loop device directly:</p>
<pre><code># mount /dev/loop2 /mnt/loop
</code></pre>]]></content></item><item><title>Locate RPM packages which contain a certain file</title><link>https://major.io/2010/12/08/locate-rpm-packages-which-contain-a-certain-file/</link><pubDate>Thu, 09 Dec 2010 02:30:00 +0000</pubDate><guid>https://major.io/2010/12/08/locate-rpm-packages-which-contain-a-certain-file/</guid><description>It&amp;rsquo;s not easy remembering which RPM packages contain certain files. If I asked you which files you&amp;rsquo;d find in packages like postfix-2.7.1-1.fc14 and bash-4.1.7-3.fc14, you would be able to name some obvious executables. However, would you be able to do the same if I mentioned a package like util-linux-ng-2.18-4.6.fc14? If the RPM is already installed, you can quickly use rpm -ql to list the files within it.
However, what if the RPM isn&amp;rsquo;t installed already?</description><content type="html"><![CDATA[<p>It&rsquo;s not easy remembering which RPM packages contain certain files. If I asked you which files you&rsquo;d find in packages like <code>postfix-2.7.1-1.fc14</code> and <code>bash-4.1.7-3.fc14</code>, you would be able to name some obvious executables. However, would you be able to do the same if I mentioned a package like <code>util-linux-ng-2.18-4.6.fc14</code>? If the RPM is already installed, you can quickly use <code>rpm -ql</code> to list the files within it.</p>
<p>However, what if the RPM isn&rsquo;t installed already? How do you figure out which one to install?</p>
<p>Fedora has well over 20,000 packages in the standard repositories without adding additional repositories like RPM Fusion. Narrowing that list down to find the package you want can be daunting, but you can use yum to help.</p>
<p>Consider this: you&rsquo;re following a guide online and the author says you need to run <code>deallocvt</code>:</p>
<pre><code># deallocvt
-bash: deallocvt: command not found
</code></pre><p>Perhaps it&rsquo;s in a package with <code>deallocvt</code> in the name:</p>
<pre><code># yum search deallocvt
Warning: No matches found for: deallocvt
No Matches found
</code></pre><p>This is where yum&rsquo;s <code>whatprovides</code> (<code>provides</code> works in recent yum versions) command works really well:</p>
<pre><code># yum whatprovides */deallocvt
kbd-1.15-11.fc14.x86_64 : Tools for configuring the console
Repo        : fedora
Matched from:
Filename    : /usr/bin/deallocvt
</code></pre><p>From there, you can install the <code>kbd</code> RPM package via yum and you&rsquo;ll be on your way.</p>
<p><em>Author&rsquo;s note: Regular readers will probably think this is pretty basic, but I often find people who don&rsquo;t know this functionality exists in yum.</em></p>
<p><strong>UPDATE:</strong> I forgot to include another handy command in this article (thanks to Jason Gill for reminding me). If you have file on your system already, but you need to know which RPM package it came from, you can do this very quickly:</p>
<pre><code># rpm -qf /usr/bin/free
procps-3.2.8-14.fc14.x86_64
</code></pre>]]></content></item><item><title>Tap into your Linux system with SystemTap</title><link>https://major.io/2010/12/07/tap-into-your-linux-system-with-systemtap/</link><pubDate>Wed, 08 Dec 2010 02:27:02 +0000</pubDate><guid>https://major.io/2010/12/07/tap-into-your-linux-system-with-systemtap/</guid><description>One of the most interesting topics I&amp;rsquo;ve seen so far during my RHCA training at Rackspace this week is SystemTap. In short, SystemTap allows you to dig out a bunch of details about your running system relatively easily. It takes scripts, converts them to C, builds a kernel module, and then runs the code within your script.
HOLD IT:The steps below are definitely not meant for those who are new to Linux.</description><content type="html"><![CDATA[<p>One of the most interesting topics I&rsquo;ve seen so far during my <a href="https://www.redhat.com/courses/rh442_red_hat_enterprise_system_monitoring_and_performance_tuning/">RHCA</a> training at <a href="http://rackspace.com/">Rackspace</a> this week is <a href="http://sourceware.org/systemtap/">SystemTap</a>. In short, SystemTap allows you to dig out a bunch of details about your running system relatively easily. It takes scripts, converts them to C, builds a kernel module, and then runs the code within your script.</p>
<p><strong><!-- raw HTML omitted -->HOLD IT:<!-- raw HTML omitted --> The steps below are <em>definitely</em> not meant for those who are new to Linux. Utilizing SystemTap on a production system is a bad idea — it can chew up significant resources while it runs and it can also cause a running system to kernel panic if you&rsquo;re not careful with the packages you install.</strong></p>
<p>These instructions will work well with Fedora, CentOS and Red Hat Enterprise Linux. Luckily, the SystemTap folks put together some instructions for <a href="http://sourceware.org/systemtap/wiki/SystemtapOnDebian">Debian</a> and <a href="http://sourceware.org/systemtap/wiki/SystemtapOnUbuntu">Ubuntu</a> as well.</p>
<p>Before you can start working with SystemTap on your RPM-based distribution, you&rsquo;ll need to get some prerequisites together:</p>
<pre><code>yum install gcc systemtap systemtap-runtime systemtap-testsuite kernel-devel
yum --enablerepo=*-debuginfo install kernel-debuginfo kernel-debuginfo-common
</code></pre><p><strong><!-- raw HTML omitted -->WHOA THERE:<!-- raw HTML omitted --> Ensure that the kernel-devel and kernel-debuginfo* packages that you install via yum match up with your running kernel. If there&rsquo;s a newer kernel available from your yum repo, yum will pull that one. If it&rsquo;s been a while since you updated, you&rsquo;ll either need to upgrade your current kernel to the latest and reboot or you&rsquo;ll need to hunt down the corresponding kernel-devel and kernel-debuginfo* packages from a repository. <em>Installing the wrong package version can lead to kernel panics.</em> Also, bear in mind that the debuginfo packages are quite large: almost 200MB in Red Hat/CentOS and almost 300MB in Fedora.</strong></p>
<p>You can&rsquo;t write the script in just any language. SystemTap uses an odd syntax to get things going:</p>
<pre><code>#! /usr/bin/env stap
probe begin { println(&quot;hello world&quot;) exit () }
</code></pre><p>Just run the script with <code>stap</code>:</p>
<pre><code># stap -v helloworld.stp
Pass 1: parsed user script and 73 library script(s) using 94380virt/21988res/2628shr kb, in 140usr/30sys/167real ms.
Pass 2: analyzed script: 1 probe(s), 1 function(s), 0 embed(s), 0 global(s) using 94776virt/22516res/2692shr kb, in 10usr/0sys/5real ms.
Pass 3: using cached /root/.systemtap/cache/bc/stap_bc368822da380b943d4e845ee15ed047_773.c
Pass 4: using cached /root/.systemtap/cache/bc/stap_bc368822da380b943d4e845ee15ed047_773.ko
Pass 5: starting run.
hello world
Pass 5: run completed in 0usr/20sys/285real ms.
</code></pre><p>The <code>systemtap-testsuite</code> package gives you a <strong>tubload</strong> of extremely handy SystemTap scripts. For example:</p>
<pre><code># cd /usr/share/systemtap/testsuite/systemtap.examples/io/
# stap iotime.stp
15138470 6351 (httpd) access /usr/share/cacti/index.php read: 0 write: 0
15142243 6351 (httpd) access /usr/share/cacti/include/auth.php read: 0 write: 0
15143780 6351 (httpd) access /usr/share/cacti/include/global.php read: 0 write: 0
15144099 6351 (httpd) access /etc/cacti/db.php read: 0 write: 0
15187641 6351 (httpd) access /usr/share/cacti/lib/adodb/adodb.inc.php read: 106486 write: 0
15187664 6351 (httpd) iotime /usr/share/cacti/lib/adodb/adodb.inc.php time: 218
15194965 6351 (httpd) access /usr/share/cacti/lib/adodb/adodb-time.inc.php read: 0 write: 0
15195692 6351 (httpd) access /usr/share/cacti/lib/adodb/adodb-iterator.inc.php read: 0 write: 0
   ... output continues ...
</code></pre><p>The <code>iotime.stp</code> script dumps out the reads and writes occurring on the system in real time. After starting the script above, I accessed my cacti instance on the server and immediately started seeing some reads as apache began picking up PHP files to parse.</p>
<p>Consider a situation in which you need to decrease interrupts on a Linux machine. This is vital for laptops and systems that need to remain in low power states. Some might suggest powertop <a href="http://www.lesswatts.org/projects/powertop/">for that, but why not give SystemTap a try?</a></p>
<pre><code># cd /usr/share/systemtap/testsuite/systemtap.examples/interrupt/
# stap interrupts-by-dev.stp
        ohci_hcd:usb3 :      1
        ohci_hcd:usb4 :      1
            hda_intel :      1
                 eth0 :      2
                 eth0 :      2
                 eth0 :      2
                 eth0 :      2
                 eth0 :      2
                 eth0 :      2
</code></pre><p>On this particular system, it&rsquo;s pretty obvious that the ethernet interface is causing a lot of interrupts.</p>
<p>If you want more examples, keep hunting around in the systemtap-testsuite package (remember <code>rpm -ql systemtap-testsuite</code>) or review the <a href="http://sourceware.org/systemtap/examples/">giant list of examples</a> on SystemTap&rsquo;s site.</p>
<p><em>Thanks again to Phil Hopkins at Rackspace for giving us a detailed explanation of system profiling during training.</em></p>
]]></content></item><item><title>Keep web servers in sync with DRBD and OCFS2</title><link>https://major.io/2010/12/02/keep-web-servers-in-sync-with-drbd-and-ocfs2/</link><pubDate>Fri, 03 Dec 2010 02:01:12 +0000</pubDate><guid>https://major.io/2010/12/02/keep-web-servers-in-sync-with-drbd-and-ocfs2/</guid><description>The guide to redundant cloud hosting that I wrote recently will need some adjustments as I&amp;rsquo;ve fallen hard for the performance and reliability of DRBD and OCFS2. As a few of my sites were gaining in popularity, I noticed that GlusterFS simply couldn&amp;rsquo;t keep up. High I/O latency and broken replication threw a wrench into my love affair with GlusterFS and I knew there had to be a better option.</description><content type="html"><![CDATA[<p>The <a href="/redundant-cloud-hosting-configuration-guide/">guide to redundant cloud hosting</a> that I wrote recently will need some adjustments as I&rsquo;ve fallen hard for the performance and reliability of DRBD and OCFS2. As a few of my sites were gaining in popularity, I noticed that GlusterFS simply couldn&rsquo;t keep up. High I/O latency and broken replication threw a wrench into my love affair with GlusterFS and I knew there had to be a better option.</p>
<p>I&rsquo;ve shared my configuration with my coworkers and I&rsquo;ve received many good questions about it. Let&rsquo;s get to the Q&amp;A:</p>
<p><strong>How does the performance compare to GlusterFS?</strong></p>
<p>On Gluster&rsquo;s best days, the data throughput speeds were quite good, but the latency to retrieve the data was often much too high. Page loads on this site were taking upwards of 3-4 seconds with GlusterFS latency accounting for well over 75% of the delays. For small files, GlusterFS&rsquo;s performance was about 20-25x slower than accessing the disk natively. The performance hit for DRBD and OCFS2 is usually between 1.5-3x for small files and difficult to notice for large file transfers.</p>
<p><strong>Couldn&rsquo;t you keep the data separate and then sync it with rsync?</strong></p>
<p>Everyone knows that rsync can be a resource consuming monster and it seems wasteful to call rsync via a cron job to keep my data in sync. There are some periods of the day where the actual data on the web root rarely changes. There are other times where it changes rapidly and I&rsquo;d end up with nodes out of sync for a few minutes.</p>
<p>To get the just-in-time synchronization that I want, I&rsquo;d have to run rsync at least once a minute. If the data isn&rsquo;t changing over a long period, rsync would end up crushing the disk and consuming CPU for no reason. DRBD only syncs data when data changes. Also, all reads with DRBD are done locally. This makes is a highly efficient and effective choice for instant synchronization.</p>
<p><strong>Why OCFS2? Isn&rsquo;t that overkill?</strong></p>
<p>When you use DRBD in dual-primary mode, it&rsquo;s functionally equivalent to having a raw storage device (like a SAN) mounted in two places. If you threw an ext4 filesystem onto a LUN on your SAN and then mounted it on two different servers, you&rsquo;d be in bad shape very quickly. Non-clustered filesystems like ext3 or ext4 can&rsquo;t handle being mounted in more than one environment.</p>
<p>OCFS2 is built primarily to be mounted in more than one place and it comes with its own distributed locking manager (DLM). The configuration files for OCFS2 are extremely simple and you mount it like any other filesystem. It&rsquo;s been part of the mainline Linux kernel since 2.6.19.</p>
<p><strong>What happens when you lose one of the nodes?</strong></p>
<p>The configuration shown above can operate with just one node in an emergency. When the failed node comes back online, DRBD will resync the block device and you can mount the OCFS2 filesystem as you normally would.</p>
<p><strong>You&rsquo;re using an Oracle product? Really?</strong></p>
<p>You&rsquo;ve got me there. I&rsquo;m not a fan of how they treat the open source community with regards to some of their projects, but the OCFS2 filesystem is robust, free, and it meets my needs.</p>
<p><strong>Where&rsquo;s the how-to?</strong></p>
<p>It&rsquo;s coming soon! Stay tuned.</p>
]]></content></item><item><title>Happy Thanksgiving</title><link>https://major.io/2010/11/25/happy-thanksgiving-2010/</link><pubDate>Thu, 25 Nov 2010 16:00:18 +0000</pubDate><guid>https://major.io/2010/11/25/happy-thanksgiving-2010/</guid><description>For those of us in the United States, we celebrate Thanksgiving today. Although it means different things for different people, I consider it to be a good day to remember those who have helped us and count the blessings that we all take for granted.
Without a doubt, I&amp;rsquo;m most thankful for my family. They&amp;rsquo;re my support base and I couldn&amp;rsquo;t get through my days without them.
I wish I could thank my coworkers, mentors, and close friends individually, but this blog post would get rather lengthy.</description><content type="html"><![CDATA[<p>For those of us in the United States, we celebrate <a href="http://en.wikipedia.org/wiki/Thanksgiving">Thanksgiving</a> today. Although it means different things for different people, I consider it to be a good day to remember those who have helped us and count the blessings that we all take for granted.</p>
<p>Without a doubt, I&rsquo;m most thankful for my family. They&rsquo;re my support base and I couldn&rsquo;t get through my days without them.</p>
<p>I wish I could thank my coworkers, mentors, and close friends individually, but this blog post would get rather lengthy. To everyone who has had taken the time to teach me something: I thank you. You&rsquo;ve inspired me to continue learning and inspire others to do the same.</p>
<p>Happy Thanksgiving to you and your families. Please travel safely and take some time to relax.</p>
]]></content></item><item><title>Monitor MySQL restore progress with pv</title><link>https://major.io/2010/11/24/monitor-mysql-restore-progress-with-pv/</link><pubDate>Wed, 24 Nov 2010 16:43:28 +0000</pubDate><guid>https://major.io/2010/11/24/monitor-mysql-restore-progress-with-pv/</guid><description>The pv command is one that I really enjoy using but it&amp;rsquo;s also one that I often forget about. You can&amp;rsquo;t get a much more concise definition of what pv does than this one:
pv allows a user to see the progress of data through a pipeline, by giving information such as time elapsed, percentage completed (with progress bar), current throughput rate, total data transferred, and ETA.
The usage certainly isn&amp;rsquo;t complicated:</description><content type="html"><![CDATA[<p>The <a href="http://linux.die.net/man/1/pv">pv</a> command is one that I really enjoy using but it&rsquo;s also one that I often forget about. You can&rsquo;t get a much more concise definition of what pv does than this one:</p>
<blockquote>
<p>pv allows a user to see the progress of data through a pipeline, by giving information such as time elapsed, percentage completed (with progress bar), current throughput rate, total data transferred, and ETA.</p>
</blockquote>
<p>The usage certainly isn&rsquo;t complicated:</p>
<blockquote>
<p>To use it, insert it in a pipeline between two processes, with the appropriate options. Its standard input will be passed through to its standard output and progress will be shown on standard error.</p>
</blockquote>
<p>A great application of pv is when you&rsquo;re restoring large amounts of data into MySQL, especially if you&rsquo;re restoring data under duress due to an accidentally-dropped table or database. (Who hasn&rsquo;t been there before?) The standard way of restoring data is something we&rsquo;re all familiar with:</p>
<!-- raw HTML omitted -->
<p>The downside of this method is that you have no idea how quickly your restore is working or when it might be done. You could always open another terminal to monitor the tables and databases as they&rsquo;re created, but that can be hard to follow.</p>
<p>Toss in pv and that problem is solved:</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>When it comes to MySQL, your restore rate is going to be different based on some different factors, so the ETA might not be entirely accurate.</p>
]]></content></item><item><title>Throwing thoughtful “403 Forbidden” responses with apache</title><link>https://major.io/2010/11/17/throwing-thoughtful-403-forbidden-responses-with-apache/</link><pubDate>Wed, 17 Nov 2010 13:47:19 +0000</pubDate><guid>https://major.io/2010/11/17/throwing-thoughtful-403-forbidden-responses-with-apache/</guid><description>If you offer a web service that users query via scripts or other applications, you&amp;rsquo;ll probably find that some people will begin to abuse the service. My icanhazip.com site is no exception.
While many of the users have reasonable usage patterns, there are some users that query the site more than once per second from the same IP address. If you haven&amp;rsquo;t used the site before, all it does is return your public IP address in plain text.</description><content type="html"><![CDATA[<p>If you offer a web service that users query via scripts or other applications, you&rsquo;ll probably find that some people will begin to abuse the service. My <a href="http://icanhazip.com/">icanhazip.com</a> site is no exception.</p>
<p>While many of the users have reasonable usage patterns, there are some users that query the site more than once per second from the same IP address. If you haven&rsquo;t used the site before, all it does is return your public IP address in plain text. Unless your IP changes rapidly, you may not need to query the site more than a few times an hour.</p>
<p>I added the following to my icanhazip.com virtual host definition to get the message across to those users that abuse the service:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-apache" data-lang="apache"><span style="color:#b58900">ErrorDocument</span> <span style="color:#2aa198">403</span> &#34;No can haz IP. Stop abusing this service. \
    Contact major at mhtx dot net for details.&#34;
<span style="color:#b58900">RewriteEngine</span> <span style="color:#719e07">On</span>
<span style="color:#b58900">RewriteCond</span> %{REMOTE_ADDR} ^12.23.34.45$ [OR]
<span style="color:#b58900">RewriteCond</span> %{REMOTE_ADDR} ^98.87.76.65$
<span style="color:#b58900">RewriteRule</span> .* nocanhaz [F]
</code></pre></div><p>The users that are caught on the business end of these 403 responses will see something like this:</p>
<pre><code>$ curl -i icanhazip.com
HTTP/1.1 403 Forbidden
Date: Wed, 17 Nov 2010 13:42:55 GMT
Server: Apache
Content-Length: 84
Connection: close
Content-Type: text/html; charset=iso-8859-1

No can haz IP. Stop abusing this service. Contact major at mhtx dot net for details.
</code></pre>]]></content></item><item><title>Accessing Rackspace Cloud Servers and Slicehost slices privately via OpenVPN</title><link>https://major.io/2010/11/16/accessing-rackspace-cloud-servers-and-slicehost-slices-privately-via-openvpn/</link><pubDate>Tue, 16 Nov 2010 13:52:53 +0000</pubDate><guid>https://major.io/2010/11/16/accessing-rackspace-cloud-servers-and-slicehost-slices-privately-via-openvpn/</guid><description>Diagram: OpenVPN to Rackspace Cloud Servers and Slicehost
A recent blog post from Mixpanel inspired me to write a quick how-to for Fedora users on using OpenVPN to talk to instances privately in the Rackspace Cloud.
The diagram at the right gives an idea of what this guide will allow you to accomplish. Consider a situation where you want to talk to the MySQL installation on db1 directly without requiring extra ssh tunnels or MySQL over SSL via the public network.</description><content type="html"><![CDATA[<p><img src="/wp-content/uploads/2010/11/openvpn-to-rackspace-cloud-diagram.png" alt="diagram"> Diagram: OpenVPN to Rackspace Cloud Servers and Slicehost</p>
<p>A recent <a href="http://code.mixpanel.com/openvpn-in-the-rackspace-cloud/">blog post from Mixpanel</a> inspired me to write a quick how-to for Fedora users on using OpenVPN to talk to instances privately in the Rackspace Cloud.</p>
<p>The diagram at the right gives an idea of what this guide will allow you to accomplish. Consider a situation where you want to talk to the MySQL installation on db1 directly without requiring extra ssh tunnels or MySQL over SSL via the public network. If you tunnel into one of your instances, you can utilize the private network to talk between your instances very easily.</p>
<p>There&rsquo;s one important thing to keep in mind here: even though you&rsquo;ll be utilizing the private network between your tunnel endpoint and your other instances, your traffic will still traverse the public network. That means that the instance with your tunnel endpoint will still get billed for the traffic flowing through your tunnel.</p>
<p>You&rsquo;ll only need the openvpn package on the server side:</p>
<pre><code>yum -y install openvpn
</code></pre><p>Throw down this simple configuration file into /etc/openvpn/server.conf:</p>
<pre><code>port 1194
proto tcp
dev tun
persist-key
persist-tun

server 10.66.66.0 255.255.255.0
ifconfig-pool-persist ipp.txt

#push &quot;route 10.0.0.0 255.0.0.0&quot;
push &quot;route 10.176.0.0 255.248.0.0&quot;
keepalive 10 120

ca      /etc/openvpn/my_certificate_authority.pem
cert    /home/major/vpn_server_cert.pem
key     /home/major/vpn_server_key.pem
dh      /etc/openvpn/easy-rsa/2.0/keys/dh1024.pem

status log/openvpn-status.log
verb 3
</code></pre><p>Here&rsquo;s a bit of explanation for some things you may want to configure:</p>
<ul>
<li><code>push</code>: These are the routes that will be sent over the VPN that are pushed to the clients. If you don&rsquo;t use any IP addresses in the 10.0.0.0/8 network block in your office, you can probably use the commented out line above. However, you may want to be more specific with the routes if you happen to use any 10.0.0.0/8 space in your office.</li>
<li><code>server</code>: These are the IP addresses that the VPN server will assign and NAT out through the private interface. I&rsquo;ve used a /24 above, but you may want to adjust the netmask if you have a lot of users making tunnels to your VPN endpoint.</li>
<li><code>ca, cert, key</code>: You will need to create a certificate authority as well as a certificate/key pair for your VPN endpoint. I already use <a href="http://simpleauthority.com/">SimpleAuthority</a> on my Mac to manage some other CA&rsquo;s and certificates, but you can use <a href="http://openvpn.net/index.php/open-source/documentation/miscellaneous/77-rsa-key-management.html">openvpn&rsquo;s easy-rsa</a> scripts if you wish. They are already included with the openvpn installation.</li>
</ul>
<p>Build your Diffie-Hellman parameters file:</p>
<pre><code>cd /etc/openvpn/easy-rsa/2.0/ &amp;&amp; ./build-dh
</code></pre><p>Tell iptables that you want to NAT your VPN endpoint traffic out to all 10.x.x.x IP addresses on the private network:</p>
<pre><code>iptables -t nat -A POSTROUTING -s 10.0.0.0/8 -o eth1 -j MASQUERADE
</code></pre><p>The last step on the server side is to ensure that the kernel will forward packets from the VPN endpoint out through the private interface. Ensure that your /etc/sysctl.conf looks like this:</p>
<pre><code># Controls IP packet forwarding
net.ipv4.ip_forward = 1
</code></pre><p>Adjusting your sysctl.conf ensures that forwarding is enabled at boot time, but you&rsquo;ll need to enable it on your VPN endpoint right now:</p>
<pre><code>echo 1 &gt; /proc/sys/net/ipv4/ip_forward
</code></pre><p>Start the openvpn server:</p>
<pre><code>/etc/init.d/openvpn start
</code></pre><p>If all is well, you should see openvpn listening on port 1194:</p>
<pre><code>[root@lb2 ~]# netstat -ntlp | grep openvpn
tcp        0      0 0.0.0.0:1194      0.0.0.0:*         LISTEN      2020/openvpn
</code></pre><p>You&rsquo;ll need to configure a client to talk to your VPN now. This involves three steps: creating a new certificate/key pair for the client (same procedure as making your server certificates), signing the client&rsquo;s certificate with your CA certificate (same one that you used above to sign your server certificates), and then configuring your client application to access the VPN.</p>
<p>There are <strong>many</strong> openvpn clients out there to choose from.</p>
<p>If you&rsquo;re using a Linux desktop, you may want to consider using the <a href="http://geraner.typepad.com/blog/2009/10/how-to-create-an-openvpn-connect-in-linux-version-2.html">built-in VPN functionality in NetworkManager</a>. For Mac users, I&rsquo;d highly recommend using <a href="http://www.thesparklabs.com/viscosity/">Viscosity</a> ($9), but there&rsquo;s also <a href="http://code.google.com/p/tunnelblick/">tunnelblick</a> (free).</p>
]]></content></item><item><title>400th post: Looking back at where this blog started</title><link>https://major.io/2010/11/13/400th-post-looking-back-at-where-this-blog-started/</link><pubDate>Sun, 14 Nov 2010 02:06:00 +0000</pubDate><guid>https://major.io/2010/11/13/400th-post-looking-back-at-where-this-blog-started/</guid><description>Last night&amp;rsquo;s post about my charity drive for the Movember Foundation was the 400th post on my blog! I started posting on rackerhacker.com way back in the spring of 2007 shortly after I was hired by Rackspace in December of 2006.
My main purpose for the blog at the beginning was to create a place where I could write quick articles about problems I found and how to fix them. Most of the people around me were using their own handy systems to store notes (Stickies on the Mac, Tomboy notes on Linux, or just simple text files), but they weren&amp;rsquo;t able to share them easily.</description><content type="html"><![CDATA[<p>Last night&rsquo;s <a href="/2010/11/12/raising-money-for-prostate-cancer-research-and-survivor-support/">post</a> about my charity drive for the <a href="http://us.movember.com/">Movember Foundation</a> was the 400th post on my blog! I started posting on rackerhacker.com way back in the spring of 2007 shortly after I was hired by <a href="http://rackspace.com/">Rackspace</a> in December of 2006.</p>
<p>My main purpose for the blog at the beginning was to create a place where I could write quick articles about problems I found and how to fix them. Most of the people around me were using their own handy systems to store notes (Stickies on the Mac, Tomboy notes on Linux, or just simple text files), but they weren&rsquo;t able to share them easily. I wanted a way to write up a solution and instantly share it with someone. I also wanted that person to be able to pass along the fix to someone else if they wanted.</p>
<p>Needless to say, it took off from there.</p>
<p>It&rsquo;s important to note that I couldn&rsquo;t have done this by myself. I&rsquo;ve learned some efficient strategies for managing large systems and troubleshooting complex issues from my peers, my managers, and colleagues outside of Rackspace. There have been many triumphs and there have been quite a few failures.</p>
<p>The <em>failures</em> have taught me the most. I&rsquo;ve made some pretty large mistakes and here are a few:</p>
<ul>
<li>inserted data into a MySQL slave in an active replication pair</li>
<li>run a fsck on an online ext3 partition</li>
<li>marked a failed drive online in a hardware RAID array</li>
<li>mangled Plesk installations in ways that you can&rsquo;t comprehend</li>
<li>typed &lsquo;reboot&rsquo; into a terminal and pressed enter, only to realize I was in the wrong terminal</li>
<li>ran UPDATE statements without a WHERE clause in MySQL (well, I only did this one twice)</li>
</ul>
<p>Even after all that, people occasionally tell me that I&rsquo;m very good at what I do. I don&rsquo;t know if that&rsquo;s true or not, but I&rsquo;m glad some people think so! Many of those folks end up asking me this question:</p>
<blockquote>
<p>How do I learn how to be a successful Linux systems administrator?</p>
</blockquote>
<p>My answer is this: Be humble. Always be thirsty for knowledge. Don&rsquo;t be afraid to make mistakes. Love what you do and the people you serve.</p>
]]></content></item><item><title>Raising money for prostate cancer research and survivor support</title><link>https://major.io/2010/11/12/raising-money-for-prostate-cancer-research-and-survivor-support/</link><pubDate>Sat, 13 Nov 2010 03:19:07 +0000</pubDate><guid>https://major.io/2010/11/12/raising-money-for-prostate-cancer-research-and-survivor-support/</guid><description>Movember 2010
This is a bit of an unusual post for my blog, but I wanted to reach out to you for charitable reasons. I&amp;rsquo;m currently trying to grow some facial hair (which is a feat in itself) with some folks from work for the Movember Foundation. I look a bit sillier than usual, but at least it&amp;rsquo;s for a good cause.
The Movember Foundation currently passes along the proceeds to the Prostate Cancer Foundation and LIVESTRONG.</description><content type="html"><![CDATA[<p><img src="/wp-content/uploads/2010/11/Movember.jpg" alt="movember"> Movember 2010</p>
<p>This is a bit of an unusual post for my blog, but I wanted to reach out to you for charitable reasons. I&rsquo;m currently trying to grow some facial hair (which is a feat in itself) with some folks from work for the <a href="http://us.movember.com/">Movember Foundation</a>. I look a bit sillier than usual, but at least it&rsquo;s for a good cause.</p>
<p>The Movember Foundation currently passes along the proceeds to the <a href="http://www.pcf.org/">Prostate Cancer Foundation</a> and <a href="http://www.livestrong.org/">LIVESTRONG</a>.</p>
<p>If you&rsquo;d like to make a donation, you can <a href="http://us.movember.com/mospace/1073998/">do that on my member page</a>.</p>
]]></content></item><item><title>Switching from GlusterFS to DRBD and OCFS2</title><link>https://major.io/2010/11/10/switching-from-glusterfs-to-drbd-and-ocfs2/</link><pubDate>Wed, 10 Nov 2010 13:55:50 +0000</pubDate><guid>https://major.io/2010/11/10/switching-from-glusterfs-to-drbd-and-ocfs2/</guid><description>As my uptime reports have shown, and as some of you have reported, my blog&amp;rsquo;s load time has increased steadily over the past few weeks. It turns out that one of my VM&amp;rsquo;s was on a physical machine that had some trouble and I was reaching a point where GlusterFS&amp;rsquo;s replicate functionality couldn&amp;rsquo;t meet my performance needs.
Instead of using GlusterFS as I had before in my redundant cloud hosting guide, I decided to use DRBD in dual-primary mode with OCFS2 as the clustering filesystem on top of it.</description><content type="html"><![CDATA[<p>As my uptime reports have shown, and as some of you have reported, my blog&rsquo;s load time has increased steadily over the past few weeks. It turns out that one of my VM&rsquo;s was on a physical machine that had some trouble and I was reaching a point where GlusterFS&rsquo;s replicate functionality couldn&rsquo;t meet my performance needs.</p>
<p>Instead of using <a href="http://en.wikipedia.org/wiki/GlusterFS">GlusterFS</a> as I had before in my <a href="/redundant-cloud-hosting-configuration-guide/">redundant cloud hosting guide</a>, I decided to use <a href="http://en.wikipedia.org/wiki/DRBD">DRBD</a> in dual-primary mode with <a href="http://en.wikipedia.org/wiki/OCFS">OCFS2</a> as the clustering filesystem on top of it. The performance is quite good so far:</p>
<p><img src="/wp-content/uploads/2010/11/pingdomresponsetime-rackerhacker.com_.png" alt="5"> Pingdom Response Time Graph for rackerhacker.com</p>
<p>I switched over the DNS late last night and the response time has fallen from the two to three second range (during times of low load) to right around one second per request. In addition to the reduced load times, I can support higher concurrency without significant performance degradation.</p>
<p>Don&rsquo;t worry - I&rsquo;ll make a detailed post on this topic later along with a guide on how to set it up yourself.</p>
]]></content></item><item><title>Upgrading Fedora 13 to Fedora 14 on Slicehost and Rackspace Cloud Servers</title><link>https://major.io/2010/11/03/upgrading-fedora-13-to-fedora-14-on-slicehost-and-rackspace-cloud-servers/</link><pubDate>Wed, 03 Nov 2010 20:02:45 +0000</pubDate><guid>https://major.io/2010/11/03/upgrading-fedora-13-to-fedora-14-on-slicehost-and-rackspace-cloud-servers/</guid><description>On most systems, using Fedora&amp;rsquo;s preupgrade package is the most reliable way to update to the next Fedora release. However, this isn&amp;rsquo;t the case with Slicehost and Rackspace Cloud Servers.
Here are the steps for an upgrade from Fedora 13 to Fedora 14 via yum:
yum -y upgrade wget http://mirror.rackspace.com/fedora/releases/14/Fedora/x86_64/os/Packages/fedora-release-14-1.noarch.rpm rpm -Uvh fedora-release-14-1.noarch.rpm yum -y install yum yum -y upgrade If you happen to be upgrading a 32-bit instance on Slicehost, simply replace x86_64 with i386 in the url shown above.</description><content type="html"><![CDATA[<p>On most systems, using Fedora&rsquo;s <a href="http://fedoraproject.org/wiki/PreUpgrade">preupgrade</a> package is the most reliable way to update to the next Fedora release. However, this isn&rsquo;t the case with Slicehost and Rackspace Cloud Servers.</p>
<p>Here are the steps for an upgrade from Fedora 13 to Fedora 14 via yum:</p>
<pre><code>yum -y upgrade
wget http://mirror.rackspace.com/fedora/releases/14/Fedora/x86_64/os/Packages/fedora-release-14-1.noarch.rpm
rpm -Uvh fedora-release-14-1.noarch.rpm
yum -y install yum
yum -y upgrade
</code></pre><p>If you happen to be upgrading a 32-bit instance on Slicehost, simply replace <code>x86_64</code> with <code>i386</code> in the url shown above.</p>
]]></content></item><item><title>Do professional certifications belong in your e-mail signature?</title><link>https://major.io/2010/10/16/do-professional-certifications-belong-in-your-e-mail-signature/</link><pubDate>Sat, 16 Oct 2010 15:53:25 +0000</pubDate><guid>https://major.io/2010/10/16/do-professional-certifications-belong-in-your-e-mail-signature/</guid><description>After a discussion amongst coworkers about professional certifications in e-mail signatures yesterday, I decided to throw the question out to Twitter to gather some feedback:
rackerhacker: Quick Twitter poll for the nerds: How many certification abbreviations do you put in your e-mail signature? [Permalink]
The question must have struck a nerve with folks as I had over 50 replies in less than 10-15 minutes. I expected to hear a lot of people say &amp;ldquo;zero&amp;rdquo;, and there were quite a few responses that didn&amp;rsquo;t surprise me:</description><content type="html"><![CDATA[<p>After a discussion amongst coworkers about professional certifications in e-mail signatures yesterday, I decided to throw the question out to <a href="http://twitter.com/rackerhacker/">Twitter</a> to gather some feedback:</p>
<blockquote>
<p><a href="http://twitter.com/rackerhacker">rackerhacker</a>: Quick Twitter poll for the nerds: How many certification abbreviations do you put in your e-mail signature? [<a href="http://twitter.com/RackerHacker/status/27448088177">Permalink</a>]</p>
</blockquote>
<p>The question must have struck a nerve with folks as I had over 50 replies in less than 10-15 minutes. I expected to hear a lot of people say &ldquo;zero&rdquo;, and there were quite a few responses that didn&rsquo;t surprise me:</p>
<blockquote>
<p><a href="http://twitter.com/minter">minter</a>: @RackerHacker Zero</p>
</blockquote>
<blockquote>
<p><a href="http://twitter.com/stwange">stwange</a>: @RackerHacker none it&rsquo;s pretentious</p>
</blockquote>
<blockquote>
<p><a href="http://twitter.com/nickboldt">nickboldt</a>: @RackerHacker Zero. My cert-fu is weak.</p>
</blockquote>
<blockquote>
<p><a href="http://twitter.com/scassiba/status/27448504377">scassiba</a>: @RackerHacker none, I don&rsquo;t feel it&rsquo;s necessary to fluff up an email signature with certifications</p>
</blockquote>
<blockquote>
<p><a href="http://twitter.com/errr_">errr_</a>: @RackerHacker 0</p>
</blockquote>
<blockquote>
<p><a href="http://twitter.com/chrstphrbrwn">chrstphrbrwn</a>: @RackerHacker Zero. I don&rsquo;t use an email signature.</p>
</blockquote>
<blockquote>
<p><a href="http://twitter.com/DamianZaremba">DamianZaremba</a>: @RackerHacker none, makes you look a bit stuck up imo</p>
</blockquote>
<blockquote>
<p><a href="http://twitter.com/saiweb">saiweb</a>: @RackerHacker 0</p>
</blockquote>
<blockquote>
<p><a href="http://twitter.com/jirahcox">jirahcox</a>: @RackerHacker 0. Job title only if absolutely necessary.</p>
</blockquote>
<blockquote>
<p><a href="http://twitter.com/jtimberman">jtimberman</a>: @RackerHacker None mine are almost all expired anyway :)</p>
</blockquote>
<blockquote>
<p><a href="http://twitter.com/ckeck">ckeck</a>: @RackerHacker zero</p>
</blockquote>
<blockquote>
<p><a href="http://twitter.com/billblum">billblum</a>: @RackerHacker None.</p>
</blockquote>
<blockquote>
<p><a href="http://twitter.com/puppetmasterd">puppetmasterd</a>: @RackerHacker zero, or preferably fewer</p>
</blockquote>
<blockquote>
<p><a href="http://twitter.com/ripienaar">ripienaar</a>: @RackerHacker zero, they dont add value. Much rather link me to your github account so I can make up my own mind :)</p>
</blockquote>
<blockquote>
<p><a href="http://twitter.com/redbluemagenta">redbluemagenta</a>: @RackerHacker None. People can see for themselves through other avenues (blog, github, references) if you&rsquo;re any good.</p>
</blockquote>
<blockquote>
<p><a href="http://twitter.com/ubuntusoren">ubuntusoren</a>: @RackerHacker none</p>
</blockquote>
<p>There were a few people who disagreed:</p>
<blockquote>
<p><a href="http://twitter.com/bwwhite">bwwhite</a>: @RackerHacker Just one because that&rsquo;s all I have :) But I think 2 should be the limit. Pick the 2 most relevant to your current role</p>
</blockquote>
<blockquote>
<p><a href="http://twitter.com/jwgoerlich">jwgoerlich</a>: Generally 2. Depends on the email topic.</p>
</blockquote>
<blockquote>
<p><a href="http://twitter.com/whitenheimer">whitenheimer</a>: @RackerHacker just one, some of them aren&rsquo;t worth putting</p>
</blockquote>
<blockquote>
<p><a href="http://twitter.com/russjohnson">russjohnson</a>: @RackerHacker Currently none but have done upto 4</p>
</blockquote>
<blockquote>
<p><a href="http://twitter.com/rbp1987">rbp1987</a>: @RackerHacker I only put the most relevant or the highest level of cert that i have. Why what do you do at the moment?</p>
</blockquote>
<blockquote>
<p><a href="http://twitter.com/hotshotsphoto">hotshotsphoto</a>: @RackerHacker MCP, MCP+I, CNA MCSE ITIL Practitioner….but only when I&rsquo;m working in the IT field</p>
</blockquote>
<p>There were quite a few that were strongly worded or humorous:</p>
<blockquote>
<p><a href="http://twitter.com/rjamestaylor">rjamestaylor</a>: @RackerHacker I hate them - R Taylor, SCJP, MCP, RHCP, BSci, SAG</p>
</blockquote>
<blockquote>
<p><a href="http://twitter.com/mshuler">mshuler</a>: .@RackerHacker I regard email signatures similar to SUVs - the size is relevant to the compensation factor</p>
</blockquote>
<blockquote>
<p><a href="http://twitter.com/iota">iota</a>: @RackerHacker zero; as number of reported certifications increase, respect for sender decreases - my law #1514</p>
</blockquote>
<blockquote>
<p><a href="http://twitter.com/raykrueger">raykrueger</a>: @RackerHacker <a href="http://theoatmeal.com/comics/email">http://theoatmeal.com/comics/email</a></p>
</blockquote>
<blockquote>
<p><a href="http://twitter.com/hjv">hjv</a>: @RackerHacker I&rsquo;m Ebay A+++ Certified.</p>
</blockquote>
<blockquote>
<p><a href="http://twitter.com/unixdaemon">unixdaemon</a>: @puppetmasterd @RackerHacker I&rsquo;d love to see &ldquo;Failed my MCP due to realising 10 minutes in that it would taint my soul. Forever.&rdquo; on a CV.</p>
</blockquote>
<blockquote>
<p><a href="http://twitter.com/swimsaftereatin">swimsaftereatin</a>: @mshuler @RackerHacker And ASCII art is to the signature as a huge purple spoiler is to a pickup truck.</p>
</blockquote>
<blockquote>
<p><a href="http://twitter.com/anoopbhat">anoopbhat</a>: @RackerHacker none. unless there is a cert whose acronym is BADASS.</p>
</blockquote>
<blockquote>
<p><a href="http://twitter.com/sarahvdv">sarahvdv</a>: @RackerHacker None because I only have one and it&rsquo;s almost embarrassing to put &ldquo;CompTIA Network+ Certified&rdquo; in my signature.</p>
</blockquote>
<blockquote>
<p><a href="http://twitter.com/0x44">0x44</a>: @RackerHacker Zero. Certifications are nerd short-hand for &ldquo;Don&rsquo;t hire me.&rdquo;</p>
</blockquote>
<p>I&rsquo;m certainly not against certifications - they&rsquo;re a good way for vendors to ensure that there are trained professionals that meet a certain set of minimum knowledge levels about their product. When you hire someone with a particular certification, you should be able to assume that they have this minimum knowledge level (for most certifications).</p>
<p>However, a certification says absolutely nothing about how a job candidate has actually <em>applied these skills</em> to their previous work. For example, consider a systems administrator with a CCNA. If you ask the job applicant something like &ldquo;So, how much experience do you have working with Cisco&rdquo; for a Cisco-heavy job position and they reply that they&rsquo;ve set up a Cisco PIX a few times, but they mainly focus on Linux administration, then what is that certification worth to your company?</p>
<p>As for e-mail signatures, I&rsquo;d leave out the certifications. If you&rsquo;re sending e-mails to coworkers that you already know, there shouldn&rsquo;t any reason for you to &ldquo;fluff&rdquo; your signature with those abbreviations. They should already be familiar with your abilities and the addition of certifications to the e-mail doesn&rsquo;t add anything valuable to the e-mail itself. If you&rsquo;re sending e-mails to people you don&rsquo;t know (especially for a job), it makes your e-mail look pretentious.</p>
]]></content></item><item><title>Securing your ssh server</title><link>https://major.io/2010/10/12/securing-your-ssh-server/</link><pubDate>Tue, 12 Oct 2010 22:39:15 +0000</pubDate><guid>https://major.io/2010/10/12/securing-your-ssh-server/</guid><description>One of the most common questions that I see in my favorite IRC channel is: “How can I secure sshd on my server?” There&amp;rsquo;s no single right answer, but most systems administrators combine multiple techniques to provide as much security as possible with the least inconvenience to the end user.
Here are my favorite techniques listed from most effective to least effective:
SSH key pairs
By disabling password-based authentication and requiring ssh key pairs, you reduce the chances of compromise via a brute force attack.</description><content type="html"><![CDATA[<p>One of the most common questions that I see in <a href="irc://irc.freenode.net/slicehost">my favorite IRC channel</a> is: “How can I secure sshd on my server?” There&rsquo;s no single right answer, but most systems administrators combine multiple techniques to provide as much security as possible with the least inconvenience to the end user.</p>
<p>Here are my favorite techniques listed from most effective to least effective:</p>
<p><strong>SSH key pairs</strong></p>
<p>By disabling password-based authentication and requiring ssh key pairs, you reduce the chances of compromise via a brute force attack. This can also help you protect against weak account passwords since a valid private key is required to gain access to the server. However, a weak account password is still a big problem if you allow your users to use sudo.</p>
<p>If you&rsquo;re new to using ssh keys, there are <a href="http://sial.org/howto/openssh/publickey-auth/">many</a> <a href="http://www.debian-administration.org/articles/530">great</a> <a href="http://www.linuxquestions.org/linux/answers/Networking/Public_key_authentication_with_ssh">guides</a> that can walk you through the process.</p>
<p><strong>Firewall</strong></p>
<p>Limiting the source IP addresses that can access your server on port 22 is simple and effective. However, if you travel on vacation often or your home IP address changes frequently, this may not be a convenient way to limit access. Acquiring a server with trusted access through your firewall would make this method easier to use, but you&rsquo;d need to <a href="http://en.wikipedia.org/wiki/Recursion">consider the security of that server as well</a>.</p>
<p>The iptables rules would look something like this:</p>
<pre><code>iptables -A INPUT -j ACCEPT -p tcp --dport 22 -s 10.0.0.20
iptables -A INPUT -j ACCEPT -p tcp --dport 22 -s 10.0.0.25
iptables -A INPUT -j DROP -p tcp --dport 22
</code></pre><p><strong>Use a non-standard port</strong></p>
<p>I&rsquo;m not a big fan of <a href="http://en.wikipedia.org/wiki/Security_through_obscurity">security through obscurity</a> and it doesn&rsquo;t work well for ssh. If someone is simply scanning a subnet to find ssh daemons, you might not be seen the first time. However, if someone is targeting you specifically, changing the ssh port doesn&rsquo;t help at all. They&rsquo;ll find your ssh banner quickly and begin their attack.</p>
<p>If you prefer this method, simply adjust the <code>Port</code> configuration parameter in your sshd_config file.</p>
<p><strong>Limit users and groups</strong></p>
<p>If you have only certain users and groups who need ssh access to your server, setting user or group limits can help increase security. Consider a server which needs ssh access for developers and a manager. Adding this to to your sshd_config would allow only those users and groups to access your ssh daemon:</p>
<pre><code>AllowGroups developers
AllowUsers jsmith pjohnson asamuels
</code></pre><p>Keep in mind that any users or groups not included in the sshd_config won&rsquo;t be able to access your ssh server.</p>
<p><strong>TCP wrappers</strong></p>
<p>While <a href="http://en.wikipedia.org/wiki/TCP_Wrapper">TCP wrappers</a> are tried and true, I consider them to be a bit old-fashioned. I&rsquo;ve found that many new systems administrators may not think of TCP wrappers when they diagnose server issues and this could possibly cause delays when adjustments need to be made later.</p>
<p>If you&rsquo;re ready to use TCP wrappers to limit ssh connections, check out <a href="http://docs.redhat.com/docs/en-US/Red_Hat_Enterprise_Linux/5/html/Deployment_Guide/s1-tcpwrappers-access.html">Red Hat&rsquo;s extensive documentation</a>.</p>
<p><strong>fail2ban and denyhosts</strong></p>
<p>For those systems administrators who want to take a bit more active stance on blocking brute force attacks, there&rsquo;s always <a href="http://en.wikipedia.org/wiki/Fail2ban">fail2ban</a> or <a href="http://en.wikipedia.org/wiki/DenyHosts">denyhosts</a>. Both fail2ban and denyhosts monitor your authentication logs for repeated failures, but denyhosts can only work with your ssh daemon. You can use fail2ban with other applications like web servers and FTP servers.</p>
<p>The only downside of using these applications is that if a valid user accidentally tries to authenticate unsuccessfully multiple times, they may be locked out for a period of time. This could be a big problem if you&rsquo;re in the middle of a server emergency.</p>
<p>A quick search on Google will give you instructions on <a href="http://www.fail2ban.org/wiki/index.php/HOWTOs">fail2ban configuration</a> as well as <a href="http://denyhosts.sourceforge.net/faq.html#2_0">denyhosts configuration</a>.</p>
<p><strong>Port knocking</strong></p>
<p>Although <a href="http://en.wikipedia.org/wiki/Port_knocking">port knocking</a> is another tried and true method to prevent unauthorized access, it can be annoying to use unless you have users who are willing to jump through additional hoops. Port knocking involves a “knock” on an arbitrary port that then allows the ssh daemon to be exposed to the user who sent the original knock.</p>
<p><a href="http://www.linuxjournal.com/article/6811">Linux Journal</a> has a great article explaining how port knocking works and it provides some sample configurations as well.</p>
<p><strong>Conclusion</strong></p>
<p>The best way to secure your ssh daemon is to apply more than one of these methods to your servers. Weighing security versus convenience of access isn&rsquo;t an easy task and it will be different for every environment. Regardless of the method or methods you choose, ensure that the rest of your team is comfortable with the changes and capable of adapting to them efficiently.</p>
]]></content></item><item><title>Installing Xen 4 on Fedora 13</title><link>https://major.io/2010/09/10/installing-xen-4-on-fedora-13/</link><pubDate>Fri, 10 Sep 2010 13:56:49 +0000</pubDate><guid>https://major.io/2010/09/10/installing-xen-4-on-fedora-13/</guid><description>Installing Xen can be a bit of a challenge for a beginner and it&amp;rsquo;s made especially difficult by distribution vendors who aren&amp;rsquo;t eager to include it in their current releases. I certainly don&amp;rsquo;t blame the distribution vendors for omitting it; the code to support Xen&amp;rsquo;s privileged domain isn&amp;rsquo;t currently in upstream kernels.
However, Pasi Kärkkäinen has written a detailed walkthrough about how to get Xen 4 running on Fedora 13. Although there are quite a few steps involved, it&amp;rsquo;s worked well for me so far.</description><content type="html"><![CDATA[<p>Installing Xen can be a bit of a challenge for a beginner and it&rsquo;s made especially difficult by distribution vendors who aren&rsquo;t eager to include it in their current releases. I certainly don&rsquo;t blame the distribution vendors for omitting it; the code to support Xen&rsquo;s privileged domain isn&rsquo;t currently in upstream kernels.</p>
<p>However, <a href="http://www.xen.org/community/spotlight/pasi.html">Pasi Kärkkäinen</a> has written a <a href="http://wiki.xensource.com/xenwiki/Fedora13Xen4Tutorial">detailed walkthrough</a> about how to get Xen 4 running on Fedora 13. Although there are quite a few steps involved, it&rsquo;s worked well for me so far.</p>
]]></content></item><item><title>A nerd’s perspective on cloud hosting</title><link>https://major.io/2010/08/25/a-nerds-perspective-on-cloud-hosting/</link><pubDate>Wed, 25 Aug 2010 13:03:52 +0000</pubDate><guid>https://major.io/2010/08/25/a-nerds-perspective-on-cloud-hosting/</guid><description>Let&amp;rsquo;s go ahead and get this out of the way: The following post contains only my personal opinions. These are not the opinions of my employer and should not be considered as such. The term &amp;ldquo;cloud hosting&amp;rdquo; has become more popular over the past few years and it seems like everyone is talking about it. I&amp;rsquo;m often asked by customers and coworkers about what cloud hosting really is. Where does traditional dedicated hosting end and cloud begin?</description><content type="html"><![CDATA[<p>Let&rsquo;s go ahead and get this out of the way: <!-- raw HTML omitted -->The following post contains only my personal opinions. These are not the opinions of my employer and should not be considered as such.<!-- raw HTML omitted --></p>
<hr>
<p>The term &ldquo;cloud hosting&rdquo; has become more popular over the past few years and it seems like everyone is talking about it. I&rsquo;m often asked by customers and coworkers about what cloud hosting really is. Where does traditional dedicated hosting end and cloud begin? Do they overlap? Who needs cloud and who doesn&rsquo;t?<!-- raw HTML omitted --></p>
<p>You can&rsquo;t talk about cloud hosting without defining it first. When I think of &ldquo;cloud&rdquo;, these are the things that come to mind:</p>
<ul>
<li>quickly add/remove resources with little or no lead time</li>
<li>hosting platforms that allow for quick provisioning of highly available systems</li>
<li>self-service adjustment of tangible and intangible resources that normally require human intervention</li>
</ul>
<p>That list may seem a bit vague at first, but try to let it sink in just a bit. Hosting applications in a &ldquo;cloud&rdquo; shouldn&rsquo;t mean that you must have a virtual instance running on Xen, KVM or VMWare, and it shouldn&rsquo;t mean that you must have an account with Rackspace Cloud, Amazon EC2, or Microsoft Azure. It means that your hosting operations are highly automated and you can rapidly allocate and deallocate resources for the requirements of your current projects.</p>
<p>Consider this: a customer of a traditional dedicated hosting provider decides to take their applications and host them on one VPS at a leading commercial provider. That provider allows the customer to spin up new VM&rsquo;s in a matter of minutes and re-image the VM&rsquo;s whenever they like. Is that cloud hosting? <strong>I&rsquo;d say yes - even if it&rsquo;s one single virtual instance.</strong> That customer has moved from a hosting system with manual interventions and extended lead times to a system where they have instant control over their resources.</p>
<p>It&rsquo;s not possible to talk about what cloud is without talking about what it isn&rsquo;t.</p>
<ul>
<li><strong>Cloud is not infinitely scalable.</strong> If any provider ever claims that their solution is &ldquo;infinitely scalable&rdquo;, you should be skeptical. Regardless of the provider, everyone eventually runs out of datacenter space, servers, network bandwidth, or power. (If you know of a provider that is infinitely scalable, please let me know as I&rsquo;d love to see their facilities and review their supply chain.)</li>
<li><strong>Cloud isn&rsquo;t right for everybody.</strong> Some applications have demands that cloud hosting might not be able to meet (yet). If an application depends on proprietary hardware that is difficult to virtualize or rapidly allocate, cloud hosting is probably not the answer for that particular application.</li>
<li><strong>Cloud doesn&rsquo;t mean VPS. VPS doesn&rsquo;t mean cloud.</strong> As I said before, having a virtual private server environment is not a pre-requisite for cloud hosting. Also, not all VPS solutions fit my definition of cloud as they don&rsquo;t allow for rapid deployments and resource adjustments.</li>
</ul>
<p>It&rsquo;s important to remember that cloud hosting is a marketing term. As for the technology of cloud, it&rsquo;s what you make of it. You should be looking to reduce costs, solidify availability and increase performance every day. If the ideals of cloud hosting help you do that, it might be the right option for you.</p>
]]></content></item><item><title>Very unscientific GlusterFS benchmarks</title><link>https://major.io/2010/08/13/very-unscientific-glusterfs-benchmarks/</link><pubDate>Fri, 13 Aug 2010 20:55:24 +0000</pubDate><guid>https://major.io/2010/08/13/very-unscientific-glusterfs-benchmarks/</guid><description>I&amp;rsquo;ve been getting requests for GlusterFS benchmarks from every direction lately and I&amp;rsquo;ve been a bit slow on getting them done. You may suspect that you know the cause of the delays, and you&amp;rsquo;re probably correct. ;-)
Quite a few different sites argue that the default GlusterFS performance translator configuration from glusterfs-volgen doesn&amp;rsquo;t allow for good performance. You can find other sites which say you should stick with the defaults that come from the script.</description><content type="html"><![CDATA[<p>I&rsquo;ve been getting requests for GlusterFS benchmarks from every direction lately and I&rsquo;ve been a bit slow on getting them done. You may suspect that you know the <a href="/2010/07/14/version-2-0-has-arrived/">cause of the delays</a>, and you&rsquo;re probably correct. ;-)</p>
<p>Quite a few different sites argue that the default GlusterFS performance translator configuration from glusterfs-volgen doesn&rsquo;t allow for good performance. You can find other sites which say you should stick with the defaults that come from the script. I decided to run some simple tests to see which was true in my environment.</p>
<p>Here&rsquo;s the testbed:</p>
<ul>
<li>GlusterFS 3.0.5 running on RHEL 5.4 Xen guests with ext3 filesystems</li>
<li>one GlusterFS client and two GlusterFS servers are running in separate Xen guests</li>
<li>cluster/replicate translator is being used to keep the servers in sync</li>
<li>the instances are served by a gigabit network</li>
</ul>
<p>It&rsquo;s about time for some pretty graphs, isn&rsquo;t it?</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>The test run on the left used default stock <a href="http://pastebin.com/MAX1kWDg">client</a> and <a href="http://pastebin.com/uyE6qkZ6">server</a> volume files as they come from glusterfs-volgen. The test run on the right used a <a href="http://pastebin.com/gqMquRpB">client volume file with no performance translators</a> (the server volume file was untouched). Between each test run, the GlusterFS mount was unmounted and remounted. I repeated this process four times (for a total of five runs) and averaged the data.</p>
<p><em>You&rsquo;ll have to forgive the color mismatches and the lack of labeling on the legend (that&rsquo;s KB/sec transferred) as I&rsquo;m far from an Excel expert.</em></p>
<p>The graphs show that running without any translators at all will drastically hinder read caching in GlusterFS - exactly as I expected. Without any translators, the performance is very even across the board. Since my instances had 256MB of RAM each, their iocache translator was limited to about 51MB of cache. That&rsquo;s reflected in the graph on the left - look for the vertical red/blue divider between the 32MB and 64MB file sizes. I&rsquo;ll be playing around with that value soon to see how it can improve performance for large and small files.</p>
<p>Keep in mind that this test was very unscientific and your results may vary depending on your configuration. While I hope to have more detailed benchmarks soon, this should help some of the folks who have been asking for something basic and easy to understand.</p>
]]></content></item><item><title>One month with GlusterFS in production</title><link>https://major.io/2010/08/11/one-month-with-glusterfs-in-production/</link><pubDate>Wed, 11 Aug 2010 13:29:02 +0000</pubDate><guid>https://major.io/2010/08/11/one-month-with-glusterfs-in-production/</guid><description>As many of you might have noticed from my previous GlusterFS blog post and my various tweets, I&amp;rsquo;ve been working with GlusterFS in production for my personal hosting needs for just over a month. I&amp;rsquo;ve also been learning quite a bit from some of the folks in the #gluster channel on Freenode. On a few occasions I&amp;rsquo;ve even been able to help out with some configuration problems from other users.</description><content type="html"><![CDATA[<p>As many of you might have noticed from my <a href="/2010/05/27/glusterfs-on-the-cheap-with-rackspaces-cloud-servers-or-slicehost/">previous GlusterFS blog post</a> and my <a href="http://twitter.com/rackerhacker">various tweets</a>, I&rsquo;ve been working with GlusterFS in production for my personal hosting needs for just over a month. I&rsquo;ve also been learning quite a bit from some of the folks in the <a href="http://java.freenode.net/index.php?channel=gluster">#gluster</a> channel on <a href="http://freenode.net/">Freenode</a>. On a few occasions I&rsquo;ve even been able to help out with some configuration problems from other users.</p>
<p>There has been quite a bit of interest in GlusterFS as of late and I&rsquo;ve been inundated with questions from coworkers, other system administrators and developers. Most folks want to know about its reliability and performance in demanding production environments. I&rsquo;ll try to do my best to cover the big points in this post.</p>
<p><strong>First off, here&rsquo;s now I&rsquo;m using it in production:</strong> I have two web nodes that keep content in sync for various web sites. They each run a GlusterFS server instance and they also mount their GlusterFS share. I&rsquo;m using the <a href="http://www.gluster.com/community/documentation/index.php/Translators/cluster/replicate">replicate translator</a> to keep both web nodes in sync with client side replication.</p>
<p>Here are my impressions after a month:</p>
<p><strong>I/O speed is often tied heavily to network throughput</strong></p>
<p>This one may seem obvious, but it&rsquo;s not always true in all environments. If you deal with a lot of small files like I do, a 40mbit/sec link between the Xen guests is plenty. Adding extra throughput didn&rsquo;t add any performance to my servers. However, if you wrangle large files on your servers regularly, you may want to consider higher throughput links between your servers. I was able to push just under 900mbit/sec by using dd to create a large file within a GlusterFS mount.</p>
<p><strong>Network and I/O latency are big factors for small file performance</strong></p>
<p>If you have a busy network and the latency creeps up from time to time, you&rsquo;ll find that your small file performance will drop significantly (especially with the replicate translator). Without getting too nerdy (you&rsquo;re welcome to read the <a href="http://ftp.zresearch.com/pub/gluster/glusterfs/doc/afr.pdf">technical document on replication</a>), replication is an intensive process. When a file is accessed, the client goes around to each server node to ensure that it not only has a copy of the file being read, but that it has the <em>correct</em> copy. If a server didn&rsquo;t save a copy of a file (due to disk failure or the server being offline when the file was written), it has to be synced across the network from one of the good nodes.</p>
<p>When you write files on replicated servers, the client has to roll through the same process first. Once that&rsquo;s done, it has to lock the file, write to the change log, then do the write operation, drop the change log entries, and then unlock the file. All of those operations must be done on <em>all of the servers</em>. High latency networks will wreak havoc on this process and cause it to take longer than it should.</p>
<p>It&rsquo;s quite obvious that if you have a fast, low-latency network between your servers, slow disks can still be a problem. If the client is waiting on the server nodes' disks to write data, the read and write performance will suffer. I&rsquo;ve tested this in environments with fast networks and very busy RAID arrays. Even if the network was very underutilized, slow disks could cut performance drastically.</p>
<p><strong>Monitoring GlusterFS isn&rsquo;t easy</strong></p>
<p>When the client has communication problems with the server nodes, some weird things can happen. I&rsquo;ve seen situations where the client loses connections to the servers (see the next section on reliability) and the client mount simply hangs. In other situations, the client has been knocked offline entirely and the process is missing from the process tree by the time I logged in. Your monitoring will need to ensure that the mount is active and is responding in a timely fashion.</p>
<p>There&rsquo;s a <a href="http://www.sirgroane.net/2010/04/monitoring-gluster-with-nagios/">handy script</a> which allows you to monitor GlusterFS mounts via nagios that Ian Rogers put together. Also, you can get some historical data with <a href="http://github.com/acrollet/munin-glusterfs">acrollet&rsquo;s munin-glusterfs plugin</a>.</p>
<p><strong>GlusterFS 3.x is pretty reliable</strong></p>
<p>When I first started working with GlusterFS, I was using a version from the 2.x tree. The Fedora package maintainer hadn&rsquo;t updated the package in quite some time, but I figured it should work well enough for my needs. I found that the small file performance was lacking and the nodes often had communication issues when many files were being accessed or written simultaneously. This improved when I built my own RPMs of 3.0.4 (and later 3.0.5) and began using those instead.</p>
<p>I did some failure testing by hard cycling the server and client nodes and found some interesting results. First off, abruptly pulling clients had no effects on the other clients or the server nodes. The connection eventually timed out and the servers logged the timeout as expected.</p>
<p>Abruptly pulling servers led to some mixed results. In the 2.x branch, I saw client hangs and timeouts when I abruptly removed a server. This appears to be mostly corrected in the 3.x branch. If you&rsquo;re using replicate, it&rsquo;s important to keep in mind that the first server volume listed in your client&rsquo;s volume file is the one that will be coordinating the file and directory locking. Should that one fall offline quickly, you&rsquo;ll see a hiccup in performance for a brief moment and the next server will be used for coordinating the locking. When your original server comes back up, the locking coordination will shift back.</p>
<p><strong>Conclusion</strong></p>
<p>I&rsquo;m really impressed with how much GlusterFS can do with the simplicity of how it operates. Sure, you can get better performance and more features (sometimes) from something like Lustre or GFS2, but the amount of work required to stand up that kind of cluster isn&rsquo;t trivial. GlusterFS really only requires that your kernel have FUSE support (it&rsquo;s been in mainline kernels since 2.6.14).</p>
<p>There are some things that GlusterFS really needs in order to succeed:</p>
<ul>
<li><strong>Documentation</strong> - The current documentation is often out of date and confusing. I&rsquo;ve even found instances where the documentation contradicts itself. While there are some good technical documents about the design of some translators, they really ought to do some more work there.</li>
<li><strong>Statistics gathering</strong> - It&rsquo;s very difficult to find out what GlusterFS is doing and where it can be optimized. Profiling your environment to find your bottlenecks is nearly impossible with the 2.x and 3.x branches. It doesn&rsquo;t make it easier when some of the performance translators actually decrease performance.</li>
<li><strong>Community involvement</strong> - This ties back into the documentation part a little, but it would be nice to see more participation from Gluster employees on IRC and via the mailing lists. They&rsquo;re a little better with mailing list responses than other companies I&rsquo;ve seen, but there is still room for improvement.</li>
</ul>
<p>If you&rsquo;re considering GlusterFS for your servers but you still have more questions, feel free to leave a comment or find me on Freenode (I&rsquo;m &lsquo;rackerhacker&rsquo;).</p>
]]></content></item><item><title>Adding comments to iptables rules</title><link>https://major.io/2010/07/26/adding-comments-to-iptables-rules/</link><pubDate>Mon, 26 Jul 2010 15:00:52 +0000</pubDate><guid>https://major.io/2010/07/26/adding-comments-to-iptables-rules/</guid><description>After I wrote a recent post on best practices for iptables, I noticed that I forgot to mention comments for iptables rules. They can be extremely handy if you have some obscure rules for odd situations.
To make an iptables rule with a comment, simply add on the following arguments to the rule:
-m comment --comment &amp;quot;limit ssh access&amp;quot; Depending on your distribution, you may need to load the ipt_comment or xt_comment modules into your running kernel first.</description><content type="html"><![CDATA[<p>After I wrote a recent post on <a href="http://rackerhacker.com/2010/04/12/best-practices-iptables/">best practices for iptables</a>, I noticed that I forgot to mention comments for iptables rules. They can be extremely handy if you have some obscure rules for odd situations.</p>
<p>To make an iptables rule with a comment, simply add on the following arguments to the rule:</p>
<pre><code>-m comment --comment &quot;limit ssh access&quot;
</code></pre><p>Depending on your distribution, you may need to load the <code>ipt_comment</code> or <code>xt_comment</code> modules into your running kernel first.</p>
<p>A full iptables rule to limit ssh access would look something like this:</p>
<pre><code>iptables -A INPUT -j DROP -p tcp --dport 22 -m comment --comment &quot;limit ssh access&quot;
</code></pre>]]></content></item><item><title>Version 2.0 has arrived</title><link>https://major.io/2010/07/14/version-2-0-has-arrived/</link><pubDate>Wed, 14 Jul 2010 17:48:54 +0000</pubDate><guid>https://major.io/2010/07/14/version-2-0-has-arrived/</guid><description>As some of you may have noticed, I haven&amp;rsquo;t made as many posts lately as I normally would. It&amp;rsquo;s probably due to this little guy you see on the right.
Evan Michael was born earlier this month and although he&amp;rsquo;s had a bit of a rough start, he&amp;rsquo;s proving that he&amp;rsquo;s a strong fellow. I&amp;rsquo;ll try to pick up where I left off with my posts when we have him settled.</description><content type="html"><![CDATA[<p><img src="/wp-content/uploads/2010/07/IMG_0860.jpg" alt="1"></p>
<p>As some of you may have noticed, I haven&rsquo;t made as many posts lately as I normally would. It&rsquo;s probably due to this little guy you see on the right.</p>
<p>Evan Michael was born earlier this month and although he&rsquo;s had a bit of a rough start, he&rsquo;s proving that he&rsquo;s a strong fellow. I&rsquo;ll try to pick up where I left off with my posts when we have him settled.</p>
]]></content></item><item><title>A modern implementation and explanation of Linux Virtual Server (LVS)</title><link>https://major.io/2010/06/27/modern-implementation-and-explanation-of-linux-virtual-server-lvs/</link><pubDate>Sun, 27 Jun 2010 16:03:27 +0000</pubDate><guid>https://major.io/2010/06/27/modern-implementation-and-explanation-of-linux-virtual-server-lvs/</guid><description>Typical configuration for a proxy-type load balancer
A typical load balancing configuration using hardware devices or software implementations will be organized such that they resemble the diagram at the right. I usually call this a proxy-type load balancing solution since the load balancer proxies your request to some other nodes. The standard order of operations looks like this:
client makes a request load balancer receives the request load balancer sends request to a web node the web server sends content back to the load balancer the load balancer responds to the client If you&amp;rsquo;re not familiar with load balancing, here&amp;rsquo;s an analogy.</description><content type="html"><![CDATA[<p><img src="/wp-content/uploads/2010/06/loadbalancer-viaproxy.png" alt="/wp-content/uploads/2010/06/loadbalancer-viaproxy.png"> Typical configuration for a proxy-type load balancer</p>
<p>A typical load balancing configuration using hardware devices or software implementations will be organized such that they resemble the diagram at the right. I usually call this a proxy-type load balancing solution since the load balancer proxies your request to some other nodes. The standard order of operations looks like this:</p>
<ul>
<li>client makes a request</li>
<li>load balancer receives the request</li>
<li>load balancer sends request to a web node</li>
<li>the web server sends content back to the load balancer</li>
<li>the load balancer responds to the client</li>
</ul>
<p>If you&rsquo;re not familiar with load balancing, here&rsquo;s an analogy. Consider a fast food restaurant. When you walk up to the counter and place an order, you&rsquo;re asking the person at the counter (the load balancer) for a hamburger. The person at the counter is going to submit your order, and then a group of people (web nodes) are going to work on it. Once your hamburger (web request) is ready, your order will be given to the person at the counter and then back to you.</p>
<p>This style of organization can become a problem as your web nodes begin to scale. It requires you to ensure that your load balancers can keep up with the requests and sustain higher transfer rates that come from having more web nodes serving a greater number of requests. Imagine the fast food restaurant where you have one person taking the orders but you have 30 people working on the food. The person at the counter may be able to take orders very quickly, but they may not be able to keep up with the orders coming out of the kitchen.</p>
<p><img src="/wp-content/uploads/2010/06/loadbalancer-ipvs.png" alt="/wp-content/uploads/2010/06/loadbalancer-ipvs.png"> LVS allows for application servers<!-- raw HTML omitted --> to respond to clients directly</p>
<p>This is where <a href="http://en.wikipedia.org/wiki/Linux_Virtual_Server">Linux Virtual Server (LVS)</a> really shines. LVS operates a bit differently:</p>
<ul>
<li>client makes a request</li>
<li>load balancer receives the request</li>
<li>load balancer sends request to a web node</li>
<li>the web server sends the response <strong>directly to the client</strong></li>
</ul>
<p>The key difference is that the load balancer sends the unaltered request to the web server and the web server responds <em>directly to the client</em>. Here&rsquo;s the fast food analogy again. If you ask the person at the counter (the load balancer) for a hamburger, that person is going to take your order and give it to the kitchen staff (the web nodes) to work on it. This time around, the person at the counter is going to advise the kitchen staff that the order needs to go directly to you once it&rsquo;s complete. When your hamburger is ready, a member of the kitchen staff will walk to the counter and give it directly to you.</p>
<p>In the fast food analogy, what are the benefits? As the number of orders and kitchen staff increases, the job of the person at the counter doesn&rsquo;t drastically increase in difficulty. While that person will have to handle more orders and keep tabs on which of the kitchen staff is working on the least amount of orders, they don&rsquo;t have to worry about returning food to customers. Also, the kitchen staff doesn&rsquo;t need to waste time handing orders to the person at the counter. Instead, they can pass these orders directly to the customer that ordered them.</p>
<p>In the world of servers, this is a large benefit. Since the web servers' responses no longer pass through the load balancer, they can spend more time on what they do best: balancing traffic. This allows for smaller, lower-powered load balancing servers from the beginning. It also allows for increases in web nodes without big changes for the load balancers.</p>
<p>There are three main implementations of LVS to consider:</p>
<p><img src="/wp-content/uploads/2010/06/Lvslogo.png" alt="2"></p>
<p><strong>LVS-DR: Direct Routing</strong></p>
<p>The load balancer receives the request and sends the packet directly to a waiting real server to process. LVS-DR has the best performance, but all of your servers must be on the same network subnet and they have to be able to share the same router (with no other routing devices in between them).</p>
<p><strong>LVS-TUN: Tunneling</strong></p>
<p>This is very similar to the direct routing approach, but the packets are <a href="http://en.wikipedia.org/wiki/IP_tunnel">encapsulated</a> and sent directly to the real servers once the load balancer receives them. This removes the restriction that all of the devices must be on the same network. Thanks to encapsulation, you can use this method to load balance between multiple datacenters.</p>
<p><strong>LVS-NAT: Network Address Translation</strong></p>
<p>Using NAT for LVS yields the least performance and scaling of all of the implementation options. In this configuration, the incoming requests are rewritten so that they will be transported correctly in a NAT environment. This puts a bigger burden on the load balancer as it must rewrite the requests quickly while still keeping up with how much work is being done by each web server.</p>
<hr>
<p><strong>Looking for a Linux Virtual Server HOWTO?</strong> Stay tuned. I&rsquo;m preparing one for my next post.<!-- raw HTML omitted --></p>
]]></content></item><item><title>Reincarnation of Twitter’s realtime XMPP search term tracking with ruby</title><link>https://major.io/2010/06/17/reincarnation-of-twitters-realtime-xmpp-search-term-tracking-with-ruby/</link><pubDate>Thu, 17 Jun 2010 18:40:48 +0000</pubDate><guid>https://major.io/2010/06/17/reincarnation-of-twitters-realtime-xmpp-search-term-tracking-with-ruby/</guid><description>When Twitter was still in its early stages, you could track certain search terms in near-realtime via Jabber. It was quite popular and its performance degraded over time as more users signed up and began posting updates. Eventually, Twitter killed the jabber bot altogether. Many users have asked when it will return.
Well, it hasn&amp;rsquo;t returned, but you can build your own replacement with ruby, a jabber account, and a few gems.</description><content type="html"><![CDATA[<p>When Twitter was still in its early stages, you could track certain search terms in near-realtime <a href="http://blog.twitter.com/2006/10/use-twitter-by-instant-message.html">via Jabber</a>. It was quite popular and its performance degraded over time as more users signed up and began posting updates. Eventually, Twitter killed the jabber bot altogether. <a href="http://www.lagesse.org/twitter-and-track/">Many users have asked when it will return</a>.</p>
<p>Well, it hasn&rsquo;t returned, but you can build your own replacement with ruby, a jabber account, and a few gems. While it won&rsquo;t do everything that the original jabber bot did, you can still track tweets mentioning certain terms very quickly.</p>
<p>Here&rsquo;s how to get started:</p>
<p>First, install the <em>tweetstream</em> and <em>xmpp4r-simple</em> gems:</p>
<!-- raw HTML omitted -->
<p>Next, you&rsquo;ll need a jabber account. You&rsquo;ll probably want to make one for the exclusive use of your jabber bot. I chose to make up a quick account at <a href="http://www.chatmask.com/">ChatMask</a> for mine.</p>
<p>The last step is to drop a copy of this script on your server:</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>You&rsquo;ll want to be sure to fill in the following:</p>
<ul>
<li>your jabber bot&rsquo;s username and password</li>
<li>the username and password for the twitter account that will monitor the stream</li>
<li>the search terms you want to track</li>
<li>the destination jabber account where the messages should be sent</li>
</ul>
<p>Ensure that your jabber account has authorized the jabber bot&rsquo;s account so that you&rsquo;ll actually receive the messages. Also, Twitter is <a href="http://apiwiki.twitter.com/Streaming-API-Documentation#FilterLimiting">very strict with their streaming API tracking terms</a>. It&rsquo;s a good idea to review their <a href="http://apiwiki.twitter.com/Streaming-API-Documentation">entire Streaming API documentation</a> to ensure that you&rsquo;re not going to end up having your account temporarily or permanently blacklisted.</p>
<p>Once everything is ready to go, you can just run the script within GNU screen or via nohup. There&rsquo;s still a bit more error checking to do around jabber reconnections, but the script has run non-stop for well over two weeks at a time without a failure.</p>
]]></content></item><item><title>Parsing mdadm output with paste</title><link>https://major.io/2010/06/14/parsing-mdadm-output-with-paste/</link><pubDate>Mon, 14 Jun 2010 14:05:57 +0000</pubDate><guid>https://major.io/2010/06/14/parsing-mdadm-output-with-paste/</guid><description>My curiosity is always piqued when I find new ways to manipulate command line output in simple ways. While working on a solution to parse /proc/mdstat output, I stumbled upon the paste utility.
The man page offers a very simple description of its features:
Write lines consisting of the sequentially corresponding lines from each FILE, separated by TABs, to standard output.
Here&amp;rsquo;s an example of how it works. Let&amp;rsquo;s say you want to parse some software raid output that looks like this:</description><content type="html"><![CDATA[<p>My curiosity is always piqued when I find new ways to manipulate command line output in simple ways. While working on a solution to parse /proc/mdstat output, I stumbled upon the <a href="http://www.gnu.org/software/coreutils/manual/html_node/paste-invocation.html">paste</a> utility.</p>
<p>The <a href="http://linux.die.net/man/1/paste">man page</a> offers a very simple description of its features:</p>
<blockquote>
<p>Write lines consisting of the sequentially corresponding lines from each FILE, separated by TABs, to standard output.</p>
</blockquote>
<p>Here&rsquo;s an example of how it works. Let&rsquo;s say you want to parse some software raid output that looks like this:</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>It would be handy if we had both on one line as that would make it easier to parse with a script. Of course, you can do this with utilities like awk and tr, but paste makes it so much easier:</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>By default, paste uses tabs to separate the lines, but you can use the <code>-d</code> argument to specify any delimiter you like:</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
]]></content></item><item><title>GlusterFS on the cheap with Rackspace’s Cloud Servers or Slicehost</title><link>https://major.io/2010/05/27/glusterfs-on-the-cheap-with-rackspaces-cloud-servers-or-slicehost/</link><pubDate>Fri, 28 May 2010 00:34:10 +0000</pubDate><guid>https://major.io/2010/05/27/glusterfs-on-the-cheap-with-rackspaces-cloud-servers-or-slicehost/</guid><description>NOTE:This post is out of date and is relevant only for GlusterFS 2.x.
*High availability is certainly not a new concept, but if there&amp;rsquo;s one thing that frustrates me with high availability VM setups, it&amp;rsquo;s storage. If you don&amp;rsquo;t mind going active-passive, you can set up DRBD, toss your favorite filesystem on it, and you&amp;rsquo;re all set.If you want to go active-active, or if you want multiple nodes active at the same time, you need to use a clustered filesystem like GFS2, OCFS2 or Lustre.</description><content type="html"><![CDATA[<p><em><!-- raw HTML omitted -->NOTE:<!-- raw HTML omitted --> This post is out of date and is relevant only for GlusterFS 2.x.</em></p>
<ul>
<li>
<ul>
<li>*High availability is certainly not a new concept, but if there&rsquo;s one thing that frustrates me with high availability VM setups, it&rsquo;s storage. If you don&rsquo;t mind going active-passive, you can set up</li>
</ul>
</li>
</ul>
<p><a href="http://en.wikipedia.org/wiki/Drbd">DRBD</a>, toss your favorite filesystem on it, and you&rsquo;re all set.<!-- raw HTML omitted --></p>
<p>If you want to go active-active, or if you want multiple nodes active at the same time, you need to use a clustered filesystem like <a href="http://en.wikipedia.org/wiki/Global_File_System">GFS2</a>, <a href="http://en.wikipedia.org/wiki/OCFS">OCFS2</a> or <a href="http://en.wikipedia.org/wiki/Lustre_(file_system)">Lustre</a>. These are certainly good options to consider but they&rsquo;re not trivial to implement. They usually rely on additional systems and scripts to provide reliable <a href="http://en.wikipedia.org/wiki/Fencing_(computing)">fencing</a> and <a href="http://en.wikipedia.org/wiki/STONITH">STONITH</a> capabilities.</p>
<p>What about the rest of us who want multiple active VM&rsquo;s with simple replicated storage that doesn&rsquo;t require any additional elaborate systems? This is where <a href="http://en.wikipedia.org/wiki/GlusterFS">GlusterFS</a> really shines. GlusterFS can ride on top of whichever filesystem you prefer, and that&rsquo;s a huge win for those who want a simple solution. However, that means that it has to use <a href="http://en.wikipedia.org/wiki/Filesystem_in_Userspace">fuse</a>, and that will limit your performance.</p>
<p><strong>Let&rsquo;s get this thing started!</strong></p>
<p>Consider a situation where you want to run a WordPress blog on two VM&rsquo;s with load balancers out front. You&rsquo;ll probably want to use GlusterFS&rsquo;s replicated volume mode (RAID 1-ish) so that the same files are on both nodes all of the time. To get started, build two small Slicehost slices or Rackspace Cloud Servers. I&rsquo;ll be using Fedora 13 in this example, but the instructions for other distributions should be very similar.</p>
<p>First things first — be sure to set a new root password and update all of the packages on the system. This should go without saying, but it&rsquo;s important to remember. We can clear out the default iptables ruleset since we will make a customized set later:</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>GlusterFS communicates over the network, so we will want to ensure that traffic only moves over the private network between the instances. We will need to add the private IP&rsquo;s and a special hostname for each instance to <code>/etc/hosts</code> on both instances. I&rsquo;ll call mine <code>gluster1</code> and <code>gluster2</code>:</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>You&rsquo;re now ready to install the required packages on both instances:</p>
<!-- raw HTML omitted -->
<p>Make the directories for the GlusterFS volumes on each instance:</p>
<!-- raw HTML omitted -->
<p>We&rsquo;re ready to make the configuration files for our storage volumes. Since we want the same files on each instance, we will use the <code>--raid 1</code> option. <strong>This only needs to be run on the first node:</strong></p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>Once that&rsquo;s done, you&rsquo;ll have four new files:</p>
<ul>
<li><code>booster.fstab</code> – you won&rsquo;t need this file</li>
<li><code>gluster1-store1-export.vol</code> – server-side configuration file for the first instance</li>
<li><code>gluster2-store1-export.vol</code> – server-side configuration file for the second instance</li>
<li><code>store1-tcp.vol</code> – client side configuration file for GlusterFS clients</li>
</ul>
<p>Copy the <code>gluster1-store1-export.vol</code> file to <code>/etc/glusterfs/glusterfsd.vol</code> on your first instance. Then, copy <code>gluster2-store1-export.vol</code> to <code>/etc/glusterfs/glusterfsd.vol</code> on your second instance. The <code>store1-tcp.vol</code> should be copied to <code>/etc/glusterfs/glusterfs.vol</code> on both instances.</p>
<p>At this point, you&rsquo;re ready to start the GlusterFS servers on each instance:</p>
<!-- raw HTML omitted -->
<p>You can now mount the GlusterFS volume on both instances:</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>You should now be able to see the new GlusterFS volume in both instances:</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>As a test, you can create a file on your first instance and verify that your second instance can read the data:</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>If you remove that file on your second instance, it should disappear from your first instance as well.</p>
<p>Obviously, this is a very simple and basic implementation of GlusterFS. You can increase performance by making dedicated VM&rsquo;s just for serving data and you can adjust the default performance options when you mount a GlusterFS volume. Limiting access to the GlusterFS servers is also a good idea.</p>
<p>If you want to read more, I&rsquo;d recommend reading the <a href="http://www.gluster.com/community/documentation/index.php/GlusterFS_Technical_FAQ">GlusterFS Technical FAQ</a> and the <a href="http://www.gluster.com/community/documentation/index.php/GlusterFS_User_Guide">GlusterFS User Guide</a>.</p>
<hr>
<p><strong>Thank you for your e-mails!</strong> I&rsquo;ll be expanding on this post later with some sample benchmarks and additional tips/tricks, so please stay tuned.<!-- raw HTML omitted --></p>
]]></content></item><item><title>How to sell: a guide for technical people</title><link>https://major.io/2010/05/26/how-to-sell-a-guide-for-technical-people/</link><pubDate>Thu, 27 May 2010 02:12:39 +0000</pubDate><guid>https://major.io/2010/05/26/how-to-sell-a-guide-for-technical-people/</guid><description>I&amp;rsquo;ll admit it right now: I love engaging customers and learning more about how what we do at Rackspace can help their business or ideas take flight. Talking with customers can be a little nerve-wracking at first since you&amp;rsquo;re not always sure what their experience level is and which products they really need. However, you can get past that initial nervousness very quickly by getting an idea of what the customer needs and what they&amp;rsquo;ve tried already (that didn&amp;rsquo;t work).</description><content type="html"><![CDATA[<p>I&rsquo;ll admit it right now: I love engaging customers and learning more about how what we do at Rackspace can help their business or ideas take flight. Talking with customers can be a little nerve-wracking at first since you&rsquo;re not always sure what their experience level is and which products they really need. However, you can get past that initial nervousness very quickly by getting an idea of what the customer needs and what they&rsquo;ve tried already (that didn&rsquo;t work).</p>
<p>You may not have realized it, but I covered the most important part of selling a technical product in the first paragraph without even mentioning the word &ldquo;sell&rdquo;. <strong>That was intentional.</strong> As a technical person, you have an innate ability to interact with customers without needing to actively <em>sell</em> them the product.</p>
<p>Whenever I meet a customer at a conference, trade show, or some other relatively informal event, I try to keep a few things in mind. I&rsquo;ll share them with you:</p>
<p><strong>Learn why your customers are seeking out your product and what they really need</strong></p>
<p>It&rsquo;s pretty obvious that this step requires more listening than talking. While the customer is explaining what they need but haven&rsquo;t found, try to keep a running tally in your brain of what technologies are important to them so that you can rank your suggestions for them. Don&rsquo;t think about which product will work best for them yet - just keep keep their general requirements in mind.</p>
<p>This is also a good opportunity to relate to what they&rsquo;ve told you. If there&rsquo;s a certain solution that ended up working really well or one that failed miserably, and you&rsquo;re familiar with one of those solutions, tell them briefly about your experiences. This will re-affirm how the customer feels about that solution and it also shows them that you&rsquo;ve been in their shoes before. They&rsquo;ll also appreciate that you&rsquo;ve been listening to their concerns and looking for ways to relate to their unique situation.</p>
<p><strong>Make thoughtful production suggestions and discuss implementation</strong></p>
<p>Some folks might say this is where the selling starts, but if you&rsquo;re doing it correctly, you&rsquo;ve been selling your product and your company the whole time. This is where things can get tricky. Most technical people I&rsquo;ve met will try to avoid being pushy when suggesting a product for a customer to use, and that&rsquo;s a good idea.</p>
<p>You need to do three things: pick the right product (or group of products), explain what needs it meets, and briefly cover some example implementations. <strong>As a technical person, this is where you really shine.</strong> Interpreting the customer&rsquo;s needs and turning it into a mini technical sales pitch is a piece of cake when you know the product well and you&rsquo;ve implemented it before.</p>
<p>It&rsquo;s great to give a customer multiple options, but it&rsquo;s a bad idea to overwhelm them. If you find that you&rsquo;re talking a bit too much, there&rsquo;s no harm in offering to talk about details later during a formal meeting. You can say things like these:</p>
<ul>
<li>&ldquo;this product will meet all your needs, but if you want to save a little money, you can use this other product like this.&rdquo;</li>
<li>&ldquo;if you combine these two products, you can meet these needs and save some time, but you can just use one and set it up like this…&rdquo;</li>
<li>&ldquo;then later on, if you need to expand, you can start using this product by…&rdquo;</li>
</ul>
<p><strong>Think about the customer&rsquo;s future growth</strong></p>
<p>Even if you have products that meet your customer&rsquo;s needs, they&rsquo;re going to be concerned about what&rsquo;s going to happen down the road. What happens when they scale to a level that they can&rsquo;t even comprehend right now? I don&rsquo;t think any customer would expect you to cover all the bases, but try to think of some basic future-proofing for the customer. Even if it might involve a product that your company doesn&rsquo;t sell, just mention it.</p>
<p>Of course, there are some things that you shouldn&rsquo;t do:</p>
<ul>
<li>Don&rsquo;t overpromise or push hard about a future product.</li>
<li>Don&rsquo;t feel obligated to know the answer to every question.</li>
<li>Don&rsquo;t use words like &ldquo;infinite&rdquo;, &ldquo;forever&rdquo;, or &ldquo;perfect&rdquo;.</li>
<li>Don&rsquo;t talk about cost constantly.</li>
<li>Don&rsquo;t force a customer to choose a product, take product literature, or take your contact information.</li>
<li>Don&rsquo;t make assumptions about the customer&rsquo;s technical level, needs, or purchasing power.</li>
<li>Don&rsquo;t let it bother you if the customer isn&rsquo;t interested in your product - it&rsquo;s not personal.</li>
</ul>
<p>And that&rsquo;s about it. If you follow those three tips and avoid the things you shouldn&rsquo;t do, you&rsquo;ll get the confidence you need to engage the customer and create the beginnings of a relationship with them.</p>
]]></content></item><item><title>Idiot’s guide to OAuth logins for Twitter</title><link>https://major.io/2010/05/19/idiots-guide-to-oauth-logins-for-twitter/</link><pubDate>Thu, 20 May 2010 01:26:07 +0000</pubDate><guid>https://major.io/2010/05/19/idiots-guide-to-oauth-logins-for-twitter/</guid><description>It certainly shouldn&amp;rsquo;t be difficult, but I always have a tough time with OAuth. Twitter is dropping support for basic authentication on June 30th, 2010. I have some automated Twitter bots that need an upgrade, so I&amp;rsquo;ve been working on a quick solution to generate tokens for my scripts.
I formulated a pretty simple script using John Nunemaker&amp;rsquo;s twitter gem that will get it done manually for any scripts you have that read from or update Twitter:</description><content type="html"><![CDATA[<p>It certainly shouldn&rsquo;t be difficult, but I always have a tough time with <a href="http://en.wikipedia.org/wiki/OAuth">OAuth</a>. Twitter is <a href="http://apiwiki.twitter.com/OAuth-FAQ#WhenareyougoingtoturnoffBasicAuth">dropping support for basic authentication</a> on <a href="http://countdowntooauth.com/">June 30th, 2010</a>. I have some automated Twitter bots that need an upgrade, so I&rsquo;ve been working on a quick solution to generate tokens for my scripts.</p>
<p>I formulated a pretty simple script using <a href="http://twitter.rubyforge.org/">John Nunemaker&rsquo;s twitter gem</a> that will get it done manually for any scripts you have that read from or update Twitter:</p>
<pre><code>#!/usr/bin/ruby
require 'rubygems'
require 'twitter'

# These credentials are specific to your *application* and not your *user*
# Get these credentials from Twitter directly: http://twitter.com/apps
application_token = '[this should be the shorter one]'
application_secret = '[this should be the longer one]'

oauth = Twitter::OAuth.new(application_token,application_secret)

request_token = oauth.request_token.token
request_secret = oauth.request_token.secret
puts &quot;Request token =&gt; #{request_token}&quot;
puts &quot;Request secret =&gt; #{request_secret}&quot;
puts &quot;Authentication URL =&gt; #{oauth.request_token.authorize_url}&quot;

print &quot;Provide the PIN that Twitter gave you here: &quot;
pin = gets.chomp

oauth.authorize_from_request(request_token,request_secret,pin)
access_token = oauth.access_token.token
access_secret = oauth.access_token.secret
puts &quot;Access token =&gt; #{oauth.access_token.token}&quot;
puts &quot;Access secret =&gt; #{oauth.access_token.secret}&quot;

oauth.authorize_from_access(access_token, access_secret)
twitter = Twitter::Base.new(oauth)
puts twitter.friends_timeline(:count =&gt; 1)
</code></pre><p>When you run the script, it will give you a request token, request secret and a URL to visit. When you access the URL, you&rsquo;ll be given a PIN. Type the PIN into the prompt and you&rsquo;ll get your access token and secret. This is what you can use to continue authenticating with Twitter, so be sure to save the access token and secret.</p>
<p>From then on, you should be able to login with a script like this:</p>
<pre><code>#!/usr/bin/ruby
require 'rubygems'
require 'twitter'

application_token = '[this should be the shorter one]'
application_secret = '[this should be the longer one]'

oauth = Twitter::OAuth.new(application_token,application_secret)

oauth.authorize_from_access(access_token, access_secret)
twitter = Twitter::Base.new(oauth)
puts twitter.friends_timeline(:count =&gt; 1)
</code></pre><p>I hope this helps!</p>
]]></content></item><item><title>Legacy tty1 and block device support for Xen guests with pvops kernels</title><link>https://major.io/2010/05/14/legacy-tty1-and-block-device-support-for-xen-guests-with-pvops-kernels/</link><pubDate>Fri, 14 May 2010 13:24:34 +0000</pubDate><guid>https://major.io/2010/05/14/legacy-tty1-and-block-device-support-for-xen-guests-with-pvops-kernels/</guid><description>The discussions about the paravirt_ops, or &amp;ldquo;pvops&amp;rdquo;, support in upstream kernels at Xen Summit 2010 last month really piqued my interest.
Quite a few distribution maintainers have gone to great lengths to keep Xen domU support in their kernels and it&amp;rsquo;s been an uphill battle. Some kernels, such as Ubuntu&amp;rsquo;s linux-ec2 kernels, have patches from 2.6.18 dragged forward into 2.6.32 and even 2.6.33. It certainly can&amp;rsquo;t be enjoyable to keep dragging those patches forward into new kernel trees.</description><content type="html"><![CDATA[<p>The discussions about the <a href="http://wiki.xensource.com/xenwiki/XenParavirtOps">paravirt_ops</a>, or &ldquo;pvops&rdquo;, support in upstream kernels at <a href="http://www.xen.org/xensummit/xensummit_spring_2010.html">Xen Summit 2010</a> last month really piqued my interest.</p>
<p>Quite a few distribution maintainers have gone to great lengths to keep Xen domU support in their kernels and it&rsquo;s been an uphill battle. Some kernels, such as Ubuntu&rsquo;s <a href="http://packages.ubuntu.com/lucid/linux-ec2">linux-ec2</a> kernels, have patches from 2.6.18 dragged forward into 2.6.32 and even 2.6.33. It certainly can&rsquo;t be enjoyable to keep dragging those patches forward into new kernel trees.</p>
<p>The paravirt_ops support for Xen guests was added in 2.6.23 and continues to be included and improved in the latest kernel trees. However, there are two significant problems with these new kernels if you&rsquo;re trying to work with legacy environments:</p>
<ul>
<li>the console is on <code>hvc0</code>, not <code>tty1</code></li>
<li>block devices are now <code>/dev/xvdX</code> rather than <code>/dev/sdX</code></li>
</ul>
<p>If you only have a few guests, these changes are generally pretty easy. Switching the console just requires some changes to your inittab or upstart configurations. Changing the block device names requires changes to the guest&rsquo;s Xen configuration file and <code>/etc/fstab</code> within the guest itself.</p>
<p>Considering the <a href="http://www.rackspacecloud.com/cloud_hosting_products/servers">amount of environments</a> I work with daily at Rackspace, changing the guest configuration is definitely not an option. I needed a way to keep the console and block devices unchanged so that our customers could have a consistent experience on our infrastructure.</p>
<p>Luckily, <a href="http://blog.warma.dk/">Soren Hansen</a> offered to pitch in and a solution became apparent. Through some <a href="http://lists.xensource.com/archives/html/xen-devel/2010-05/msg00712.html">relatively small patches</a>, the legacy console and block device support was available in the latest 2.6.32 version (2.6.32.12 as of this post&rsquo;s writing).</p>
<p>So far, I&rsquo;ve tested x86_64 and i386 versions of 2.6.32.12 with the console and block device patches. It&rsquo;s gone through its paces on Xen 3.0.3, 3.1.2, 3.3.0 and 3.4.2. All revisions of Fedora, CentOS, Ubuntu, Debian, Gentoo and Arch made within the last two years are working well with the new kernels.</p>
]]></content></item><item><title>Taking a posting break</title><link>https://major.io/2010/05/05/taking-a-posting-break/</link><pubDate>Wed, 05 May 2010 23:06:43 +0000</pubDate><guid>https://major.io/2010/05/05/taking-a-posting-break/</guid><description>I&amp;rsquo;d planned to add a new post this week, but I was sidelined with recovery from a surgery. Once I&amp;rsquo;m back on my feet, I&amp;rsquo;ll get to work on the blog again.
Thanks for the e-mails and support!</description><content type="html"><![CDATA[<p>I&rsquo;d planned to add a new post this week, but I was sidelined with recovery from a surgery. Once I&rsquo;m back on my feet, I&rsquo;ll get to work on the blog again.</p>
<p>Thanks for the e-mails and support!</p>
]]></content></item><item><title>Xen Summit: Day One Highlights</title><link>https://major.io/2010/04/29/xen-summit-day-one-highlights/</link><pubDate>Thu, 29 Apr 2010 15:52:41 +0000</pubDate><guid>https://major.io/2010/04/29/xen-summit-day-one-highlights/</guid><description>I flew out to San Jose, California with two other Rackers for the Xen Summit at AMD&amp;rsquo;s headquarters. The first day of the two-day conference was very informative. Lots of people asked for some highlights, so I&amp;rsquo;ll provide those here.
Ian Pratt and Keir Fraser started off the day with an overview of Xen&amp;rsquo;s current roadmap. They talked about their largest products and where they&amp;rsquo;re going.
Open source Xen development is going strong with the recent release of Xen 4.</description><content type="html"><![CDATA[<p>I flew out to San Jose, California with two <a href="http://twitter.com/ajmesserli">other</a> <a href="http://twitter.com/h1nch">Rackers</a> for the Xen Summit at AMD&rsquo;s headquarters. The first day of the two-day conference was very informative. Lots of people asked for some highlights, so I&rsquo;ll provide those here.</p>
<p><a href="http://en.wikipedia.org/wiki/Ian_Pratt_(computer_scientist)">Ian Pratt</a> and <a href="http://www.xen.org/community/spotlight/keirfraser.html">Keir Fraser</a> started off the day with an overview of Xen&rsquo;s current roadmap. They talked about their largest products and where they&rsquo;re going.</p>
<p>Open source Xen development is going strong with the recent release of Xen 4.0 and there are some great features in the works for Xen 4.1. There should be a new credit scheduler called &ldquo;<a href="http://wiki.xensource.com/xenwiki/Credit2_Scheduler_Development">credit2</a>&rdquo; in that release. Also, the <a href="http://www.xen.org/products/cloudxen.html">Xen Cloud Platform</a> project is improving and the feature set is growing. The <a href="http://openvswitch.org/">Open vSwitch</a> was recently integrated with XCP.</p>
<p>The <a href="http://www.xen.org/products/xci.html">Xen Client Initiative</a> caught my attention quickly as it allows a user to run a very thin hypervisor on a client machine, such as a laptop or desktop computer, and then run multiple operating systems on top of that hypervisor. This would reduce the need for products like VMWare Fusion, VirtualBox or Parallels Desktop.</p>
<p>Still not impressed? Watch Ian Pratt do a quick demonstration of XCI:</p>
<p><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --></p>
<p>Tom Woller from AMD talked about some of the new hardware enhancements that allow Xen to virtualize devices more efficiently. While much of the low-level hardware discussion was a little over my head (I&rsquo;m terrible with hardware), the improvements make sense and should improve the relationship with Xen.</p>
<p><a href="http://mesl.ucsd.edu/yuvraj/">Yuvraj Agarwal</a> from the University of California San Diego talked about SleepServers. He&rsquo;s done some impressive work with Xen to reduce the energy usage of PC&rsquo;s left on when their users are no longer in the office.</p>
<p>Jonathan Ludlum covered the roadmap for Xen Cloud Platform in greater detail. This is definitely going to be a product to watch as it has a tight integration between various products, including XAPI, Open vSwitch, and the Xen hypervisor itself. It currently runs a CentOS-based userland and Jonathan said there are no plans to change it.</p>
<p>Later in the afternoon, <a href="http://www.linkedin.com/in/darnok">Konrad Wilk</a> from Oracle talked about the current status of the Xen kernels. The PVOps kernels are used with Xen 4.0 for the dom0 (and soon for the 3.4.x branch as well). The domU support has been present in PVOps code in the upstream kernels for some time. Many distribution vendors are making one kernel for bare metal and virtualized instances by simply adding PVOps support when they build their kernels. This eliminates the need for the traditional &ldquo;kernel-xen&rdquo; kernels that needed to be loaded for virtualized instances to work properly. <a href="http://www.goop.org/~jeremy/">Jeremy Fitzhardinge</a> helped Konrad answer some questions from the audience and their presentation was one of the most informative ones of the day.</p>
<p><a href="http://www.xen.org/community/spotlight/dunlap.html">George Dunlap</a> talked about the new credit2 scheduler that is due to be released with Xen 4.1 later this year. He found that the current scheduler negatively affects VM&rsquo;s running applications that have low CPU requirements but are affected greatly by higher latency. He tested this with a VM that was playing back an audio stream. When the rest of the VM&rsquo;s on the server used a lot of CPU time, the audio skipped many times. The credit2 scheduler allows for these low-CPU, latency-sensitive applications to keep running as expected without interruptions. I&rsquo;m looking forward to testing this out later this year.</p>
<p>The last presentation of the day was from <a href="http://uk.linkedin.com/in/stabellini">Stefano Stabellini</a>. He covered the work being done to simplify how applications interact with Xen. The new library, libxenlight, strives to be a common layer between client applications and the Xen hypervisor.</p>
<p>The day wrapped up with a great party at Dave and Buster&rsquo;s. We had the opportunity to meet many people from Citrix as well as other people working in the virtualization space. Overall, the first day was very informative and I&rsquo;m eager to hear some of the presentations scheduled for the second day.</p>
]]></content></item><item><title>Best practices: iptables</title><link>https://major.io/2010/04/12/best-practices-iptables/</link><pubDate>Mon, 12 Apr 2010 13:35:31 +0000</pubDate><guid>https://major.io/2010/04/12/best-practices-iptables/</guid><description>Anyone who has used iptables before has locked themselves out of a remote server at least once. It&amp;rsquo;s easily avoided, but often forgotten. Lots of people have asked me for a list of best practices for iptables firewalls and I certainly hope this post helps.
Understand how iptables operates
Before you can begin using iptables, you need to fully understand how it matches packets with chains and rules. There is a terrific diagram in Wikipedia that will make it easier to understand.</description><content type="html"><![CDATA[<p>Anyone who has used <a href="http://en.wikipedia.org/wiki/Iptables">iptables</a> before has locked themselves out of a remote server at least once. It&rsquo;s easily avoided, but often forgotten. Lots of people have asked me for a list of best practices for iptables firewalls and I certainly hope this post helps.</p>
<p><strong>Understand how iptables operates</strong></p>
<p>Before you can begin using iptables, you need to fully understand how it matches packets with chains and rules. There is a <a href="http://en.wikipedia.org/wiki/Iptables#Operational_summary">terrific diagram in Wikipedia</a> that will make it easier to understand. It&rsquo;s imperative to remember that iptables rules are read top-down until a matching rule is found. If no matching rule is found, the default policy of the chain will be applied (more on that in a moment).</p>
<p><strong>Don&rsquo;t set the default policy to DROP</strong></p>
<p>All iptables chains have a default policy setting. If a packet doesn&rsquo;t match any of the rules in a relevant chain, it will match the default policy and will be handled accordingly. I&rsquo;ve seen quite a few users set their default policy to DROP, and this can bring about some unintended consequences.</p>
<p>Consider a situation where your INPUT chain contains quite a few rules allowing traffic, and you&rsquo;ve set the default policy to DROP. Later on, another administrator logs into the server and flushes the rules (which isn&rsquo;t a good practice, either). I&rsquo;ve met quite a few good systems administrators who are unaware of the default policy for iptables chains. Your server will be completely inaccessible immediately. All of the packets will be dropped since they match the default policy in the chain.</p>
<p>Instead of using the default policy, I normally recommend making an explicit DROP/REJECT rule at the bottom of your chain that matches everything. You can leave your default policy set to ACCEPT and this should reduce the chance of blocking all access to the server.</p>
<p><strong>Don&rsquo;t blindly flush iptables rules</strong></p>
<p>Before running <code>iptables -F</code>, always check each chain&rsquo;s default policy. If the INPUT chain is set to DROP, you&rsquo;ll need to set it to ACCEPT if you want to access the server after the rules are flushed. Also, consider the security implications of your network when you clear the rules. Your services will be completely exposed and any masquerading or NAT rules will be removed.</p>
<p><strong>Remember localhost</strong></p>
<p>Lots of applications require access to the <code>lo</code> interface. Ensure that you set up your rules carefully so that the <code>lo</code> interface is not disturbed.</p>
<p><strong>Split complicated rule groups into separate chains</strong></p>
<p>Even if you&rsquo;re the only systems administrator for your particular network, it&rsquo;s important to keep your iptables rules manageable. If you have a certain subset of rules that may be a little complicated, consider breaking them out into their own chain. You can just add in a jump to that chain from your default set of chains.</p>
<p><strong>Use REJECT until you know your rules are working properly</strong></p>
<p>When you&rsquo;re writing iptables rules, you&rsquo;ll probably be testing them pretty often. One way to speed up that process is to use the REJECT target rather than DROP. You&rsquo;ll get an immediate rejection of your traffic (a TCP reset) instead of wondering if your packet is being dropped or if it&rsquo;s making it to your server at all. Once you&rsquo;re done with your testing, you can flip the rules from REJECT to DROP if you prefer.</p>
<p><em>For those folks working towards their RHCE, this is a huge help during the test. When you&rsquo;re nervous and in a hurry, the immediate packet rejection is a welcomed sight.</em></p>
<p><strong>Be stringent with your rules</strong></p>
<p>Try to make your rules as specific as possible for your needs. For example, I like to allow ICMP pings on my servers so that I can run network tests against them. I could easily toss a rule into my INPUT chain that looks like this:</p>
<!-- raw HTML omitted -->
<p>However, I don&rsquo;t want to simply allow all ICMP traffic. There have been some ICMP flaws from time to time and I&rsquo;d rather keep as low of a profile as possible. There are <a href="http://en.wikipedia.org/wiki/Internet_Control_Message_Protocol#List_of_permitted_control_messages_.28incomplete_list.29">many types of ICMP control messages</a>, but I only want to allow echo requests:</p>
<!-- raw HTML omitted -->
<p>This will allow echo requests (standard ICMP pings), but it won&rsquo;t explicitly allow any other ICMP traffic to pass through the firewall.</p>
<p><strong>Use comments for obscure rules</strong></p>
<p>If you have rules to cover edge cases that other administrators might not understand, consider using iptables comments by adding the following arguments to your rules:</p>
<!-- raw HTML omitted -->
<p>The comments will appear in the iptables output if you list the current rules. They will also appear in your saved iptables rules.</p>
<p><strong>Always save your rules</strong></p>
<p>Most distributions offer some way to save your iptables rules so that they persist through reboots. Red Hat-based distributions offer <code>/etc/init.d/iptables save</code>, but Debian and Ubuntu require some <a href="http://rackerhacker.com/2009/11/16/automatically-loading-iptables-on-debianubuntu/">manual labor</a>. An errant reboot would easily take out your unsaved rules, so save them often.</p>
]]></content></item><item><title>Adjusting tty’s in Fedora 13 with upstart</title><link>https://major.io/2010/03/26/adjusting-ttys-in-fedora-13-with-upstart/</link><pubDate>Fri, 26 Mar 2010 14:09:13 +0000</pubDate><guid>https://major.io/2010/03/26/adjusting-ttys-in-fedora-13-with-upstart/</guid><description>Fedora 13 has quite a few changes related to upstart, and one of the biggest ones is how terminals are configured. Most distributions tuck the tty configuration away in /etc/inittab, /etc/event.d/ or /etc/init/. If you want to adjust the number of tty&amp;rsquo;s in Fedora 13, you&amp;rsquo;ll need to look in /etc/sysconfig/init:
new RH6.0 bootup # verbose =&amp;gt; old-style bootup # anything else =&amp;gt; new style bootup without ANSI colors or positioning BOOTUP=color # column to start &amp;quot;[ OK ]&amp;quot; label in RES_COL=60 # terminal sequence to move to that column.</description><content type="html"><![CDATA[<p>Fedora 13 has quite a few changes related to <a href="http://en.wikipedia.org/wiki/Upstart">upstart</a>, and one of the biggest ones is how terminals are configured.  Most distributions tuck the tty configuration away in <code>/etc/inittab</code>, <code>/etc/event.d/</code> or <code>/etc/init/</code>.  If you want to adjust the number of tty&rsquo;s in Fedora 13, you&rsquo;ll need to look in <code>/etc/sysconfig/init</code>:</p>
<pre><code> new RH6.0 bootup
# verbose =&gt; old-style bootup
# anything else =&gt; new style bootup without ANSI colors or positioning
BOOTUP=color
# column to start &quot;[  OK  ]&quot; label in
RES_COL=60
# terminal sequence to move to that column. You could change this
# to something like &quot;tput hpa ${RES_COL}&quot; if your terminal supports it
MOVE_TO_COL=&quot;echo -en \\033[${RES_COL}G&quot;
# terminal sequence to set color to a 'success' color (currently: green)
SETCOLOR_SUCCESS=&quot;echo -en \\033[0;32m&quot;
# terminal sequence to set color to a 'failure' color (currently: red)
SETCOLOR_FAILURE=&quot;echo -en \\033[0;31m&quot;
# terminal sequence to set color to a 'warning' color (currently: yellow)
SETCOLOR_WARNING=&quot;echo -en \\033[0;33m&quot;
# terminal sequence to reset to the default color.
SETCOLOR_NORMAL=&quot;echo -en \\033[0;39m&quot;
# default kernel loglevel on boot (syslog will reset this)
LOGLEVEL=3
# Set to anything other than 'no' to allow hotkey interactive startup...
PROMPT=yes
# Set to 'yes' to allow probing for devices with swap signatures
AUTOSWAP=no
# What ttys should gettys be started on?
ACTIVE_CONSOLES=/dev/tty[1-6]
</code></pre><p>The very last line controls the number of tty&rsquo;s that are kept alive on your system. If you need more tty&rsquo;s, simply increase the 6 to a higher number. If you only want one terminal (which is usually what I want in Xen), just make this adjustment:</p>
<pre><code># What ttys should gettys be started on?
ACTIVE_CONSOLES=/dev/tty1
</code></pre><p>A normal <code>telinit q</code> doesn&rsquo;t seem to adjust the terminals on the fly as it did before upstart was involved. I&rsquo;m not sure if this is a bug or an intended feature. Either way, a reboot solves the problem and you should see the changes afterwards.</p>
]]></content></item><item><title>Why I’m a Racker</title><link>https://major.io/2010/03/26/why-im-a-racker/</link><pubDate>Fri, 26 Mar 2010 13:38:07 +0000</pubDate><guid>https://major.io/2010/03/26/why-im-a-racker/</guid><description>I normally try to keep my work-related items separate from this blog, but I felt that I needed to break tradition for a moment. The new Rackspace Talent site was released a few weeks ago and Michael Long asked me to write a blog post about what it means to be a Racker (that&amp;rsquo;s the term we use for employees of Rackspace).
After the post went up, I received as much feedback from people outside of Rackspace as I received from Rackers.</description><content type="html"><![CDATA[<p><a href="http://rackblogs.com/talent/rackers/why-im-a-racker/"><!-- raw HTML omitted --></a>I normally try to keep my work-related items separate from this blog, but I felt that I needed to break tradition for a moment.  The new <a href="http://www.rackertalent.com/" title="Rackspace Talent site">Rackspace Talent</a> site was released a few weeks ago and <a href="http://twitter.com/theredrecruiter" title="Twitter: theredrecruiter">Michael Long</a> asked me to write a blog post about what it means to be a Racker (that&rsquo;s the term we use for employees of Rackspace).</p>
<p>After the post went up, I received as much feedback from people outside of Rackspace as I received from Rackers.  The negative feedback I received was centered around the assertion that the post&rsquo;s content was &ldquo;fluffed&rdquo; to make the Rackspace experience seem better than it actually is.  That couldn&rsquo;t be further from the truth.</p>
<p>If you want to make comments on the post, or if you want to know more about working at Rackspace, let me know.  Although I&rsquo;m not in sales and I&rsquo;m not in recruiting, I always enjoy talking to people about using Rackspace&rsquo;s services or working for Rackspace.</p>
<p>Here&rsquo;s a link to the post: <a href="http://www.rackertalent.com/rackers/why-im-a-racker/" title="Why I'm A Racker - Major Hayden">Rackspace Talent - Why I&rsquo;m a Racker</a></p>
]]></content></item><item><title>Testing network throughput with iperf</title><link>https://major.io/2010/03/20/testing-network-throughput-with-iperf/</link><pubDate>Sat, 20 Mar 2010 21:38:07 +0000</pubDate><guid>https://major.io/2010/03/20/testing-network-throughput-with-iperf/</guid><description>When you need to measure network throughput and capacity, I haven&amp;rsquo;t found a simpler solution than iperf. There isn&amp;rsquo;t much to say about the operation of iperf — it&amp;rsquo;s a very simple application.
In short, iperf can be installed on two machines within your network. You&amp;rsquo;ll run one as a server, and one as a client. On the server side, simply run:
On the client side, run:
The client side will try to shove TCP packets through the network interface as quickly as possible for a period of 10 seconds by default.</description><content type="html"><![CDATA[<p>When you need to measure network throughput and capacity, I haven&rsquo;t found a simpler solution than <a href="http://sourceforge.net/projects/iperf/">iperf</a>. There isn&rsquo;t <a href="http://en.wikipedia.org/wiki/Iperf">much to say</a> about the operation of iperf — it&rsquo;s a very simple application.</p>
<p>In short, iperf can be installed on two machines within your network. You&rsquo;ll run one as a server, and one as a client. On the server side, simply run:</p>
<!-- raw HTML omitted -->
<p>On the client side, run:</p>
<!-- raw HTML omitted -->
<p>The client side will try to shove TCP packets through the network interface as quickly as possible for a period of 10 seconds by default. Once that&rsquo;s complete, you&rsquo;ll see a report on the server and client that will look like this:</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>The previous test was run over an 802.11n network between a wired and wireless device. The <a href="http://en.wikipedia.org/wiki/Comparison_of_wireless_data_standards#Throughput">typical downlink</a> for an 802.11n network is about 40Mbit/s, so it&rsquo;s obvious that my home network could use an adjustment.</p>
<p>You can also run bidirectional tests from the client either at the same time (<code>-d</code> flag) or one after the other (<code>-r</code> flag). The server side will keep running until you stop it, so you can leave it running and run tests from multiple locations over time. You can daemonize the server end if that makes things easier.</p>
<p>For the full list of options, refer to <a href="http://staff.science.uva.nl/~jblom/gigaport/tools/man/iperf.html">iperf&rsquo;s man page</a>.</p>
]]></content></item><item><title>SIGTERM vs. SIGKILL</title><link>https://major.io/2010/03/18/sigterm-vs-sigkill/</link><pubDate>Thu, 18 Mar 2010 13:25:59 +0000</pubDate><guid>https://major.io/2010/03/18/sigterm-vs-sigkill/</guid><description>Sending signals to processes using kill on a Unix system is not a new topic for most systems administrators, but I&amp;rsquo;ve been asked many times about the difference between kill and kill -9.
Anytime you use kill on a process, you&amp;rsquo;re actually sending the process a signal (in almost all situations – I&amp;rsquo;ll get into that soon). Standard C applications have a header file that contains the steps that the process should follow if it receives a particular signal.</description><content type="html"><![CDATA[<p>Sending signals to processes using <a href="http://en.wikipedia.org/wiki/Kill_(command)">kill</a> on a Unix system is not a new topic for most systems administrators, but I&rsquo;ve been asked many times about the difference between <code>kill</code> and <code>kill -9</code>.</p>
<p>Anytime you use <code>kill</code> on a process, you&rsquo;re actually sending the process a signal (in almost all situations – I&rsquo;ll get into that soon). Standard C applications have a <a href="http://en.wikipedia.org/wiki/Signal.h">header file</a> that contains the steps that the process should follow if it receives a particular signal. You can get an entire list of the available signals on your system by checking the man page for <code>kill</code>.</p>
<p>Consider a command like this:</p>
<!-- raw HTML omitted -->
<p>This would send a signal called <a href="http://en.wikipedia.org/wiki/SIGTERM">SIGTERM</a> to the process. Once the process receives the notice, a few different things can happen:</p>
<ul>
<li>the process may stop immediately</li>
<li>the process may stop after a short delay after cleaning up resources</li>
<li>the process may keep running indefinitely</li>
</ul>
<p>The application can determine what it wants to do once a SIGTERM is received. While most applications will clean up their resources and stop, some may not. An application may be configured to do something completely different when a SIGTERM is received. Also, if the application is in a bad state, such as waiting for disk I/O, it may not be able to act on the signal that was sent.</p>
<p>Most system administrators will usually resort to the more abrupt signal when an application doesn&rsquo;t respond to a SIGTERM:</p>
<!-- raw HTML omitted -->
<p>The <code>-9</code> tells the <code>kill</code> command that you want to send signal #9, which is called <a href="http://en.wikipedia.org/wiki/SIGKILL">SIGKILL</a>. With a name like that, it&rsquo;s obvious that this signal carries a little more weight.</p>
<p>Although SIGKILL is defined in the same signal header file as SIGTERM, it cannot be ignored by the process. In fact, the process isn&rsquo;t even made aware of the SIGKILL signal since the signal goes straight to <!-- raw HTML omitted -->the kernel<!-- raw HTML omitted --> init. At that point, init will stop the process. The process never gets the opportunity to catch the signal and act on it.</p>
<p>However, the kernel may not be able to successfully kill the process in some situations. If the process is waiting for network or disk I/O, the kernel won&rsquo;t be able to stop it. <a href="http://en.wikipedia.org/wiki/Zombie_process">Zombie processes</a> and processes caught in an <a href="http://en.wikipedia.org/wiki/Uninterruptible_sleep">uninterruptible sleep</a> cannot be stopped by the kernel, either. A reboot is required to clear those processes from the system.</p>
]]></content></item><item><title>Rackspace Cloud Tech Podcast Episode 2</title><link>https://major.io/2010/03/08/rackspace-cloud-tech-podcast-episode-2/</link><pubDate>Tue, 09 Mar 2010 01:51:39 +0000</pubDate><guid>https://major.io/2010/03/08/rackspace-cloud-tech-podcast-episode-2/</guid><description>I participated in a podcast for the Rackspace Cloud with Robert Collazo last week. We covered some important topics including network security and convenient deployment tools.</description><content type="html"><![CDATA[<p><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --></p>
<p>I participated in a <a href="http://www.rackspacecloud.com/blog/2010/03/05/tech-cloud-cast-episode-2/">podcast</a> for the <a href="http://rackspacecloud.com/">Rackspace Cloud</a> with <a href="http://twitter.com/rcollazo">Robert Collazo</a> last week. We covered some important topics including network security and convenient deployment tools.</p>
]]></content></item><item><title>Sticky shift key with synergy in Fedora 12</title><link>https://major.io/2010/03/03/sticky-shift-key-with-synergy-in-fedora-12/</link><pubDate>Thu, 04 Mar 2010 02:44:12 +0000</pubDate><guid>https://major.io/2010/03/03/sticky-shift-key-with-synergy-in-fedora-12/</guid><description>My synergy setup at work is relatively simple. I have a MacBook Pro running Snow Leopard that acts as a synergy server and a desktop running Fedora 12 as a synergy client. On the Mac, I use SynergyKM to manage the synergy server. The Fedora box uses my gdm strategy for starting synergy at the login screen and in GNOME.
I kept having an issue where the shift key would become stuck regardless of the settings I set for the client or server.</description><content type="html"><![CDATA[<p>My synergy setup at work is relatively simple. I have a MacBook Pro running Snow Leopard that acts as a synergy server and a desktop running Fedora 12 as a synergy client. On the Mac, I use SynergyKM to manage the synergy server. The Fedora box uses <a href="http://rackerhacker.com/2008/07/30/automatically-starting-synergy-in-gdm-in-ubuntufedora/">my gdm strategy</a> for starting synergy at the login screen and in GNOME.</p>
<p>I kept having an issue where the shift key would become stuck regardless of the settings I set for the client or server. The  <code>halfDuplexCapsLock</code> <a href="http://synergy2.sourceforge.net/configuration.html">configuration option</a> had no effect. After installing <code>xkeycaps</code>, I found that both shift keys were getting stuck if I brought the mouse back and forth between Mac and Fedora twice.</p>
<p>I decided to run a test. I started the client with the debug argument and moved the mouse to my Fedora box. At that point, I pressed the letter &lsquo;a&rsquo; and saw:</p>
<pre><code>DEBUG1: CXWindowsKeyState.cpp,195:   032 (00000000) up
DEBUG1: CXWindowsKeyState.cpp,195:   03e (00000000) up
DEBUG1: CXWindowsKeyState.cpp,195:   026 (00000000) down
DEBUG1: CXWindowsKeyState.cpp,195:   032 (00000000) down
DEBUG1: CXWindowsKeyState.cpp,195:   03e (00000000) down
DEBUG1: CXWindowsKeyState.cpp,195:   026 (00000000) up
</code></pre><p>I brought the mouse back to the Mac and then back to Fedora. I pressed &lsquo;a&rsquo; again and saw:</p>
<pre><code>DEBUG1: CXWindowsKeyState.cpp,195:   026 (00000000) down
DEBUG1: CXWindowsKeyState.cpp,195:   026 (00000000) up
DEBUG1: CXWindowsKeyState.cpp,195:   026 (00000000) down
DEBUG1: CXWindowsKeyState.cpp,195:   026 (00000000) up
</code></pre><p>After dumping the keyboard layout with <code>xmodmap</code> I found the keys that corresponded with the key numbers:</p>
<ul>
<li>032 - Left shift</li>
<li>03e - Right shift</li>
<li>026 - a</li>
</ul>
<p>If I tapped the left shift, I could clear the key press, but I couldn&rsquo;t clear the right shift key (it was stuck down according to Fedora&rsquo;s X server). When I hooked up a physical keyboard and mouse, I was able to use them normally without any keybinding problems.</p>
<p><!-- raw HTML omitted -->The root cause:<!-- raw HTML omitted --> When synergy started in <code>/etc/gdm/PreSession/Default</code> after the gdm login, the keyboard layout wasn&rsquo;t set up properly. The X server was setting up the keyboard layout later in the startup process and this confusion caused the shift keys to get stuck. Fedora 12 uses evdev to probe for keyboards during X&rsquo;s startup and eventually settles on a default layout if none are explicitly defined.</p>
<p><!-- raw HTML omitted -->The fix:<!-- raw HTML omitted --> I added the synergy startup to the GNOME startup items and it works flawlessly.</p>
]]></content></item><item><title>Private network interfaces: the forgotten security hole</title><link>https://major.io/2010/03/01/private-network-interfaces-the-forgotten-security-hole/</link><pubDate>Tue, 02 Mar 2010 00:55:07 +0000</pubDate><guid>https://major.io/2010/03/01/private-network-interfaces-the-forgotten-security-hole/</guid><description>Regardless of the type of hosting you&amp;rsquo;re using - dedicated or cloud - it&amp;rsquo;s important to take network interface security seriously. Most often, threats from the internet are the only ones mentioned. However, if you share a private network with other customers, you have just as much risk on that interface.
Many cloud providers allow you access to a private network environment where you can exchange data with other instances or other services offered by the provider.</description><content type="html"><![CDATA[<p>Regardless of the type of hosting you&rsquo;re using - dedicated or cloud - it&rsquo;s important to take network interface security seriously. Most often, threats from the internet are the only ones mentioned. However, if you share a private network with other customers, you have just as much risk on that interface.</p>
<p>Many cloud providers allow you access to a private network environment where you can exchange data with other instances or other services offered by the provider. The convenience of this access comes with a price: other instances can access your instance on the private network just as easily as they could on the public interface.</p>
<p>Here are some security tips for your private interfaces:</p>
<p><strong>Disable the private interface</strong></p>
<p>This one is pretty simple. If you have only one instance or server, and you don&rsquo;t need to communicate privately with any other instances, just disable the interface. Remember to configure your networking scripts to leave the interface disabled after reboots.</p>
<p><strong>Use packet filtering</strong></p>
<p>The actual mechanism will vary based on your operating system, but filtering packets is the one of the simplest ways to secure your private interface. You can take some different approaches with them, but I find the easiest method is to allow access from your other instances and reject all other traffic.</p>
<p>For additional security, you can limit access based on ports as well as source IP addresses. This could prevent an attacker from having easy access to your other instances if they&rsquo;re able to break into one of them.</p>
<p><strong>Configure your daemons to listen on the appropriate interfaces</strong></p>
<p>If there are services that don&rsquo;t need to be listening on the private network, don&rsquo;t allow them to listen on your private interface. For example, MySQL might need to listen on the private interface so the web server can talk to it, but apache won&rsquo;t need to listen on the private interface. This reduces the profile of your instance on the private network and makes it a less likely target for attack.</p>
<p><strong>Use hosts.allow and hosts.deny</strong></p>
<p>Many new systems administrators forget about how handy tcpwrappers can be for limiting access. If your firewall is down in error, host.allow and hosts.deny could be an extra layer of protection. It&rsquo;s important to ensure that the daemons you are attempting to control are build with tcpwrappers support. Daemons like sshd support it, but apache and MySQL do not.</p>
<p><strong>Encrypt all traffic on the private network</strong></p>
<p>Just because it&rsquo;s called a &ldquo;private&rdquo; network doesn&rsquo;t mean that your traffic can traverse the network privately. You should always err on the side of caution and encrypt all traffic traversing the private network. You can use ssh tunnels, stunnel, or the built-in SSL features found in most daemons.</p>
<p>This also brings up an important point: <strong>you should know how your provider&rsquo;s private network works</strong>. Are there safeguards to prevent sniffing? Could someone else possibly ARP spoof your instance&rsquo;s private IP addresses? Is your private network&rsquo;s subnet shared among many customers?</p>
<p>With all of that said, it&rsquo;s also very important to have proper change control policies so that administrators working after you are fully aware of the security measures in place and why they are important. This will ensure that all of the administrators on your instances will understand the security of the system and they should be able to make sensible adjustments later for future functionality.</p>
]]></content></item><item><title>System Administration Inspiration: If it’s broken, break it a little more</title><link>https://major.io/2010/02/28/system-administration-inspiration-if-its-broken-break-it-a-little-more/</link><pubDate>Sun, 28 Feb 2010 16:47:16 +0000</pubDate><guid>https://major.io/2010/02/28/system-administration-inspiration-if-its-broken-break-it-a-little-more/</guid><description>Earlier this year, I started a series of posts to encourage systems administrators to refine their troubleshooting abilities. This is the second post in that series.
Almost every system administrator has found themselves in a situation where they&amp;rsquo;re confronted with a server which has a problem. However, if you&amp;rsquo;re not the primary administrator for the server, you may not always know what has changed recently or you may not be aware of changes in the server&amp;rsquo;s environment.</description><content type="html"><![CDATA[<p>Earlier this year, <a href="http://rackerhacker.com/2010/01/03/a-new-year-system-administrator-inspiration/">I started a series of posts</a> to encourage systems administrators to refine their troubleshooting abilities. This is the second post in that series.</p>
<p>Almost every system administrator has found themselves in a situation where they&rsquo;re confronted with a server which has a problem. However, if you&rsquo;re not the primary administrator for the server, you may not always know what has changed recently or you may not be aware of changes in the server&rsquo;s environment. In these situations, if the fix isn&rsquo;t obvious, try going through these steps:</p>
<p><strong>Localize the problem to a specific daemon or service</strong></p>
<p>In the case of a problem where a website isn&rsquo;t loading properly, is it a problem with the web server itself? Could something other than the actual web server daemon be having an issue?</p>
<p>As an example, consider a ruby on rails application which runs through apache&rsquo;s mod_proxy_balancer and queries data from MySQL. If any of those individual puzzle pieces were not functioning correctly, you&rsquo;d get a different result. A downed MySQL instance could make the application throw errors or appear to be unresponsive. If the mongrel cluster had failed, apache might be returning internal server errors. Your browser might return a connection refused if apache was down. These are all relatively easy to determine.</p>
<p>What if you are unable to determine which daemon is causing the problem?</p>
<p><strong>If it&rsquo;s broken, break it a little more</strong></p>
<p>Let&rsquo;s say that you&rsquo;ve reviewed the process list and all of the appropriate daemons appear to be running. However, the website is still not loading properly. What do you do? Bring down a service and try again. Did something change? Did a new error appear? If not, bring that daemon back up and try taking down one of the other ones.</p>
<p>I&rsquo;ve also had some good results by making small adjustments in the web server&rsquo;s configuration file. If you have a virtual host that isn&rsquo;t returning the correct data, try commenting it out temporarily. For rewrite rules, try removing them temporarily or strip them down to a more basic form. Test again, and then begin adding lines back incrementally. As much as a single period or quotation mark can derail a perfectly good set of rewrite rules.</p>
<p>In short - try to think outside the box when you&rsquo;re troubleshooting a difficult issue on an unfamiliar system. Always remember to back up your configurations before making changes and ensure your daemons will start properly if you bring them down.</p>
]]></content></item><item><title>MySQL: The total number of locks exceeds the lock table size</title><link>https://major.io/2010/02/16/mysql-the-total-number-of-locks-exceeds-the-lock-table-size-2/</link><pubDate>Tue, 16 Feb 2010 18:00:29 +0000</pubDate><guid>https://major.io/2010/02/16/mysql-the-total-number-of-locks-exceeds-the-lock-table-size-2/</guid><description>If you&amp;rsquo;re running an operation on a large number of rows within a table that uses the InnoDB storage engine, you might see this error:
ERROR 1206 (HY000): The total number of locks exceeds the lock table size
MySQL is trying to tell you that it doesn&amp;rsquo;t have enough room to store all of the row locks that it would need to execute your query. The only way to fix it for sure is to adjust innodb_buffer_pool_size and restart MySQL.</description><content type="html"><![CDATA[<p>If you&rsquo;re running an operation on a large number of rows within a table that uses the InnoDB storage engine, you might see this error:</p>
<p><code>ERROR 1206 (HY000): The total number of locks exceeds the lock table size</code></p>
<p>MySQL is trying to tell you that it doesn&rsquo;t have enough room to store all of the row locks that it would need to execute your query. The only way to fix it for sure is to adjust <code>innodb_buffer_pool_size</code> and restart MySQL. By default, this is set to only 8MB, which is too small for anyone who is using InnoDB to do anything.</p>
<p><strong>If you need a temporary workaround,</strong> reduce the amount of rows you&rsquo;re manipulating in one query. For example, if you need to delete a million rows from a table, try to delete the records in chunks of 50,000 or 100,000 rows. If you&rsquo;re inserting many rows, try to insert portions of the data at a single time.</p>
<p>Further reading:</p>
<ul>
<li><a href="http://bugs.mysql.com/bug.php?id=15667">MySQL Bug #15667 - The total number of locks exceeds the lock table size</a></li>
<li><a href="http://mrothouse.wordpress.com/2006/10/20/mysql-error-1206/">MySQL Error 1206 » Mike R&rsquo;s Blog</a></li>
</ul>
]]></content></item><item><title>WordPress + W3 Total Cache + MaxCDN How-To</title><link>https://major.io/2010/02/13/wordpress-w3-total-cache-maxcdn/</link><pubDate>Sun, 14 Feb 2010 03:56:30 +0000</pubDate><guid>https://major.io/2010/02/13/wordpress-w3-total-cache-maxcdn/</guid><description>It&amp;rsquo;s no secret that I&amp;rsquo;m a big fan of WordPress as a blog and CMS platform. While it does have its problems, it&amp;rsquo;s relatively simple to set up, it&amp;rsquo;s extensible, and - when properly configured - it has great performance. The WP Super Cache plugin has been a staple on my WordPress blogs for quite some time and it has solved almost all of my performance problems.
However, when you load up quite a few plugins or a heavy theme, the performance will dip due to the increased number of stylesheets, javascript files, and images.</description><content type="html"><![CDATA[<p>It&rsquo;s no secret that I&rsquo;m a big fan of <a href="http://wordpress.org/">WordPress</a> as a blog and CMS platform. While it does have its problems, it&rsquo;s relatively simple to set up, it&rsquo;s extensible, and - when properly configured - it has great performance. The <a href="http://wordpress.org/extend/plugins/wp-super-cache/">WP Super Cache</a> plugin has been a staple on my WordPress blogs for quite some time and it has solved almost all of my performance problems.</p>
<p>However, when you load up quite a few plugins or a heavy theme, the performance will dip due to the increased number of stylesheets, javascript files, and images. You can compress and combine the stylesheets and javascript to decrease load times, but this may not get the performance to a level you like.</p>
<p>I was in this situation and I found a great solution: the <a href="http://wordpress.org/extend/plugins/w3-total-cache/">W3 Total Cache</a> plugin and the <a href="http://www.maxcdn.com/">MaxCDN</a> service.</p>
<p>To get started, <a href="http://www.maxcdn.com/">visit MaxCDN&rsquo;s site</a> and set up an account. Their current promotion gives you 1TB of CDN bandwidth for one year for $10 (regularly $99). Once you sign up, do the following:</p>
<ul>
<li>Click <strong>Manage Zones</strong></li>
<li>Click <strong>Create pull zone</strong></li>
</ul>
<p>At this point, you&rsquo;ll see a list of form fields to complete:</p>
<ul>
<li>Enter an alias for the pull zone name</li>
<li>The origin server URL is the URL that&rsquo;s normally used to access your site (i.e. rackerhacker.com)</li>
<li>The custom CDN domain is the URL you want to use for your CDN (i.e. cdn.rackerhacker.com)</li>
<li>The label can be anything you&rsquo;d like to use to remember which zone is which</li>
<li>Enabling compression is generally a good idea</li>
</ul>
<p>Once you save the zone, MaxCDN will give you a new domain name. You&rsquo;ll want to create a CNAME record that points from your CDN URL (for me, that&rsquo;s cdn.rackerhacker.com) to the really long URL that MaxCDN provides.</p>
<p><!-- raw HTML omitted -->STOP HERE:<!-- raw HTML omitted --> Ensure that all of your DNS servers are replying with the CNAME record before you continue with the W3 Total Cache installation and CDN setup. If you proceed without waiting for that, some of your blog&rsquo;s visitors will get errors when they try to load content via your CDN domain.</p>
<p>You&rsquo;re ready for W3 Total Cache now. Install the plugin within your WordPress installation and activate it. Hop into the settings for the plugin and make these adjustments:</p>
<ul>
<li>Enable <strong>Page Caching</strong> and set it to <strong>Disk (enhanced)</strong></li>
<li>Enable <strong>Minify</strong> and set it to <strong>Disk</strong></li>
<li>Enable <strong>Database Caching</strong> and set it to <strong>Disk</strong></li>
<li>Leave the CDN disabled for now, but flip the <strong>CDN Type</strong> to <strong>Origin Pull (Mirror)</strong></li>
<li>Press <strong>Save changes</strong></li>
</ul>
<p>Click <strong>CDN Settings</strong> at the top of the page and configure the CDN:</p>
<ul>
<li>Enter your CDN domain (for me, it&rsquo;s cdn.rackerhacker.com) in the top form field</li>
<li>Leave the other options as they are by default and click <strong>Save changes</strong></li>
</ul>
<p>W3 Total Cache should prompt you to clear out your page cache, and that would be recommended at this step. If you fully reload your blog&rsquo;s main page in your browser (may require you to hold SHIFT while you click reload/refresh) and check the page source, you should see your CDN URL appear for some of the javascript or CSS files.</p>
<p>You may discover that some CSS files, stylesheets, or images aren&rsquo;t being loaded via the CDN automatically. Luckily, that&rsquo;s an easy fix. Under the <strong>Minify Settings</strong> section of the W3 Total Cache plugin settings, scroll to the very bottom. Add in your javascript or CSS files via the form fields at the bottom and the plugin should handle the minifying (is that even a word?) and the CDN URL rewriting for you.</p>
<p>Further reading:</p>
<ul>
<li><a href="http://www.maxcdn.com/wordpress-cdn-module.php">MaxCDN WordPress Integration Overview</a></li>
<li><a href="http://wordpress.org/extend/plugins/w3-total-cache/">W3 Total Cache plugin page at wordpress.org</a></li>
<li><a href="http://www.w3-edge.com/wordpress-plugins/w3-total-cache/">W3 Total Cache main site</a></li>
</ul>
]]></content></item><item><title>MySQL: The total number of locks exceeds the lock table size</title><link>https://major.io/2010/01/29/mysql-the-total-number-of-locks-exceeds-the-lock-table-size/</link><pubDate>Fri, 29 Jan 2010 13:12:21 +0000</pubDate><guid>https://major.io/2010/01/29/mysql-the-total-number-of-locks-exceeds-the-lock-table-size/</guid><description>This problem has cropped up for me a few times, but I&amp;rsquo;ve always forgotten to make a post about it. If you&amp;rsquo;re working with a large InnoDB table and you&amp;rsquo;re updating, inserting, or deleting a large volume of rows, you may stumble upon this error:
InnoDB stores its lock tables in the main buffer pool. This means that the number of locks you can have at the same time is limited by the innodb_buffer_pool_size variable that was set when MySQL was started.</description><content type="html"><![CDATA[<p>This problem has cropped up for me a few times, but I&rsquo;ve always forgotten to make a post about it. If you&rsquo;re working with a large InnoDB table and you&rsquo;re updating, inserting, or deleting a large volume of rows, you may stumble upon this error:</p>
<!-- raw HTML omitted -->
<p>InnoDB stores its lock tables in the main buffer pool. This means that the number of locks you can have at the same time is limited by the <code>innodb_buffer_pool_size</code> variable that was set when MySQL was started. By default, MySQL leaves this at 8MB, which is pretty useless if you&rsquo;re doing anything with InnoDB on your server.</p>
<p>Luckily, the fix for this issue is very easy: adjust <code>innodb_buffer_pool_size</code> to a more reasonable value. However, that fix does require a restart of the MySQL daemon. There&rsquo;s simply no way to adjust this variable on the fly (with the current stable MySQL versions as of this post&rsquo;s writing).</p>
<p>Before you adjust the variable, make sure that your server can handle the additional memory usage. The <code>innodb_buffer_pool_size</code> variable is a server wide variable, not a per-thread variable, so it&rsquo;s shared between all of the connections to the MySQL server (like the query cache). If you set it to something like 1GB, MySQL won&rsquo;t use all of that up front. As MySQL finds more things to put in the buffer, the memory usage will gradually increase until it reaches 1GB. At that point, the oldest and least used data begins to get pruned when new data needs to be present.</p>
<p><strong>So, you need a workaround without a MySQL restart?</strong></p>
<p>If you&rsquo;re in a pinch, and you need a workaround, break up your statements into chunks. If you need to delete a million rows, try deleting 5-10% of those rows per transaction. This may allow you to sneak under the lock table size limitations and clear out some data without restarting MySQL.</p>
<p>To learn more about InnoDB&rsquo;s parameters, visit the <a href="http://dev.mysql.com/doc/refman/5.0/en/innodb-parameters.html">MySQL documentation</a>.</p>
]]></content></item><item><title>Switching between audible and visual bells in screen</title><link>https://major.io/2010/01/21/switching-between-audible-and-visual-bells-in-screen/</link><pubDate>Thu, 21 Jan 2010 14:37:09 +0000</pubDate><guid>https://major.io/2010/01/21/switching-between-audible-and-visual-bells-in-screen/</guid><description>About a year ago, I was introduced to the joys of using irssi and screen to access irc servers. Before that time, I&amp;rsquo;d usually used graphical clients like Colloquy, and I always enjoyed getting Growl notifications when someone mentioned a word or string that I set up as a trigger.
Once I started using irssi in screen, I found that the visual bell in screen didn&amp;rsquo;t get my attention quickly. Luckily, someone in the #slicehost channel let me know about screen&amp;rsquo;s audible bell.</description><content type="html"><![CDATA[<p>About a year ago, I was introduced to the joys of using <a href="http://www.irssi.org/">irssi</a> and <a href="http://www.gnu.org/software/screen/">screen</a> to access irc servers. Before that time, I&rsquo;d usually used graphical clients like <a href="http://colloquy.info/">Colloquy</a>, and I always enjoyed getting <a href="http://growl.info/">Growl</a> notifications when someone mentioned a word or string that I set up as a trigger.</p>
<p>Once I started using irssi in screen, I found that the visual bell in screen didn&rsquo;t get my attention quickly. Luckily, someone in the <a href="irc://irc.freenode.net/slicehost">#slicehost</a> channel let me know about screen&rsquo;s audible bell. You can flip between the visual and audible bell with <strong>CTRL-A</strong> and then <strong>CTRL-G</strong>. If you keep repeating that key combination, you&rsquo;ll switch back and forth between the two (with a status update at the bottom left).</p>
<p>You can also set up your visual bell configuration in your .screenrc via some configuration parameters:</p>
<pre><code>vbell [on|off]
vbell_msg [message]
vbellwait sec
</code></pre>]]></content></item><item><title>Crash course in dsh</title><link>https://major.io/2010/01/20/crash-course-in-dsh/</link><pubDate>Wed, 20 Jan 2010 14:47:56 +0000</pubDate><guid>https://major.io/2010/01/20/crash-course-in-dsh/</guid><description>Thanks to a recommendation from [Michael][1] and [Florian][2], I&amp;rsquo;ve been using [dsh][3] with a lot of success for quite some time. In short, dsh is a small application which will allow you to run commands across many servers via ssh very quickly.
You may be wondering: &amp;ldquo;Why not just use ssh in a for loop?&amp;rdquo; Sure, you could do something like this in bash:
But dsh allows you to do this: In addition, dsh allows you to run the commands concurrently (-c) or one after the other (-w).</description><content type="html"><![CDATA[<p>Thanks to a recommendation from [Michael][1] and [Florian][2], I&rsquo;ve been using [dsh][3] with a lot of success for quite some time. In short, dsh is a small application which will allow you to run commands across many servers via ssh very quickly.</p>
<p>You may be wondering: &ldquo;Why not just use ssh in a for loop?&rdquo; Sure, you could do something like this in bash:</p>
<pre><code>

But dsh allows you to do this:

</code></pre><p>In addition, dsh allows you to run the commands concurrently (-c) or one after the other (-w). You can tell it to prepend each line with the machine&rsquo;s name (-M) or it can omit the machine name from the output (-H). If you need to pass extra options, such as which ssh key to use, or an alternative port, you can do that as well (-o). All of these command line options can be tossed into a configuration file if you have a default set of options you prefer.</p>
<p>Another thing that makes dsh more powerful is the groups feature. Let&rsquo;s say you have three groups of servers - some are in California, others in Texas, and still others in New York. You could make three files for the groups:</p>
<ul>
<li>~/.dsh/group/california</li>
<li>~/.dsh/group/texas</li>
<li>~/.dsh/group/newyork</li>
</ul>
<p>Inside each file, you just need to list the hosts one after the other. Here&rsquo;s the <code>~/.dsh/group/texas</code> group file:</p>
<pre><code>db1.tx.mydomain.com
db2.tx.mydomain.com
web1.tx.mydomain.com
web2.tx.mydomain.com
#web3.tx.mydomain.com
</code></pre><p>As you can see, dsh handles comments in the hosts file. In the above example, the web3 server will be skipped since it&rsquo;s prepended with a comment. Let&rsquo;s say you want to check the uptime on all of the Texas servers as fast as possible:</p>
<pre><code>

That will run the `uptime` command on all of the servers in the Texas group concurrently. If you need to run it on two groups at once, just pass another group (eg. `-g texas -g california`) as an argument. You can also run the commands against all of your groups (-a).

The dsh command can really help you if you need to gather information or run simple commands on many remote servers. If you find yourself using it often for systems management, you may want to consider something like [puppet][4].

 [1]: http://twitter.com/mshuler
 [2]: http://twitter.com/pandemicsyn
 [3]: http://www.netfort.gr.jp/~dancer/software/dsh.html.en
 [4]: http://reductivelabs.com/products/puppet/
</code></pre>]]></content></item><item><title>Change the escape keystrokes in screen</title><link>https://major.io/2010/01/07/change-the-escape-keystrokes-in-screen/</link><pubDate>Thu, 07 Jan 2010 14:11:23 +0000</pubDate><guid>https://major.io/2010/01/07/change-the-escape-keystrokes-in-screen/</guid><description>One of my favorite (and most used) applications on any Linux machine is screen. Once you fire up a screen session, you can start something and keep it running indefinitely. Even if your internet connection drops or you accidentally close your terminal window, the screen session will remain open on the remote server.
Detaching from a screen session is done by pressing CTRL-A and then d (for detach). However, when I&amp;rsquo;m on my Mac, CTRL-A and CTRL-E send my cursor to the beginning and end of lines, respectively.</description><content type="html"><![CDATA[<p>One of my favorite (and most used) applications on any Linux machine is <a href="http://www.gnu.org/software/screen/">screen</a>. Once you fire up a screen session, you can start something and keep it running indefinitely. Even if your internet connection drops or you accidentally close your terminal window, the screen session will remain open on the remote server.</p>
<p>Detaching from a screen session is done by pressing CTRL-A and then d (for detach). However, when I&rsquo;m on my Mac, CTRL-A and CTRL-E send my cursor to the beginning and end of lines, respectively. Once I launch screen, I lose the CTRL-A functionality because screen thinks I&rsquo;m trying to send it a command.</p>
<p>Luckily, this can be changed in your <code>~/.screenrc</code>:</p>
<!-- raw HTML omitted -->
<p>With this change, you can press CTRL-W, then press d, and you&rsquo;ll detach from the screen session. For all of the screen options, run <code>man screen</code> on your local machine or review the <a href="http://linuxmanpages.com/man1/screen.1.php">man page online</a>.</p>
]]></content></item><item><title>A New Year System Administrator Inspiration</title><link>https://major.io/2010/01/03/a-new-year-system-administrator-inspiration/</link><pubDate>Mon, 04 Jan 2010 02:53:53 +0000</pubDate><guid>https://major.io/2010/01/03/a-new-year-system-administrator-inspiration/</guid><description>Happy New Year! I certainly hope it&amp;rsquo;s a great one for you, your family, and your business. As the new year begins, I figured it would be a good time to sit down and answer a question that I hear very often:
How do I become a better systems administrator?
The best way to become a better systems administrator is to fully understand the theory of what&amp;rsquo;s happening in your server&amp;rsquo;s environment.</description><content type="html"><![CDATA[<p>Happy New Year! I certainly hope it&rsquo;s a great one for you, your family, and your business. As the new year begins, I figured it would be a good time to sit down and answer a question that I hear very often:</p>
<p><em>How do I become a better systems administrator?</em></p>
<p>The best way to become a better systems administrator is to <strong>fully understand the theory</strong> of what&rsquo;s happening in your server&rsquo;s environment.</p>
<p>What do I mean by that? Learn why things aren&rsquo;t happening as you expected and think about all of the factors that could possibly be involved. Instead of thinking purely about cause and effect, you&rsquo;ll find it much easier and rewarding to consider everything inside and outside your environment before you make any changes.</p>
<p>This still may be a little difficult to fully understand, so he&rsquo;s an example. Let&rsquo;s say you&rsquo;re handling an issue where a customer can&rsquo;t reach a website hosted on their server. When you ask them for more details, they might give you the dreaded reply: &ldquo;It&rsquo;s not coming up.&rdquo; Start by making a mental list of the problems that are easiest to check:</p>
<ul>
<li>Is the web server daemon running?</li>
<li>If a database server is being used, is it running and accessible?</li>
<li>Is there a software/hardware firewall blocking port 80?</li>
<li>Is a script stuck on the server tying up resources?</li>
<li>Could there be a DNS resolution problem?</li>
<li>Is the server up?</li>
<li>Did a switch fail?</li>
<li>Is the server&rsquo;s hard disk out of space?</li>
<li>Can the customer reach other websites like Google or Yahoo?</li>
<li>If SELinux is involved, have the appropriate contexts been set?</li>
<li>Could the site be a target of a denial of service attack?</li>
<li>Has the server reached its connection tracking limit?</li>
</ul>
<p>Of course, this is a relatively short list, but these are all easy to check. If you&rsquo;re thinking about cause and effect, you might only consider the web server daemon and some basic network issues. By considering all of the other factors that may be related, you&rsquo;ve ensured that all of the basics are covered before you consider more complex problems.</p>
<p>Most systems administrators have taken an error message and tossed it in en masse into Google before. Occasionally, no results will appear for the search. If you find yourself in this situation, try to understand the individual parts of the error message. Work outward from what you know already. You should know which daemon said it, and you may have an idea of what the application was doing when the error occurred. Take time to consider what the daemon is trying to tell you within the context of what it was doing at the time.</p>
<p>One of the easiest ways to force yourself to be immersed into this way of thinking is to host applications for non-technical people. You&rsquo;ll find that many customers want things done differently, and they&rsquo;re all at different levels of technical aptitude. Some may find it a frustrating experience at first, but you&rsquo;ll think yourself later. It will force you to consider all aspects of how a server operates since you might not always know what&rsquo;s happening within a customer&rsquo;s application.</p>
<p>As always, if you find yourself stumbling, remember to ask your peers and colleagues. Even if they haven&rsquo;t seen the particular issue, they will probably be able to guide you closer to the solution you seek.</p>
]]></content></item><item><title>WordPress and PHP 5.3.x: update_comment_type_cache() expected to be a reference</title><link>https://major.io/2009/12/21/wordpress-and-php-5-3-x-update_comment_type_cache-expected-to-be-a-reference/</link><pubDate>Mon, 21 Dec 2009 14:02:02 +0000</pubDate><guid>https://major.io/2009/12/21/wordpress-and-php-5-3-x-update_comment_type_cache-expected-to-be-a-reference/</guid><description>I upgraded a Fedora 11 instance to Fedora 12 and found the following error at the top of one of my WordPress blogs:
Parameter 1 to update_comment_type_cache() expected to be a reference, value given in wp-includes/plugin.php on line 166 The problem wasn&amp;rsquo;t in a plugin, actually. It was within my theme&amp;rsquo;s (R755-light) functions.php:
function update_comment_type_cache(&amp;amp;$queried_posts) { The temporary fix is to remove the &amp;amp; from that line so it looks like this:</description><content type="html"><![CDATA[<p>I upgraded a Fedora 11 instance to Fedora 12 and found the following error at the top of one of my WordPress blogs:</p>
<pre><code>Parameter 1 to update_comment_type_cache() expected to be a reference,
value given in wp-includes/plugin.php on line 166
</code></pre><p>The problem wasn&rsquo;t in a plugin, actually. It was within my theme&rsquo;s (<a href="http://wordpress.org/extend/themes/r755-light">R755-light</a>) functions.php:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-php" data-lang="php"><span style="color:#719e07">function</span> <span style="color:#268bd2">update_comment_type_cache</span>(<span style="color:#719e07">&amp;</span><span style="color:#268bd2">$queried_posts</span>) {
</code></pre></div><p>The temporary fix is to remove the <code>&amp;</code> from that line so it looks like this:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-php" data-lang="php"><span style="color:#719e07">function</span> <span style="color:#268bd2">update_comment_type_cache</span>(<span style="color:#268bd2">$queried_posts</span>) {
</code></pre></div><p>After clearing out the WP Super Cache, the page was loading properly again. It turns out that the function actually calculates how many comments are available for a given post, so that functionality is working properly right now. A few theme authors are already releasing new versions to fix this bug, but my theme&rsquo;s author has not.</p>
<blockquote>
<p>The credit for the fix goes to someone in the <a href="http://wordpress.org/support/topic/297878?replies=8">WordPress forums</a>.</p>
</blockquote>
]]></content></item><item><title>Upgraded to WordPress 2.9</title><link>https://major.io/2009/12/21/upgraded-to-wordpress-2-9/</link><pubDate>Mon, 21 Dec 2009 13:47:04 +0000</pubDate><guid>https://major.io/2009/12/21/upgraded-to-wordpress-2-9/</guid><description>If you haven&amp;rsquo;t upgraded your WordPress installation to version 2.9 yet, you might want to consider doing that soon. There are quite a few improvements, bug fixes and security features available in the new version.
The automatic upgrade via the admin interface actually worked just fine for me. Of course, I backed up my database and files first, just to be sure.</description><content type="html"><![CDATA[<p>If you haven&rsquo;t <a href="http://codex.wordpress.org/Upgrading_WordPress">upgraded</a> your <a href="http://wordpress.org/">WordPress</a> installation to version 2.9 yet, you might want to consider doing that soon. There are quite a few <a href="http://core.trac.wordpress.org/query?status=closed&amp;milestone=2.9">improvements, bug fixes and security features</a> available in the new version.</p>
<p>The automatic upgrade via the admin interface actually worked just fine for me. Of course, I backed up my database and files first, just to be sure.</p>
]]></content></item><item><title>Parse kernel.org changelogs with wget and grep</title><link>https://major.io/2009/12/15/parse-kernel-org-changelogs-with-wget-and-grep/</link><pubDate>Tue, 15 Dec 2009 23:14:47 +0000</pubDate><guid>https://major.io/2009/12/15/parse-kernel-org-changelogs-with-wget-and-grep/</guid><description>I try to keep up with the latest kernel update from kernel.org, but parsing through the output can be a pain if there are a lot of changes taking place. Here&amp;rsquo;s a handy one-liner to make it easier to read:
wget --quiet -O - http://www.kernel.org/pub/linux/kernel/v2.6/ChangeLog-2.6.31.8 | grep -A 4 ^commit | grep -B 1 &amp;quot;^--&amp;quot; | grep -v &amp;quot;^--&amp;quot; It should give you some output like this:
Linux 2.6.31.8 ext4: Fix potential fiemap deadlock (mmap_sem vs.</description><content type="html"><![CDATA[<p>I try to keep up with the latest kernel update from <a href="http://kernel.org/">kernel.org</a>, but parsing through the output can be a pain if there are a lot of changes taking place. Here&rsquo;s a handy one-liner to make it easier to read:</p>
<pre><code>wget --quiet -O - http://www.kernel.org/pub/linux/kernel/v2.6/ChangeLog-2.6.31.8 | grep -A 4 ^commit | grep -B 1 &quot;^--&quot; | grep -v &quot;^--&quot;
</code></pre><p>It should give you some output like this:</p>
<pre><code>Linux 2.6.31.8
    ext4: Fix potential fiemap deadlock (mmap_sem vs. i_data_sem)
    signal: Fix alternate signal stack check
    SCSI: scsi_lib_dma: fix bug with dma maps on nested scsi objects
    SCSI: osd_protocol.h: Add missing #include
    SCSI: megaraid_sas: fix 64 bit sense pointer truncation
    ..
</code></pre>]]></content></item><item><title>Upgrading Fedora 11 to 12 using yum</title><link>https://major.io/2009/12/07/upgrading-fedora-11-to-12-using-yum/</link><pubDate>Tue, 08 Dec 2009 02:28:06 +0000</pubDate><guid>https://major.io/2009/12/07/upgrading-fedora-11-to-12-using-yum/</guid><description>As with the Fedora 10 to 11 upgrade, you can upgrade Fedora 11 to Fedora 12 using yum. I find this to be the easiest and most reliable way to upgrade a Fedora installation whether you use it as a server or desktop.
To reduce the total data downloaded, I&amp;rsquo;d recommend installing the yum-presto package first. It downloads delta RPM&amp;rsquo;s and builds them on the fly, which allows you to upgrade packages without having to download the entire RPM&amp;rsquo;s.</description><content type="html"><![CDATA[<p>As with the <a href="/2009/06/11/upgrading-from-fedora-10-cambridge-to-fedora-11-leonidas/">Fedora 10 to 11 upgrade</a>, you can upgrade Fedora 11 to Fedora 12 using yum. I find this to be the easiest and most reliable way to upgrade a Fedora installation whether you use it as a server or desktop.</p>
<p>To reduce the total data downloaded, I&rsquo;d recommend installing the <code>yum-presto</code> package first. It downloads delta RPM&rsquo;s and builds them on the fly, which allows you to upgrade packages without having to download the entire RPM&rsquo;s.</p>
<!-- raw HTML omitted -->
<p>Now, upgrade your current system to the latest packages and clean up yum&rsquo;s metadata:</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>Get the latest <code>fedora-release</code> package and install it (replace <strong>x86_64</strong> with <strong>x86</strong> if you&rsquo;re using a 32-bit system):</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>Now, upgrade your system to Fedora 12:</p>
<!-- raw HTML omitted -->
<blockquote>
<p>For detailed documentation on the entire process, refer to <a href="http://fedoraproject.org/wiki/YumUpgradeFaq">Fedora using yum</a> on the FedoraProject Wiki.</p>
</blockquote>
]]></content></item><item><title>Disable acceleration for Apple’s Magic Mouse</title><link>https://major.io/2009/12/03/disable-acceleration-for-apples-magic-mouse/</link><pubDate>Thu, 03 Dec 2009 13:55:41 +0000</pubDate><guid>https://major.io/2009/12/03/disable-acceleration-for-apples-magic-mouse/</guid><description>Edit: After further research, I found that this fix only adjusts the speed at which your mouse moves. It doesn&amp;rsquo;t do anything for the acceleration curve.
I recently picked up a Magic Mouse and discovered that I like almost all of its features. The biggest headache is the funky mouse acceleration curve that it applies by default. When you make small movements, they barely even register on the screen. When you make big movements and slow down a little mid-move, the pointer slows down much too rapidly.</description><content type="html"><![CDATA[<p><strong>Edit:</strong> After further research, I found that this fix only adjusts the speed at which your mouse moves. It doesn&rsquo;t do anything for the acceleration curve.</p>
<p>I recently picked up a <a href="http://www.apple.com/magicmouse/">Magic Mouse</a> and discovered that I like almost all of its features. The biggest headache is the funky mouse acceleration curve that it applies by default. When you make small movements, they barely even register on the screen. When you make big movements and slow down a little mid-move, the pointer slows down much too rapidly.</p>
<p>A quick Google search revealed a <a href="http://discussions.apple.com/thread.jspa?messageID=10640835">support discussion post</a> where users were discussing possible solutions. Someone suggested running this in the terminal:</p>
<!-- raw HTML omitted -->
<p>That improved things a little for me, but it&rsquo;s not perfect. If you adjust the tracking speed in System Preferences after running this command, the acceleration curve will be reset to the default.</p>
<p><strong>Update:</strong> After some tinkering (and <a href="http://reviews.cnet.com/8301-13727_7-10392736-263.html">further Googling</a>), I found that `` or <code>.1</code> seemed to work better for me than <code>-1</code>.</p>
]]></content></item><item><title>Automatically loading iptables rules on Debian/Ubuntu</title><link>https://major.io/2009/11/16/automatically-loading-iptables-on-debianubuntu/</link><pubDate>Tue, 17 Nov 2009 04:39:52 +0000</pubDate><guid>https://major.io/2009/11/16/automatically-loading-iptables-on-debianubuntu/</guid><description>If you want your iptables rules automatically loaded every time your networking comes up on your Debian or Ubuntu server, you can follow these easy steps.
First, get your iptables rules set up the way you like them. Once you&amp;rsquo;ve verified that everything works, save the rules:
Next, open up /etc/network/if-up.d/iptables in your favorite text editor and add the following:
Once you save it, make it executable:
Now, the rules will be restored each time your networking scripts start (or restart).</description><content type="html"><![CDATA[<p>If you want your iptables rules automatically loaded every time your networking comes up on your Debian or Ubuntu server, you can follow these easy steps.</p>
<p>First, get your iptables rules set up the way you like them. Once you&rsquo;ve verified that everything works, save the rules:</p>
<!-- raw HTML omitted -->
<p>Next, open up <code>/etc/network/if-up.d/iptables</code> in your favorite text editor and add the following:</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>Once you save it, make it executable:</p>
<!-- raw HTML omitted -->
<p>Now, the rules will be restored each time your networking scripts start (or restart). If you need to save changes to your rules in the future, you can manually edit <code>/etc/firewall.conf</code> or you can adjust your rules live and run:</p>
<!-- raw HTML omitted -->
<p><em>Thanks to <a href="http://twitter.com/ajmesserli">Ant</a> for this handy tip.</em></p>
]]></content></item><item><title>Upgraded to WordPress 2.8.6 with some theme changes</title><link>https://major.io/2009/11/16/upgraded-to-wordpress-2-8-6-with-some-theme-changes/</link><pubDate>Tue, 17 Nov 2009 04:32:22 +0000</pubDate><guid>https://major.io/2009/11/16/upgraded-to-wordpress-2-8-6-with-some-theme-changes/</guid><description>I&amp;rsquo;ve upgraded the blog to WordPress 2.8.6 after I read about the registered user exploits.
Also, I&amp;rsquo;ve dropped the Adsimple theme I was using, and I&amp;rsquo;m now using a slightly modified Dojo theme. It&amp;rsquo;s a little easier on the eyes, but it&amp;rsquo;s still lightweight enough to be fast on mobile broadband connections.
Let me know what you think!</description><content type="html"><![CDATA[<p>I&rsquo;ve upgraded the blog to WordPress 2.8.6 after I read about the <a href="http://wordpress.org/development/2009/11/wordpress-2-8-6-security-release/">registered user exploits</a>.</p>
<p>Also, I&rsquo;ve dropped the <a href="http://wordpress.org/extend/themes/adsimple">Adsimple</a> theme I was using, and I&rsquo;m now using a slightly modified <a href="http://spaceninja.com/dojo/">Dojo</a> theme. It&rsquo;s a little easier on the eyes, but it&rsquo;s still lightweight enough to be fast on mobile broadband connections.</p>
<p>Let me know what you think!</p>
]]></content></item><item><title>Changing the time zone in irssi</title><link>https://major.io/2009/11/03/changing-the-time-zone-in-irssi/</link><pubDate>Tue, 03 Nov 2009 14:34:42 +0000</pubDate><guid>https://major.io/2009/11/03/changing-the-time-zone-in-irssi/</guid><description>I usually set the time zone on my servers to UTC, but that makes it a bit confusing for me when I use irssi. If you have perl support built into irssi, you can run these commands to alter your time zone within irssi only:
For example, I&amp;rsquo;m in Central Time, so I&amp;rsquo;d use:
To update the time in your status bar, simply /whois yourself and you should see the updated time zone.</description><content type="html"><![CDATA[<p>I usually set the time zone on my servers to UTC, but that makes it a bit confusing for me when I use irssi. If you have perl support built into irssi, you can run these commands to alter your time zone within irssi only:</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>For example, I&rsquo;m in Central Time, so I&rsquo;d use:</p>
<!-- raw HTML omitted -->
<p>To update the time in your status bar, simply /whois yourself and you should see the updated time zone. If you want more handy irssi tips, look no further than <a href="http://irssi.org/documentation/tips">irssi&rsquo;s documentation</a>.</p>
]]></content></item><item><title>Please help! Found a lost dog – New Braunfels, TX</title><link>https://major.io/2009/10/14/please-help-found-a-lost-dog-new-braunfels-tx/</link><pubDate>Thu, 15 Oct 2009 02:06:53 +0000</pubDate><guid>https://major.io/2009/10/14/please-help-found-a-lost-dog-new-braunfels-tx/</guid><description>Update: The dog has made it back home to its family. Thanks to all that helped spread the word!
This is not the usual type of post I&amp;rsquo;d make on this blog, but I&amp;rsquo;m trying to find a home for a lost dog that my wife found in New Braunfels, TX. More details are on my Posterous blog.</description><content type="html"><![CDATA[<p><strong>Update</strong>: The dog has made it back home to its family. Thanks to all that helped spread the word!</p>
<p>This is not the usual type of post I&rsquo;d make on this blog, but I&rsquo;m trying to find a home for a lost dog that my wife found in New Braunfels, TX. More details are on my <a href="http://majorhayden.com/help-us-find-the-owner-of-this-lost-dog">Posterous blog</a>.</p>
]]></content></item><item><title>Twitter direct message notifications with Prowl</title><link>https://major.io/2009/09/19/twitter-direct-message-notifications-with-prowl/</link><pubDate>Sat, 19 Sep 2009 17:48:27 +0000</pubDate><guid>https://major.io/2009/09/19/twitter-direct-message-notifications-with-prowl/</guid><description>If you want instant notifications of a direct message on Twitter on your iPhone, you have a few options. You could set up your mobile device to receive SMS messages and get them quickly, but you&amp;rsquo;re stuck paying for all of those incoming SMS messages. However, it&amp;rsquo;s annoying when you follow an account, and then they follow and DM you at 4AM. Twitter allows you to set quiet hours, but then you don&amp;rsquo;t see any notifications on your phone when you wake up.</description><content type="html"><![CDATA[<p>If you want instant notifications of a direct message on Twitter on your iPhone, you have a few options. You could set up your mobile device to receive SMS messages and get them quickly, but you&rsquo;re stuck paying for all of those incoming SMS messages. However, it&rsquo;s annoying when you follow an account, and then they follow and DM you at 4AM. Twitter <a href="http://twitter.com/devices">allows you to set quiet hours</a>, but then you don&rsquo;t see any notifications on your phone when you wake up.</p>
<p>You could always pick up Boxcar [<a href="http://itunes.apple.com/WebObjects/MZStore.woa/wa/viewSoftware?id=321493542&amp;mt=8">iTunes Link</a>] from iTunes and use it&rsquo;s plug-and-play setup routine to begin receiving notifications of direct messages. The downside of the app is that the notifications are fairly delayed (sometimes 5-10 minutes late in my experience) and you&rsquo;re paying $2.99 for an app that can only handle one Twitter account. Also, the app only handles Twitter direct messages and replies – nothing else.</p>
<p>Here&rsquo;s where Prowl [<a href="http://itunes.apple.com/WebObjects/MZStore.woa/wa/viewSoftware?id=320876271&amp;mt=8">iTunes Link</a>] comes in. You can toss anything at the <a href="http://prowl.weks.net/api.php">Prowl API</a> and it comes up on your phone within seconds. The app is $2.99, but you can sent up to 1,000 notifications per hour through the API at no additional cost. <strong>Seriously – would you ever need 1,000 notifications per hour?</strong></p>
<p>First, you&rsquo;ll need two ruby gems for this project. The first of which is the <a href="http://github.com/augustl/ruby-prowl">prowl</a> gem by <a href="http://github.com/augustl">August Lilleaas</a>, and the second is <a href="http://tmail.rubyforge.org/">tmail</a>. If you haven&rsquo;t installed gems from github before, you&rsquo;ll need to run this first:</p>
<!-- raw HTML omitted -->
<p>Install the gems:</p>
<!-- raw HTML omitted -->
<p>Once they&rsquo;re installed, you can toss this script into <code>/usr/local/bin/twitterdm-prowl.rb</code>:</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>At the beginning of the script, we rope in rubygems, tmail and prowl. We&rsquo;re going to read a message in from standard input (I&rsquo;ll show you how that works in a moment) and then parse it with tmail. The subject and body are stripped down to make it easier to display on your iPhone&rsquo;s screen. Finally, we wrap up by sending the data to the Prowl API with a normal priority.</p>
<p>As far as customizations go, you&rsquo;ll obviously need to input your Prowl API key for the script to work. You can adjust the regular expressions to include different parts of the e-mail if you need them. Also, feel free to adjust the application name (“TWITDM”) in the script to something else. The priority can run from 2 (OMG emergency) to -2 (I don&rsquo;t care). I chose 0 (normal) for my script.</p>
<p>Now, make sure the script is executable:</p>
<!-- raw HTML omitted -->
<p>We&rsquo;ll need to set up an alias in <code>/etc/aliases</code> to feed in this e-mails to our script:</p>
<!-- raw HTML omitted -->
<p>Run <code>newaliases</code> to ensure that your alias ends up in the hashed aliases table.</p>
<p>At this point, you&rsquo;ll need to ensure that your emails actually make it into the alias you set up. You&rsquo;ll need something that can filter your e-mails and forward them based on the filter. If you want a strictly Linux solution, you could use procmail for that. If you&rsquo;re e-mail provider has server-side filters that can forward e-mails (<a href="http://www.rackspace.com/email_hosting/">as mine does</a>), that&rsquo;s the easiest method. Twitter provides some great e-mail headers that you can use for filtering:</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>I use <code>X-Twitteremailtype: direct_message</code> to filter and forward, but it&rsquo;s up to you on how you do it.</p>
<p>Now, use another account to send yourself a direct message and you should get a notification pretty quickly (see the image on the right). If you didn&rsquo;t get a notification, try going to the Prowl site and <a href="https://prowl.weks.net/add_notification.php">send yourself a test notification</a>. If the test notification fails, then you may have an issue with Prowl itself (this hasn&rsquo;t ever happened to me).</p>
<p>If the test notification works, but your Twitter notifications don&rsquo;t, check your mail logs. Your filters may not be sending the direct message e-mails to your alias, your alias may not be configured properly, or the permissions on the ruby script you made may be incorrect.</p>
<p>If you want to keep testing without sending yourself a ton of direct messages, just send yourself one direct message. Once you receive the e-mail notification from Twitter, open it up and view the source of the message in your e-mail client (command-option-U in Apple Mail). Copy the entire source of the message to your clipboard, paste it into your favorite text editor on your server, and save it. You can call your script with the e-mail just like your mail server would with the following:</p>
<!-- raw HTML omitted -->
<p>By going that route, you be testing the script itself, and your e-mail server will be removed from the equation.</p>
<p>If you&rsquo;re still having trouble, let me know and I&rsquo;ll be glad to help!</p>
]]></content></item><item><title>Fix MacFusion on Snow Leopard</title><link>https://major.io/2009/08/28/fix-macfusion-on-snow-leopard/</link><pubDate>Fri, 28 Aug 2009 16:21:23 +0000</pubDate><guid>https://major.io/2009/08/28/fix-macfusion-on-snow-leopard/</guid><description>Running OS X 10.6.3? William Fennie found a fix on Google Groups.
First off, credit for this fix on OS X 10.6.2 goes to Geoff Watts from his two tweets.
If you&amp;rsquo;re using Snow Leopard, you&amp;rsquo;ll find that the current version of MacFusion refuses to complete a connection to a remote server. You can fix this in two steps:
First, quit MacFusion.
Second, open System Preferences and then open the MacFUSE pane.</description><content type="html"><![CDATA[<p><strong>Running OS X 10.6.3?</strong> William Fennie <a href="http://groups.google.com/group/macfuse/browse_thread/thread/3c611784177843f0/3f02a6efd38f4b30?show_docid=3f02a6efd38f4b30">found a fix on Google Groups</a>.</p>
<p>First off, credit for this fix on <strong>OS X 10.6.2</strong> goes to <a href="https://twitter.com/geoffwatts">Geoff Watts</a> from his <a href="http://twitter.com/geoffwatts/status/3605414263">two</a> <a href="http://twitter.com/geoffwatts/status/3605464669">tweets</a>.</p>
<p>If you&rsquo;re using Snow Leopard, you&rsquo;ll find that the current version of MacFusion refuses to complete a connection to a remote server. You can fix this in two steps:</p>
<p>First, quit MacFusion.</p>
<p>Second, open System Preferences and then open the MacFUSE pane. Check the “Show Beta Versions” box and click “Check For Updates”. Go ahead and update MacFUSE.</p>
<p>Third, open up a terminal and do the following:</p>
<!-- raw HTML omitted -->
<p>Your MacFusion installation should now be working on Snow Leopard. I&rsquo;ve tested SSH and FTP connectivity so far, and they both appear to be working. Thanks again to Geoff for the fix!</p>
]]></content></item><item><title>Fedora 11 httpd: alloc_listener: failed to get a socket for (null)</title><link>https://major.io/2009/08/14/fedora-11-httpd-alloc_listener-failed-to-get-a-socket-for-null/</link><pubDate>Fri, 14 Aug 2009 17:14:02 +0000</pubDate><guid>https://major.io/2009/08/14/fedora-11-httpd-alloc_listener-failed-to-get-a-socket-for-null/</guid><description>If you use Fedora 11 in a virtualized environment, you may have seen this error recently if you&amp;rsquo;ve updated to apr-1.3.8-1:
The issue is related to three kernel calls that are used in apr-1.3.8-1: accept4(), dup3() and epoll_create1(). Without these calls, apache is unable to start.
Update on August 17, 2009:the Fedora team has pushed apr-1.3.8-2 into the stable repositories for Fedora 11, which eliminates the need for the temporary fix shown below.</description><content type="html"><![CDATA[<p>If you use Fedora 11 in a virtualized environment, you may have seen this error recently if you&rsquo;ve updated to apr-1.3.8-1:</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>The issue is related to three kernel calls that are used in apr-1.3.8-1: accept4(), dup3() and epoll_create1(). Without these calls, apache is unable to start.</p>
<p><strong><!-- raw HTML omitted -->Update on August 17, 2009:<!-- raw HTML omitted --> the Fedora team has <a href="https://bugzilla.redhat.com/show_bug.cgi?id=516331#c12">pushed apr-1.3.8-2 into the stable repositories</a> for Fedora 11, which eliminates the need for the temporary fix shown below.</strong></p>
<p><strong>Deprecated solution:</strong> There is a <a href="https://bugzilla.redhat.com/show_bug.cgi?id=516331">bug open</a> with the Fedora team, and there is a <a href="https://bugzilla.redhat.com/show_bug.cgi?id=516331#c10">temporary fix</a> available:</p>
<!-- raw HTML omitted -->
]]></content></item><item><title>Installing the mysql gem in Fedora 11 64-bit</title><link>https://major.io/2009/08/07/installing-the-mysql-gem-in-fedora-11-64-bit/</link><pubDate>Fri, 07 Aug 2009 18:57:22 +0000</pubDate><guid>https://major.io/2009/08/07/installing-the-mysql-gem-in-fedora-11-64-bit/</guid><description>On some systems, getting the mysql gem to build can be a little tricky. Fedora 11 x86_64 will require a bit of extra finesse to get the gem installed. First off, ensure that you&amp;rsquo;ve installed the mysql-devel package:
I&amp;rsquo;ll assume that you already installed the rubygems package. You can install the mysql gem like this:</description><content type="html"><![CDATA[<p>On some systems, getting the mysql gem to build can be a little tricky. Fedora 11 x86_64 will require a bit of extra finesse to get the gem installed. First off, ensure that you&rsquo;ve installed the <code>mysql-devel</code> package:</p>
<!-- raw HTML omitted -->
<p>I&rsquo;ll assume that you already installed the <code>rubygems</code> package. You can install the mysql gem like this:</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
]]></content></item><item><title>Graphical representation of Cisco’s BGP issues this morning</title><link>https://major.io/2009/08/04/graphical-representation-of-ciscos-bgp-issues-this-morning/</link><pubDate>Tue, 04 Aug 2009 14:45:45 +0000</pubDate><guid>https://major.io/2009/08/04/graphical-representation-of-ciscos-bgp-issues-this-morning/</guid><description>If you haven&amp;rsquo;t checked out bgplay, it&amp;rsquo;s pretty handy.</description><content type="html"><![CDATA[<p>If you haven&rsquo;t checked out <a href="http://bgplay.routeviews.org/bgplay/">bgplay</a>, it&rsquo;s pretty handy.</p>
]]></content></item><item><title>Get the public-facing IP for any server with icanhazip.com</title><link>https://major.io/2009/07/31/get-the-public-facing-ip-for-any-server-with-icanhazip-com/</link><pubDate>Fri, 31 Jul 2009 13:41:38 +0000</pubDate><guid>https://major.io/2009/07/31/get-the-public-facing-ip-for-any-server-with-icanhazip-com/</guid><description>There are a ton of places on the internet where you can check the public-facing IP for the device you are using. I&amp;rsquo;ve used plenty of them, but I&amp;rsquo;ve always wanted one that just returned text. You can get pretty close with checkip.dyndns.org, but there is still HTML in the output:
I wanted something simpler, so I set up icanhazip.com:</description><content type="html"><![CDATA[<p>There are a ton of places on the internet where you can check the public-facing IP for the device you are using. I&rsquo;ve used plenty of them, but I&rsquo;ve always wanted one that just returned text. You can get pretty close with <a href="http://checkip.dyndns.org/">checkip.dyndns.org</a>, but there is still HTML in the output:</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>I wanted something simpler, so I set up <a href="http://icanhazip.com/">icanhazip.com</a>:</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
]]></content></item><item><title>Requiring SSL encryption for WordPress administration</title><link>https://major.io/2009/07/31/requiring-ssl-encryption-for-wordpress-administration/</link><pubDate>Fri, 31 Jul 2009 13:13:26 +0000</pubDate><guid>https://major.io/2009/07/31/requiring-ssl-encryption-for-wordpress-administration/</guid><description>I was digging around for WordPress plugins last night that would allow me to secure the administrative login page for my WordPress installations. Most of the plugins are only compatible with WordPress 2.7.x or earlier, so I was a little concerned about them working with WordPress 2.8.2.
Then I stumbled upon the WordPress documentation that shows you how to require SSL with no plugins at all. If you&amp;rsquo;re using WordPress 2.</description><content type="html"><![CDATA[<p>I was digging around for <a href="http://wordpress.org/">WordPress</a> plugins last night that would allow me to secure the administrative login page for my WordPress installations. Most of the plugins are only compatible with WordPress 2.7.x or earlier, so I was a little concerned about them working with WordPress 2.8.2.</p>
<p>Then I stumbled upon the <a href="http://codex.wordpress.org/Administration_Over_SSL">WordPress documentation</a> that shows you how to require SSL with no plugins at all. If you&rsquo;re using WordPress 2.6+, you can use these super-simple instructions:</p>
<p>Require encryption just for the /wp-admin/ login, but leave the rest of the administrative area on HTTP:</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>To encrypt the login and the entire administrative area:</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>Of course, for this to work, you&rsquo;ll need virtual hosts on ports 80 and 443 for your blog. Also, you&rsquo;ll need an SSL certificate for your blog. You can snag one from a <a href="https://ssl.trustwave.com/solutions-overview.php">reputable provider</a> or <a href="http://rackerhacker.com/2007/08/02/generate-self-signed-certificate-and-key-in-one-line/">make your own</a>.</p>
]]></content></item><item><title>Upgraded WordPress to 2.8.2</title><link>https://major.io/2009/07/29/upgraded-wordpress-to-2-8-2/</link><pubDate>Wed, 29 Jul 2009 22:22:13 +0000</pubDate><guid>https://major.io/2009/07/29/upgraded-wordpress-to-2-8-2/</guid><description>I finally made it around to upgrading WordPress to 2.8.2 and it seems to be working like a charm. It looks like there were some bug fixes and a XSS vulnerability was patched.</description><content type="html">&lt;p>I finally made it around to upgrading WordPress to 2.8.2 and it seems to be working like a charm. It looks like there were some bug fixes and a &lt;a href="http://wordpress.org/development/2009/07/wordpress-2-8-2/">XSS vulnerability was patched&lt;/a>.&lt;/p></content></item><item><title>Rotating rails logs when using Phusion Passenger</title><link>https://major.io/2009/06/26/rotating-rails-logs-when-using-phusion-passenger/</link><pubDate>Fri, 26 Jun 2009 15:09:54 +0000</pubDate><guid>https://major.io/2009/06/26/rotating-rails-logs-when-using-phusion-passenger/</guid><description>I found a great post on Overstimulate about handling the rotation of rails logs when you use Phusion Passenger. Most of the data for your application should end up in the apache logs, but if your site is highly dynamic, you may end up with a giant production log if you&amp;rsquo;re not careful.
Toss this into /etc/logrotate.d/yourrailsapplication:
For a detailed explanation, see the post on Overstimulate.</description><content type="html"><![CDATA[<p>I found a <a href="http://overstimulate.com/articles/logrotate-rails-passenger">great post</a> on <a href="http://overstimulate.com/">Overstimulate</a> about handling the rotation of rails logs when you use Phusion Passenger. Most of the data for your application should end up in the apache logs, but if your site is highly dynamic, you may end up with a giant production log if you&rsquo;re not careful.</p>
<p>Toss this into /etc/logrotate.d/yourrailsapplication:</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>For a detailed explanation, <a href="http://overstimulate.com/articles/logrotate-rails-passenger">see the post</a> on <a href="http://overstimulate.com/">Overstimulate</a>.</p>
]]></content></item><item><title>Deleting all e-mail messages in your inbox with mutt</title><link>https://major.io/2009/06/19/deleting-all-e-mail-messages-in-your-inbox-with-mutt/</link><pubDate>Fri, 19 Jun 2009 17:37:58 +0000</pubDate><guid>https://major.io/2009/06/19/deleting-all-e-mail-messages-in-your-inbox-with-mutt/</guid><description>Occasionally, I&amp;rsquo;ll end up with a mailbox full of random data, alerts, or other useless things. If you have SSH access to the server, you can always clear out your mail spool, but if you connect to an IMAP server, you can use mutt to do the same thing.
First, use mutt to connect to your server remotely (via IMAP over SSL in this example):
mutt -f imaps://mail.yourdomain.com/ Once you&amp;rsquo;ve connected and logged in, press SHIFT-D (uppercase d).</description><content type="html"><![CDATA[<p>Occasionally, I&rsquo;ll end up with a mailbox full of random data, alerts, or other useless things. If you have SSH access to the server, you can always clear out your mail spool, but if you connect to an IMAP server, you can use mutt to do the same thing.</p>
<p>First, use mutt to connect to your server remotely (via IMAP over SSL in this example):</p>
<pre><code>mutt -f imaps://mail.yourdomain.com/
</code></pre><p>Once you&rsquo;ve connected and logged in, press <strong>SHIFT-D</strong> (uppercase d). The status bar of mutt should show:</p>
<pre><code>Delete messages matching:
</code></pre><p>Type in <code>~s .*</code> so that the line looks like:</p>
<pre><code>Delete messages matching: ~s .*
</code></pre><p>When you press enter, mutt will put a <strong>D</strong> next to all of the messages, which marks them for deletion. Press <code>q</code> to quit, and then <code>y</code> to confirm the deletion. After a brief moment, all of those messages will be deleted and mutt will exit.</p>
<p><strong>Update:</strong> There&rsquo;s an even faster way to remove all of the messages in a mailbox with mutt. Just hold shift while pressing D, ~ (tilde), and A to select everything:</p>
<pre><code>D~A
</code></pre>]]></content></item><item><title>Two great signals: SIGSTOP and SIGCONT</title><link>https://major.io/2009/06/15/two-great-signals-sigstop-and-sigcont/</link><pubDate>Mon, 15 Jun 2009 18:16:19 +0000</pubDate><guid>https://major.io/2009/06/15/two-great-signals-sigstop-and-sigcont/</guid><description>The best uses I&amp;rsquo;ve found for the SIGSTOP and SIGCONT signals are times when a process goes haywire, or when a script spawns too many processes at once.
You can issue the signals like this:
kill -SIGSTOP [pid] kill -SIGCONT [pid] Wikipedia has great definitions for SIGSTOP:
When SIGSTOP is sent to a process, the usual behaviour is to pause that process in its current state. The process will only resume execution if it is sent the SIGCONT signal.</description><content type="html"><![CDATA[<p>The best uses I&rsquo;ve found for the SIGSTOP and SIGCONT signals are times when a process goes haywire, or when a script spawns too many processes at once.</p>
<p>You can issue the signals like this:</p>
<pre><code>kill -SIGSTOP [pid]
kill -SIGCONT [pid]
</code></pre><p>Wikipedia has great definitions for <a href="http://en.wikipedia.org/wiki/SIGSTOP">SIGSTOP</a>:</p>
<blockquote>
<p>When SIGSTOP is sent to a process, the usual behaviour is to pause that process in its current state. The process will only resume execution if it is sent the SIGCONT signal. SIGSTOP and SIGCONT are used for job control in the Unix shell, among other purposes. SIGSTOP cannot be caught or ignored.</p>
</blockquote>
<p>and <a href="http://en.wikipedia.org/wiki/SIGCONT">SIGCONT</a>:</p>
<blockquote>
<p>When SIGSTOP or SIGTSTP is sent to a process, the usual behaviour is to pause that process in its current state. The process will only resume execution if it is sent the SIGCONT signal. SIGSTOP and SIGCONT are used for job control in the Unix shell, among other purposes.</p>
</blockquote>
<p>In short, SIGSTOP tells a process to “hold on” and SIGCONT tells a process to “pick up where you left off”. This can work really well for rsync jobs since you can pause the job, clear up some space on the destination device, and then resume the job. The source rsync process just thinks that the destination rsync process is taking a long time to respond.</p>
<p>In the <code>ps</code> output, stopped processes will have a status containing <em>T</em>. Here&rsquo;s an example with <code>crond</code>:</p>
<pre><code># kill -SIGSTOP `pgrep crond`
# ps aufx | grep crond
root      3499  0.0  0.0 100328  1236 ?        Ts   Jun11   0:01 crond
# kill -SIGCONT `pgrep crond`
# ps aufx | grep crond
root      3499  0.0  0.0 100328  1236 ?        Ss   Jun11   0:01 crond
</code></pre>]]></content></item><item><title>Ugly upgrade path from WordPress 2.7.1 to 2.8</title><link>https://major.io/2009/06/13/ugly-upgrade-path-from-wordpress-2-7-1-to-2-8/</link><pubDate>Sat, 13 Jun 2009 17:48:15 +0000</pubDate><guid>https://major.io/2009/06/13/ugly-upgrade-path-from-wordpress-2-7-1-to-2-8/</guid><description>When I tried to do an automatic upgrade from WordPress 2.7.1 to 2.8 yesterday, it failed miserably. The files were all put in place, but when I tried to load /wp-admin/upgrade.php, this error popped up:
I was perplexed at the error, so I restored from a backup and began upgrading manually. The manual upgrades have always worked well for me in the past, so I figured this would probably fix the problem.</description><content type="html"><![CDATA[<p>When I tried to do an automatic upgrade from WordPress 2.7.1 to 2.8 yesterday, it failed miserably. The files were all put in place, but when I tried to load <code>/wp-admin/upgrade.php</code>, this error popped up:</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>I was perplexed at the error, so I restored from a backup and began <a href="http://codex.wordpress.org/Upgrading_WordPress">upgrading manually</a>. The manual upgrades have always worked well for me in the past, so I figured this would probably fix the problem. After the upgrade, I went to <code>/wp-admin/upgrade.php</code> and saw:</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p><strong>What the heck is going on?</strong> I restored from a backup, tried the manual upgrade again, and it still failed. I took a look at the lines causing the problem in <code>schema.php</code>:</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>I figured I could comment out the if statement and probably still be safe:</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p><strong>Success?</strong> I could make it through the <code>upgrade.php</code> part fine at this point, but whenever I tried to add a tag to a post, it wasn&rsquo;t saving to the database. I caught this error in my apache logs:</p>
<p><code>[Fri Jun 12 23:45:03 2009] [error] [client 72.183.200.144] WordPress database error Duplicate entry 'debian' for key 'slug' for query INSERT INTO wp_terms (`name`,`slug`,`term_group`) VALUES ('debian','debian','0') made by wp_insert_term, referer: http://rackerhacker.com/wp-admin/post.php?action=edit&amp;post=877</code></p>
<p><strong>Frustration quickly ensued.</strong> I moved my <code>/wp-content/</code> folder out of the way and replaced it with the standard WordPress stuff, but that didn&rsquo;t help. I moved plugins out of the way, one by one, but that didn&rsquo;t fix it either. Then I spotted a strange file sitting in <code>/wp-content/</code> called <code>db.php</code>. When I opened it, I found a <a href="http://pastie.org/private/rmbmk3ohgmdbujotnrg">lot of database setup classes for mysqli</a>.</p>
<p>I renamed it to <code>db.pleasedonteverrunthisphp</code> and I was able to save tags properly. So far, I haven&rsquo;t found any issues after I made chat change.</p>
<p>Does anyone know where that file might have come from? I don&rsquo;t remember adding it myself, so I&rsquo;m wondering if it was ever packaged with a WordPress plugin or a WordPress installation. I hope this helps someone else!</p>
]]></content></item><item><title>Upgrading from Fedora 10 (Cambridge) to Fedora 11 (Leonidas)</title><link>https://major.io/2009/06/11/upgrading-from-fedora-10-cambridge-to-fedora-11-leonidas/</link><pubDate>Thu, 11 Jun 2009 17:48:39 +0000</pubDate><guid>https://major.io/2009/06/11/upgrading-from-fedora-10-cambridge-to-fedora-11-leonidas/</guid><description>There are two main ways to upgrade Fedora 10 (Cambridge) to Fedora 11 (Leonidas):
» What the Fedora developers suggest:
Of course, if you&amp;rsquo;re doing this on a Fedora desktop, you can use preupgrade (rather than preupgrade-cli) to upgrade with a GUI.
» The method I prefer (and it works properly on Slicehost):
At this point, you would normally just start upgrading packages, but the Fedora developers threw us a curveball.</description><content type="html"><![CDATA[<p>There are two main ways to upgrade Fedora 10 (Cambridge) to Fedora 11 (Leonidas):</p>
<p><strong>» What the Fedora developers suggest:</strong></p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>Of course, if you&rsquo;re doing this on a Fedora desktop, you can use <code>preupgrade</code> (rather than <em>preupgrade-cli</em>) to upgrade with a GUI.</p>
<p><strong>» The method I prefer (and it works properly on <a href="http://slicehost.com/">Slicehost</a>):</strong></p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>At this point, you would normally just start upgrading packages, but the Fedora developers threw us a curveball. Since yum in Fedora 10 doesn&rsquo;t support metalinks, your upgrades will fail with something like this:</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>It&rsquo;s easily fixed, however. Open up <code>/etc/yum.repos.d/fedora.repo</code> and <code>/etc/yum.repos.d/fedora-updates.repo</code> in your favorite text editor and change the <code>mirrorlist</code> URL&rsquo;s like so:</p>
<p><strong>Fedora Repository</strong></p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p><strong>Fedora Updates Repository</strong></p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>Once you make those changes, finish out the upgrade:</p>
<!-- raw HTML omitted -->
<p>This process will take a little while to complete, but there shouldn&rsquo;t be any interaction required. Once it&rsquo;s done, change the <code>mirrorlist</code> lines back to the original values so you can benefit from the speedups provided by the metalink format.</p>
]]></content></item><item><title>Simple SOCKS proxy using SSH</title><link>https://major.io/2009/05/26/simple-socks-proxy-using-ssh/</link><pubDate>Tue, 26 May 2009 19:29:55 +0000</pubDate><guid>https://major.io/2009/05/26/simple-socks-proxy-using-ssh/</guid><description>Sometimes we find ourselves in places where we don&amp;rsquo;t trust the network that we&amp;rsquo;re using. I&amp;rsquo;ve found myself in quite a few situations where I know my data is being encrypted, but I want an additional layer of protection. Luckily, that protection is built into SSH if you&amp;rsquo;d like to use it.
Create a simple SOCKS proxy with SSH by using the -D flag:
That command will open up a SOCKS proxy on your workstation on port 2400.</description><content type="html"><![CDATA[<p>Sometimes we find ourselves in places where we don&rsquo;t trust the network that we&rsquo;re using. I&rsquo;ve found myself in quite a few situations where I know my data is being encrypted, but I want an additional layer of protection. Luckily, that protection is built into SSH if you&rsquo;d like to use it.</p>
<p>Create a simple SOCKS proxy with SSH by using the <code>-D</code> flag:</p>
<!-- raw HTML omitted -->
<p>That command will open up a SOCKS proxy on your workstation on port 2400. If you configure your application to use the local SOCKS proxy, any traffic using the proxy will be sent through an encrypted SSH connection to your remote server and out to the internet. Inbound traffic through the proxy is encrypted through the same connection.</p>
<p>You can pair that with autossh to keep your proxy connected at all times:</p>
<!-- raw HTML omitted -->
]]></content></item><item><title>Comparing MySQL result sets quickly</title><link>https://major.io/2009/05/05/comparing-mysql-result-sets-quickly/</link><pubDate>Tue, 05 May 2009 15:51:09 +0000</pubDate><guid>https://major.io/2009/05/05/comparing-mysql-result-sets-quickly/</guid><description>I found a really helpful tip on Xaprb for comparing result sets in MySQL:
It&amp;rsquo;s a quick way to determine if you have two tables that are properly in sync. Although there are better ways to compare tables in replicated environments, this method can get it done pretty quickly.</description><content type="html"><![CDATA[<p>I found a really helpful tip on <a href="http://www.xaprb.com/blog/2009/03/25/mysql-command-line-tip-compare-result-sets/">Xaprb</a> for comparing result sets in MySQL:</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>It&rsquo;s a quick way to determine if you have two tables that are properly in sync. Although there are <a href="http://www.maatkit.org/doc/mk-table-checksum.html">better ways</a> to compare tables in replicated environments, this method can get it done pretty quickly.</p>
]]></content></item><item><title>Re-scan the SCSI bus in Linux after hot-swapping a drive</title><link>https://major.io/2009/04/23/re-scan-the-scsi-bus-in-linux-after-hot-swapping-a-drive/</link><pubDate>Thu, 23 Apr 2009 17:00:54 +0000</pubDate><guid>https://major.io/2009/04/23/re-scan-the-scsi-bus-in-linux-after-hot-swapping-a-drive/</guid><description>Servers with hot swappable drive bays are always handy. However, things can turn ugly if the SCSI controller doesn&amp;rsquo;t like a new drive when it is inserted. You may end up with these errors in your dmesg output:
The errors show that the SCSI bus is having issues bringing the new drive online, and it won&amp;rsquo;t be seen by the OS until the SCSI controller is pleased. You can force the controller to re-scan the drives attached to it, and this should correct the problem:</description><content type="html"><![CDATA[<p>Servers with hot swappable drive bays are always handy. However, things can turn ugly if the SCSI controller doesn&rsquo;t like a new drive when it is inserted. You may end up with these errors in your dmesg output:</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>The errors show that the SCSI bus is having issues bringing the new drive online, and it won&rsquo;t be seen by the OS until the SCSI controller is pleased. You can force the controller to re-scan the drives attached to it, and this should correct the problem:</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>Replace the <strong>X</strong> with the proper controller number of your SCSI controller. If you&rsquo;re not sure which controller is which, try running:</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p><em>Credit for this find goes to Tony Dolan</em></p>
]]></content></item><item><title>Piping log files to a syslog server</title><link>https://major.io/2009/04/21/piping-log-files-to-a-syslog-server/</link><pubDate>Tue, 21 Apr 2009 22:59:21 +0000</pubDate><guid>https://major.io/2009/04/21/piping-log-files-to-a-syslog-server/</guid><description>If you have a centralized syslog server, or you use Splunk for log tracking, you may find the need to get older log files into a syslog port on that server.
Edit: Using logger (as suggested by David and Jerry below) will give you a more reliable way to send the data to a syslog server:
You&amp;rsquo;ll also be able to set a label for the text before it&amp;rsquo;s piped into the syslog server, which would be handy if you&amp;rsquo;re sorting or parsing the data later on.</description><content type="html"><![CDATA[<p>If you have a centralized syslog server, or you use Splunk for log tracking, you may find the need to get older log files into a syslog port on that server.</p>
<p><strong>Edit:</strong> Using logger (as suggested by David and Jerry below) will give you a more reliable way to send the data to a syslog server:</p>
<!-- raw HTML omitted -->
<p>You&rsquo;ll also be able to set a label for the text before it&rsquo;s piped into the syslog server, which would be handy if you&rsquo;re sorting or parsing the data later on.</p>
<p>Also, you can send your data in the raw using netcat:</p>
<!-- raw HTML omitted -->
]]></content></item><item><title>PHPMyAdmin 3.x hides the table indexes</title><link>https://major.io/2009/04/03/phpmyadmin-3x-hides-the-table-indexes/</link><pubDate>Sat, 04 Apr 2009 00:51:48 +0000</pubDate><guid>https://major.io/2009/04/03/phpmyadmin-3x-hides-the-table-indexes/</guid><description>Users of PHPMyAdmin 3.x may find that the table indexes are automatically hidden at the bottom of the page. I find this to be a huge annoyance since table indexes are tremendously important to the structure of the table.
If you don&amp;rsquo;t want to downgrade to PHPMyAdmin 2.x, just add the following line to the top of your config.inc.php file:
This will cause the indexes to be displayed when you click Structure for a certain table.</description><content type="html"><![CDATA[<p>Users of PHPMyAdmin 3.x may find that the table indexes are automatically hidden at the bottom of the page. I find this to be a huge annoyance since table indexes are tremendously important to the structure of the table.</p>
<p>If you don&rsquo;t want to downgrade to PHPMyAdmin 2.x, just add the following line to the top of your config.inc.php file:</p>
<!-- raw HTML omitted -->
<p>This will cause the indexes to be displayed when you click <strong>Structure</strong> for a certain table. By default, they are hidden.</p>
<p><em><strong>Sidenote:</strong> Some of you might be thinking: “Hey, you&rsquo;re a DBA, you should know MySQL queries without needing PHPMyAdmin.” You&rsquo;re right. I do know how to get the job done without PHPMyAdmin, but I enjoy the way PHPMyAdmin allows me to visualize my table structures. Also, it&rsquo;s a handy way to present data to others very quickly.</em></p>
]]></content></item><item><title>Enabling VNC as a pseudo-KVM with VMWare Server</title><link>https://major.io/2009/03/24/enabling-vnc-as-a-pseudo-kvm-with-vmware-server/</link><pubDate>Wed, 25 Mar 2009 01:28:59 +0000</pubDate><guid>https://major.io/2009/03/24/enabling-vnc-as-a-pseudo-kvm-with-vmware-server/</guid><description>Mac users feel a little left out when it comes to VMWare Server clients. There&amp;rsquo;s one for Windows and Linux, but Mac users are out of luck. Sure, you can VNC into a Linux box, use X forwarding, or use RDC to access a Windows box, but a real Mac client would really be helpful.
However, I stumbled upon some documentation that will allow you to VNC to a VMWare Server VM&amp;rsquo;s main screen.</description><content type="html"><![CDATA[<p>Mac users <a href="http://communities.vmware.com/thread/155201">feel a little left</a> out when it comes to <a href="http://www.vmware.com/products/server/">VMWare Server</a> clients. There&rsquo;s one for Windows and Linux, but Mac users are out of luck. Sure, you can VNC into a Linux box, use X forwarding, or use RDC to access a Windows box, but a real Mac client would really be helpful.</p>
<p>However, I stumbled upon some documentation that will allow you to VNC to a VMWare Server VM&rsquo;s main screen. It&rsquo;s equivalent to having a network KVM connected to the VM so you can have out-of-band management. With VMWare server 2.x, you can enable it by following these steps:</p>
<p><strong>Step 1.</strong> Create a new VM in VMWare Server, but <em>don&rsquo;t start the VM</em>.</p>
<p><strong>Step 2.</strong> SSH to the server and find your VM&rsquo;s .vmx file. Normally, you can find the file in a location like <code>/var/lib/vmware/[vmname]/[vmname].vmx</code>.</p>
<p><strong>Step 3.</strong> Add the following lines to the end of the .vmx file:</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p><strong>Step 4.</strong> Change the VNC port and password to values that suit your environment and then start the VM.</p>
<p><strong>DUH!</strong> Don&rsquo;t set two VM&rsquo;s to use the same vnc port, but that should go without saying.</p>
]]></content></item><item><title>Compare the RPM packages installed on two different servers</title><link>https://major.io/2009/03/10/compare-the-rpm-packages-installed-on-two-different-servers/</link><pubDate>Tue, 10 Mar 2009 23:31:49 +0000</pubDate><guid>https://major.io/2009/03/10/compare-the-rpm-packages-installed-on-two-different-servers/</guid><description>Setting up new servers can be a pain if you&amp;rsquo;re not able to clone them from a server that is known to be working. Many VPS providers, like Slicehost, allow you to clone a system to a new system. Without that option, you can pull a list of RPM&amp;rsquo;s without their version number for a fairly quick and basic comparison.
First, pull a list of RPM package by name only:</description><content type="html"><![CDATA[<p>Setting up new servers can be a pain if you&rsquo;re not able to clone them from a server that is known to be working. Many VPS providers, like <a href="http://slicehost.com/">Slicehost</a>, allow you to clone a system to a new system. Without that option, you can pull a list of RPM&rsquo;s without their version number for a fairly quick and basic comparison.</p>
<p>First, pull a list of RPM package by name only:</p>
<!-- raw HTML omitted -->
<p>Once you&rsquo;ve done that on both servers, just use diff to compare the two files:</p>
<!-- raw HTML omitted -->
]]></content></item><item><title>Prevent gnome-keyring from asking for a password when NetworkManager starts</title><link>https://major.io/2009/02/26/prevent-gnome-keyring-from-asking-for-a-password-when-networkmanager-starts/</link><pubDate>Fri, 27 Feb 2009 00:21:21 +0000</pubDate><guid>https://major.io/2009/02/26/prevent-gnome-keyring-from-asking-for-a-password-when-networkmanager-starts/</guid><description>I recently tossed Ubuntu 8.10 on my Mac Mini at home to use it as a home theater PC (with Boxee). When I connected to my wireless network via NetworkManager, I entered my WPA2 passphrase, and then I was prompted to enter a password for gnome-keyring. I went back to the couch, SSH&amp;rsquo;ed in, and continued configuring it remotely. When it rebooted, it never came back online.
Once I switched the TV back over to the Mini, I saw that gnome-keyring had popped up and it was asking for my password.</description><content type="html"><![CDATA[<p>I recently tossed Ubuntu 8.10 on my Mac Mini at home to use it as a home theater PC (with <a href="http://boxee.tv/">Boxee</a>). When I connected to my wireless network via NetworkManager, I entered my WPA2 passphrase, and then I was prompted to enter a password for gnome-keyring. I went back to the couch, SSH&rsquo;ed in, and continued configuring it remotely. When it rebooted, it never came back online.</p>
<p>Once I switched the TV back over to the Mini, I saw that gnome-keyring had popped up and it was asking for my password. I entered it, and the Mini joined the wireless network. Each time I rebooted, I had to go through this procedure (which is annoying to do with a HTPC that is across the room). I found a <a href="http://ubuntu-tutorials.com/2007/07/12/automatically-unlocking-the-default-gnome-keyring-pam-keyring/">pretty fancy solution</a>, but it looked a little complicated for my setup.</p>
<p>Here&rsquo;s how I did it in a simpler way in Ubuntu 8.10:</p>
<ul>
<li>Click <strong>Applications</strong> &gt; <strong>Accessories</strong> &gt; <strong>Passwords and Encryption Keys</strong></li>
<li>Click <strong>Edit</strong> &gt; <strong>Preferences</strong></li>
<li>Click your keyring name (usually <em>default</em>)</li>
<li>Click <strong>Change Unlock Password</strong></li>
<li>Enter your current password in the top box, but leave the bottom two boxes blank</li>
<li>Click <strong>OK</strong></li>
<li>Click <strong>Use unsafe storage</strong> when you are prompted</li>
<li>Click <strong>Close</strong></li>
</ul>
<p>If you reboot your machine, it should not ask for a password for your keyring any longer. This allowed my system to log into my wireless network automatically.</p>
<p><strong>WHOA THERE:</strong> Since the only password being stored on the device is my WPA2 password, I&rsquo;m not concerned about the security of the keyring. If you&rsquo;re doing this on a laptop or desktop that other people use, I would highly recommend not following these steps. All of your passwords and keys will be stored unencrypted.</p>
]]></content></item><item><title>A new customer service blog: dontbeafraidofcustomers.com</title><link>https://major.io/2009/02/24/a-new-customer-service-blog-dontbeafraidofcustomerscom/</link><pubDate>Wed, 25 Feb 2009 03:36:25 +0000</pubDate><guid>https://major.io/2009/02/24/a-new-customer-service-blog-dontbeafraidofcustomerscom/</guid><description>I&amp;rsquo;m striving to keep the topics of this blog as technical as possible since that what most readers keep asking for. Tech talk is always something I enjoy, but one of my other big passions is customer service. It may sound trite, but I&amp;rsquo;m actually genuine about it.
I started a new customer service blog with a different approach. The blog will cover different situations where companies often make errors while handling customer service issues.</description><content type="html"><![CDATA[<p>I&rsquo;m striving to keep the topics of this blog as technical as possible since that what most readers keep asking for. Tech talk is always something I enjoy, but one of my other big passions is customer service. It may sound trite, but I&rsquo;m actually genuine about it.</p>
<p>I started a <a href="http://dontbeafraidofcustomers.com/">new customer service blog</a> with a different approach. The blog will cover different situations where companies often make errors while handling customer service issues.</p>
<p>If you have suggestions for a post, please <a href="http://dontbeafraidofcustomers.com/contact/">let me know</a>!</p>
]]></content></item><item><title>Boxcheck v2.0 is on the way</title><link>https://major.io/2009/02/24/boxcheck-v20-is-on-the-way/</link><pubDate>Tue, 24 Feb 2009 13:20:32 +0000</pubDate><guid>https://major.io/2009/02/24/boxcheck-v20-is-on-the-way/</guid><description>I&amp;rsquo;ve been a bit slow on new posts lately for two main reasons. First, I&amp;rsquo;ve been working hard with my new position at Slicehost, (I&amp;rsquo;m still with Rackspace). Also, I&amp;rsquo;ve thrown out the old code for Boxcheck.com and I&amp;rsquo;m re-writing it from scratch.
Most of you probably know that the first version of Boxcheck threw out some odd errors for some tests and caused confusion for those that depended on the results.</description><content type="html"><![CDATA[<p>I&rsquo;ve been a bit slow on new posts lately for two main reasons. First, I&rsquo;ve been working hard with my <a href="http://www.slicehost.com/articles/2009/2/23/it-s-getting-crowded-in-here">new position at Slicehost</a>, (<a href="http://www.slicehost.com/articles/2008/10/22/big-news-today">I&rsquo;m still with Rackspace</a>). Also, I&rsquo;ve thrown out the old code for Boxcheck.com and I&rsquo;m re-writing it from scratch.</p>
<p>Most of you probably know that the first version of Boxcheck threw out some odd errors for some tests and caused confusion for those that depended on the results. The code is being overhauled to handle errors and special cases much more efficiently. I&rsquo;m implementing multiple technologies on the back-end to make the interface simpler and easier to use. There may also be an API that folks can script against - but I haven&rsquo;t gotten that far yet.</p>
<p>In short - thanks for all of your e-mails! Boxcheck isn&rsquo;t going away at all. It&rsquo;s going to be back soon! If you have any questions or suggestions, feel free to give me a shout.</p>
<p><strong>UPDATE 2010-03-26:</strong> I&rsquo;ve abandoned the project for now, but if anything changes, I&rsquo;ll be sure to make another post.</p>
]]></content></item><item><title>Upgrade Debian etch to lenny</title><link>https://major.io/2009/02/18/upgrade-debian-etch-to-lenny/</link><pubDate>Wed, 18 Feb 2009 13:28:39 +0000</pubDate><guid>https://major.io/2009/02/18/upgrade-debian-etch-to-lenny/</guid><description>I&amp;rsquo;ve tested this Debian etch to lenny upgrade process a few times so far, and it seems to be working well.</description><content type="html"><![CDATA[<p>I&rsquo;ve tested this Debian etch to lenny upgrade process a few times so far, and it seems to be working well.</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
]]></content></item><item><title>New Linux security advisory Twitter bots</title><link>https://major.io/2009/02/05/new-linux-security-advisory-twitter-bots/</link><pubDate>Thu, 05 Feb 2009 17:00:03 +0000</pubDate><guid>https://major.io/2009/02/05/new-linux-security-advisory-twitter-bots/</guid><description>Thanks to the simplicity of ruby as well as the twitter and httparty gems, I&amp;rsquo;ve whipped up some Twitter bots today. The three bots will alert you when there are Red Hat, Ubuntu, or Debian security advisories. I&amp;rsquo;d set one up for Fedora, but their feed is broken today.
@redhaterrata - Red Hat Errata Updates
@ubuntusecurity - Ubuntu Security Notifications
@debiansecurity - Debian Security Advisories
As always, you can let me know if you have any suggestions for improvements, or other bots you&amp;rsquo;d like to see.</description><content type="html"><![CDATA[<p>Thanks to the simplicity of <a href="http://ruby-lang.org/">ruby</a> as well as the <a href="http://twitter.rubyforge.org/">twitter</a> and <a href="http://github.com/jnunemaker/httparty/tree/master">httparty</a> gems, I&rsquo;ve whipped up some Twitter bots today. The three bots will alert you when there are Red Hat, Ubuntu, or Debian security advisories. I&rsquo;d set one up for Fedora, but their feed is broken today.</p>
<p><strong><a href="http://twitter.com/redhaterrata/">@redhaterrata</a></strong> - Red Hat Errata Updates</p>
<p><strong><a href="http://twitter.com/ubuntusecurity/">@ubuntusecurity</a></strong> - Ubuntu Security Notifications</p>
<p><strong><a href="http://twitter.com/debiansecurity/">@debiansecurity</a></strong> - Debian Security Advisories</p>
<p>As always, you can <a href="http://twitter.com/rackerhacker/">let me know</a> if you have any suggestions for improvements, or other bots you&rsquo;d like to see.</p>
]]></content></item><item><title>Linux: emergency reboot or shutdown with magic commands</title><link>https://major.io/2009/01/29/linux-emergency-reboot-or-shutdown-with-magic-commands/</link><pubDate>Fri, 30 Jan 2009 02:07:06 +0000</pubDate><guid>https://major.io/2009/01/29/linux-emergency-reboot-or-shutdown-with-magic-commands/</guid><description>Most linux distributions use some type of mechanism to gracefully stop daemons and unmount storage volumes during a reboot or shutdown. It&amp;rsquo;s most commonly done via scripts that will wait for each daemon to shut down gracefully before proceeding to the next daemon.
As we know, sometimes servers misbehave due to things put them through, and you can quickly end up in a situation where things are going badly. I&amp;rsquo;m talking about the type of situation where you&amp;rsquo;re connected via SSH to a server that controls phone lines for five million people and it sits in a tiny building 400 miles away from the nearest human being.</description><content type="html"><![CDATA[<p>Most linux distributions use some type of mechanism to gracefully stop daemons and unmount storage volumes during a reboot or shutdown. It&rsquo;s most commonly done via scripts that will wait for each daemon to shut down gracefully before proceeding to the next daemon.</p>
<p>As we know, sometimes servers misbehave due to things put them through, and you can quickly end up in a situation where things are going badly. I&rsquo;m talking about the type of situation where you&rsquo;re connected via SSH to a server that controls phone lines for five million people and it sits in a tiny building 400 miles away from the nearest human being. We&rsquo;re talking bad. If you issue a plain <code>reboot</code> command, it might not even make it that far. Once SSH stops running, you&rsquo;re going to be out of luck.</p>
<p>If you find yourself in this situation (and I hope you won&rsquo;t!), you have some options to get your way with a misbehaving server remotely. You can force an immediate reboot with the following:</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p><!-- raw HTML omitted --><!-- raw HTML omitted -->WHOA THERE!<!-- raw HTML omitted --><!-- raw HTML omitted --> This is pretty much the same as pressing the reset button on the server (if equipped). No daemons will be shut down gracefully, no filesystem sync will occur, and you may get the wrath of a fsck (or worse, a non-booting server) upon reboot. To do things a little more carefully, read on.</p>
<p>These are called <a href="http://en.wikipedia.org/wiki/Magic_SysRq_key">magic commands</a>, and they&rsquo;re pretty much synonymous with holding down Alt-SysRq and another key on older keyboards. Dropping <code>1</code> into <code>/proc/sys/kernel/sysrq</code> tells the kernel that you want to enable SysRq access (it&rsquo;s usually disabled). The second command is equivalent to pressing Alt-SysRq-b on a QWERTY keyboard.</p>
<p>There&rsquo;s a better way of rebooting a misbehaving server that <a href="http://en.wikipedia.org/wiki/Magic_SysRq_key#.22Raising_Elephants.22_mnemonic_device">Wikipedia shows</a> with the mnemonic “Reboot Even If System Utterly Broken”:</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>I can&rsquo;t vouch for this actually working, but I&rsquo;m interested to try it. <strong>UPDATE:</strong> I&rsquo;ve been told that doing this series of commands with ReiserFS is a very bad idea.</p>
<p>If you want to shut the machine down entirely (please think about it before using this on a remote system):</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>If you want to keep SysRq enabled all the time, you can do that with an entry in your server&rsquo;s <code>sysctl.conf</code>:</p>
<!-- raw HTML omitted -->
]]></content></item><item><title>Linux: Adjust storage kernel module load order</title><link>https://major.io/2009/01/26/linux-adjust-storage-kernel-module-load-order/</link><pubDate>Mon, 26 Jan 2009 20:40:01 +0000</pubDate><guid>https://major.io/2009/01/26/linux-adjust-storage-kernel-module-load-order/</guid><description>I set up a system at home that has two SATA controllers: one is on the motherboard (nvidia chipset), while the other is on a Silicon Image SATA card that has three eSATA ports. Here is the relevant lspci output:
There are two primary drives connected to the onboard controller and four connected to the controller card. One of the primary drives on the onboard controller contains the operating system (Ubuntu, in this case), while the other drive is blank.</description><content type="html"><![CDATA[<p>I set up a system at home that has two SATA controllers: one is on the motherboard (nvidia chipset), while the other is on a Silicon Image SATA card that has three eSATA ports. Here is the relevant <code>lspci</code> output:</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>There are two primary drives connected to the onboard controller and four connected to the controller card. One of the primary drives on the onboard controller contains the operating system (Ubuntu, in this case), while the other drive is blank.</p>
<p>When the system booted, the sata_sil24 driver for the add-on card always loaded before the sata_nv drivers for the onboard storage controller:</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>After specifying an explicit order in /etc/modules and /etc/modprobe.conf, I wasn&rsquo;t able to see any changes. The sata_sil24 driver still loaded before the onboard sata_nv driver. Luckily, a <a href="http://twitter.com/Twirrim">very wise person</a> on <a href="http://twitter.com/">Twitter</a> <a href="http://twitter.com/Twirrim/status/1148330615">gave me a strategy</a> that <a href="http://tinyurl.com/d53f6e">worked just fine</a>.</p>
<p>I added sata_sil24 to the bottom of my /etc/modprobe.d/blacklist file first. Then, in /etc/modules, I listed sata_nv first, followed by sata_sil24. When the system booted, I got the result that I wanted:</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>The sata_nv driver is loading first, and Ubuntu boots off of it without an issue. The sata_sil24 driver loads next so that the drives connected to the card show up lower in the boot order.</p>
<p><em>Many thanks to <a href="http://twitter.com/Twirrim">@Twirrim</a> on Twitter for the suggestion!</em></p>
]]></content></item><item><title>Writing a Ruby on Rails application without using a database</title><link>https://major.io/2009/01/09/writing-a-ruby-on-rails-application-without-using-a-database/</link><pubDate>Fri, 09 Jan 2009 17:00:44 +0000</pubDate><guid>https://major.io/2009/01/09/writing-a-ruby-on-rails-application-without-using-a-database/</guid><description>Some of you may be wondering “why would you want to use Rails without a database?” There are several situations why a database would not be needed, and I&amp;rsquo;ve run into quite a few of them. One of the specific cases was when I wanted to write a web interface for an application that only had a REST interface available to the public.
If you find yourself needing to write a Rails application without a database, just do the following:</description><content type="html"><![CDATA[<p>Some of you may be wondering “why would you want to use Rails without a database?” There are several situations why a database would not be needed, and I&rsquo;ve run into quite a few of them. One of the specific cases was when I wanted to write a web interface for an application that only had a REST interface available to the public.</p>
<p>If you find yourself needing to write a Rails application without a database, just do the following:</p>
<p><strong>For Rails 1.0 and up:</strong></p>
<p><code>config/environment.rb</code>:</p>
<!-- raw HTML omitted -->
<p><code>test/test_helper.rb</code></p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p><strong>For Rails 2.1 and up:</strong> Comment out both of the lines that begin with <code>ActiveRecord::Base</code> in <code>config/initializers/new_rails_defaults.rb</code>:</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>For more details, review the <a href="http://wiki.rubyonrails.org/rails/pages/HowToUseRailsWithoutADatabase">full article</a> on <a href="http://rubyonrails.org">rubyonrails.org</a>.</p>
]]></content></item><item><title>CPAN: Automatically install dependencies without confirmation</title><link>https://major.io/2009/01/01/cpan-automatically-install-dependencies-without-confirmation/</link><pubDate>Fri, 02 Jan 2009 01:44:51 +0000</pubDate><guid>https://major.io/2009/01/01/cpan-automatically-install-dependencies-without-confirmation/</guid><description>I enjoy using CPAN because it installs Perl modules with a simple interface, fetches dependencies, and warns you when things are about to end badly. However, one of my biggest complaints is when it constantly confirms installing dependencies. While this is an annoyance if you have to install a module with many dependencies (or if you&amp;rsquo;re working with CPAN on a new server), you can tell CPAN to automatically confirm the installation of dependencies.</description><content type="html"><![CDATA[<p>I enjoy using <a href="http://www.cpan.org/">CPAN</a> because it installs Perl modules with a simple interface, fetches dependencies, and warns you when things are about to end badly. However, one of my biggest complaints is when it constantly confirms installing dependencies. While this is an annoyance if you have to install a module with many dependencies (or if you&rsquo;re working with CPAN on a new server), you can tell CPAN to automatically confirm the installation of dependencies.</p>
<p>To do this, simply bring up a CPAN shell:</p>
<!-- raw HTML omitted -->
<p>Run these two commands in the CPAN shell:</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>Now, exit the CPAN shell, start the CPAN shell, and try to install a module that you need. All dependencies will be automatically confirmed, downloaded and installed.</p>
<p>The first line sets your dependency policy to <em>follow</em> rather than <em>ask</em> (the default). The second line tells CPAN to write the changes to your user&rsquo;s CPAN configuration file to make them permanent.</p>
<p>A big thanks goes out to <a href="http://lee.hambley.name/about">Lee Hambley</a> for the <a href="http://lee.hambley.name/2008/05/cpan-automatically-accept-dependencies">fix</a>.</p>
<p><strong>WARNING:</strong> <em>There are some occasions where you would not want to install dependencies from CPAN. Examples of these situations are when your operating system&rsquo;s package manager (yum, up2date, apt-get, aptitude, etc) has installed Perl modules in an alternative location or when you have manually installed modules in a non-standard way. I&rsquo;m a Red Hat guy, and these problems rarely arise on Red Hat/Fedora systems, but your mileage may vary.</em></p>
]]></content></item><item><title>Upgraded to WordPress 2.7</title><link>https://major.io/2008/12/25/upgraded-to-wordpress-27/</link><pubDate>Fri, 26 Dec 2008 03:45:39 +0000</pubDate><guid>https://major.io/2008/12/25/upgraded-to-wordpress-27/</guid><description>I&amp;rsquo;ve upgraded the blog successfully to WordPress 2.7 (with a bit of frustration). When I attempted the upgrade, I received this error:
Call to undefined method wpdb::has_cap() in schema.php on line 22
Even though I followed the instructions on the WordPress site, and I disabled all of my plugins, the error kept appearing. The only parts of the site that remained unchanged after the upgrade were the wp-config.php page and the wp-content directory.</description><content type="html"><![CDATA[<p>I&rsquo;ve upgraded the blog successfully to <a href="http://wordpress.org/">WordPress 2.7</a> (with a bit of frustration). When I attempted the upgrade, I received this error:</p>
<p><code>Call to undefined method wpdb::has_cap() in schema.php on line 22</code></p>
<p>Even though I followed the instructions on the WordPress site, and I disabled all of my plugins, the error kept appearing. The only parts of the site that remained unchanged after the upgrade were the <code>wp-config.php</code> page and the <code>wp-content</code> directory. I merged the changes in wp-config.php, but the error was still present.</p>
<p>In frustration, I renamed the wp-content directory to a temporary name and uploaded the <code>wp-content</code> provided from the WordPress tarball. The upgrade completed without a problem! I deleted the new <code>wp-content</code> directory, put the old <code>wp-content</code> directory back in place, and WordPress sprang to life!</p>
<p>It&rsquo;s certainly possible that a WordPress plugin blocked the upgrade process even though it was disabled in the database.</p>
]]></content></item><item><title>Reducing inode and dentry caches to keep OOM killer at bay</title><link>https://major.io/2008/12/03/reducing-inode-and-dentry-caches-to-keep-oom-killer-at-bay/</link><pubDate>Thu, 04 Dec 2008 00:44:20 +0000</pubDate><guid>https://major.io/2008/12/03/reducing-inode-and-dentry-caches-to-keep-oom-killer-at-bay/</guid><description>When it comes to frustrating parts of the Linux kernel, OOM killer takes the cake. If it finds that applications are using too much memory on the server, it will kill process abruptly to free up memory for the system to use. I spent much of this week wrestling with a server that was in the clutches of OOM killer.
There are a few processes on the server that keep it fairly busy.</description><content type="html"><![CDATA[<p>When it comes to frustrating parts of the Linux kernel, <a href="http://linux-mm.org/OOM_Killer">OOM killer</a> takes the cake. If it finds that applications are using too much memory on the server, it will kill process abruptly to free up memory for the system to use. I spent much of this week wrestling with a server that was in the clutches of OOM killer.</p>
<p>There are a few processes on the server that keep it fairly busy. Two of the processes are vital to the server&rsquo;s operation – if they are stopped, lots of work is required to get them running properly again. I found that a certain java process was being killed by OOM killer regularly, and another perl process was being killed occasionally.</p>
<p>Naturally, my disdain for java made me think that the java process was the source of the issue. The process was configured to use a small amount of RAM, so it was ruled out. The other perl process used even less memory, so it was ruled out as well. When I checked the sysstat data with sar, I found that the server was only using about 2-3GB out of 4GB of physical memory at the time when OOM killer was started. <em>At this point, I was utterly perplexed.</em></p>
<p>I polled some folks around the office and gathered some ideas. After putting some ideas together, I found that the server was actually caching too much data in the <code>ext3_inode_cache</code> and <code>dentry_cache</code>. These caches hold recently accessed files and directories on the server, and they&rsquo;re purged as the files and directories become stale. Since the operations on the server read and write large amounts of data locally and via NFS, I knew these caches had to be gigantic. If you want to check your own caches, you can use the <code>slabtop</code> command. For those who like things more difficult, you can also <code>cat</code> the contents of <code>/proc/slabinfo</code> and grep for the caches that are important to you.</p>
<p>An immense amount of Googling revealed very little, but I discovered a <a href="http://www.linuxinsight.com/proc_sys_vm_drop_caches.html">dirty hack</a> to fix the issue <strong>(don&rsquo;t run this yet)</strong>:</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p><strong><!-- raw HTML omitted -->There are huge consequences to dumping these caches and running <code>sync</code>.<!-- raw HTML omitted --></strong> If you are writing data at the time you run these commands, you&rsquo;ll actually be dumping the data out of the filesystem cache before it reaches the disk, which could lead to very bad things.</p>
<p>While discussing the issue with a coworker, he <a href="http://www.linuxinsight.com/proc_sys_vm_vfs_cache_pressure.html">found a different method</a> for correcting the issue that was <strong>much</strong> safer. You can echo values into <strong>/proc/sys/vm/vfs_cache_pressure</strong> to tell the kernel what priority it should take when clearing out the inode/dentry caches. LinuxInsight explains the range of values well:</p>
<blockquote>
<p>At the default value of vfs_cache_pressure = 100 the kernel will attempt to reclaim dentries and inodes at a “fair” rate with respect to pagecache and swapcache reclaim. Decreasing vfs_cache_pressure causes the kernel to prefer to retain dentry and inode caches. Increasing vfs_cache_pressure beyond 100 causes the kernel to prefer to reclaim dentries and inodes.</p>
</blockquote>
<p>In short, values less than 100 won&rsquo;t reduce the caches very much as all. Values over 100 will signal to the kernel that you want to clear out the caches at a higher priority. I found that no matter what value you use, the kernel clears the caches at a slow rate. I&rsquo;ve been using a value of 10000 on the server I talked about earlier in the article, and it has kept the caches down to a reasonable level.</p>
]]></content></item><item><title>Simple server monitoring with xinetd</title><link>https://major.io/2008/12/02/simple-server-monitoring-with-xinetd/</link><pubDate>Wed, 03 Dec 2008 00:13:10 +0000</pubDate><guid>https://major.io/2008/12/02/simple-server-monitoring-with-xinetd/</guid><description>You can use the simple but powerful xinetd on your Linux server to monitor almost anything on the server. Since xinetd just holds open a port and waits for a connection, you can tell it to run a script and return the output directly to the network stream.
To start, you&amp;rsquo;ll need a script which will return data to stdout. In this example, I&amp;rsquo;ll use a very simple script like the following:</description><content type="html"><![CDATA[<p>You can use the simple but powerful xinetd on your Linux server to monitor almost anything on the server. Since xinetd just holds open a port and waits for a connection, you can tell it to run a script and return the output directly to the network stream.</p>
<p>To start, you&rsquo;ll need a script which will return data to stdout. In this example, I&rsquo;ll use a very simple script like the following:</p>
<pre><code>#!/bin/bash
echo `uptime | egrep -o 'up ([0-9]+) days' | awk '{print $2}'`
</code></pre><p>This script pulls the number of days that the server has been online. Make the script executable with a <code>chmod +x</code>.</p>
<p>Now, you&rsquo;ll need to choose a port on which to run the xinetd service. I normally find a service in <code>/etc/services</code> that I won&rsquo;t be using on the server. In this example, I&rsquo;ll use <em>isdnlog</em>, which runs on port 20011. Create a file called <code>/etc/xinetd.d/myscript</code> and include the following in the file:</p>
<pre><code>service isdnlog
{
	disable	= no
	socket_type	= stream
	protocol	= tcp
	wait		= no
	user		= root
	server		= /path/to/script.sh
	server_args	= test
}
</code></pre><p>Depending on your xinetd version, you may need to enable your new configuration and restart xinetd:</p>
<pre><code>chkconfig myscript on
/etc/init.d/xinetd restart
</code></pre><p>You can test your new script using netcat:</p>
<pre><code>$ uptime
18:10:30 up 141 days, 19:17,  1 user,  load average: 0.65, 1.47, 1.14
$ nc localhost 20011
141
</code></pre><p>If you need to pass arguments to your script, just adjust the <em>server_args</em> line in the xinetd configuration. Also, be sure that your script is set up to handle the arguments.</p>
]]></content></item><item><title>MySQLTuner 1.0.0 is now available</title><link>https://major.io/2008/12/01/mysqltuner-100-is-now-available/</link><pubDate>Tue, 02 Dec 2008 00:31:43 +0000</pubDate><guid>https://major.io/2008/12/01/mysqltuner-100-is-now-available/</guid><description>It has finally arrived. MySQLTuner 1.0.0 is now available for download. Major improvements include full support for MySQL 5.1 and Solaris. You can review other changes in the changelog.
As always, I&amp;rsquo;d like to personally thank everyone who has made this release possible. Your efforts have provided benefits from MySQL users worldwide, and they have shown me how rewarding it is to maintain open source software.</description><content type="html"><![CDATA[<p>It has finally arrived. MySQLTuner 1.0.0 is now available for <a href="http://mysqltuner.pl/">download</a>. Major improvements include full support for MySQL 5.1 and Solaris. You can review other changes in the <a href="http://wiki.mysqltuner.com/Changelog">changelog</a>.</p>
<p>As always, I&rsquo;d like to personally thank everyone who has made this release possible. Your efforts have provided benefits from MySQL users worldwide, and they have shown me how rewarding it is to maintain open source software.</p>
]]></content></item><item><title>Upgraded to WordPress 2.6.5</title><link>https://major.io/2008/12/01/upgraded-to-wordpress-265/</link><pubDate>Tue, 02 Dec 2008 00:18:51 +0000</pubDate><guid>https://major.io/2008/12/01/upgraded-to-wordpress-265/</guid><description>If you run a WordPress blog, and you haven&amp;rsquo;t upgraded to WordPress 2.6.5, you might want to make the move soon. There were a few bugs and security issues fixed, along with other changes.</description><content type="html"><![CDATA[<p>If you run a WordPress blog, and you haven&rsquo;t <a href="http://codex.wordpress.org/Upgrading_WordPress">upgraded</a> to <a href="http://wordpress.org/download/">WordPress 2.6.5</a>, you might want to make the move soon. There were a few bugs and security issues fixed, along with <a href="http://codex.wordpress.org/Version_2.6.5">other changes</a>.</p>
]]></content></item><item><title>Importing Excel files into MySQL with PHP</title><link>https://major.io/2008/11/07/importing-excel-files-into-mysql-with-php/</link><pubDate>Fri, 07 Nov 2008 19:42:45 +0000</pubDate><guid>https://major.io/2008/11/07/importing-excel-files-into-mysql-with-php/</guid><description>If you have Excel files that need to be imported into MySQL, you can import them easily with PHP. First, you will need to download some prerequisites:
PHPExcelReader - http://sourceforge.net/projects/phpexcelreader/
Spreadsheet_Excel_Writer - http://pear.php.net/package/Spreadsheet_Excel_Writer
Once you&amp;rsquo;ve downloaded both items, upload them to your server. Your directory listing on your server should have two directories: Excel (from PHPExcelReader) and Spreadsheet_Excel_Writer-x.x.x (from Spreadsheet_Excel_Writer). To work around a bug in PHPExcelReader, copy oleread.inc from the Excel directory into a new path:</description><content type="html"><![CDATA[<p>If you have Excel files that need to be imported into MySQL, you can import them easily with PHP. First, you will need to download some prerequisites:</p>
<p>PHPExcelReader - <a href="http://sourceforge.net/projects/phpexcelreader/">http://sourceforge.net/projects/phpexcelreader/</a></p>
<p>Spreadsheet_Excel_Writer - <a href="http://pear.php.net/package/Spreadsheet_Excel_Writer">http://pear.php.net/package/Spreadsheet_Excel_Writer</a></p>
<p>Once you&rsquo;ve downloaded both items, upload them to your server. Your directory listing on your server should have two directories: <code>Excel</code> <em>(from PHPExcelReader)</em> and <code>Spreadsheet_Excel_Writer-x.x.x</code> <em>(from Spreadsheet_Excel_Writer)</em>. To work around a bug in PHPExcelReader, copy <code>oleread.inc</code> from the <code>Excel</code> directory into a new path:</p>
<p><code>Spreadsheet/Excel/Reader/OLERead.php</code></p>
<p>The PHPExcelReader code will expect <code>OLERead.php</code> to be in that specific location. Once that is complete, you&rsquo;re ready to use the PHPExcelReader class. I made an example Excel spreadsheet like this:</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>After that, I created a PHP script to pick up the data and insert it into the database, row by row:</p>
<pre><code>require_once 'Excel/reader.php';
$data = new Spreadsheet_Excel_Reader();
$data-&gt;setOutputEncoding('CP1251');
$data-&gt;read('exceltestsheet.xls');

$conn = mysql_connect(&quot;hostname&quot;,&quot;username&quot;,&quot;password&quot;);
mysql_select_db(&quot;database&quot;,$conn);

for ($x = 2; $x &lt; = count($data-&gt;sheets[0][&quot;cells&quot;]); $x++) {
    $name = $data-&gt;sheets[0][&quot;cells&quot;][$x][1];
    $extension = $data-&gt;sheets[0][&quot;cells&quot;][$x][2];
    $email = $data-&gt;sheets[0][&quot;cells&quot;][$x][3];
    $sql = &quot;INSERT INTO mytable (name,extension,email)
        VALUES ('$name',$extension,'$email')&quot;;
    echo $sql.&quot;\n&quot;;
    mysql_query($sql);
}
</code></pre><p>After the script ran, each row had been added to the database table successfully. If you have additional columns to insert, just repeat these lines, using an appropriate variable for each column:</p>
<pre><code>sheets[0][&quot;cells&quot;][$row_number][$column_number];
</code></pre><p>For more details, you can refer to a <a href="http://devzone.zend.com/article/3336-Reading-and-Writing-Spreadsheets-with-PHP">post in Zend&rsquo;s Developer Zone</a>.</p>
]]></content></item><item><title>Plesk: Upgrade to 8.4 causes “no such user” error in maillog</title><link>https://major.io/2008/11/06/plesk-upgrade-to-84-causes-no-such-user-error-in-maillog/</link><pubDate>Thu, 06 Nov 2008 17:04:08 +0000</pubDate><guid>https://major.io/2008/11/06/plesk-upgrade-to-84-causes-no-such-user-error-in-maillog/</guid><description>If you have a Plesk server where short mail names are enabled, upgrading to Plesk 8.4 can cause some issues. Valid logins may be rejected, and they&amp;rsquo;ll appear in your /usr/local/psa/var/log/maillog as &amp;ldquo;no such user&amp;rdquo;. You can correct the issue by switching to long mail names (click Server -&amp;gt; Mail in Plesk), or you can run a shell script provided by Parallels.
For further details, refer to the Plesk KB article &amp;ldquo;Mail users cannot get or send mail after upgrade to Plesk 8.</description><content type="html"><![CDATA[<p>If you have a Plesk server where short mail names are enabled, upgrading to Plesk 8.4 can cause some issues. Valid logins may be rejected, and they&rsquo;ll appear in your /usr/local/psa/var/log/maillog as &ldquo;no such user&rdquo;. You can correct the issue by switching to long mail names (click Server -&gt; Mail in Plesk), or you can <a href="http://kb.parallels.com/Attachments/4889/Attachments/mail_fix.sh">run a shell script</a> provided by <a href="http://parallels.com/">Parallels</a>.</p>
<p>For further details, refer to the Plesk KB article <a href="http://kb.parallels.com/en/5256">&ldquo;Mail users cannot get or send mail after upgrade to Plesk 8.4&rdquo;</a></p>
]]></content></item><item><title>Viewing documentation for your ruby gems</title><link>https://major.io/2008/11/05/viewing-documentation-for-your-ruby-gems/</link><pubDate>Thu, 06 Nov 2008 00:14:57 +0000</pubDate><guid>https://major.io/2008/11/05/viewing-documentation-for-your-ruby-gems/</guid><description>I stumbled into this four line ruby script that will serve up all of the rdoc documentation for your server&amp;rsquo;s currently installed gems:
#!/usr/bin/env ruby require &amp;quot;rubygems/server&amp;quot; options = {:gemdir =&amp;gt; Gem.dir, :port =&amp;gt; 4242, :daemon =&amp;gt; true} Gem::Server::run(options) Thanks to Daniel for the ruby code!</description><content type="html"><![CDATA[<p>I stumbled into this four line ruby script that will serve up all of the rdoc documentation for your server&rsquo;s currently installed gems:</p>
<pre><code>#!/usr/bin/env ruby
require &quot;rubygems/server&quot;
options = {:gemdir =&gt; Gem.dir, :port =&gt; 4242, :daemon =&gt; true}
Gem::Server::run(options)
</code></pre><p><em>Thanks to Daniel for the ruby code!</em></p>
]]></content></item><item><title>What is ‘steal time’ in my sysstat output?</title><link>https://major.io/2008/11/04/what-is-steal-time-in-my-sysstat-output/</link><pubDate>Tue, 04 Nov 2008 17:19:08 +0000</pubDate><guid>https://major.io/2008/11/04/what-is-steal-time-in-my-sysstat-output/</guid><description>After running sar on my new slice from SliceHost*, I noticed a new column called steal. It&amp;rsquo;s generally very low on my virtual machine, and I&amp;rsquo;ve never seen it creep over 1-2%.
IBM&amp;rsquo;s definition of steal time is actually pretty good:
Steal time is the percentage of time a virtual CPU waits for a real CPU while the hypervisor is servicing another virtual processor.
So, relatively speaking, what does this mean?</description><content type="html"><![CDATA[<p>After running sar on my new slice from <a href="http://slicehost.com/">SliceHost</a>*, I noticed a new column called <em>steal</em>. It&rsquo;s generally very low on my virtual machine, and I&rsquo;ve never seen it creep over 1-2%.</p>
<p>IBM&rsquo;s <a href="http://www.ibm.com/developerworks/linux/linux390/perf/tuning_rec_CPUtimes_virtual.html">definition</a> of steal time is actually pretty good:</p>
<blockquote>
<p>Steal time is the percentage of time a virtual CPU waits for a real CPU while the hypervisor is servicing another virtual processor.</p>
</blockquote>
<p>So, relatively speaking, what does this mean?</p>
<p>A high steal percentage may mean that you may be outgrowing your virtual machine with your hosting company. Other virtual machines may have a larger slice of the CPU&rsquo;s time and you may need to ask for an upgrade in order to compete. Also, a high steal percentage may mean that your hosting company is overselling virtual machines on your particular server. If you upgrade your virtual machine and your steal percentage doesn&rsquo;t drop, you may want to seek another provider.</p>
<p>A low steal percentage can mean that your applications are working well with your current virtual machine. Since your VM is not wrestling with other VM&rsquo;s constantly for CPU time, your VM will be more responsive. This may also suggest that your hosting provider is underselling their servers, which is definitely a good thing.</p>
<ul>
<li><em>I&rsquo;ve been a customer of <a href="http://slicehost.com/">SliceHost</a> for a while (prior to <a href="http://www.slicehost.com/articles/2008/10/22/big-news-today">Rackspace&rsquo;s acquisition</a>), and I recommend them to anyone who needs a solid VM solution. If you want to help out with my hosting costs, you&rsquo;re welcome to use my <a href="https://manage.slicehost.com/customers/new?referrer=6fc0943c343da4f6b87dbe5abf500c2e">SliceHost referral link</a>.</em></li>
</ul>
]]></content></item><item><title>Syncing an iPhone with a new Mac without hassles</title><link>https://major.io/2008/11/02/syncing-an-iphone-with-a-new-mac-without-hassles/</link><pubDate>Sun, 02 Nov 2008 16:56:23 +0000</pubDate><guid>https://major.io/2008/11/02/syncing-an-iphone-with-a-new-mac-without-hassles/</guid><description>I know I usually talk about Linux server related topics on this blog, but I&amp;rsquo;m pretty proud of what I&amp;rsquo;ve figured out this morning on my Mac. As you know, the iPhone can really only fully sync with one machine, and if you want to connect it to a new Mac that you&amp;rsquo;ve purchased, you have to fully erase the iPhone and start over. (Of course, if you used the Migration Assistant to set up your new Mac, this won&amp;rsquo;t be necessary.</description><content type="html"><![CDATA[<p>I know I usually talk about Linux server related topics on this blog, but I&rsquo;m pretty proud of what I&rsquo;ve figured out this morning on my Mac. As you know, the iPhone can really only fully sync with one machine, and if you want to connect it to a new Mac that you&rsquo;ve purchased, you have to fully erase the iPhone and start over. (Of course, if you used the Migration Assistant to set up your new Mac, this won&rsquo;t be necessary.)</p>
<p>Here are the steps to migrate your iTunes data from one Mac to another without having to erase and re-sync your iPhone:</p>
<ul>
<li><strong>Make sure that iTunes is not running on <em>both</em> Macs.</strong></li>
<li><strong>Disconnect your iPhone/iPod from <em>both</em> Macs.</strong></li>
<li>Copy your iTunes folder.
<code>/Users/username/Music/iTunes</code></li>
<li>Copy your iPhone/iPod backups.
<code>/Users/username/Library/Application Support/MobileSync</code></li>
<li>Copy your iTunes configuration files.
<code>/Users/username/Library/Preferences/com.apple.iTunes*</code></li>
<li>Open iTunes on your new Mac and verify that <em>Applications</em> and <em>Ringtones</em> appear.</li>
<li>Connect your iPhone/iPod to the new Mac and accept any new authorizations.</li>
<li>Use iTunes on your old Mac to de-authorize the computer.</li>
</ul>
<p>If you choose to keep your MP3&rsquo;s separate from iTunes (and not in the library), this will only copy over the references to the MP3 files themselves.</p>
]]></content></item><item><title>Installing Microsoft’s TrueType fonts on Linux servers</title><link>https://major.io/2008/10/23/installing-microsofts-truetype-fonts-on-linux-servers/</link><pubDate>Fri, 24 Oct 2008 00:31:23 +0000</pubDate><guid>https://major.io/2008/10/23/installing-microsofts-truetype-fonts-on-linux-servers/</guid><description>Although the idea of putting something from Microsoft on a Linux box might sound awful at first, you may find a reason to use Microsoft TrueType fonts on a Linux server. If you&amp;rsquo;re using GD to render an image, these fonts may come in handy.
If you have an RPM-based linux distribution, you can use a spec file that is available on SourceForge. You can follow the instructions on the project&amp;rsquo;s page, or you can follow these abbreviated instructions here:</description><content type="html"><![CDATA[<p>Although the idea of <a href="http://www.rtr.com/fpsupport/">putting something from Microsoft on a Linux box</a> might sound awful at first, you may find a reason to use Microsoft TrueType fonts on a Linux server. If you&rsquo;re using GD to render an image, these fonts may come in handy.</p>
<p>If you have an RPM-based linux distribution, you can use a spec file that is available on <a href="http://corefonts.sourceforge.net/">SourceForge</a>. You can follow the instructions on the <a href="http://corefonts.sourceforge.net/">project&rsquo;s page</a>, or you can follow these abbreviated instructions here:</p>
<p>Install some prerequisites:</p>
<pre><code>// RHEL 4
up2date -i rpm-build wget ttmkfdir
// RHEL 5
yum install rpm-build wget ttmkfdir
</code></pre><p>Install <a href="http://rpmfind.net/linux/rpm2html/search.php?query=cabextract&amp;submit=Search+...">cabextract</a>.</p>
<p>Build the RPM:</p>
<pre><code>wget -O /usr/src/redhat/SPECS/msttcorefonts-2.0-1.spec http://corefonts.sourceforge.net/msttcorefonts-2.0-1.spec
rpmbuild -bb msttcorefonts-2.0-1.spec
rpm -Uvh /usr/src/redhat/SPECS/msttcorefonts-2.0-1.spec
</code></pre><p>Test it to be sure that they&rsquo;re installed:</p>
<pre><code>xlsfonts | grep ^-microsoft
rpm -ql msttcorefonts
</code></pre>]]></content></item><item><title>ext3_dx_add_entry: Directory index full!</title><link>https://major.io/2008/10/13/ext3_dx_add_entry-directory-index-full/</link><pubDate>Mon, 13 Oct 2008 17:00:51 +0000</pubDate><guid>https://major.io/2008/10/13/ext3_dx_add_entry-directory-index-full/</guid><description>I found a server last week that was having severe issues with disk I/O to the point where most operations were taking many minutes to complete. The server wasn&amp;rsquo;t under much load, but a quick run of dmesg threw quite a bit of these lines out onto the screen:
EXT3-fs warning (device sda5): ext3_dx_add_entry: Directory index full!
After a thorough amount of searching, I couldn&amp;rsquo;t find out what the error actually meant.</description><content type="html"><![CDATA[<p>I found a server last week that was having severe issues with disk I/O to the point where most operations were taking many minutes to complete. The server wasn&rsquo;t under much load, but a quick run of <code>dmesg</code> threw quite a bit of these lines out onto the screen:</p>
<p><code>EXT3-fs warning (device sda5): ext3_dx_add_entry: Directory index full!</code></p>
<p>After a thorough amount of searching, I couldn&rsquo;t find out what the error actually meant. As with most errors starting with <code>EXT3-fs warning</code>, I figured that a fsck might be the best option.</p>
<p>During the fsck, several inodes were repaired and the check completed after 10-15 minutes. I jotted down some notes about the directories that popped up on the screen during the fsck. The server rebooted it came up without any problems.</p>
<p>I reviewed the directories that appeared during the fsck and they were full of files. Some of the directories contained upwards of 200,000 files. Many of the files were moved into lost+found after the fsck, so they had to be restored from their backups. I still don&rsquo;t know what caused the original issue as the hardware checked out fine. If you run into this error, a fsck should help, but make sure that you have backups handy.</p>
]]></content></item><item><title>Working hard on PleskHacker</title><link>https://major.io/2008/09/25/working-hard-on-pleskhacker/</link><pubDate>Thu, 25 Sep 2008 17:00:19 +0000</pubDate><guid>https://major.io/2008/09/25/working-hard-on-pleskhacker/</guid><description>Some of you may have already noticed from my Twitter feed, but I&amp;rsquo;ve started a complete re-write of pleskhacker.com in an easier-to-use format. The old site was done in Dokuwiki, and I found that it didn&amp;rsquo;t scale too well. The new site uses WordPress, and the performance is already improving.
Another improvement to the site is that it is now hosted on Mosso. This should allow the site to handle a lot more traffic and be more redundant than the original configuration.</description><content type="html"><![CDATA[<p>Some of you may have already noticed from my <a href="http://twitter.com/rackerhacker/">Twitter feed</a>, but I&rsquo;ve started a complete re-write of <a href="http://pleskhacker.com/">pleskhacker.com</a> in an easier-to-use format. The old site was done in Dokuwiki, and I found that it didn&rsquo;t scale too well. The new site uses WordPress, and the performance is already improving.</p>
<p>Another improvement to the site is that it is now hosted on <a href="http://mosso.com/">Mosso</a>. This should allow the site to handle a lot more traffic and be more redundant than the original configuration. Eventually, I&rsquo;ll move this blog to Mosso| as well.</p>
<p><strong>What is PleskHacker?</strong> It&rsquo;s part of an effort to provide more detailed documentation of Plesk for system administrators. PleskHacker has documentation covering one third of the Plesk database so far. It includes definitions for each column and table, but it also shows relationships between tables so you can create your own JOIN queries with ease.</p>
]]></content></item><item><title>Apache 2.2: internal dummy connection</title><link>https://major.io/2008/09/23/apache-22-internal-dummy-connection/</link><pubDate>Wed, 24 Sep 2008 01:42:21 +0000</pubDate><guid>https://major.io/2008/09/23/apache-22-internal-dummy-connection/</guid><description>After working with some RHEL 5 servers fairly regularly, I noticed a reduction in Apache 2.2 performance when many connections were made to the server. There were messages like these streaming into the access_log as well:
127.0.0.1 - - [21/Aug/2008:12:00:10 -0400] &amp;quot;GET / HTTP/1.0&amp;quot; 200 2269 &amp;quot;-&amp;quot; &amp;quot;Apache/2.2.3 (Red Hat) (internal dummy connection)&amp;quot;&amp;lt;br /&amp;gt; 127.0.0.1 - - [21/Aug/2008:12:00:11 -0400] &amp;quot;GET / HTTP/1.0&amp;quot; 200 2269 &amp;quot;-&amp;quot; &amp;quot;Apache/2.2.3 (Red Hat) (internal dummy connection)&amp;quot;&amp;lt;br /&amp;gt; 127.</description><content type="html"><![CDATA[<p>After working with some RHEL 5 servers fairly regularly, I noticed a reduction in Apache 2.2 performance when many connections were made to the server. There were messages like these streaming into the access_log as well:</p>
<p><code>127.0.0.1 - - [21/Aug/2008:12:00:10 -0400] &quot;GET / HTTP/1.0&quot; 200 2269 &quot;-&quot; &quot;Apache/2.2.3 (Red Hat) (internal dummy connection)&quot;&lt;br /&gt; 127.0.0.1 - - [21/Aug/2008:12:00:11 -0400] &quot;GET / HTTP/1.0&quot; 200 2269 &quot;-&quot; &quot;Apache/2.2.3 (Red Hat) (internal dummy connection)&quot;&lt;br /&gt; 127.0.0.1 - - [21/Aug/2008:12:00:13 -0400] &quot;GET / HTTP/1.0&quot; 200 2269 &quot;-&quot; &quot;Apache/2.2.3 (Red Hat) (internal dummy connection)&quot;&lt;br /&gt; 127.0.0.1 - - [21/Aug/2008:12:00:14 -0400] &quot;GET / HTTP/1.0&quot; 200 2269 &quot;-&quot; &quot;Apache/2.2.3 (Red Hat) (internal dummy connection)&quot;&lt;br /&gt; 127.0.0.1 - - [21/Aug/2008:12:00:15 -0400] &quot;GET / HTTP/1.0&quot; 200 2269 &quot;-&quot; &quot;Apache/2.2.3 (Red Hat) (internal dummy connection)&quot;</code></p>
<p>On servers with ipv6 enabled, you might see a line like this one:</p>
<p><code>::1 - - [21/Aug/2008:12:00:15 -0400] &quot;GET / HTTP/1.0&quot; 200 2269 &quot;-&quot; &quot;Apache/2.2.3 (Red Hat) (internal dummy connection)&quot;</code></p>
<p>I began to wonder why Apache was making these connections back onto itself and initiating a <code>GET /</code>. Apache&rsquo;s <a href="http://wiki.apache.org/httpd/InternalDummyConnection">documentation</a> had the following:</p>
<blockquote>
<p>When the Apache HTTP Server manages its child processes, it needs a way to wake up processes that are listening for new connections. To do this, it sends a simple HTTP request back to itself. This request will appear in the access_log file with the remote address set to the loop-back interface (typically 127.0.0.1 or ::1 if IPv6 is configured). If you log the User-Agent string (as in the combined log format), you will see the server signature followed by &ldquo;(internal dummy connection)&rdquo; on non-SSL servers. During certain periods you may see up to one such request for each httpd child process.</p>
<p>These requests are perfectly normal and you do not, in general, need to worry about them. They can simply be ignored.</p>
</blockquote>
<p>Sure, I could easily ignore the requests, but the requests were increasing the load on my server more than I liked. Apache&rsquo;s documentation suggested omitting the lines from the logs by adding the following to the Apache configuration:</p>
<p><code>SetEnvIf Remote_Addr &quot;127\.0\.0\.1&quot; loopback</code></p>
<p>And then adding <code>env=!loopback</code> to your <code>CustomLog</code> lines ensures that the data won&rsquo;t show up in your access logs. However, you&rsquo;ll still end up with <code>Directory index forbidden by Options directive: /var/www/html/</code> filling up your error_logs. A quick search revealed a <a href="http://www.inventivelabs.com.au/weblog/post/apache-s-internal-dummy-connection">handy mod_rewrite</a> rule to get rid of these requests as quickly as possible with the lowest effort required from Apache:</p>
<p><code>RewriteCond %{HTTP_USER_AGENT} ^.*internal\ dummy\ connection.*$ [NC]&lt;br /&gt; RewriteRule .* - [F,L]</code></p>
<p>At this point, the requests to the localhost should receive a 403 immediately. Since you can&rsquo;t keep Apache from sending all of these requests to itself, the best you can do is respond to them in a manner that requires the lowest possible resources.</p>
]]></content></item><item><title>New look</title><link>https://major.io/2008/09/20/new-look/</link><pubDate>Sat, 20 Sep 2008 20:19:05 +0000</pubDate><guid>https://major.io/2008/09/20/new-look/</guid><description>I&amp;rsquo;ve decided to adjust the look of this blog to make it easier to read and faster to download. My old Mandigo theme was great, but it required each user to download about 200kb worth of content to see the blog. The new theme is about 30-40kb per download and it renders faster in all the browsers I&amp;rsquo;ve tested.
Leave a comment and let me know what you think of the changes.</description><content type="html"><![CDATA[<p>I&rsquo;ve decided to adjust the look of this blog to make it easier to read and faster to download. My old Mandigo theme was great, but it required each user to download about 200kb worth of content to see the blog. The new theme is about 30-40kb per download and it renders faster in all the browsers I&rsquo;ve tested.</p>
<p>Leave a comment and let me know what you think of the changes. Have a great weekend!</p>
]]></content></item><item><title>Compress your web content for better performance</title><link>https://major.io/2008/09/19/compress-your-web-content-for-better-performance/</link><pubDate>Fri, 19 Sep 2008 17:00:47 +0000</pubDate><guid>https://major.io/2008/09/19/compress-your-web-content-for-better-performance/</guid><description>Most web developers expend a lot of energy optimizing queries, reducing the overhead of functions, and streamlining their application&amp;rsquo;s overall flow. However, many forget that one of the simplest adjustments is the compression of data as it leaves the web server.
Luckily, mod_deflate makes this easy, and the Apache documentation has a handy initial configuration available:
&amp;lt;Location /&amp;gt; SetOutputFilter DEFLATE BrowserMatch ^Mozilla/4 gzip-only-text/html BrowserMatch ^Mozilla/4\.0[678] no-gzip BrowserMatch \bMSI[E] !no-gzip !gzip-only-text/html SetEnvIfNoCase Request_URI \.</description><content type="html"><![CDATA[<p>Most web developers expend a lot of energy optimizing queries, reducing the overhead of functions, and streamlining their application&rsquo;s overall flow. However, many forget that one of the simplest adjustments is the compression of data as it leaves the web server.</p>
<p>Luckily, <a href="http://httpd.apache.org/docs/2.0/mod/mod_deflate.html">mod_deflate</a> makes this easy, and the Apache documentation has a <a href="http://httpd.apache.org/docs/2.0/mod/mod_deflate.html#recommended">handy initial configuration</a> available:</p>
<pre><code>&lt;Location /&gt;
  SetOutputFilter DEFLATE
  BrowserMatch ^Mozilla/4 gzip-only-text/html
  BrowserMatch ^Mozilla/4\.0[678] no-gzip
  BrowserMatch \bMSI[E] !no-gzip !gzip-only-text/html
  SetEnvIfNoCase Request_URI \.(?:gif|jpe?g|png)$ no-gzip dont-vary
  Header append Vary User-Agent env=!dont-vary
&lt;/Location&gt;
</code></pre><p>This configuration will compress everything except for images. Of course, you can&rsquo;t test this with curl, but you can test it with Firefox and <a href="https://addons.mozilla.org/en-US/firefox/addon/3829">LiveHTTPHeaders</a>. If you don&rsquo;t have Firefox handy, you can try a very handy <a href="http://www.gidnetwork.com/tools/gzip-test.php">web application</a> that will give you the statistics about the compression of your site&rsquo;s data.</p>
]]></content></item><item><title>Thanks for making RackerHacker so popular!</title><link>https://major.io/2008/09/14/thanks-for-making-rackerhacker-so-popular/</link><pubDate>Mon, 15 Sep 2008 03:31:09 +0000</pubDate><guid>https://major.io/2008/09/14/thanks-for-making-rackerhacker-so-popular/</guid><description>After a quick review of the stats this evening, I found that this blog has made it to 500,000 sessions so far this year. Page views are just past 2.5 million! In 2007, there were over 65,000 sessions and over 427,000 million page views in 2007 as a final count.
Here&amp;rsquo;s the exact numbers as of tonight:
2008 Year to Date:
Sessions: 501,166 Pageviews: 2,506,011 Hits: 7,414,333 Bytes Transferred: 65.</description><content type="html"><![CDATA[<p>After a quick review of the stats this evening, I found that this blog has made it to 500,000 sessions so far this year. Page views are just past 2.5 million! In 2007, there were over 65,000 sessions and over 427,000 million page views in 2007 as a final count.</p>
<p>Here&rsquo;s the exact numbers as of tonight:</p>
<p>2008 Year to Date:</p>
<ul>
<li>Sessions: 501,166</li>
<li>Pageviews: 2,506,011</li>
<li>Hits: 7,414,333</li>
<li>Bytes Transferred: 65.19GB</li>
</ul>
<p>2007 Totals:</p>
<ul>
<li>Sessions: 65,849</li>
<li>Pageviews: 427,923</li>
<li>Hits: 1,199,403</li>
<li>Bytes Transferred: 36.22GB</li>
</ul>
<p>Thanks to everyone for making this blog so popular! It started as a simple place to stash knowledge that I&rsquo;ve gained during my work as a Linux systems administrator, but it&rsquo;s turned into something that helps people around the world with their daily work.</p>
<p>Thank you!</p>
]]></content></item><item><title>Upgraded to WordPress 2.6.2</title><link>https://major.io/2008/09/09/upgraded-to-wordpres-262/</link><pubDate>Wed, 10 Sep 2008 03:16:53 +0000</pubDate><guid>https://major.io/2008/09/09/upgraded-to-wordpres-262/</guid><description>I&amp;rsquo;ve updated the blog to WordPress 2.6.2 tonight. There are some relatively serious security concerns with some of the code in WordPress 2.6.1, so I&amp;rsquo;d recommend upgrading at your earliest convenience.
Slide on over to the WordPress site and download it now.</description><content type="html"><![CDATA[<p>I&rsquo;ve updated the blog to WordPress 2.6.2 tonight. There are some <a href="http://codex.wordpress.org/Changelog/2.6.2">relatively serious security concerns</a> with some of the code in WordPress 2.6.1, so I&rsquo;d recommend upgrading at your earliest convenience.</p>
<p>Slide on over to the <a href="http://wordpress.org/">WordPress</a> site and <a href="http://wordpress.org/download/">download it now</a>.</p>
]]></content></item><item><title>SquirrelMail: 127 Can’t execute command</title><link>https://major.io/2008/09/08/squirrelmail-127-cant-execute-command/</link><pubDate>Mon, 08 Sep 2008 17:16:00 +0000</pubDate><guid>https://major.io/2008/09/08/squirrelmail-127-cant-execute-command/</guid><description>I found a Plesk 8.3 server running RHEL 4 last month that was presenting errors when users attempted to send e-mail via SquirrelMail:
ERROR: Email delivery error Server replied: 127 Can't execute command '/usr/sbin/sendmail -i -t -fsomeuser@somedomain.com'. The error was appearing because safe_mode was enabled and SquirrelMail was unable to drop e-mails into /usr/sbin/squirrelmail. After disabling safe_mode on the server, the users were able to send e-mails via SquirrelMail.</description><content type="html"><![CDATA[<p>I found a Plesk 8.3 server running RHEL 4 last month that was presenting errors when users attempted to send e-mail via SquirrelMail:</p>
<pre><code>ERROR:
Email delivery error
Server replied: 127 Can't execute command '/usr/sbin/sendmail -i -t -fsomeuser@somedomain.com'.
</code></pre><p>The error was appearing because safe_mode was enabled and SquirrelMail was unable to drop e-mails into /usr/sbin/squirrelmail. After disabling safe_mode on the server, the users were able to send e-mails via SquirrelMail.</p>
]]></content></item><item><title>MySQLTuner 0.9.9 is now available</title><link>https://major.io/2008/09/07/mysqltuner-099-is-now-available/</link><pubDate>Mon, 08 Sep 2008 02:05:03 +0000</pubDate><guid>https://major.io/2008/09/07/mysqltuner-099-is-now-available/</guid><description>MySQLTuner 0.9.9 is now available for download. There&amp;rsquo;s a new wiki for the script at wiki.mysqltuner.com. The new wiki contains links to downloads, changelogs, and other details.
If you want to know when the latest releases are available, there are multiple options:
Follow RackerHacker on Twitter Sign up for the MySQLTuner mailing list Use the --checkversion option when you run the script</description><content type="html"><![CDATA[<p>MySQLTuner 0.9.9 is now available for <a href="http://mysqltuner.pl/">download</a>. There&rsquo;s a new wiki for the script at <a href="http://wiki.mysqltuner.com">wiki.mysqltuner.com</a>. The new wiki contains links to downloads, changelogs, and other details.</p>
<p>If you want to know when the latest releases are available, there are multiple options:</p>
<ul>
<li>Follow <a href="http://twitter.com/RackerHacker/">RackerHacker</a> on <a href="http://twitter.com/">Twitter</a></li>
<li>Sign up for the <a href="http://wiki.mysqltuner.com/MySQLTuner#Mailing_List">MySQLTuner mailing list</a></li>
<li>Use the <code>--checkversion</code> option when you run the script</li>
</ul>
]]></content></item><item><title>Listing of VMWare configuration parameters</title><link>https://major.io/2008/09/05/listing-of-vmware-configuration-parameters/</link><pubDate>Fri, 05 Sep 2008 17:09:54 +0000</pubDate><guid>https://major.io/2008/09/05/listing-of-vmware-configuration-parameters/</guid><description>For a recent project, I needed to automatically provision VM&amp;rsquo;s for testing. I wanted to make the .vmx files on the fly with the exact configuration required, but I couldn&amp;rsquo;t find documentation for the options that are allowed in the .vmx files. Luckily, a fellow named Ulli Hankeln has made an impressive list available on his site.
The listings contain tons of options that I wasn&amp;rsquo;t aware of, and it also provides hints on which ones you shouldn&amp;rsquo;t adjust.</description><content type="html"><![CDATA[<p>For a recent project, I needed to automatically provision VM&rsquo;s for testing. I wanted to make the .vmx files on the fly with the exact configuration required, but I couldn&rsquo;t find documentation for the options that are allowed in the .vmx files. Luckily, a fellow named Ulli Hankeln has <a href="http://sanbarrow.com/vmx.html">made an impressive list</a> available on his site.</p>
<p>The listings contain tons of options that I wasn&rsquo;t aware of, and it also provides hints on which ones you shouldn&rsquo;t adjust.</p>
]]></content></item><item><title>CentOS/RHEL x86_64 + VMWare: Use of uninitialized value in string</title><link>https://major.io/2008/09/03/centosrhel-x86_64-vmware-use-of-uninitialized-value-in-string/</link><pubDate>Wed, 03 Sep 2008 17:06:20 +0000</pubDate><guid>https://major.io/2008/09/03/centosrhel-x86_64-vmware-use-of-uninitialized-value-in-string/</guid><description>I was working with a CentOS 5 x86_64 installation running VMWare server last week when I stumbled upon this error:
Use of uninitialized value in string eq at /usr/lib64/perl5/site_perl/5.8.8/x86_64-linux-thread-multi/VMware/VmPerl.pm line 114. You can run the vmware-cmd application with this error (it&amp;rsquo;s not a fatal error) and keep going with your normal business. However, if you want to remove the error, comment out lines 114 and 115 in the Perl module referenced by the error:</description><content type="html"><![CDATA[<p>I was working with a CentOS 5 x86_64 installation running VMWare server last week when I stumbled upon this error:</p>
<pre><code>Use of uninitialized value in string eq at
/usr/lib64/perl5/site_perl/5.8.8/x86_64-linux-thread-multi/VMware/VmPerl.pm line 114.
</code></pre><p>You can run the vmware-cmd application with this error (it&rsquo;s not a fatal error) and keep going with your normal business. However, if you want to remove the error, comment out lines 114 and 115 in the Perl module referenced by the error:</p>
<pre><code>die &quot;Perl API Version does not match dynamic library version.&quot;
    unless (version() eq $VERSION);
</code></pre>
<p>Commenting out these lines does not affect the VMWare server in any way.</p>
]]></content></item><item><title>Encrypted filesystems and partitions on RHEL 5</title><link>https://major.io/2008/09/01/encrypted-filesystems-and-partitions-on-rhel-5/</link><pubDate>Tue, 02 Sep 2008 01:55:36 +0000</pubDate><guid>https://major.io/2008/09/01/encrypted-filesystems-and-partitions-on-rhel-5/</guid><description>I spoke with a customer last week who was curious about enabling encrypted partitions on a DAS connected to their server. I wasn&amp;rsquo;t entirely sure if it was possible in RHEL 5 since I couldn&amp;rsquo;t remember if it was available in Fedora 6. According to Red Hat&amp;rsquo;s release notes, it is possible. Here&amp;rsquo;s an excerpt from their release notes:  Encrypted Swap Partitions and Non-root File Systems
Red Hat Enterprise Linux 5 now provides basic support for encrypted swap partitions and non-root file systems.</description><content type="html"><![CDATA[<p>I spoke with a customer last week who was curious about enabling encrypted partitions on a DAS connected to their server.  I wasn&rsquo;t entirely sure if it was possible in RHEL 5 since I couldn&rsquo;t remember if it was available in Fedora 6.  According to <a href="http://www.redhat.com/docs/manuals/enterprise/RHEL-5-manual/release-notes/RELEASE-NOTES-x86-en.html">Red Hat&rsquo;s release notes</a>, it is possible.  Here&rsquo;s an excerpt from their release notes: </p>
<blockquote>
<p>Encrypted Swap Partitions and Non-root File Systems</p>
</blockquote>
<blockquote>
<p>Red Hat Enterprise Linux 5 now provides basic support for encrypted swap partitions and non-root file systems. To use these features, add the appropriate entries to /etc/crypttab and reference the created devices in /etc/fstab.</p>
<p>Below is a sample /etc/crypttab entry:</p>
<p>my_swap /dev/hdb1 /dev/urandom swap,cipher=aes-cbc-essiv:sha256</p>
</blockquote>
<blockquote>
<p>This creates the encrypted block device /dev/mapper/my_swap, which can be referenced in /etc/fstab.</p>
<p>Below is a sample /etc/crypttab entry for a file system volume:</p>
<p>my_volume /dev/hda5 /etc/volume_key cipher=aes-cbc-essiv:sha256</p>
</blockquote>
<blockquote>
<p>The /etc/volume_key file contains a plaintext encryption key. You can also specify none as the key file name; this configures the system to ask for the encryption key during boot instead.</p>
<p>It is recommended to use LUKS (Linux Unified Key Setup) for setting up file system volumes. To do this, follow these steps:</p>
<p>Create the encrypted volume using cryptsetup luksFormat.</p>
<p>Add the necessary entry to /etc/crypttab.</p>
<p>Set up the volume manually using cryptsetup luksOpen (or reboot).</p>
<p>Create a file system on the encrypted volume.</p>
<p>Add the necessary entry to /etc/fstab.</p>
</blockquote>
<p>After scouring the Red Hat Enterprise Linux manuals and knowledge base, I couldn&rsquo;t find specific instructions to set it up. However, there was an <a href="http://www.redhatmagazine.com/2007/01/18/disk-encryption-in-fedora-past-present-and-future/">article in the Red Hat Magazine</a> that may help.</p>
]]></content></item><item><title>Upgraded to WordPress 2.6.1</title><link>https://major.io/2008/08/30/upgraded-to-wordpress-261/</link><pubDate>Sun, 31 Aug 2008 02:17:15 +0000</pubDate><guid>https://major.io/2008/08/30/upgraded-to-wordpress-261/</guid><description>I&amp;rsquo;ve just upgraded this blog to WordPress 2.6.1. If you haven&amp;rsquo;t upgraded yours, you may want to get started. You can review the changelog as well.</description><content type="html"><![CDATA[<p>I&rsquo;ve just upgraded this blog to WordPress 2.6.1.  If you haven&rsquo;t upgraded yours, you may want to <a href="http://wordpress.org/download/">get started</a>. You can review the <a href="http://codex.wordpress.org/Changelog/2.6.1">changelog</a> as well.</p>
]]></content></item><item><title>Basic procmail configuration with spamassassin filtering</title><link>https://major.io/2008/08/13/basic-procmail-configuration-with-spamassassin-filtering/</link><pubDate>Wed, 13 Aug 2008 17:00:48 +0000</pubDate><guid>https://major.io/2008/08/13/basic-procmail-configuration-with-spamassassin-filtering/</guid><description>I&amp;rsquo;ve used this extremely basic procmail configuration a million times, and it&amp;rsquo;s a great start for any server configuration. It passes e-mails through spamassassin (if they&amp;rsquo;re smaller than 256KB) and then filters any e-mail marked as spam to /dev/null:
LOGFILE=/var/log/procmail.log DROPPRIVS=yes&amp;lt;/p&amp;gt; &amp;lt;p&amp;gt;:0fw | /usr/bin/spamc&amp;lt;/p&amp;gt; &amp;lt;p&amp;gt;:0 * ^X-Spam-Status: Yes /dev/null Of course, you can make this much more complicated with some additional customization.</description><content type="html"><![CDATA[<p>I&rsquo;ve used this extremely basic procmail configuration a million times, and it&rsquo;s a great start for any server configuration. It passes e-mails through spamassassin (if they&rsquo;re smaller than 256KB) and then filters any e-mail marked as spam to /dev/null:</p>
<pre><code>LOGFILE=/var/log/procmail.log
DROPPRIVS=yes&lt;/p&gt;
&lt;p&gt;:0fw
| /usr/bin/spamc&lt;/p&gt;
&lt;p&gt;:0
* ^X-Spam-Status: Yes
/dev/null
</code></pre><p>Of course, you can make this much more complicated with some additional customization.</p>
]]></content></item><item><title>Congratulations to RAX</title><link>https://major.io/2008/08/12/congratulations-to-rax/</link><pubDate>Tue, 12 Aug 2008 15:00:32 +0000</pubDate><guid>https://major.io/2008/08/12/congratulations-to-rax/</guid><description>It&amp;rsquo;s moments like these that make me proud to be a Racker! In just a year and a half that I&amp;rsquo;ve been there, I&amp;rsquo;ve seen a lot of great changes and this one is the best. Congratulations to RAX!</description><content type="html"><![CDATA[<p><a href="http://www.flickr.com/photos/texas1emt/2754581569/"><img src="http://farm4.static.flickr.com/3269/2754581569_b57694b1fa_m.jpg" alt="RAX" title="RAX"></a></p>
<p>It&rsquo;s moments like these that make me proud to be a Racker! In just a year and a half that I&rsquo;ve been there, I&rsquo;ve seen a lot of great changes and this one is the best. Congratulations to <a href="http://finance.google.com/finance?q=NYSE%3ARAX&amp;hl=en">RAX</a>!</p>
]]></content></item><item><title>Enabling Ruby on Rails support for a domain in Plesk</title><link>https://major.io/2008/08/11/enabling-ruby-on-rails-support-for-a-domain-in-plesk/</link><pubDate>Tue, 12 Aug 2008 01:16:18 +0000</pubDate><guid>https://major.io/2008/08/11/enabling-ruby-on-rails-support-for-a-domain-in-plesk/</guid><description>If you have Plesk 8.1 or later, you have support available for Ruby on Rails. Unfortunately, clicking the FastCGI checkbox in Plesk won&amp;rsquo;t get you all of the support you need (and expect). The folks over at Parallels created a relatively simple process to get Ruby on Rails working properly on your site:
Go to your domain that you want to adjust, and click Setup. Make sure the CGI and FastCGI options are enabled.</description><content type="html"><![CDATA[<p>If you have Plesk 8.1 or later, you have support available for Ruby on Rails. Unfortunately, clicking the FastCGI checkbox in Plesk won&rsquo;t get you all of the support you need (and expect). The folks over at Parallels created a <a href="http://kb.parallels.com/en/5489">relatively simple process</a> to get Ruby on Rails working properly on your site:</p>
<p>Go to your domain that you want to adjust, and click <strong>Setup</strong>. Make sure the <strong>CGI</strong> and <strong>FastCGI</strong> options are enabled. Pick a name for your application and make the directory for your application in the <strong>httpdocs</strong> directory. Upload your files to that directory.</p>
<p>Once you&rsquo;ve done that, create an <strong>.htaccess</strong> file in the <strong>httpdocs</strong> directory with the following text inside:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-apache" data-lang="apache"><span style="color:#b58900">RewriteEngine</span> <span style="color:#719e07">On</span>
<span style="color:#b58900">RewriteRule</span> ^$ <span style="color:#2aa198">/public/index.html</span> [L]
<span style="color:#b58900">RewriteCond</span> % !^/railsapp/public
<span style="color:#b58900">RewriteRule</span> ^(.*)$ <span style="color:#2aa198">/public/</span>$1 [L]
<span style="color:#b58900">RewriteCond</span> % !-f
<span style="color:#b58900">RewriteRule</span> ^(.*)$ public/dispatch.fcgi/$1 [QSA,L]
</code></pre></div><p>Remove the <strong>.htaccess</strong> file within the <strong>public</strong> directory of your application and add a file called <strong>dispatch.fcgi</strong> to that directory which contains:</p>
<pre><code>#!/usr/bin/ruby
</code></pre><p>You should be able to access your application at <a href="http://domain.com/railsapp/">http://domain.com/railsapp/</a>.</p>
]]></content></item><item><title>Reduce disk I/O for small reads using memory</title><link>https://major.io/2008/08/07/reduce-disk-io-for-small-reads-using-memory/</link><pubDate>Thu, 07 Aug 2008 17:00:27 +0000</pubDate><guid>https://major.io/2008/08/07/reduce-disk-io-for-small-reads-using-memory/</guid><description>Many applications that are used on a standard server perform quite a few of small writes to the disk (like MySQL or Apache). These writes can pile up and limit the performance of your applications. If you have kernel 2.6.9 or later, you can adjust how these small writes are handled to allow for better performance.
There&amp;rsquo;s two main kernel variables to know:
vm.dirty_ratio - The highest % of your memory that can be used to hold dirty data.</description><content type="html"><![CDATA[<p>Many applications that are used on a standard server perform quite a few of small writes to the disk (like MySQL or Apache). These writes can pile up and limit the performance of your applications. If you have kernel 2.6.9 or later, you can adjust how these small writes are handled to allow for better performance.</p>
<p>There&rsquo;s two main kernel variables to know:</p>
<p><strong>vm.dirty_ratio</strong> - The highest % of your memory that can be used to hold dirty data. If you set this to a low value, the kernel will flush small writes to the disk more often. Higher values allow the small writes to stack up in memory. They&rsquo;ll go to the disk in bigger chunks.</p>
<p><strong>vm.dirty_background_ratio</strong> - The lowest % of your memory where pdflush is told to stop when it is writing dirty data. You&rsquo;ll want to keep this set as low as possible.</p>
<p>These might confuse you. In short, when your memory begins filling with little pieces of data that needs to be written to the disk, it will keep filling until it reaches the dirty_ratio. At that point, pdflush will start up, and it will write data until it reduces the dirty data to the value set by dirty_background_ratio.</p>
<p>Stock 2.6.9 kernels have a dirty_background_ratio of 10% and a dirty_ratio of 40%. Some distributions tweak these defaults to something different, so you may want to review the settings on your system. On a system with heavy disk I/O, you can increase the dirty_ratio and reduce the dirty_background_ratio. A little experimentation may be necessary to find the perfect setting for your server.</p>
<p>If you want to play with the variables, just use your standard echo:</p>
<pre><code>echo 5 &gt; /proc/sys/vm/dirty_background_ratio
echo 60 &gt; /proc/sys/vm/dirty_ratio
</code></pre><p>Once you&rsquo;ve found the right setting, you can set it permanently by adding lines to your /etc/sysctl.conf:</p>
<pre><code>vm.dirty_background_ratio = 5
vm.dirty_ratio = 60
</code></pre><p>If you have a reliable server with a good RAID card and power supply, you could set the dirty_ratio to 100 and the dirty_background_ratio to 1. This was recommended by a buddy of mine who runs quite a few servers running virtual machines.</p>
]]></content></item><item><title>Automatically starting synergy in GDM in Ubuntu/Fedora</title><link>https://major.io/2008/07/30/automatically-starting-synergy-in-gdm-in-ubuntufedora/</link><pubDate>Wed, 30 Jul 2008 17:00:09 +0000</pubDate><guid>https://major.io/2008/07/30/automatically-starting-synergy-in-gdm-in-ubuntufedora/</guid><description>Before you follow this guide, be sure to read about the issue I had in Fedora 12 with this strategy.
At work, I have a Mac Mini as my main workstation with one monitor. There&amp;rsquo;s another monitor to the right which is connected to my Linux box. I run a synergy server on the Mac, and I run a synergy client in Linux. However, I was getting pretty frustrated when I&amp;rsquo;d have to manually start the synergy client on the Linux box with another keyboard.</description><content type="html"><![CDATA[<p><strong>Before you follow this guide,</strong> be sure to <a href="http://rackerhacker.com/2010/03/03/sticky-shift-key-with-synergy-in-fedora-12/">read about the issue</a> I had in Fedora 12 with this strategy.</p>
<p>At work, I have a Mac Mini as my main workstation with one monitor. There&rsquo;s another monitor to the right which is connected to my Linux box. I run a synergy server on the Mac, and I run a synergy client in Linux. However, I was getting pretty frustrated when I&rsquo;d have to manually start the synergy client on the Linux box with another keyboard.</p>
<p>After a bit of Google searching, I found a solution that will enable synergy at the GDM login as well as after the login (when the window manager starts). Here&rsquo;s the process:</p>
<p>Open <strong>/etc/gdm/Init/Default</strong> in your editor of choice and go to the bottom of the file. Just before <code>exit 0</code>, add the following:</p>
<pre><code>/usr/bin/killall synergyc
sleep 1
/usr/bin/synergyc 111.222.333.444
</code></pre><p>Next, you can create the <strong>/etc/gdm/PostLogin/Default</strong> file as an empty file, or you can copy over the template file from <strong>/etc/gdm/PostLogin/Default.sample</strong> to <strong>/etc/gdm/PostLogin/Default</strong>. Either way, add the following to that file:</p>
<pre><code>/usr/bin/killall synergyc
sleep 1
</code></pre><p>Finally, edit the <strong>/etc/gdm/Presession/Default</strong> file and add in the following before <code>exit 0</code>:</p>
<pre><code>/usr/bin/killall synergyc
sleep 1
/usr/bin/synergyc 111.222.333.444
</code></pre><p>Once that&rsquo;s done, you can log out and log back in to see the changes. You can also reboot your Linux desktop or switch to runlevel 3 and back to 5 (if your OS supports runlevel changes).</p>
]]></content></item><item><title>Plesk 8.4.0: Unable to use short names for POP3/IMAP</title><link>https://major.io/2008/07/28/plesk-840-unable-to-use-short-names-for-pop3imap/</link><pubDate>Mon, 28 Jul 2008 16:08:50 +0000</pubDate><guid>https://major.io/2008/07/28/plesk-840-unable-to-use-short-names-for-pop3imap/</guid><description>If you recently upgraded to Plesk 8.4.0 with short names enabled, you may have found that it&amp;rsquo;s working with SMTP, but it doesn&amp;rsquo;t work with POP3 or IMAP. There&amp;rsquo;s a bug in the Plesk version that prevents the courier configuration from being updated.
To correct the issue, first make sure that Plesk has short names enabled (Server &amp;gt; Mail). Once you&amp;rsquo;ve confirmed that Plesk thinks it&amp;rsquo;s enabled, add SHORTNAMES=1 to the following configuration files:</description><content type="html"><![CDATA[<p>If you recently upgraded to Plesk 8.4.0 with short names enabled, you may have found that it&rsquo;s working with SMTP, but it doesn&rsquo;t work with POP3 or IMAP. There&rsquo;s a bug in the Plesk version that prevents the courier configuration from being updated.</p>
<p>To correct the issue, first make sure that Plesk has short names enabled (Server &gt; Mail). Once you&rsquo;ve confirmed that Plesk thinks it&rsquo;s enabled, add <code>SHORTNAMES=1</code> to the following configuration files:</p>
<ul>
<li>/etc/courier-imap/imapd</li>
<li>/etc/courier-imap/imapd-ssl</li>
<li>/etc/courier-imap/pop3d</li>
<li>/etc/courier-imap/pop3d-ssl</li>
</ul>
<p>Restart courier-imap with <code>/etc/init.d/courier-imap restart</code> and you should be all set.</p>
]]></content></item><item><title>First post using the WordPress iPhone application</title><link>https://major.io/2008/07/22/first-post-using-the-wordpress-iphone-application/</link><pubDate>Tue, 22 Jul 2008 14:50:55 +0000</pubDate><guid>https://major.io/2008/07/22/first-post-using-the-wordpress-iphone-application/</guid><description>If you haven&amp;rsquo;t downloaded the new WordPress application for the iPhone yet, you might want to check it out. So far, it seems to work really well!</description><content type="html">&lt;p>If you haven&amp;rsquo;t downloaded the new WordPress application for the iPhone yet, you might want to check it out. So far, it seems to work really well!&lt;/p></content></item><item><title>MySQLTuner 0.9.8 is now available</title><link>https://major.io/2008/07/21/mysqltuner-098-is-now-available/</link><pubDate>Mon, 21 Jul 2008 17:17:14 +0000</pubDate><guid>https://major.io/2008/07/21/mysqltuner-098-is-now-available/</guid><description>MySQLTuner 0.9.8 is now available for download and it is full of new changes! So far, MySQLTuner has been downloaded almost 24,000 times. Thanks for making it so popular.
Easy download!
You can download it simply by running wget mysqltuner.pl. You will automatically be redirected to the script.
Access servers remotely
If you have multiple servers that you need to tune and monitor, simply download MySQLTuner onto one machine. You can run it against other servers by specifying a hostname, port, and login credentials.</description><content type="html"><![CDATA[<p>MySQLTuner 0.9.8 is now available for <a href="http://rackerhacker.com/mysqltuner/">download</a> and it is full of new changes! So far, MySQLTuner has been downloaded almost 24,000 times. Thanks for making it so popular.</p>
<p><strong>Easy download!</strong></p>
<p>You can download it simply by running <code>wget mysqltuner.pl</code>. You will automatically be redirected to the script.</p>
<p><strong>Access servers remotely</strong></p>
<p>If you have multiple servers that you need to tune and monitor, simply download MySQLTuner onto one machine. You can run it against other servers by specifying a hostname, port, and login credentials. Two new options are available: <code>--host</code> and <code>--port</code>.</p>
<p><strong>Pass login credentials on the command line</strong></p>
<p>Two new options are available: <code>--user</code> and <code>--pass</code>. You can pass a username and password on the command line to log in quickly. This can be especially helpful in conjunction with cron.</p>
<p><strong>Manually set the amount of RAM and swap memory installed</strong></p>
<p>I&rsquo;ve received a few bug reports where users on certain virtual environments saw incorrect memory calculations when they ran MySQLTuner. You can now set the current amount of RAM and swap memory installed with <code>--forcemem</code> and <code>--forceswap</code> respectively. (Thanks to Jason Gill for the patch and the bug report!)</p>
<p><strong>Checking for updates is now optional</strong></p>
<p>Some operating system distributions and environments can&rsquo;t allow for automatic update checking due to security concerns, so I&rsquo;ve made update checks optional. The <code>--skipversion</code> option has been removed and it was replaced with <code>--checkversion</code>. <em>As a side note, the only data that I collected was the version number being run and the IP from which the update originated. This data has only been used for aggregate statistical purposes.</em></p>
<p>As you might imagine, MySQLTuner 1.0 is just around the corner. I&rsquo;ve been holding out for the MySQL 5.1 GA release, but I may release the script sooner. Don&rsquo;t worry - as soon as MySQL 5.1 becomes a GA release, I&rsquo;ll be hard at work to support any new optimization options which it provides.</p>
]]></content></item><item><title>Calling all creative types: I need a slogan</title><link>https://major.io/2008/07/15/calling-all-creative-types-i-need-a-slogan/</link><pubDate>Tue, 15 Jul 2008 20:37:00 +0000</pubDate><guid>https://major.io/2008/07/15/calling-all-creative-types-i-need-a-slogan/</guid><description>As some of you may know, MySQLTuner is one of my favorite projects. It&amp;rsquo;s approaching the 1.0 release and I feel like celebrating. I thought about putting together some T-shirts (because as a nerd, I enjoy the occasional techy T-shirt), but I can&amp;rsquo;t come up with a good slogan.
I&amp;rsquo;ve heard of some pretty hilarious slogans for database-related activities, like &amp;ldquo;DBA&amp;rsquo;s know how to take dumps&amp;rdquo; and &amp;ldquo;don&amp;rsquo;t overflow my buffer&amp;rdquo;, but I&amp;rsquo;m trying to come up with some good ones for MySQLTuner.</description><content type="html"><![CDATA[<p>As some of you may know, MySQLTuner is one of my favorite projects. It&rsquo;s approaching the 1.0 release and I feel like celebrating. I thought about putting together some T-shirts (because as a nerd, I enjoy the occasional techy T-shirt), but I can&rsquo;t come up with a good slogan.</p>
<p>I&rsquo;ve heard of some pretty hilarious slogans for database-related activities, like &ldquo;DBA&rsquo;s know how to take dumps&rdquo; and &ldquo;don&rsquo;t overflow my buffer&rdquo;, but I&rsquo;m trying to come up with some good ones for MySQLTuner. Whoever picks the slogan/joke that wins and goes on the shirt will get a free shirt from me. Yes, I did say <strong>free</strong>.</p>
]]></content></item><item><title>WordPress 2.6</title><link>https://major.io/2008/07/15/wordpress-26/</link><pubDate>Tue, 15 Jul 2008 20:30:43 +0000</pubDate><guid>https://major.io/2008/07/15/wordpress-26/</guid><description>WordPress 2.6 is now available, and I&amp;rsquo;ve just upgraded today. If you haven&amp;rsquo;t upgraded yet, download it and follow the instructions.</description><content type="html"><![CDATA[<p><a href="http://wordpress.org/development/2008/07/wordpress-26-tyner/">WordPress 2.6</a> is now available, and I&rsquo;ve just upgraded today. If you haven&rsquo;t upgraded yet, <a href="http://wordpress.org/download/">download it</a> and <a href="http://codex.wordpress.org/Upgrading_WordPress">follow the instructions</a>.</p>
]]></content></item><item><title>Enabling all tests with Nessus</title><link>https://major.io/2008/07/15/enabling-all-tests-with-nessus/</link><pubDate>Tue, 15 Jul 2008 17:00:03 +0000</pubDate><guid>https://major.io/2008/07/15/enabling-all-tests-with-nessus/</guid><description>Nessus is one of those applications that makes me happy and drives me crazy at the same time. It does what I need it to, but it&amp;rsquo;s often hard to get it rolling when it needs to do something for me. When I run it, I run it in batch mode, which requires me to have a .nessusrc file. However, there is almost no documentation on how to create one of these files.</description><content type="html"><![CDATA[<p><a href="http://nessus.org">Nessus</a> is one of those applications that makes me happy and drives me crazy at the same time. It does what I need it to, but it&rsquo;s often hard to get it rolling when it needs to do something for me. When I run it, I run it in batch mode, which requires me to have a .nessusrc file. However, there is almost no documentation on how to create one of these files.</p>
<p>Luckily, a smart fellow by the name of George Theall created <a href="http://www.tifaware.com/perl/update-nessusrc/">update-nessusrc</a>. It&rsquo;s a handy perl script that will take a basic .nessusrc file and do things with it based on the options you pass it. As I said before, I want every test enabled, so here&rsquo;s the steps I performed:</p>
<p>First, I ran a batch scan to make a basic .nessurc file:</p>
<pre><code># nessus -xqV -T txt localhost 1241 username password targets.txt results.txt
</code></pre><p>The <strong>x</strong> skips the SSL certificate warning, <strong>q</strong> enables batch mode, <strong>V</strong> prints verbose status messages to the screen and <strong>-T txt</strong> makes the report come out in a text format.</p>
<p>Once it started, I pressed CTRL-C to stop it, and then I had a .nessusrc file ready to go. I <a href="http://www.tifaware.com/perl/update-nessusrc/">downloaded update-nessusrc</a> and ran it to enable all plugins:</p>
<pre><code># ./update-nessusrc-2.37 -ds -c &quot;_all_&quot; .nessusrc
</code></pre><p>The <strong>d</strong> enables debug mode (and saves the new .nessusrc to a new file name), the <strong>s</strong> prints a summary, and <strong>-c &ldquo;_all_&quot;</strong> tells the script to enable all plugin categories. You now have a .nessurc file for use with batch scans that will utilize all of the available plugins.</p>
<p>If you&rsquo;re lazy, you can <a href="/wp-content/nessusrc.txt">download my pre-made .nessusrc</a> that I made today with Nessus 3.2.1.</p>
]]></content></item><item><title>MySQLTuner is now a Fedora 9 package!</title><link>https://major.io/2008/06/27/mysqltuner-is-now-a-fedora-9-package/</link><pubDate>Fri, 27 Jun 2008 17:16:56 +0000</pubDate><guid>https://major.io/2008/06/27/mysqltuner-is-now-a-fedora-9-package/</guid><description>Thanks to some work started by Ville Skyttä, MySQLTuner is now included in Fedora 9 repositories:
# cat /etc/fedora-release Fedora release 9 (Sulphur) # yum info mysqltuner Loaded plugins: fastestmirror, priorities, refresh-packagekit Loading mirror speeds from cached hostfile * updates: mirrors.usc.edu * fedora: mirror.unl.edu * livna: mirrors.tummy.com Available Packages Name : mysqltuner Arch : noarch Version : 0.9.1 Release : 4 Size : 11 k Repo : updates Summary : MySQL high performance tuning script URL : http://mysqltuner.</description><content type="html"><![CDATA[<p>Thanks to <a href="https://bugzilla.redhat.com/show_bug.cgi?id=452172">some work started by Ville Skyttä</a>, MySQLTuner is now included in Fedora 9 repositories:</p>
<pre><code># cat /etc/fedora-release
Fedora release 9 (Sulphur)
# yum info mysqltuner
Loaded plugins: fastestmirror, priorities, refresh-packagekit
Loading mirror speeds from cached hostfile
 * updates: mirrors.usc.edu
 * fedora: mirror.unl.edu
 * livna: mirrors.tummy.com
Available Packages
Name       : mysqltuner
Arch       : noarch
Version    : 0.9.1
Release    : 4
Size       : 11 k
Repo       : updates
Summary    : MySQL high performance tuning script
URL        : http://mysqltuner.com/
License    : GPLv3+
Description: MySQLTuner is a MySQL high performance tuning script written in perl that will provide you with a snapshot of a MySQL server's health. Based
           : on the statistics gathered, specific recommendations will be provided that will increase a MySQL server's efficiency and performance.  The
           : script gives you automated MySQL tuning that is on the level of what you would receive from a MySQL DBA.`

In addition to Ville, I'd like to thank Jason Tibbitts for reviewing and approving the new package.
</code></pre>]]></content></item><item><title>MySQL: ERROR 1040: Too many connections</title><link>https://major.io/2008/06/24/mysql-error-1040-too-many-connections/</link><pubDate>Tue, 24 Jun 2008 17:00:47 +0000</pubDate><guid>https://major.io/2008/06/24/mysql-error-1040-too-many-connections/</guid><description>If you run a fairly busy and/or badly configured MySQL server, you may receive something like this when attempting to connect:
MySQL is telling you that it is handling the maximum connections that you have configured it to handle. By default, MySQL will handle 100 connections simultaneously. This is very similar to the situation when Apache reaches the MaxClients setting. You won&amp;rsquo;t even be able to connect to MySQL to find out what is causing the connections to be used up, so you will be forced to restart the MySQL daemon to troubleshoot the issue.</description><content type="html"><![CDATA[<p>If you run a fairly busy and/or badly configured MySQL server, you may receive something like this when attempting to connect:</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>MySQL is telling you that it is handling the maximum connections that you have configured it to handle. By default, MySQL will handle 100 connections simultaneously. This is very similar to the situation when Apache reaches the MaxClients setting. You won&rsquo;t even be able to connect to MySQL to find out what is causing the connections to be used up, so you will be forced to restart the MySQL daemon to troubleshoot the issue.</p>
<p>What causes MySQL to run out of connections? Here&rsquo;s a list of reasons that may cause MySQL to run out of available connections, listed in order of what you should check:</p>
<p><strong>Bad MySQL configuration</strong></p>
<p>Verify that you have set MySQL&rsquo;s buffers and caches to appropriate levels for the type of data you&rsquo;re storing and the types of queries that you are running. One quick way to check this information is via <a href="http://rackerhacker.com/mysqltuner/">MySQLTuner</a>. The script will tell you how well your server is performing along with the corrections you should make. Running the script only takes a few moments and it doesn&rsquo;t require a DBA to decipher the results.</p>
<p><strong>Data storage techniques</strong></p>
<p>Remember that MySQL works best when moving vertically, not horizontally. If you have a table with 20 columns, breaking it into two tables with 10 columns each will improve performance. Even if you need to join the two tables together to get your data, it will still perform at a higher level. Also, use the right data types for the right data. If you&rsquo;re storing an integer only, don&rsquo;t use a CHAR or VARCHAR data type. If your integer will be small, then use something like a TINYINT or SMALLINT rather than INT. This means MySQL will use less memory, pull less data from the disk, and have higher performing joins.</p>
<p><strong>Slow queries</strong></p>
<p>These are generally pretty easy to fix. If you have queries that don&rsquo;t use indexes, or if queries run slowly with indexes in place, you need to rethink how you&rsquo;re pulling your data. Should your data be split into multiple tables? Are you pulling more data than you need? Keep these questions in mind, enable the slow query log, and re-work your queries to find where the bottlenecks occur.</p>
<p><strong>Division of labor</strong></p>
<p>Most people who use MySQL have a dynamic site written in a scripting language, like PHP, Perl or Python. It&rsquo;s obvious that your server will need to do some work to parse the scripts, send data back to the client, and communicate with MySQL. If you find that your server is overworked, consider moving MySQL to its own dedicated hardware. Among many other things, this will reduce your disk I/O, allow you to better utilize memory, and it will help you when you need to scale even further. Be sure to keep your MySQL server close to your web servers, however, as increased latency will only make your performance problem first.</p>
<p><strong>Right hardware</strong></p>
<p>Do you have the right hardware for the job? Depending on your budget, you may need to make the move for hardware that gives you better I/O throughput and more useable cores. MySQL is a multi-threaded application, so it can utilize multiple cores to serve data quickly. Also, writing logs, reading tables, and adjusting indexes are disk-intensive tasks that need fast drives to perform well. When you look for a dedicated server for MySQL, be sure to choose multiple-core machines with low latency RAM, fast drives (SCSI/SAS), and a reliable network interface.</p>
<p>By reviewing these bottlenecks, you can reduce the load on your MySQL server without increasing your maximum connections. Simply increasing the maximum connections <strong>is a very bad idea</strong>. This can cause MySQL to consume unnecessary resources on your server and it may lead to an unstable system (crash!).</p>
]]></content></item><item><title>MySQLTuner 0.9.1 available for Mandriva users</title><link>https://major.io/2008/06/21/mysqltuner-091-available-for-mandriva-users/</link><pubDate>Sun, 22 Jun 2008 02:54:03 +0000</pubDate><guid>https://major.io/2008/06/21/mysqltuner-091-available-for-mandriva-users/</guid><description>Thanks to some hard work from Oden Eriksson and Frederik Himpe, MySQLTuner 0.9.1 is available in a Mandriva package.</description><content type="html">&lt;p>Thanks to some hard work from Oden Eriksson and Frederik Himpe, MySQLTuner 0.9.1 is available in a &lt;a href="http://rpmfind.net/linux/RPM/mandriva/devel/cooker/i586/media/contrib/release/mysqltuner-0.9.1-1mdv2009.0.noarch.html">Mandriva package&lt;/a>.&lt;/p></content></item><item><title>Rebuilding statistics from previous months on Plesk 8.3</title><link>https://major.io/2008/06/20/rebuilding-statistics-from-previous-months-on-plesk-83/</link><pubDate>Fri, 20 Jun 2008 17:00:06 +0000</pubDate><guid>https://major.io/2008/06/20/rebuilding-statistics-from-previous-months-on-plesk-83/</guid><description>There was a bug in versions of Plesk prior to 8.3 where the AWStats statistics for the previous months were unavailable. It was a bug within Plesk&amp;rsquo;s AWStat&amp;rsquo;s implementation, and it was fixed in Plesk 8.3.
However, the fix only corrected the issue moving forward after the upgrade. There was no automated way to rebuild the previous months' statistics, even though the AWStats data was right there on the disk!</description><content type="html"><![CDATA[<p>There was a bug in versions of Plesk prior to 8.3 where the AWStats statistics for the previous months were unavailable. It was a bug within Plesk&rsquo;s AWStat&rsquo;s implementation, and it was fixed in Plesk 8.3.</p>
<p>However, the fix only corrected the issue moving forward after the upgrade. There was no automated way to rebuild the previous months' statistics, <em>even though the AWStats data was right there on the disk</em>!</p>
<p>I saw this blog post about the issue, and the fix is quite elegant:</p>
<p><a href="http://www.europheus.com/?p=67">Plesk 8.3 AWStats on Linux - Rebuilding Previous Month Statistics</a></p>
]]></content></item><item><title>Why I interviewed at Google and stayed at Rackspace</title><link>https://major.io/2008/06/19/my-interview-experience-at-google/</link><pubDate>Thu, 19 Jun 2008 17:00:10 +0000</pubDate><guid>https://major.io/2008/06/19/my-interview-experience-at-google/</guid><description>As some of you might know, I interviewed for a position at Google in April of this year. It wasn&amp;rsquo;t a position that I sought out, but it all came about after I received an e-mail and phone call from a recruiter. Obviously, there&amp;rsquo;s some things I can&amp;rsquo;t talk about with regards to the interview process, but there&amp;rsquo;s quite a few things that can be said.
How it started
The initial recruiter that I spoke with was a very friendly fellow that didn&amp;rsquo;t seem too technical.</description><content type="html"><![CDATA[<p>As some of you might know, I interviewed for a position at Google in April of this year. It wasn&rsquo;t a position that I sought out, but it all came about after I received an e-mail and phone call from a recruiter. Obviously, there&rsquo;s some things I can&rsquo;t talk about with regards to the interview process, but there&rsquo;s quite a few things that can be said.</p>
<p><strong>How it started</strong></p>
<p>The initial recruiter that I spoke with was a very friendly fellow that didn&rsquo;t seem too technical. He didn&rsquo;t get into the job description much, but he was interested mostly in whether I wanted to relocate and what type of job I enjoy most. We ran through a few cursory technical questions and he tried to find out what my skill level was in certain areas. When it was all said and done, he said I&rsquo;d be contacted from someone else at Google within a few weeks.</p>
<p>Two weeks later, I received some e-mails, went through [redacted] phone screens (with some pretty intelligent people), and learned more about the position. The folks from Google that I spoke with ranged from friendly and chatty to very direct and somewhat terse. Overall, I got the idea that they weren&rsquo;t interested in running a quiz, but they wanted to know how deep my knowledge and understanding was with regards to critical topics relating to the position. I know this sounds vague, but it&rsquo;s about as much as I can tell you.</p>
<p><strong>The middle</strong></p>
<p>I received a few more e-mails after the phone screens and my recruiter wanted to bring me out to California. Travel arrangements were made, I flew out to San Jose, and then drove the short drive to Mountain View. The city and the surrounding areas were a little different than I was used to. Most of the buildings and structures look as if they were built between 1960 and 1980 and they had a peculiar architecture. I stayed in the Hotel Avante (which was quite comfortable) and made the short drive to the Googleplex in the morning.</p>
<p>This was about the point where I slapped myself and said <strong>&ldquo;Holy crap, I&rsquo;m interviewing at GOOGLE!&quot;</strong></p>
<p>When I arrived, I went into the wrong buildings twice until I found the right one, but some Google employees finally pointed me in the right direction. I met with my recruiter, who was actually pretty entertaining, and he gave me a run down of how the day would go. I spent the morning interviewing, and then I joined a Google employee for lunch. He answered many of my questions about the cost of living, job benefits, and how he liked Google. When that was over, I went back to interviewing and was escorted out of the building at the end of the day.</p>
<p><strong>Towards the end</strong></p>
<p>I spoke with my recruiter a few more times after the interview for some basic paperwork-related issues, and he worked hard to keep me in the loop on my application status. There wasn&rsquo;t much of a concern job-wise as I work for one of the <a href="http://www.rackspace.com/">best companies</a> in my industry already. However, I was getting ready to move to a new home, so I let my recruiter know that I was in a bit of a time crunch.</p>
<p>You&rsquo;ll probably want to know what happened next, but there&rsquo;s not really anything that I&rsquo;m allowed to say about it! What I can tell you is that I&rsquo;m still with the <a href="http://www.rackspace.com/">best company</a> in my industry, and I&rsquo;m still enjoying it each day.</p>
<p>So I know what you&rsquo;re probably thinking…</p>
<p><strong>Why did you stay at Rackspace?</strong></p>
<p>It&rsquo;s easy to answer this question: I learn something new every day at Rackspace. Sometimes it&rsquo;s something technical, and sometimes it&rsquo;s something related to managing people or designing technology. The people that I share this learning opportunity with make it all worthwhile. I&rsquo;ve never worked for a company where my managers cared so much about my personal and technical development. Also, I&rsquo;ve never worked at a company where, as a manager, I&rsquo;m encouraged to care for my own technicians' personal and technical development.</p>
<p>If you have any more questions about why I love working at Rackspace, please let me know. I&rsquo;ll be happy to fill you in.</p>
]]></content></item><item><title>MySQLTuner v0.9.1 is now available</title><link>https://major.io/2008/06/18/mysqltuner-v091-is-now-available/</link><pubDate>Thu, 19 Jun 2008 02:53:58 +0000</pubDate><guid>https://major.io/2008/06/18/mysqltuner-v091-is-now-available/</guid><description>MySQLTuner v0.9.1 is now available! This long-awaited update includes bug fixes, feature enhancements, and compatibility improvements.
MySQLTuner now checks for fragmented tables
When deletes or updates are made on tables, MySQL will often leave holes behind that it hopes to fill in later. If the size and quantity keep climbing, the holes can cause performance degradation for writes and reads. Fragmentation can be corrected with OPTIMIZE TABLE, and the script recommends it if needed.</description><content type="html"><![CDATA[<p>MySQLTuner v0.9.1 is <a href="http://rackerhacker.com/mysqltuner/">now available</a>! This long-awaited update includes bug fixes, feature enhancements, and compatibility improvements.</p>
<p><strong>MySQLTuner now checks for fragmented tables</strong></p>
<p>When deletes or updates are made on tables, MySQL will often leave holes behind that it hopes to fill in later. If the size and quantity keep climbing, the holes can cause performance degradation for writes and reads. Fragmentation can be corrected with <code>OPTIMIZE TABLE</code>, and the script recommends it if needed.</p>
<p><strong>Fixed a bug where zero-length passwords cause authentication to repeat</strong></p>
<p>The script will now allow you to have a zero-length password, and it won&rsquo;t re-prompt for the password over and over again.</p>
<p><strong>Fixed a wget 1.11 timestamp bug</strong></p>
<p>This can cause the version check to fail if the .wgetrc has timestamps enabled.</p>
<p><strong>Corrected a math error in the temporary table calculation</strong></p>
<p>The script should now be able to more accurately determine the relative quantity of temporary tables created on disk.</p>
<p><strong>Fixed an error when the status variable <code>Open_tables</code> returned zero</strong></p>
<p>The divide by zero error has been corrected.</p>
<p><strong>Added table cache changes in preparation for MySQL 6</strong></p>
<p>It&rsquo;s still in the early stages, but MySQLTuner should have full support for MySQL 6 by the time it reaches RC status.</p>
<p><strong>Thanks for the contributions!</strong></p>
<p>This update would not have been possible without help from Ville Skytta, Trent Hornibrook and Luuk Vosslamber.</p>
<p>To download the latest copy, visit the <a href="http://rackerhacker.com/mysqltuner/">MySQLTuner page</a>.</p>
]]></content></item><item><title>Remove backticks from MySQL dumps</title><link>https://major.io/2008/06/18/remove-backticks-from-mysql-dumps/</link><pubDate>Wed, 18 Jun 2008 17:00:01 +0000</pubDate><guid>https://major.io/2008/06/18/remove-backticks-from-mysql-dumps/</guid><description>I found myself in a peculiar situation last week. I&amp;rsquo;d been asked to downgrade a server from MySQL 4.1 to MySQL 3.23. Believe me, I tried to advise against the request, but I didn&amp;rsquo;t succeed.
I made a MySQL 3.23 compatible dump with --compatible=mysql323, but the dump came out with backticks around the database names. This works with some 3.23 versions, but it doesn&amp;rsquo;t work with others. Apparently RHEL 3&amp;rsquo;s MySQL 3.</description><content type="html"><![CDATA[<p>I found myself in a peculiar situation last week. I&rsquo;d been asked to downgrade a server from MySQL 4.1 to MySQL 3.23. Believe me, I tried to advise against the request, but I didn&rsquo;t succeed.</p>
<p>I made a MySQL 3.23 compatible dump with <code>--compatible=mysql323</code>, but the dump came out with backticks around the database names. This works with some 3.23 versions, but it doesn&rsquo;t work with others. Apparently RHEL 3&rsquo;s MySQL 3.23 is one of those versions where it simply won&rsquo;t work.</p>
<p>This sed line came in handy to strip the backticks from the <code>USE</code> lines in the dump:</p>
<pre><code>sed -e &quot;s/^USE \`\(.*\)\`/USE \1/g&quot;
</code></pre>]]></content></item><item><title>Why I use Plesk</title><link>https://major.io/2008/06/17/why-i-use-plesk/</link><pubDate>Tue, 17 Jun 2008 17:00:47 +0000</pubDate><guid>https://major.io/2008/06/17/why-i-use-plesk/</guid><description>It seems like I have a conversation like this one at least once a week:
Them: &amp;ldquo;Hey Major, you&amp;rsquo;re a pretty nerdy guy, what server distro do you use?&amp;rdquo;
Me: &amp;ldquo;I use CentOS 4 right now.&amp;rdquo;
Them: &amp;ldquo;CentOS? Why not use something more cutting edge, like Fedora or Gentoo?&amp;rdquo;
Me: &amp;ldquo;Well, I like those, but Plesk works really well with CentOS.&amp;rdquo;
Them: &amp;ldquo;Seriously? You use Plesk? WHY?&amp;rdquo;
My CentOS server hosts a fair amount of domains for people I know and people I don&amp;rsquo;t know.</description><content type="html"><![CDATA[<p>It seems like I have a conversation like this one at least once a week:</p>
<p><strong>Them:</strong> &ldquo;Hey Major, you&rsquo;re a pretty nerdy guy, what server distro do you use?&rdquo;</p>
<p><strong>Me:</strong> &ldquo;I use CentOS 4 right now.&rdquo;</p>
<p><strong>Them:</strong> &ldquo;CentOS? Why not use something more cutting edge, like Fedora or Gentoo?&rdquo;</p>
<p><strong>Me:</strong> &ldquo;Well, I like those, but Plesk works really well with CentOS.&rdquo;</p>
<p><strong>Them:</strong> &ldquo;Seriously? You use Plesk? WHY?&rdquo;</p>
<p>My CentOS server hosts a fair amount of domains for people I know and people I don&rsquo;t know. I share the box with a colleague of mine, and most of the customers are his. I&rsquo;d rather not handle calls at 3AM for a user that wants to change a password or a user who wants to make a new mailbox. Plesk is a burden to work around at times, yes, but it saves me from more headaches than it creates.</p>
<p>Some might be saying, &ldquo;well, why Plesk when there&rsquo;s other panels around that are better?&rdquo; This question is highly subjective. Not all panels will work for all people, and that&rsquo;s why there&rsquo;s competition (although Parallels has been buying so many panels lately, they might eradicate the competition).</p>
<p>Here&rsquo;s why I like Plesk:</p>
<p><strong>Ease of use</strong></p>
<p>For the customers on the server, Plesk looks similar to their Windows XP desktop. Anyone that knows me will know that I dislike Windows and their user interfaces, but I give kudos to Plesk for giving users something that looks familiar to them. This reduces the questions that administrators receive, and it empowers the individual customer to do more for themselves.</p>
<p><strong>Good integration with enterprise operating systems</strong></p>
<p>Red Hat is a fairly solid OS platform, and Plesk works well with it. It uses RPM&rsquo;s and it can even be upgraded using up2date and yum (if you&rsquo;re so inclined). The autoinstaller is just a script that automatically downloads RPM&rsquo;s and executes them in groups. Although Plesk upgrades can be a little sketchy at times, having the ability to use RPM to add and remove packages can get you out of a bind in a more organized fashion.</p>
<p><strong>Extensive back-end utilities that I&rsquo;m familiar with</strong></p>
<p>I work on Plesk servers daily as a <a href="http://www.rackspace.com/">Racker</a>, so performing advanced tasks with Plesk is a fairly straightforward process. While Plesk uses some mediocre daemons (courier-imap, proftpd) for some tasks and downright awful daemons for others (qmail), I can usually sort out any issues that pop up.</p>
<p><strong>Responsive and interested development team</strong></p>
<p>I&rsquo;ve talked to Plesk developers via phone and in person several times. They are genuinely interested in writing a solid, user-friendly product, and they&rsquo;re open to suggestions. Of course, mistakes are made (dhparam issues, bind chroot debacles) but they do their best to get updates out. The lead developer, Andrey, is a very friendly guy with a lot of good ideas. Also, his ideas make sense - he&rsquo;s not trying to make Plesk into something more than it needs to be.</p>
<p><strong>If Plesk didn&rsquo;t exist, what would you use?</strong></p>
<p>If I didn&rsquo;t have the time to write my own (which I don&rsquo;t right now), I&rsquo;d use DirectAdmin. It&rsquo;s extremely fast, but lacking in features. However, it&rsquo;s a pretty solid panel and the pricing is very reasonable. Again, it&rsquo;s another solid panel on CentOS/RHEL, which is a plus for me.</p>
]]></content></item><item><title>Adjusting CPAN mirror list</title><link>https://major.io/2008/06/16/adjusting-cpan-mirror-list/</link><pubDate>Mon, 16 Jun 2008 17:00:01 +0000</pubDate><guid>https://major.io/2008/06/16/adjusting-cpan-mirror-list/</guid><description>One of the most frustrating aspects of CPAN is connecting to mirrors via FTP. Most of the time, the mirrors are extraordinarily slow when it comes to FTP logins, and they often fail. As we all know, RHEL enjoys pulling some shenanigans (Scalar::Util - enough said) when perl receives an upgrade, and when I need CPAN to work quickly, it often does the opposite.
I was struggling to find a way to reconfigure CPAN to use HTTP mirrors rather than FTP, but I couldn&amp;rsquo;t figure out where CPAN was holding this data.</description><content type="html"><![CDATA[<p>One of the most frustrating aspects of CPAN is connecting to mirrors via FTP. Most of the time, the mirrors are extraordinarily slow when it comes to FTP logins, and they often fail. As we all know, RHEL enjoys pulling some shenanigans (Scalar::Util - enough said) when perl receives an upgrade, and when I need CPAN to work quickly, it often does the opposite.</p>
<p>I was struggling to find a way to reconfigure CPAN to use HTTP mirrors rather than FTP, but I couldn&rsquo;t figure out where CPAN was holding this data. It wasn&rsquo;t in ~/.cpan and there was nothing in /etc for it. However, I found that you can reconfigure CPAN by running the following command:</p>
<pre><code># perl -MCPAN -e shell
CPAN: File::HomeDir loaded ok (v0.69)
cpan shell -- CPAN exploration and modules installation (v1.9205)
ReadLine support enabled
cpan[1]&gt; o conf init
</code></pre><p>The configuration script will run again as if you had never configured CPAN. Best of all, if you need to stop mid-way through the reconfiguration, your original configuration is still there. If you&rsquo;d rather just adjust your mirror list rather than starting over completely with the CPAN configuration, use the following:</p>
<p>Display your current mirrors:</p>
<pre><code>o conf urllist
</code></pre><p>Delete the first mirror in your list:</p>
<pre><code>o conf urllist shift
</code></pre><p>Delete the last mirror in your list:</p>
<pre><code>o conf urllist pop
</code></pre><p>Add on a new mirror:</p>
<pre><code>o conf urllist push http://cpan.mirror.facebook.com/
</code></pre><p>Save your mirror changes:</p>
<pre><code>o conf urllist commit
</code></pre>]]></content></item><item><title>HP Servers: hwraidinfo and hwraidstatus in Linux</title><link>https://major.io/2008/06/13/hp-servers-hwraidinfo-and-hwraidstatus-in-linux/</link><pubDate>Fri, 13 Jun 2008 17:09:31 +0000</pubDate><guid>https://major.io/2008/06/13/hp-servers-hwraidinfo-and-hwraidstatus-in-linux/</guid><description>Working with the RAID configurations on Linux can be a little involved if all you have is hpacucli. Luckily, the folks using HP&amp;rsquo;s OS distributions will get tools like hwraidinfo and hwraid status, but you can get these going in Linux as well.
Here&amp;rsquo;s a bash script equivalent of hwraidinfo which will work in Linux:
#!/bin/sh SLOTLIST=$(hpacucli ctrl all show | \ grep Slot | sed -e 's/^.*Slot //g' -e 's/ .</description><content type="html"><![CDATA[<p>Working with the RAID configurations on Linux can be a little involved if all you have is hpacucli. Luckily, the folks using HP&rsquo;s OS distributions will get tools like hwraidinfo and hwraid status, but you can get these going in Linux as well.</p>
<p>Here&rsquo;s a bash script equivalent of hwraidinfo which will work in Linux:</p>
<pre><code>#!/bin/sh
SLOTLIST=$(hpacucli ctrl all show | \
grep Slot | sed -e 's/^.*Slot //g' -e 's/ .*$//g')
for i in $SLOTLIST
do
echo
hpacucli ctrl slot=$i show | grep -v &quot;^$&quot;
echo
hpacucli ctrl slot=$i ld all show | grep -v &quot;^$&quot;
hpacucli ctrl slot=$i pd all show | grep -v &quot;^$&quot;
done
echo
</code></pre><p>And here is the script equivalent of hwraidstatus:</p>
<pre><code>#!/bin/sh
SLOTLIST=$(hpacucli ctrl all show | \
grep Slot | sed -e 's/^.*Slot //g' -e 's/ .*$//g')
for i in $SLOTLIST
do
echo
hpacucli ctrl slot=$i show status | grep -v &quot;^$&quot;
echo
hpacucli ctrl slot=$i ld all show status | grep -v &quot;^$&quot;
hpacucli ctrl slot=$i pd all show status | grep -v &quot;^$&quot;
done
echo
</code></pre><p>Save these to the filesystem, run <code>chmod +x</code> and move them to /usr/sbin (or /usr/local/sbin) so that the root user can use them.</p>
]]></content></item><item><title>Screencast topic requests?</title><link>https://major.io/2008/06/12/screencast-topic-requests/</link><pubDate>Thu, 12 Jun 2008 16:00:14 +0000</pubDate><guid>https://major.io/2008/06/12/screencast-topic-requests/</guid><description>I&amp;rsquo;ve received some great feedback on my first screencast. The suggestions have been helpful, and the compliments have made me want to create more free screencasts.
At this point, I&amp;rsquo;m opening it up to you - what would you like to learn? Of course, I&amp;rsquo;m still planning to make a full set of screencasts for the MySQL DBA certification, but that will come later.</description><content type="html"><![CDATA[<p>I&rsquo;ve received some great feedback on my first <a href="http://rackerhacker.com/2008/06/04/screencast-write-a-php-script-to-query-mysql/">screencast</a>. The suggestions have been helpful, and the compliments have made me want to create more free screencasts.</p>
<p>At this point, I&rsquo;m opening it up to you - what would you like to learn? Of course, I&rsquo;m still planning to make a full set of screencasts for the MySQL DBA certification, but that will come later.</p>
]]></content></item><item><title>MySQL: Can’t drop one or more of the requested users</title><link>https://major.io/2008/06/11/mysql-cant-drop-one-or-more-of-the-requested-users/</link><pubDate>Wed, 11 Jun 2008 23:59:37 +0000</pubDate><guid>https://major.io/2008/06/11/mysql-cant-drop-one-or-more-of-the-requested-users/</guid><description>MySQL has quite a few cryptic error messages, and this one is one of the best:
mysql&amp;gt; DROP USER 'forums'@'db1.myserver.com'; ERROR 1268 (HY000): Can't drop one or more of the requested users Naturally, I was quite interested to know why MySQL wasn&amp;rsquo;t going to allow me to remove this user. There was nothing special about the user, but then again, this wasn&amp;rsquo;t a server that I personally managed, so I wasn&amp;rsquo;t sure what kind of configuration was in place.</description><content type="html"><![CDATA[<p>MySQL has quite a few cryptic error messages, and this one is one of the best:</p>
<pre><code>mysql&gt; DROP USER 'forums'@'db1.myserver.com';
ERROR 1268 (HY000): Can't drop one or more of the requested users
</code></pre><p>Naturally, I was quite interested to know why MySQL wasn&rsquo;t going to allow me to remove this user. There was nothing special about the user, but then again, this wasn&rsquo;t a server that I personally managed, so I wasn&rsquo;t sure what kind of configuration was in place.</p>
<p>It&rsquo;s always a good idea to get your bearings, so I checked the current grants:</p>
<pre><code>mysql&gt; SHOW GRANTS FOR 'forums'@'db1.myserver.com';
+----------------------------------------------------------------------+
| Grants for forums@db1.myserver.com                                   |
+----------------------------------------------------------------------+
| GRANT USAGE ON *.* TO 'forums'@'db1.myserver.com' WITH GRANT OPTION  |
+----------------------------------------------------------------------+
1 row in set (0.00 sec)
</code></pre>
<p>The GRANT OPTION was causing my grief. It was the only privilege that the user had on the server. I revoked the privilege and attempted to drop the user yet again:</p>
<pre><code>mysql&gt; REVOKE GRANT OPTION ON *.* FROM 'forums'@'db1.myserver.com';
Query OK, 0 rows affected (0.00 sec)
mysql&gt; DROP USER 'forums'@'db1.myserver.com';
Query OK, 0 rows affected (0.00 sec)
</code></pre><p>It&rsquo;s key to remember that revoking the GRANT OPTION is a completely separate process. Revoking ALL PRIVILEGES doesn&rsquo;t include GRANT OPTION, so be sure to specify it separately:</p>
<pre><code>mysql&gt; REVOKE ALL PRIVILEGES, GRANT OPTION ON *.* FROM 'user'@'host';
</code></pre>]]></content></item><item><title>Backing up MySQL to Amazon’s S3</title><link>https://major.io/2008/06/05/backing-up-mysql-to-amazons-s3/</link><pubDate>Fri, 06 Jun 2008 00:18:49 +0000</pubDate><guid>https://major.io/2008/06/05/backing-up-mysql-to-amazons-s3/</guid><description>I received an e-mail from Tim Linden about a post he made in his blog about backing up MySQL data to Amazon&amp;rsquo;s S3.
The article goes over installing the Net::Amazon::S3 Perl module via WHM (which is handy for the cPanel users). However, if you&amp;rsquo;re not a cPanel user, you can install it via CPAN:
# perl -MCPAN -e 'install Net::Amazon::S3' If you&amp;rsquo;d rather install it through Webmin, go to the &amp;lsquo;Others&amp;rsquo; section, and click &amp;lsquo;Perl Modules&amp;rsquo;.</description><content type="html"><![CDATA[<p>I received an e-mail from <a href="http://www.timlinden.com/">Tim Linden</a> about a <a href="http://www.timlinden.com/blog/server/backup-mysql-amazon-s3/">post he made in his blog</a> about backing up MySQL data to <a href="http://en.wikipedia.org/wiki/Amazon_S3">Amazon&rsquo;s S3</a>.</p>
<p>The article goes over installing the Net::Amazon::S3 Perl module via WHM (which is handy for the cPanel users). However, if you&rsquo;re not a cPanel user, you can install it via CPAN:</p>
<pre><code># perl -MCPAN -e 'install Net::Amazon::S3'
</code></pre><p>If you&rsquo;d rather install it through Webmin, go to the &lsquo;Others&rsquo; section, and click &lsquo;Perl Modules&rsquo;.</p>
<p>Also, Tim mentions configuring a <a href="http://www.rjonna.com/ext/s3fox.php">Firefox extension for accessing S3</a> that works very well. However, I find myself using Safari most often, so I prefer to use <a href="http://www.jungledisk.com/">Jungle Disk</a> or <a href="http://www.panic.com/transmit/">Transmit</a> on my Mac.</p>
<p>Overall, it&rsquo;s a great post, and I&rsquo;m glad Tim told me about it!</p>
]]></content></item><item><title>MySQLTuner is the Debian Package of the Day!</title><link>https://major.io/2008/05/28/mysqltuner-is-the-debian-package-of-the-day/</link><pubDate>Thu, 29 May 2008 02:23:30 +0000</pubDate><guid>https://major.io/2008/05/28/mysqltuner-is-the-debian-package-of-the-day/</guid><description>I just received a Google Alert about MySQLTuner being the Debian Package of the Day!</description><content type="html"><![CDATA[<p>I just received a <a href="http://google.com/alerts">Google Alert</a> about MySQLTuner being the <a href="http://potd.redsymbol.net/?p=mysqltuner">Debian Package of the Day</a>!</p>
]]></content></item><item><title>Parallels Summit 2008 – Day 1</title><link>https://major.io/2008/05/19/parallels-summit-2008-day-1/</link><pubDate>Tue, 20 May 2008 03:52:38 +0000</pubDate><guid>https://major.io/2008/05/19/parallels-summit-2008-day-1/</guid><description>I&amp;rsquo;m really enjoying the Parallels Summit in Washington D.C. this week. The folks from Parallels have been great, and the vendors we have met are selling some pretty tremendous products.
Lots of folks have been asking for a summary of what&amp;rsquo;s going on after my Twitter posts appeared earlier today. Here&amp;rsquo;s the latest information I&amp;rsquo;ve seen and heard today:
Parallels Development Strategy
» Create an SMB edition, solidity the Application Packaging Standard (APS), and further integrate billing</description><content type="html"><![CDATA[<p>I&rsquo;m really enjoying the <a href="http://www.parallels.com/en/summit/">Parallels Summit in Washington D.C.</a> this week. The folks from Parallels have been great, and the vendors we have met are selling some pretty tremendous products.</p>
<p>Lots of folks have been asking for a summary of what&rsquo;s going on after my <a href="http://twitter.com/RackerHacker/">Twitter posts</a> appeared earlier today. Here&rsquo;s the latest information I&rsquo;ve seen and heard today:</p>
<p><strong>Parallels Development Strategy</strong></p>
<p>» Create an SMB edition, solidity the Application Packaging Standard (APS), and further integrate billing</p>
<p>» 80+ Research and development engineers currently work on Plesk products</p>
<p>» Increase density and make a more useable API</p>
<p>» Make Plesk the dominant panel through integration, SaaS deployments and scaling</p>
<p><strong>Plesk 8 Series</strong></p>
<p>» Plesk 8.4 brought DomainKeys and additional features</p>
<p>» Odd point revisions will contain minor bug fixes while even ones will contain new features</p>
<p>» Plesk 8.6 will have IPv6 support, full ModernBill integration and the APS software catalog</p>
<p><strong>Plesk 9 Series</strong></p>
<p>» Reseller level accounts will be available, and they will fit between the server administrator and client accounts</p>
<p>» Clients can be promoted to resellers</p>
<p>» Domain administrators can be promoted to clients (and resellers)</p>
<p>» Home and desktop pages will be merged</p>
<p>» Better rollover tooltips will be provided</p>
<p>» Hosting setup will be easier and setting limits will be more intuitive</p>
<p>» Server administrators will be able to hide certain services and functions</p>
<p>» Overselling will be allowed so that soft and hard limits can be set</p>
<p>» API improvements will be backward compatible</p>
<p>» Reseller billing will be integrated with ModernBill</p>
<p>» Choose to use Postfix rather than Qmail</p>
<p>» Greylisting is built into the panel</p>
<p><strong>Plesk Future Ideas</strong></p>
<p>» Vast majority of Plesk deployments have five domains or less</p>
<p>» SMB edition with support for one domain and one administrator (SaaS)</p>
<p>» Corporate fileserver and collaboration will be in the SMB version</p>
<p>» Use PHP via FastCGI rather than Apache module</p>
<p>» Lighttpd support for Plesk panel as well as hosted sites</p>
<p>» Multiple MTA&rsquo;s (in addition to Postfix/Qmail)</p>
<p>» Upgrade checks can be run prior to upgrading Plesk so that issues can be avoided</p>
<p>» Recovery tools will be available for broken upgrades</p>
<p>» Local Plesk mirrors can be used to do remote upgrades</p>
<p>» Multiple level backup/restore options for admin, reseller, client or domain administrator</p>
<p>» Migration of shared hosting environment into Virtuozzo container or Plesk Server container</p>
<p>» SiteBuilder 5.0 will have mobile web site design support</p>
<p>» Event calendar is coming</p>
<p><strong>ModernBill</strong></p>
<p>» Will be integrated into Plesk</p>
<p>» Support for 20+ payment gateways</p>
<p>» 100% ready in Q4 2008</p>
<p><strong>Application Packaging Standard (APS)</strong></p>
<p>» Applications will be available directly from Parallels within the Plesk panel</p>
<p>» Modules are easy to install and update</p>
<p>» Applications can be maintained by packagers and kept up to date</p>
<p>» Centralized license management</p>
<p>» Installation and configuration can be completely scripted</p>
<p>» Install scripts can get Plesk configuration data right from Plesk so the interface within the application matches Plesk&rsquo;s interface</p>
<p><strong>Notes from Sergei&rsquo;s Keynote</strong></p>
<p>» Hosting growth is slowing, but it mature</p>
<p>» Managed hosting is growing a lot, and SaaS has entered the marked</p>
<p>» Mentioned Rackspace&rsquo;s pending IPO as a sign of the changing market</p>
<p>» Large SaaS entries: Google&rsquo;s products, Amazon&rsquo;s S3/EC2, Microsoft&rsquo;s Hosted Exchange</p>
<p>» Google is a monopoly, the most dangerous, won&rsquo;t partner with anyone</p>
<p>» Microsoft works harder on partnerships, but was more dangerous without Yahoo!</p>
<p>» EMC quietly launched direct hosting, storage hosting, cloud computing</p>
<p>» IT is mocing off-premises, which is both a threat and opportunity</p>
<p>» Hosting companies must stay competitive via pricing, high levels of service</p>
<p>» Create stickyness with critical service offerings and owning the customer (and their data)</p>
<p>» Hosting companies can improve with broad sets of service plans, flexible upgrade paths, and close ties with customers/partners</p>
<p>» Parallels vision includes partnering, automation, virtualization, and standardization</p>
<p>I&rsquo;ll try to take good notes tomorrow and get an update together for you then!</p>
]]></content></item><item><title>Taking a short posting break</title><link>https://major.io/2008/05/08/taking-a-short-posting-break/</link><pubDate>Thu, 08 May 2008 23:24:41 +0000</pubDate><guid>https://major.io/2008/05/08/taking-a-short-posting-break/</guid><description>I&amp;rsquo;ll be taking a short break from posting, but I should be back underway within week! Here&amp;rsquo;s what I&amp;rsquo;m working on during the break:
MySQLTuner - Thank you for all of the e-mails with suggestions, patches and complaints. The 1.0 version of MySQLTuner is in the works and I hope that it helps you get more done with your MySQL server. Boxcheck - A new version of boxcheck.com is coming and it will include more tests for you to use.</description><content type="html"><![CDATA[<p>I&rsquo;ll be taking a short break from posting, but I should be back underway within week!  Here&rsquo;s what I&rsquo;m working on during the break:</p>
<ul>
<li><strong>MySQLTuner</strong> - Thank you for all of the e-mails with suggestions, patches and complaints. The 1.0 version of <a href="http://mysqltuner.com">MySQLTuner</a> is in the works and I hope that it helps you get more done with your MySQL server.</li>
<li><strong>Boxcheck</strong> - A new version of <a href="http://boxcheck.com/">boxcheck.com</a> is coming and it will include more tests for you to use. Also, the tests that it already has will be more reliable. I might even throw in a little AJAX to get everyone excited.</li>
<li><strong>PleskHacker</strong> - I&rsquo;m still working on the <a href="http://pleskhacker.com/">Plesk database documentation</a>, but this project is a lower priority than the first two.</li>
</ul>
<p>I&rsquo;m <a href="http://www.flickr.com/photos/texas1emt/sets/72157604953201884/detail/">moving into a new house</a> over the weekend, so please <a href="http://xkcd.com/386/">don&rsquo;t be discouraged</a> if you e-mail me and you don&rsquo;t receive a quick response. For all of the fans of Twitter out there, you can find me on Twitter as <a href="http://twitter.com/RackerHacker/">RackerHacker</a>.</p>
]]></content></item><item><title>Forcing qmail to process e-mail in the queue</title><link>https://major.io/2008/05/02/forcing-qmail-to-process-e-mail-in-the-queue/</link><pubDate>Fri, 02 May 2008 17:00:51 +0000</pubDate><guid>https://major.io/2008/05/02/forcing-qmail-to-process-e-mail-in-the-queue/</guid><description>Normally, qmail will be able to process the mail queue without any interaction from the system administrator, however, if you want to force it to process everything that is in the queue right now, you can do so:
kill -ALRM `pgrep qmail-send` If for some peculiar reason you don&amp;rsquo;t have pgrep on your server, you can go about it a slightly different way:
kill -ALRM `ps ax | grep qmail-send | grep -v grep | awk '{print $1}'` Your logs should begin filling up with data about e-mails rolling through the queue.</description><content type="html"><![CDATA[<p>Normally, qmail will be able to process the mail queue without any interaction from the system administrator, however, if you want to force it to process everything that is in the queue right now, you can do so:</p>
<pre><code>kill -ALRM `pgrep qmail-send`
</code></pre><p>If for some peculiar reason you don&rsquo;t have pgrep on your server, you can go about it a slightly different way:</p>
<pre><code>kill -ALRM `ps ax | grep qmail-send | grep -v grep | awk '{print $1}'`
</code></pre><p>Your logs should begin filling up with data about e-mails rolling through the queue.</p>
]]></content></item><item><title>After Plesk upgrade, “Cannot initialize InnoDB”</title><link>https://major.io/2008/05/01/after-plesk-upgrade-cannot-initialize-innodb/</link><pubDate>Thu, 01 May 2008 17:00:09 +0000</pubDate><guid>https://major.io/2008/05/01/after-plesk-upgrade-cannot-initialize-innodb/</guid><description>Upgrading Plesk from 7.5.x to 8.x will change your Plesk-related MySQL tables from MyISAM to InnoDB. This allows for better concurrency in the Plesk panel when a lot of users are logged in simultaneously. However, some server administrators will disable InnoDB support in MySQL to save resources. This will cause problems after the upgrade.
Plesk may display an error on a white page that looks something like:
Cannot initialize InnoDB</description><content type="html"><![CDATA[<p>Upgrading Plesk from 7.5.x to 8.x will change your Plesk-related MySQL tables from MyISAM to InnoDB. This allows for better concurrency in the Plesk panel when a lot of users are logged in simultaneously. However, some server administrators will disable InnoDB support in MySQL to save resources. This will cause problems after the upgrade.</p>
<p>Plesk may display an error on a white page that looks something like:</p>
<blockquote>
<p>Cannot initialize InnoDB</p>
</blockquote>
<p>This could mean that InnoDB support was disabled when MySQL was started. To correct this issue, search through the /etc/my.cnf for this line:</p>
<p><code>skip-innodb</code></p>
<p>If you find it in your configuration, remove it, and then restart MySQL. To test that InnoDB is enabled, you can refresh the Plesk page, or you can log into MySQL and run <code>SHOW ENGINES</code>. The output from the <code>SHOW ENGINES</code> statement should show <strong>YES</strong> on the line with InnoDB.</p>
<p>Should <strong>DISABLED</strong> appear instead, you may have an issue with your InnoDB configuration in your /etc/my.cnf. Be sure to check for innodb_data_file_path and make sure that it is set to an appropriate value.</p>
<p>A value of <strong>NO</strong> is not a good sign. This means that your version of MySQL was compiled without InnoDB support. This means that it cannot be enabled at runtime because MySQL wasn&rsquo;t built with any support for InnoDB. Be sure to recompile MySQL with <code>--with-innodb</code> or obtain a new package for your operating system which includes InnoDB support.</p>
<p>If you suspect that your MySQL InnoDB configuration is incorrect, you may want to review this documentation on MySQL&rsquo;s site:</p>
<p>For MySQL 5: <a href="http://dev.mysql.com/doc/refman/5.0/en/innodb-configuration.html">13.2.3. InnoDB Configuration</a></p>
<p>For MySQL 4/3.23: <a href="http://dev.mysql.com/doc/refman/4.1/en/innodb-configuration.html">13.2.4. InnoDB Configuration</a></p>
]]></content></item><item><title>Proposed MySQL DBA Screencasts</title><link>https://major.io/2008/04/30/proposed-mysql-dba-screencasts/</link><pubDate>Wed, 30 Apr 2008 23:33:12 +0000</pubDate><guid>https://major.io/2008/04/30/proposed-mysql-dba-screencasts/</guid><description>After running the idea by some of my fellow technical folks, I&amp;rsquo;ve considered making an array of screencasts aimed to prepare people for the MySQL DBA exam. I haven&amp;rsquo;t decided to make them free or charge for them as of yet, but if I did charge a fee, it would be much less than getting the training from MySQL ($2,499USD in most locations).
So, I have two questions for the general techy community:</description><content type="html"><![CDATA[<p>After running the idea by some of my fellow technical folks, I&rsquo;ve considered making an array of screencasts aimed to prepare people for the MySQL DBA exam. I haven&rsquo;t decided to make them free or charge for them as of yet, but if I did charge a fee, it would be much less than getting the training from MySQL ($2,499USD in most locations).</p>
<p><strong>So, I have two questions for the general techy community:</strong></p>
<p><strong>1.</strong> What tools/applications would you recommend on a Mac for making high-quality screencasts that have a professional feel?</p>
<p><strong>2.</strong> Would you pay for these screencasts (if they are really good), and if so, how much would you want to spend in total to get all of the screencasts for the DBA certification?</p>
<p>Feel free to add comments to this post, or you&rsquo;re welcome to drop me an e-mail at <em>major at mhtx dot net</em>. Your feedback is greatly appreciated!</p>
]]></content></item><item><title>Best PHP and MySQL development book</title><link>https://major.io/2008/04/29/best-php-and-mysql-development-book/</link><pubDate>Tue, 29 Apr 2008 17:00:54 +0000</pubDate><guid>https://major.io/2008/04/29/best-php-and-mysql-development-book/</guid><description>I finally remembered this book when someone asked me about how to get started with PHP and MySQL development. If you get the chance, get a copy of this book:
PHP and MySQL Web Developmentby Luke Welling, Laura Thomson
Barnes &amp;amp; Noble: http://snurl.com/265xp
Why do I like this book so much?
Teaching by application - The book teaches fundamentals by showing how to apply techniques to an active website. There&amp;rsquo;s not a ton of theory to wade through, and you feel like you&amp;rsquo;re learning the material faster.</description><content type="html"><![CDATA[<p>I finally remembered this book when someone asked me about how to get started with PHP and MySQL development. If you get the chance, get a copy of this book:</p>
<p><!-- raw HTML omitted -->PHP and MySQL Web Development<!-- raw HTML omitted --> <em>by Luke Welling, Laura Thomson</em></p>
<p>Barnes &amp; Noble: <a href="http://snurl.com/265xp">http://snurl.com/265xp</a></p>
<p>Why do I like this book so much?</p>
<ul>
<li><strong>Teaching by application</strong> - The book teaches fundamentals by showing how to apply techniques to an active website. There&rsquo;s not a ton of theory to wade through, and you feel like you&rsquo;re learning the material faster.</li>
<li><strong>Intertwined strategies</strong> - You learn how to get PHP working with MySQL, and then you learn how to optimize your code. It&rsquo;s important to know which work is best done by PHP and which is best done by MySQL. This book teaches both.</li>
<li><strong>Lots of examples</strong> - The CD-ROM comes with tons of code examples that actually relate to something you can use.</li>
</ul>
<p>I&rsquo;d be happy to loan my copy, but I&rsquo;ve loaned it out and it never returned.</p>
]]></content></item><item><title>Upgraded to WordPress 2.5.1</title><link>https://major.io/2008/04/28/upgraded-to-wordpress-251/</link><pubDate>Tue, 29 Apr 2008 03:15:39 +0000</pubDate><guid>https://major.io/2008/04/28/upgraded-to-wordpress-251/</guid><description>I&amp;rsquo;ve just upgraded RackerHacker to WordPress 2.5.1. If you haven&amp;rsquo;t upgraded your own blog installation yet, I&amp;rsquo;d recommend doing so soon!
Download It
Upgrade It</description><content type="html"><![CDATA[<p>I&rsquo;ve just upgraded RackerHacker to WordPress 2.5.1. If you haven&rsquo;t upgraded your own blog installation yet, I&rsquo;d recommend doing so soon!</p>
<p><a href="http://wordpress.org/download/">Download It</a></p>
<p><a href="http://codex.wordpress.org/Upgrading_WordPress">Upgrade It</a></p>
]]></content></item><item><title>Hosting the politicians</title><link>https://major.io/2008/04/24/hosting-the-politicians/</link><pubDate>Thu, 24 Apr 2008 17:12:24 +0000</pubDate><guid>https://major.io/2008/04/24/hosting-the-politicians/</guid><description>Call me weird - but I like to know where people host their sites. When I get a link to a nice-looking site, I&amp;rsquo;ll get there and think, &amp;ldquo;Hmm, I wonder where they host.&amp;rdquo;
Part of this curiosity probably stems from being a server administrator. I&amp;rsquo;ve used seven different dedicated server providers before along with 5 different VPS providers. There&amp;rsquo;s quite a few shared hosters thrown into the mix as well.</description><content type="html"><![CDATA[<p>Call me weird - but I like to know where people host their sites. When I get a link to a nice-looking site, I&rsquo;ll get there and think, &ldquo;Hmm, I wonder where they host.&rdquo;</p>
<p>Part of this curiosity probably stems from being a server administrator. I&rsquo;ve used seven different dedicated server providers before along with 5 different VPS providers. There&rsquo;s quite a few shared hosters thrown into the mix as well. I like to know which companies that people use when they have a site that is tremendously vital to their everyday lives.</p>
<p>Without further ado, here&rsquo;s some of the politicians' hosters that I&rsquo;ve found (in alphabetical order):</p>
<p><strong>Bob Barr</strong> - <em>Rackspace</em> (67.192.188.81)</p>
<p><strong>Hillary Clinton</strong> - <em>Rackspace</em> (72.32.103.48)</p>
<p><strong>John Edwards</strong> - <em>Internap</em> (70.42.42.155)</p>
<p><strong>Rudy Giuliani</strong> - <em>Core NAP</em> (64.20.231.77)</p>
<p><strong>Mike Gravel</strong> - <em>Media Temple</em> (64.13.237.176)</p>
<p><strong>Mike Huckabee</strong> - <em>HostMySite</em> (208.112.83.62)</p>
<p><strong>Alan Keyes</strong> - <em>ThePlanet</em> (74.52.145.91)</p>
<p><strong>John McCain</strong> - <em>Smartech Corporation</em> (64.203.107.147)</p>
<p><strong>Ralph Nader</strong> - <em>FutureQuest</em> (69.5.9.21)</p>
<p><strong>Barack Obama</strong> - <em>Pair Networks</em> (66.39.143.229)</p>
<p><strong>Ron Paul</strong> - <em>Rackspace</em> (74.205.22.51)</p>
<p><strong>Bill Richardson</strong> - <em>Engine Yard</em> (65.74.179.43)</p>
<p><strong>Mitt Romney</strong> - <em>Rackspace</em> (72.32.175.83)</p>
]]></content></item><item><title>Plesk: Disabling TRACE/TRACK methods globally</title><link>https://major.io/2008/04/23/plesk-disabling-tracetrack-methods-globally/</link><pubDate>Wed, 23 Apr 2008 23:40:50 +0000</pubDate><guid>https://major.io/2008/04/23/plesk-disabling-tracetrack-methods-globally/</guid><description>UPDATE: The TRACE/TRACK methods are disabled in Plesk 8.4 right out of the box!
It&amp;rsquo;s always been a bit of a challenge to disable TRACE and TRACK methods with Plesk. The only available options were to create a ton of vhost.conf files or adjust the httpd.include files and prevent modifications with chattr (which is a bad idea on many levels).
Luckily, Parallels has made things easier with a new knowledge base article.</description><content type="html"><![CDATA[<p><strong>UPDATE:</strong> The TRACE/TRACK methods are disabled in Plesk 8.4 right out of the box!</p>
<p>It&rsquo;s always been a <a href="http://rackerhacker.com/2007/08/28/apache-disable-trace-and-track-methods/">bit of a challenge</a> to disable TRACE and TRACK methods with Plesk. The only available options were to create a ton of vhost.conf files or adjust the httpd.include files and prevent modifications with <code>chattr</code> (which is a bad idea on many levels).</p>
<p>Luckily, Parallels has made things easier with a <a href="http://kb.parallels.com/en/4638">new knowledge base article</a>.</p>
]]></content></item><item><title>Reducing locking delays in MySQL</title><link>https://major.io/2008/04/16/reducing-locking-delays-in-mysql/</link><pubDate>Wed, 16 Apr 2008 17:32:50 +0000</pubDate><guid>https://major.io/2008/04/16/reducing-locking-delays-in-mysql/</guid><description>Before getting started, it&amp;rsquo;s important to understand why MySQL uses locks. In short - MySQL uses locks to prevent multiple clients from corrupting data due to simultaneous writes while also protecting clients from reading partially-written data.
Some of you may be thinking, &amp;ldquo;Okay, this makes sense.&amp;rdquo; If that&amp;rsquo;s you, skip the next two paragraphs. If not, keep reading.
Analogies can help understand topics like these. Here&amp;rsquo;s one that I came up with during a training class.</description><content type="html"><![CDATA[<p>Before getting started, it&rsquo;s important to understand why <a href="http://dev.mysql.com/">MySQL</a> uses <a href="http://dev.mysql.com/doc/refman/5.0/en/locking-issues.html">locks</a>. In short - MySQL uses locks to prevent multiple clients from corrupting data due to simultaneous writes while also protecting clients from reading partially-written data.</p>
<p>Some of you may be thinking, &ldquo;Okay, this makes sense.&rdquo; If that&rsquo;s you, skip the next two paragraphs. If not, keep reading.</p>
<p>Analogies can help understand topics like these. Here&rsquo;s one that I came up with during a training class. Consider two people sitting in front of a notepad on a table. Let&rsquo;s say that a sentence like &ldquo;The quick brown fox jumps over the lazy dog&rdquo; is already written on the notepad. If both people want to read the sentence simultaneously, they can do so without getting in each other&rsquo;s way. A third or fourth person could show up and they could all read it at the same time.</p>
<p>Well, let&rsquo;s say one of the people at the table is writing a screenplay for <a href="http://www.imdb.com/title/tt0085382/">Cujo</a>, and they want to change &ldquo;lazy&rdquo; to &ldquo;crazy&rdquo;. That person erases the &ldquo;l&rdquo; in &ldquo;lazy&rdquo; and then adds a &ldquo;cr&rdquo; to the front to spell &ldquo;crazy&rdquo;. So if the other person is reading the sentence while the first person is writing, they will see &ldquo;lazy&rdquo; turn into &ldquo;azy&rdquo;, then &ldquo;c_azy&rdquo;, and then finally, &ldquo;crazy&rdquo;. This isn&rsquo;t a big issue in real life, but on the database level, this could be dangerous. If the person who was reading the sentence showed up during the middle of the letter changes, they would think that the dog was &ldquo;azy&rdquo;, and they&rsquo;d walk away wondering what the adjective &ldquo;azy&rdquo; means. To get around this, MySQL uses locking to block clients from reading data while it&rsquo;s being written and it blocks clients from writing data simultaneously.</p>
<p>Now that we&rsquo;re all familiar with what locks are, and why MySQL uses them, let&rsquo;s talk about some ways to reduce the delays caused by locking. Here&rsquo;s some situations you might be running up against:</p>
<p><strong>Writes are delayed because reads have locked the tables</strong></p>
<p>This is the most common occurrence from the servers that I have seen. When you run a <code>SHOW PROCESSLIST</code>, you may see a few reads at the top of the queue that are in the status of &ldquo;Copying to tmp table&rdquo; and/or &ldquo;Sending data&rdquo;. On optimized servers running optimized queries, these should clear out quickly. If you&rsquo;re finding that they are not clearing out quickly, try the following:</p>
<ul>
<li>Use <code>EXPLAIN</code> on your queries to be sure that they are optimized</li>
<li>Add indexes to tables that you query often</li>
<li>Reduce the amount of rows that are being returned per query</li>
<li>Upgrade the networking equipment between web and database servers (if applicable)</li>
<li>Consider faster hardware with larger amounts of RAM</li>
<li>Use <a href="http://rackerhacker.com/mysqltuner/">MySQLTuner</a> to check your current server&rsquo;s configuration for issues</li>
<li>Consider moving to InnoDB to utilize row-based locking</li>
</ul>
<p><strong>Reads and writes are delayed because writes have locked the tables</strong></p>
<p>Situations like these are a little different. There&rsquo;s two main factors to consider here: either MySQL cannot write data to the disk fast enough, or your write queries (or tables) are not optimized. If you suspect a hardware issue, check your iowait with <code>sar</code> and see if it stays at about 10-20% or higher during the day. If it does, slow hardware may be the culprit. Try moving to SCSI disks and be sure to use RAID 5 or 10 for additional reliability and speed. SAN or DAS units may also help due to higher throughput and more disk spindles.</p>
<p>If you already have state-of-the-art hardware, be sure that your tables and queries are optimized. Run <code>OPTIMIZE TABLES</code> regularly if your data changes often to defragment the tables and clear out any holes from removed or updated data. Slow <code>UPDATE</code> queries suggest that you are updating too many rows, or you may be using a column in the WHERE clause that is not indexed. If you do a large amount of <code>INSERT</code> queries, use this syntax to enter multiple rows simultaneously:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sql" data-lang="sql"><span style="color:#719e07">INSERT</span> <span style="color:#719e07">INTO</span> <span style="color:#719e07">table</span> (col1,col2) <span style="color:#719e07">VALUES</span> (<span style="color:#2aa198">&#39;a&#39;</span>,<span style="color:#2aa198">&#39;1&#39;</span>), (<span style="color:#2aa198">&#39;b&#39;</span>,<span style="color:#2aa198">&#39;2&#39;</span>), (<span style="color:#2aa198">&#39;c&#39;</span>,<span style="color:#2aa198">&#39;3&#39;</span>);
</code></pre></div><p>This syntax tells MySQL to hold off on updating indexes until the entire query is complete. If you are updating a <strong>very large</strong> amount of rows, and you need to use multiple queries to avoid reaching the <a href="http://dev.mysql.com/doc/refman/5.0/en/server-system-variables.html#option_mysqld_max_allowed_packet">max_allowed_packet</a> directive, you can do something like this:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sql" data-lang="sql"><span style="color:#719e07">ALTER</span> <span style="color:#719e07">TABLE</span> <span style="color:#719e07">table</span> DISABLE KEYS;
<span style="color:#719e07">INSERT</span> <span style="color:#719e07">INTO</span> <span style="color:#719e07">table</span> (col1,col2) <span style="color:#719e07">VALUES</span> (<span style="color:#2aa198">&#39;a&#39;</span>,<span style="color:#2aa198">&#39;1&#39;</span>), (<span style="color:#2aa198">&#39;b&#39;</span>,<span style="color:#2aa198">&#39;2&#39;</span>), (<span style="color:#2aa198">&#39;c&#39;</span>,<span style="color:#2aa198">&#39;3&#39;</span>);
<span style="color:#719e07">~~~</span> many <span style="color:#719e07">more</span> inserts <span style="color:#719e07">~~~</span>
<span style="color:#719e07">ALTER</span> <span style="color:#719e07">TABLE</span> <span style="color:#719e07">table</span> ENABLE KEYS;
</code></pre></div><p>This forces MySQL to not calculate any new index information until you re-enable the keys or run <code>OPTIMIZE TABLE</code>. If all of this does not help, consider using InnoDB as your storage engine. You can benefit from the row-level locking, which reduces locking in mixed read/write scenarios. In addition, InnoDB is able to write data much more efficiently than MyISAM.</p>
]]></content></item><item><title>mchk: Unable to initialize quota settings for someuser@somedomain.com</title><link>https://major.io/2008/04/14/mchk-unable-to-initialize-quota-settings-for-someusersomedomaincom/</link><pubDate>Mon, 14 Apr 2008 17:00:21 +0000</pubDate><guid>https://major.io/2008/04/14/mchk-unable-to-initialize-quota-settings-for-someusersomedomaincom/</guid><description>If you&amp;rsquo;re working in Plesk and you receive this error:
mchk: Unable to initialize quota settings for someuser@somedomain.com
Run this command to fix the issue, but be patient:
find /var/qmail/mailnames -type d -name '.*' ! -name '.spamassassin' -ls -exec touch '{}'/maildirfolder \; -exec chown popuser:popuser '{}'/maildirfolder \; Thanks to Mike Jackson for this one.</description><content type="html"><![CDATA[<p>If you&rsquo;re working in Plesk and you receive this error:</p>
<p><strong>mchk: Unable to initialize quota settings for <a href="mailto:someuser@somedomain.com">someuser@somedomain.com</a></strong></p>
<p>Run this command to fix the issue, but be patient:</p>
<pre><code>find /var/qmail/mailnames -type d -name '.*' ! -name '.spamassassin' -ls -exec touch '{}'/maildirfolder \; -exec chown popuser:popuser '{}'/maildirfolder \;
</code></pre><blockquote>
<p>Thanks to <a href="http://barking-dog.net/">Mike Jackson</a> for this one.</p>
</blockquote>
]]></content></item><item><title>MySQLTuner v0.9.0 is now available</title><link>https://major.io/2008/04/06/mysqltuner-v090-is-now-available/</link><pubDate>Sun, 06 Apr 2008 14:30:28 +0000</pubDate><guid>https://major.io/2008/04/06/mysqltuner-v090-is-now-available/</guid><description>MySQLTuner v0.9.0 is now available. There is a bug fix and also a new feature!
Fixed a bug in the enumeration/sizing of tables in MySQL 5
In MySQL 5 on some distributions, a NULL is returned for the storage engine and data length. Luuk Vosslamber quickly e-mailed me about the bug yesterday and it has been fixed.
MySQLTuner version checking
MySQLTuner will now check to see if a new version is available when the script runs.</description><content type="html"><![CDATA[<p>MySQLTuner v0.9.0 is <a href="http://rackerhacker.com/mysqltuner/">now available</a>. There is a bug fix and also a new feature!</p>
<p><strong>Fixed a bug in the enumeration/sizing of tables in MySQL 5</strong></p>
<p>In MySQL 5 on some distributions, a NULL is returned for the storage engine and data length. Luuk Vosslamber quickly e-mailed me about the bug yesterday and it has been fixed.</p>
<p><strong>MySQLTuner version checking</strong></p>
<p>MySQLTuner will now check to see if a new version is available when the script runs. You can disable the check with the <code>--skipversion</code> option if you do not wish to perform the check.</p>
<p>The version check does not submit any information about the server, the MySQL installation, or any of your MySQL data. It simply queries a page on mysqltuner.com with the version number of your currently running script. Based on the value returned by the page, MySQLTuner will alert you if a new version is available.</p>
<p>To download the new version right now, please go to the <a href="http://rackerhacker.com/mysqltuner/">project page</a> and use the download links.</p>
]]></content></item><item><title>MySQLTuner v0.8.9 is now available</title><link>https://major.io/2008/04/05/mysqltuner-v089-is-now-available/</link><pubDate>Sat, 05 Apr 2008 16:03:14 +0000</pubDate><guid>https://major.io/2008/04/05/mysqltuner-v089-is-now-available/</guid><description>MySQLTuner v0.8.9 is now available. There are a few bug fixes, performance improvements, and readability adjustments.
Table enumeration and sizing can now be skipped
I&amp;rsquo;ve received reports that MySQLTuner will stall while enumerating tables on servers that contain a lot of tables or a lot of large tables. You can now use the --skipsize option to skip over the table enumeration process and let the script finish. However, you will not be able to receive some recommendations since the script was unable to determine which storage engines are enabled.</description><content type="html"><![CDATA[<p>MySQLTuner v0.8.9 is <a href="http://rackerhacker.com/mysqltuner/" title="MySQLTuner v0.8.9">now available</a>.  There are a few bug fixes, performance improvements, and readability adjustments.</p>
<p><strong>Table enumeration and sizing can now be skipped</strong></p>
<p>I&rsquo;ve received reports that MySQLTuner will stall while enumerating tables on servers that contain a lot of tables or a lot of large tables.  You can now use the <code>--skipsize</code> option to skip over the table enumeration process and let the script finish.  However, you will not be able to receive some recommendations since the script was unable to determine which storage engines are enabled.</p>
<p><strong>New table enumeration and sizing method for MySQL 5</strong></p>
<p>The script now uses information_schema to enumerate tables and their sizes for MySQL 5 servers.  This has provided a drastic improvement in performance.</p>
<p><strong>Readability improvements</strong></p>
<p>The recommendations for query_cache_limit and max_heap_table_size/tmp_table_size have been improved.</p>
<p>As always, I welcome your suggestions, bug reports, and questions!  Please feel free to drop a comment on this blog posting or send me an e-mail (it&rsquo;s in the script).  Also, don&rsquo;t forget to <a href="http://rackerhacker.com/mysqltuner/" title="MySQLTuner Mailing List">sign up for the MySQLTuner version announcement list</a>.</p>
]]></content></item><item><title>Small Companies: How to hire and fire a technical person</title><link>https://major.io/2008/04/02/small-companies-how-to-hire-and-fire-a-technical-person/</link><pubDate>Wed, 02 Apr 2008 18:00:06 +0000</pubDate><guid>https://major.io/2008/04/02/small-companies-how-to-hire-and-fire-a-technical-person/</guid><description>DISCLAIMER: Okay, technical folks - I&amp;rsquo;m doing this as a favor to the general community of people that aren&amp;rsquo;t very technical, but they need to know some tips for ridding themselves of a technical person that is harming their business. If you look at it this way, there&amp;rsquo;s a 50/50 chance that this article might get you hired instead of fired.
No one has every really asked me &amp;ldquo;hey, if I want to fire my technical guy and get a new one, how do I do it?</description><content type="html"><![CDATA[<p><strong>DISCLAIMER:</strong> Okay, technical folks - I&rsquo;m doing this as a favor to the general community of people that aren&rsquo;t very technical, but they need to know some tips for ridding themselves of a technical person that is harming their business. If you look at it this way, there&rsquo;s a 50/50 chance that this article might get you hired instead of fired.</p>
<p>No one has every really asked me &ldquo;hey, if I want to fire my technical guy and get a new one, how do I do it?&rdquo; So, how can I answer this question with any authority? Simple. I used to run my own company doing technical work for homes and businesses, I was hired and fired by people (much more hiring than firing), and I&rsquo;ve learned a lot from being &ldquo;the tech guy&rdquo;. Also, from working at Rackspace, and a previous job, I&rsquo;ve seen many situations in which a company lets their technical person go without any plan in place. You never realize how valuable your IT staff are until they&rsquo;re not in the office, and your e-mail server falls apart.</p>
<p><strong>Firing your technician</strong></p>
<p>I&rsquo;ll start with how to fire your current technical person. It should go without saying, but be sure that you&rsquo;re firing this person for a substantial and legal reason. If you&rsquo;re firing this person for something trivial or petty, <strong>stop right here and re-evaluate</strong>. But, if this is the kind of person that ignores your phone calls, takes down services to increase job security, or prints pornography on the office laserjet, then it&rsquo;s time for them to go.</p>
<p>First, create a plan. How much does this technical person know about the company that could be detrimental if they were fired abruptly? You&rsquo;ll need to consider things like their access to your buildings, computers, corporate credit cards, cars, and colocated/dedicated servers. Take an inventory of the access that they have, and also how they access these items. For example, if they have multiple user accounts on your computer network, then make sure that all of those accounts are accounted for. If you have secret passwords with any of your service providers, be sure those are documented as well.</p>
<p>If you don&rsquo;t know some of these items, but your technical person does, you might want to get this information from them in a careful manner. I&rsquo;d recommend against going in and saying something like &ldquo;we need to inventory all of our user accounts before you&rsquo;re canned&rdquo;. You need to find a plausible excuse for you to have a list of this information, and it needs to be something that the technical person won&rsquo;t argue with. Some good ones I&rsquo;ve heard are PCI, SOX or SAS/70 compliance. Let your employee know that compliance with these standards requires that you keep all of the access to all of these services in a safe place.</p>
<p>By the time you reach this stage, you really should have a new technician in mind. Interview them after work, or at night time so that the current technical person doesn&rsquo;t become suspicious. I&rsquo;ll talk more about how to find a new technical person a little later.</p>
<p>At this point, if you can trust your technical person, you should have a proper list of their entry points into your infrastructure. It&rsquo;s more likely that you don&rsquo;t trust this person at this point since you&rsquo;re firing them after all. Some might argue with me here, I&rsquo;d recommend bringing in some other technical person that you undoubtedly trust. The reason for this is that your technical person may have given you a partial list, or they may have left &ldquo;backdoors&rdquo; so that they can access your infrastructure after they leave. A trusted tech can review your company for any possible issues and can give you a heads up if they find any red flags. Of course, if your current technical person has set up traps to know when someone logs in, then you may have blown your cover entirely. I would certainly hope that your situation wouldn&rsquo;t end this badly.</p>
<p>Now that you have a complete list of everything to which your former tech had access, you have an idea of what will be involved in changing everything over. Most people like to fire employees on Fridays to reduce the chance of violence or uncomfortable moments, so here&rsquo;s my recommendation. If anything financial needs to be taken care of, get it done late on Thursday or early on Friday. Then, on Friday, set a time with your current technical person to have a short meeting. Coordinate this time with your trusted technical person so they can begin changing passwords on accounts which the current tech has access to. Change the most sensitive passwords first, like the passwords on database servers. Also, change the root passwords as a high priority, but make sure you eventually change them all since you can be bitten by sudo or SSH keys.</p>
<p>When your current technical person exits the meeting, you&rsquo;re covered. If the meeting goes well, and the current technical person is amicable, then you&rsquo;re going to be covered since their access is revoked. If the meeting goes badly, your still covered in case they try to do something nasty. Their access to your network and corporate infrastructure should be eliminated or minimized before they can do anything destructive.</p>
<p><strong>Hiring your technician</strong></p>
<p>Luckily, hiring a new technical person is a bit easier than firing one. However, if you do a bad job on the hiring, you&rsquo;ll be referring to the beginning of this article fairly soon.</p>
<p>The best way to find a new technical person is via recommendations from another person. They&rsquo;ve probably had interactions with the tech, and they can give you an idea of their technical prowess and social skills (yes, these are important). If you can&rsquo;t find any techs through recommendations, you can always check big job sites like Monster, LinkedIn, or Dice. <strong>Whichever route you choose, be sure to meet the technician in person.</strong> Don&rsquo;t hire someone based on the initials after their name, their previous job experience, or how they sound over the phone. Your technical person is like a central pillar in your organization, and this needs to be a responsible, sensible, and practical person.</p>
<p>Once you&rsquo;ve found one or more technicians that you&rsquo;d like to hire, you need to test them just a little. I&rsquo;d recommend contacting them late in the evening (8-10PM) or early/late on the weekend. See how receptive and cordial they are at these times, because when something explodes later, you&rsquo;ll probably be calling them after business hours. You don&rsquo;t want to pick up the phone at 4AM on Saturday when your Exchange server dies only to hear your tech tell you that he&rsquo;ll be in on Monday to fix it for you, and that it can wait until then. When you talk to the technician on the phone, ask them to do something that forces them to go use the computer. For example, send them an e-mail for something reasonable that they need to respond to. Or, tell them that you discovered some neat product or service, and you want to know if they could start working at your company and maintain that product or service. If they respond quickly and they don&rsquo;t give you the vibe that you&rsquo;ve just inconvenienced them horribly, then that&rsquo;s a good sign that you&rsquo;ve found a worthy technician.</p>
<p>It&rsquo;s up to you when you bring them on at your company. Some people might want to hold off until the current technician is out of the way, but some might want to bring the technician in a little early to help with the cleanup of the last technician. Either way is good in my opinion. However, I would recommend against having both of them employed at your company simultaneously. If your old technician is upset about something, that could rub off on the new guy, and you may be returning to the top part of this article sooner rather than later.</p>
<p>Also, don&rsquo;t expect the new technician to be knee-deep in your problems immediately. They will need some time to figure out your network, review your vital services, and get an idea how everything works together. If you have the giant list your previous tech made, be sure to furnish it to the new technician so they have an idea of where to go to fix a certain problem.</p>
<p>I certainly hope this article helps! If you have any questions, drop me a comment and I&rsquo;ll be happy to give additional recommendations.</p>
]]></content></item><item><title>Setting the maximum mail size in qmail</title><link>https://major.io/2008/03/24/setting-the-maximum-mail-size-in-qmail/</link><pubDate>Mon, 24 Mar 2008 18:54:35 +0000</pubDate><guid>https://major.io/2008/03/24/setting-the-maximum-mail-size-in-qmail/</guid><description>On a Plesk server, the maximum size for an individual e-mail sent through qmail is unlimited. You can limit this size by adding a number to the /var/qmail/control/databytes file.
If you wanted to limit this to something like 10MB, you can just run the following command:
echo &amp;quot;10485760&amp;quot; &amp;gt; /var/qmail/control/databytes This will limit the size of messages (including attachments) to 10MB as a maximum.</description><content type="html"><![CDATA[<p>On a Plesk server, the maximum size for an individual e-mail sent through qmail is unlimited. You can limit this size by adding a number to the /var/qmail/control/databytes file.</p>
<p>If you wanted to limit this to something like 10MB, you can just run the following command:</p>
<pre><code>echo &quot;10485760&quot; &gt; /var/qmail/control/databytes
</code></pre><p>This will limit the size of messages (including attachments) to 10MB as a maximum.</p>
]]></content></item><item><title>Importing MySQL dumps made on 64-bit servers</title><link>https://major.io/2008/03/21/importing-mysql-dumps-made-on-64-bit-servers/</link><pubDate>Fri, 21 Mar 2008 17:51:56 +0000</pubDate><guid>https://major.io/2008/03/21/importing-mysql-dumps-made-on-64-bit-servers/</guid><description>It&amp;rsquo;s tough to find examples of dumps that can&amp;rsquo;t be properly reimported on other servers. However, if you have a 64-bit server, and you make a MySQL dump file from it, you may see this issue when importing the dump on a 32-bit MySQL server:
ERROR 1118 (42000) at line 1686: Row size too large. The maximum row size for the used table type, not counting BLOBs, is 65535. You have to change some columns to TEXT or BLOBs You really don&amp;rsquo;t have any options in this situation.</description><content type="html"><![CDATA[<p>It&rsquo;s tough to find examples of dumps that can&rsquo;t be properly reimported on other servers. However, if you have a 64-bit server, and you make a MySQL dump file from it, you may see this issue when importing the dump on a 32-bit MySQL server:</p>
<pre><code>ERROR 1118 (42000) at line 1686: Row size too large. The maximum row size for the used table type, not counting BLOBs, is 65535. You have to change some columns to TEXT or BLOBs
</code></pre><p>You really don&rsquo;t have any options in this situation. You&rsquo;ll need to adjust your table on the 64-bit server for good and then make a new dump file, or you will just have to live with the fact that it can&rsquo;t be imported into a 32-bit instance of MySQL.</p>
]]></content></item><item><title>Reduce iowait in Plesk: put qmail’s queue on a ramdisk</title><link>https://major.io/2008/03/14/reduce-iowait-in-plesk-put-qmails-queue-on-a-ramdisk/</link><pubDate>Fri, 14 Mar 2008 18:16:57 +0000</pubDate><guid>https://major.io/2008/03/14/reduce-iowait-in-plesk-put-qmails-queue-on-a-ramdisk/</guid><description>I really dislike qmail. But, since I use Plesk, I&amp;rsquo;m stuck with it. However, I found a way to improve it&amp;rsquo;s awful mail queue performance by putting the mail queue onto a ramdisk. This is actually pretty darned easy to do.
First, toss a line like this into your /etc/fstab:
none /mailqueue tmpfs defaults,size=100m,nr_inodes=999k,mode=775 0 0 This will make a 100MB ramdisk on /mailqueue. Now, just symlink /var/qmail/mqueue to /mailqueue and move your e-mail over:</description><content type="html"><![CDATA[<p>I really dislike qmail. But, since I use Plesk, I&rsquo;m stuck with it. However, I found a way to improve it&rsquo;s awful mail queue performance by putting the mail queue onto a ramdisk. This is actually pretty darned easy to do.</p>
<p>First, toss a line like this into your /etc/fstab:</p>
<pre><code>none    /mailqueue      tmpfs   defaults,size=100m,nr_inodes=999k,mode=775      0       0
</code></pre><p>This will make a 100MB ramdisk on /mailqueue. Now, just symlink /var/qmail/mqueue to /mailqueue and move your e-mail over:</p>
<pre><code># mount /mailqueue
# chmod 750 /mailqueue
# chown qmailq:qmail /mailqueue
# mv /var/qmail/mqueue /var/qmail/mqueue-old
# ln -s /mailqueue /var/qmail/mqueue
# rsync -av /var/qmail/mqueue-old /mailqueue
</code></pre><p>This has significantly cut the iowait on my server during heavy e-mail periods. In addition, tools like <a href="http://sourceforge.net/projects/qmhandle">qmHandle</a> now fly through my mail queue and give me reports very quickly.</p>
]]></content></item><item><title>Plesk Professional Website Editor hangs at login</title><link>https://major.io/2008/03/13/plesk-professional-website-editor-hangs-at-login/</link><pubDate>Thu, 13 Mar 2008 18:11:57 +0000</pubDate><guid>https://major.io/2008/03/13/plesk-professional-website-editor-hangs-at-login/</guid><description>One of my biggest Plesk gripes is dealing with the Plesk Professional Website Editor. One of the most common occurrences with PPWSE is that it hangs when you attempt to log into the server. Normally, this happens when a server is behind a firewall, and it is using private IP&amp;rsquo;s.
Plesk will actually query the DNS for the domain (rather than simply connecting to the localhost), try to reach the public IP, and the traffic will be blocked by the firewall.</description><content type="html"><![CDATA[<p>One of my biggest Plesk gripes is dealing with the Plesk Professional Website Editor. One of the most common occurrences with PPWSE is that it hangs when you attempt to log into the server. Normally, this happens when a server is behind a firewall, and it is using private IP&rsquo;s.</p>
<p>Plesk will actually query the DNS for the domain (rather than simply connecting to the localhost), try to reach the public IP, and the traffic will be blocked by the firewall. This creates a login session that appears to hang, and then it shows &ldquo;FTP: not connected&rdquo; in the interface.</p>
<p>The fix is actually quite easy:</p>
<ol>
<li>
<p>Be sure that the ftp.domain.com CNAME/A record exists</p>
</li>
<li>
<p>Add a line to /etc/hosts that forces ftp.domain.com to resolve to the proper private IP address.</p>
</li>
</ol>
<p>The third item should be to stop using PPWSE, but that&rsquo;s the hardest one to work out. I&rsquo;d recommend using something like <a href="http://macromates.com/">TextMate</a> on a <a href="http://www.apple.com/macbookpro/">Mac</a> along with <a href="http://www.panic.com/transmit/">Transmit</a>, but you can get some good results out of Dreamweaver as well. Whatever you do, don&rsquo;t use Contribute.</p>
]]></content></item><item><title>What is the difference between file data and metadata?</title><link>https://major.io/2008/03/12/what-is-the-difference-between-file-data-and-metadata/</link><pubDate>Wed, 12 Mar 2008 18:01:59 +0000</pubDate><guid>https://major.io/2008/03/12/what-is-the-difference-between-file-data-and-metadata/</guid><description>Just in case some of you out there enjoy nomenclature and theory behind Linux filesystems, here&amp;rsquo;s some things to keep in mind. The modification time (mtime) of a file describes when the actual data blocks that hold the file changed. The changed time (ctime) of a file describes when the metadata was last changed.
Also, metadata is stored within a different location than the data blocks. The metadata fits in the inode while the file&amp;rsquo;s data goes within data blocks.</description><content type="html"><![CDATA[<p>Just in case some of you out there enjoy nomenclature and theory behind Linux filesystems, here&rsquo;s some things to keep in mind. The modification time (mtime) of a file describes when the actual data blocks that hold the file changed. The changed time (ctime) of a file describes when the metadata was last changed.</p>
<p>Also, metadata is stored within a different location than the data blocks. The metadata fits in the inode while the file&rsquo;s data goes within data blocks. The inode information contains the owner, owner&rsquo;s group, time related data (atime, ctime, mtime), and the mode (permissions).</p>
<p>The name of the file itself is actually stored within the file that makes up the directory. And, the directory is simply a file that masquerades as a directory once the filesystem is mounted and read.</p>
]]></content></item><item><title>Hunting down elusive sources of iowait</title><link>https://major.io/2008/03/11/hunting-down-elusive-sources-of-iowait/</link><pubDate>Tue, 11 Mar 2008 18:00:18 +0000</pubDate><guid>https://major.io/2008/03/11/hunting-down-elusive-sources-of-iowait/</guid><description>A question I&amp;rsquo;m asked daily is “How can I find out what is generating iowait on my server?” Sure, you can dig through pages of lsof output, restart services, or run strace, but it can be a frustrating process. I saw a process on this blog post, and I changed the regexes to fit Red Hat and CentOS systems a bit better:
# /etc/init.d/syslog stop # echo 1 &amp;gt; /proc/sys/vm/block_dump # dmesg | egrep &amp;quot;READ|WRITE|dirtied&amp;quot; | egrep -o '([a-zA-Z]*)' | sort | uniq -c | sort -rn | head 1526 mysqld 819 httpd 429 kjournald 35 qmail 27 in 7 imapd 6 irqbalance 5 pop 4 pdflush 3 spamc In my specific situation, it looks like MySQL is the biggest abuser of my disk, followed by Apache and the filesystem journaling.</description><content type="html"><![CDATA[<p>A question I&rsquo;m asked daily is “How can I find out what is generating iowait on my server?” Sure, you can dig through pages of lsof output, restart services, or run strace, but it can be a frustrating process. I saw a process on <a href="http://blog.eikke.com/index.php/ikke/2007/03/22/who_s_abusing_my_sata_controller">this blog post</a>, and I changed the regexes to fit Red Hat and CentOS systems a bit better:</p>
<pre><code># /etc/init.d/syslog stop
# echo 1 &gt; /proc/sys/vm/block_dump
# dmesg | egrep &quot;READ|WRITE|dirtied&quot; | egrep -o '([a-zA-Z]*)' | sort | uniq -c | sort -rn | head
   1526 mysqld
    819 httpd
    429 kjournald
     35 qmail
     27 in
      7 imapd
      6 irqbalance
      5 pop
      4 pdflush
      3 spamc
</code></pre><p>In my specific situation, it looks like MySQL is the biggest abuser of my disk, followed by Apache and the filesystem journaling. As expected, qmail is a large contender, too.</p>
<p><strong>Don&rsquo;t forget to set things back to their normal state when you&rsquo;re done!</strong></p>
<pre><code># echo 0 &gt; /proc/sys/vm/block_dump
# /etc/init.d/syslog start
</code></pre>]]></content></item><item><title>Strange error with Horde 3.1.3 and Plesk 8.1.1</title><link>https://major.io/2008/03/10/strange-error-with-horde-313-and-plesk-811/</link><pubDate>Tue, 11 Mar 2008 02:49:05 +0000</pubDate><guid>https://major.io/2008/03/10/strange-error-with-horde-313-and-plesk-811/</guid><description>I saw a ticket the other day where a customer received this error from Horde when trying to expand items on the left pane of the interface:
Fatal error: Cannot use string offset as an array in /www/horde/lib/Horde/Block/Layout/Manager.php on line 389
It turns out that Plesk 8.1.1 bundles Horde 3.1.3 which has an occasional bug within the interface. Upgrading to Plesk 8.2.0 corrects the issue as Horde 3.1.4 is installed with the upgrade.</description><content type="html"><![CDATA[<p>I saw a ticket the other day where a customer received this error from Horde when trying to expand items on the left pane of the interface:</p>
<p><code>Fatal error: Cannot use string offset as an array in /www/horde/lib/Horde/Block/Layout/Manager.php on line 389</code></p>
<p>It turns out that Plesk 8.1.1 bundles Horde 3.1.3 which has an occasional bug within the interface. Upgrading to Plesk 8.2.0 corrects the issue as Horde 3.1.4 is installed with the upgrade.</p>
<p>See <a href="http://bugs.horde.org/ticket/?id=4070">Horde&rsquo;s bug page</a> for more information.</p>
]]></content></item><item><title>MySQLTuner v0.8.6 is now available</title><link>https://major.io/2008/02/26/mysqltuner-v086-is-now-available/</link><pubDate>Wed, 27 Feb 2008 03:24:52 +0000</pubDate><guid>https://major.io/2008/02/26/mysqltuner-v086-is-now-available/</guid><description>Version 0.8.6 of MySQLTuner is now available. It contains a few bug fixes and readability improvements:
Newlines are placed between the sections for increased readability
Each section now contains extra lines to set the sections apart. It makes the output a little longer, but easier to read as well.
Storage engine status color bug
Even if the -nocolor option was passed, the storage engine statuses were being shown in color.</description><content type="html"><![CDATA[<p>Version 0.8.6 of MySQLTuner is <a href="http://mysqltuner.com/">now available</a>. It contains a few bug fixes and readability improvements:</p>
<p><strong>Newlines are placed between the sections for increased readability</strong></p>
<p>Each section now contains extra lines to set the sections apart. It makes the output a little longer, but easier to read as well.</p>
<p><strong>Storage engine status color bug</strong></p>
<p>Even if the -nocolor option was passed, the storage engine statuses were being shown in color.</p>
<p><strong>Excluded information_schema from storage engine calculations</strong></p>
<p>The information_schema database was causing extra MEMORY tables to show up in the calculations. They&rsquo;re now excluded when the calculations are being made.</p>
<p>Shawn Ashlee has also been added as a contributor as he&rsquo;s been a constant help for the project. He&rsquo;s recommended implementation ideas and he has worked to create internal MySQLTuner RPM&rsquo;s for use at Rackspace. Thanks, Shawn!</p>
]]></content></item><item><title>ntpd_initres: ntpd returns a permission denied error</title><link>https://major.io/2008/02/20/ntpd_initres-ntpd-returns-a-permission-denied-error/</link><pubDate>Wed, 20 Feb 2008 18:30:02 +0000</pubDate><guid>https://major.io/2008/02/20/ntpd_initres-ntpd-returns-a-permission-denied-error/</guid><description>I recently came across a server that was throwing this error into its message log:
ntpd_initres[2619]: ntpd returns a permission denied error!
It would only appear about every five minutes on the server, and restarting ntpd didn&amp;rsquo;t correct the issue. I stopped ntpd entirely, but the error still appeared a few minutes later.
After examining the running processes, I found that there was a lonely ntpd process that was running using a non-standard method.</description><content type="html"><![CDATA[<p>I recently came across a server that was throwing this error into its message log:</p>
<p><code>ntpd_initres[2619]: ntpd returns a permission denied error!</code></p>
<p>It would only appear about every five minutes on the server, and restarting ntpd didn&rsquo;t correct the issue. I stopped ntpd entirely, but the error still appeared a few minutes later.</p>
<p>After examining the running processes, I found that there was a lonely ntpd process that was running using a non-standard method. I killed that process, started the default instance of ntpd using the init scripts, and the issue went away.</p>
<p>It turns out that ntpd daemon that was started manually was unable to access some of the required paths and sockets that is necessary for ntpd to run properly. These configuration items are set up in the init scripts, but they&rsquo;re not included when ntpd is running manually.</p>
<p><em>This was tested on Red Hat Enterprise Linux 4.</em></p>
]]></content></item><item><title>MySQLTuner now has a mailing list</title><link>https://major.io/2008/02/19/mysqltuner-now-has-a-mailing-list/</link><pubDate>Wed, 20 Feb 2008 02:23:15 +0000</pubDate><guid>https://major.io/2008/02/19/mysqltuner-now-has-a-mailing-list/</guid><description>Due to popular demand, I&amp;rsquo;ve created a mailing list for MySQLTuner. It will be a low-traffic list consisting of new version announcements and bug fixes.
If you want to subscribe, simply send an e-mail with SUBSCRIBE in the subject line to mysqltuner-announce-request@mysqltuner.com. Be sure to add mysqltuner-announce@mysqltuner.com to your mail provider&amp;rsquo;s whitelist.</description><content type="html"><![CDATA[<p>Due to popular demand, I&rsquo;ve created a mailing list for MySQLTuner. It will be a low-traffic list consisting of new version announcements and bug fixes.</p>
<p>If you want to subscribe, simply send an e-mail with <strong>SUBSCRIBE</strong> in the subject line to <a href="mailto:mysqltuner-announce-request@mysqltuner.com?subject=SUBSCRIBE">mysqltuner-announce-request@mysqltuner.com</a>. Be sure to add <strong><a href="mailto:mysqltuner-announce@mysqltuner.com">mysqltuner-announce@mysqltuner.com</a></strong> to your mail provider&rsquo;s whitelist.</p>
]]></content></item><item><title>sendmail: savemail panic</title><link>https://major.io/2008/02/18/sendmail-savemail-panic/</link><pubDate>Mon, 18 Feb 2008 18:56:37 +0000</pubDate><guid>https://major.io/2008/02/18/sendmail-savemail-panic/</guid><description>If you see a large mail queue and your system&amp;rsquo;s I/O is increasing, you may find messages like these in your syslog:
Losing q5/qfg9N5EwE3004499: savemail panic&amp;lt;br /&amp;gt; SYSERR(root): savemail: cannot save rejected email anywhere
In this situation, there&amp;rsquo;s some reason why sendmail cannot deliver e-mail to the postmaster address. There&amp;rsquo;s a few issues that can create this problem:
Missing postmaster alias in /etc/aliases Hard disk is full The mail spool for the postmaster has the wrong ownership The mbox file for the postmaster is over 2GB and procmail can&amp;rsquo;t deliver the e-mail First, correct the situation that is preventing sendmail from delivering the e-mail to the postmaster user.</description><content type="html"><![CDATA[<p>If you see a large mail queue and your system&rsquo;s I/O is increasing, you may find messages like these in your syslog:</p>
<p><code>Losing q5/qfg9N5EwE3004499: savemail panic&lt;br /&gt; SYSERR(root): savemail: cannot save rejected email anywhere</code></p>
<p>In this situation, there&rsquo;s some reason why sendmail cannot deliver e-mail to the postmaster address. There&rsquo;s a few issues that can create this problem:</p>
<ul>
<li>Missing postmaster alias in /etc/aliases</li>
<li>Hard disk is full</li>
<li>The mail spool for the postmaster has the wrong ownership</li>
<li>The mbox file for the postmaster is over 2GB and procmail can&rsquo;t deliver the e-mail</li>
</ul>
<p>First, correct the situation that is preventing sendmail from delivering the e-mail to the postmaster user. Then, stop sendmail, clear the e-mail queue, and start sendmail again.</p>
<p>I found this issue on a Red Hat Enterprise Linux 4 server and then found the solution on <a href="http://www.brandonhutchinson.com/savemail_panic_in_Sendmail.html">Brandon&rsquo;s</a> site.</p>
]]></content></item><item><title>MySQLTuner v0.8.5 is now available</title><link>https://major.io/2008/02/11/mysqltuner-v085-is-now-available/</link><pubDate>Mon, 11 Feb 2008 18:02:11 +0000</pubDate><guid>https://major.io/2008/02/11/mysqltuner-v085-is-now-available/</guid><description>To get the latest copy, head over to the download page! Here&amp;rsquo;s the changes from 0.8.0 to 0.8.5 for MySQLTuner:
Fixed a copy/paste bug
There was a bug in 0.8.0 that displayed &amp;ldquo;OK&amp;rdquo; in red rather than showing &amp;ldquo;!!&amp;rdquo;. It affected the informational &amp;ldquo;-&amp;rdquo; outputs as well. Thanks to Nils Breunese for pointing out that confusing bug!
Fixed a data length calculation bug with MySQL 4.0
If the script was run against MySQL 4.</description><content type="html"><![CDATA[<p>To get the latest copy, head over to the <a href="http://mysqltuner.com/">download page</a>! Here&rsquo;s the changes from 0.8.0 to 0.8.5 for MySQLTuner:</p>
<p><strong>Fixed a copy/paste bug</strong></p>
<p>There was a bug in 0.8.0 that displayed &ldquo;OK&rdquo; in red rather than showing &ldquo;!!&rdquo;. It affected the informational &ldquo;-&rdquo; outputs as well. Thanks to Nils Breunese for pointing out that confusing bug!</p>
<p><strong>Fixed a data length calculation bug with MySQL 4.0</strong></p>
<p>If the script was run against MySQL 4.0 servers, it would return the Max_data_length rather than Data_length, and this was returning some horribly incorrect results for the size of tables in use with certain storage engines.</p>
<p><strong>Fixed a key buffer calculation bug with MySQL 4.0</strong></p>
<p>It&rsquo;s not possible to ask a MySQL 4.0 server about how much of the key buffer is in use, so the functionality for MySQL 4.0 now matches 3.23 for key buffer calculations.</p>
<p><strong>Added a notification for well-optimized servers</strong></p>
<p>For situations where MySQLTuner can&rsquo;t make any recommendations for performance increases, it actually says so now.</p>
<p><strong>Version bump to 0.8.5</strong></p>
<p>It&rsquo;s getting close to a full 1.0 release!</p>
]]></content></item><item><title>WordPress upgraded to 2.3.3</title><link>https://major.io/2008/02/05/wordpress-upgraded-to-233/</link><pubDate>Wed, 06 Feb 2008 00:48:17 +0000</pubDate><guid>https://major.io/2008/02/05/wordpress-upgraded-to-233/</guid><description>I botched my upgrade to 2.3.3 for a short while (sorry for the errors!), but everything is back to normal now. There&amp;rsquo;s a critical XMLRPC vulnerability in 2.3.2, so you might want to upgrade soon!</description><content type="html"><![CDATA[<p>I botched my upgrade to 2.3.3 for a short while (sorry for the errors!), but everything is back to normal now. There&rsquo;s a critical XMLRPC vulnerability in 2.3.2, so you might want to <a href="http://wordpress.org/download/">upgrade soon</a>!</p>
]]></content></item><item><title>Taking a couple of weeks off</title><link>https://major.io/2008/02/04/taking-a-couple-of-weeks-off/</link><pubDate>Tue, 05 Feb 2008 00:23:30 +0000</pubDate><guid>https://major.io/2008/02/04/taking-a-couple-of-weeks-off/</guid><description>I&amp;rsquo;m going to take two weeks off from posting on RackerHacker and I&amp;rsquo;m going to put that time towards bringing the next version of boxcheck.com to fruition. I&amp;rsquo;ve been hit with a lot of requests, and it seems to be a tool that people find pretty useful!
I&amp;rsquo;ll keep updating the blog as I get closer to releasing the next version. Also, as soon as a beta is available, I will certainly let you know!</description><content type="html"><![CDATA[<p>I&rsquo;m going to take two weeks off from posting on RackerHacker and I&rsquo;m going to put that time towards bringing the next version of <a href="http://boxcheck.com/">boxcheck.com</a> to fruition. I&rsquo;ve been hit with a lot of requests, and it seems to be a tool that people find pretty useful!</p>
<p>I&rsquo;ll keep updating the blog as I get closer to releasing the next version. Also, as soon as a beta is available, I will certainly let you know!</p>
]]></content></item><item><title>MySQLTuner v0.8.0 is now available</title><link>https://major.io/2008/01/31/mysqltuner-v080-is-now-available/</link><pubDate>Fri, 01 Feb 2008 01:01:35 +0000</pubDate><guid>https://major.io/2008/01/31/mysqltuner-v080-is-now-available/</guid><description>Thanks to an e-mail from Joe Calderon, I&amp;rsquo;ve corrected a MySQLTuner bug where indexes were not being calculated properly when symbolic links are used. It was a quick fix, and the new version is now available for download.
Also, MySQLTuner now has a standardized versioning scheme. I&amp;rsquo;m starting at 0.8.0 since I&amp;rsquo;m close to a completely stable release. There&amp;rsquo;s still a few kinks when working with MySQL cluster nodes, and I&amp;rsquo;m hoping to work that out soon.</description><content type="html"><![CDATA[<p>Thanks to an e-mail from Joe Calderon, I&rsquo;ve corrected a MySQLTuner bug where indexes were not being calculated properly when symbolic links are used. It was a quick fix, and the new version is <a href="http://mysqltuner.com/">now available for download</a>.</p>
<p>Also, MySQLTuner now has a standardized versioning scheme. I&rsquo;m starting at 0.8.0 since I&rsquo;m close to a completely stable release. There&rsquo;s still a few kinks when working with MySQL cluster nodes, and I&rsquo;m hoping to work that out soon.</p>
<p>Thanks for downloading!</p>
]]></content></item><item><title>High iowait on RHEL 4 with Plesk and SpamAssassin</title><link>https://major.io/2008/01/31/high-iowait-on-rhel-4-with-plesk-and-spamassassin/</link><pubDate>Thu, 31 Jan 2008 18:38:58 +0000</pubDate><guid>https://major.io/2008/01/31/high-iowait-on-rhel-4-with-plesk-and-spamassassin/</guid><description>One of my biggest complaints on RHEL 4 is the large resource usage by the version of SpamAssassin that is installed. When it runs, it uses a ton of CPU time and causes a lot of disk I/O as well. When running top, you may see multiple spamd processes. For a high-volume e-mail server (like the one I administer), this is simply unacceptable.
I decided to do something about it, and here are the steps:</description><content type="html"><![CDATA[<p>One of my biggest complaints on RHEL 4 is the large resource usage by the version of SpamAssassin that is installed. When it runs, it uses a ton of CPU time and causes a lot of disk I/O as well. When running top, you may see multiple spamd processes. For a high-volume e-mail server (like the one I administer), this is simply unacceptable.</p>
<p>I decided to do something about it, and here are the steps:</p>
<p><strong>First,</strong> you will need two RPMs:</p>
<p>Latest <a href="http://dag.wieers.com/rpm/packages/spamassassin/">SpamAssassin RPM from Dag</a></p>
<p>The <a href="http://autoinstall.plesk.com/">psa-spamassassin RPM from SWSoft/Parallels</a>.</p>
<p>Once you have them both on the server, install the new SpamAssassin package from Dag:</p>
<p><code># rpm -Uvh spamassassin-(version).el4.rf.(arch).rpm</code></p>
<p>At this point, Plesk&rsquo;s spamassassin scripts will be non-functional, but the next step will fix it:</p>
<p><code># rpm -Uvh --force psa-spamassassin-(version).(arch).rpm</code></p>
<p><strong>NOTE: DO NOT REMOVE the psa-spamassassin RPM. This will begin stripping your system of all SpamAssassin configurations and it cannot be reversed!</strong></p>
<p>Plesk&rsquo;s SpamAssassin scripts have been restored at this point in the process. Now, we need to do the part that really makes SpamAssassin work efficiently:</p>
<p><code># sa-update; sa-compile;</code></p>
<p>This will update the SpamAssassin rules, and it will compile the rules with re2c (you may also need to <a href="http://dag.wieers.com/rpm/packages/re2c/">get this RPM from Dag</a>). This compilation means less disk access, and less CPU time being used to process e-mails.</p>
<p>To activate the compiled rules within SpamAssassin, uncomment the plugin line in /etc/mail/spamassassin/v320.pre:</p>
<p><code># Rule2XSBody - speedup by compilation of ruleset to native code&lt;br /&gt; #&lt;br /&gt; loadplugin Mail::SpamAssassin::Plugin::Rule2XSBody</code></p>
<p>Please bear in mind that this process is done <em>at your own risk</em>. This may cause issues getting support from SWSoft or your hosting company. This has been tested on Red Hat Enterprise Linux 4 64-bit with Plesk 8.1.1, 8.2.0, and 8.2.1 with SpamAssassin 3.2.3 and 3.2.4.</p>
]]></content></item><item><title>Plesk and MySQL 5</title><link>https://major.io/2008/01/30/plesk-and-mysql-5/</link><pubDate>Wed, 30 Jan 2008 18:29:18 +0000</pubDate><guid>https://major.io/2008/01/30/plesk-and-mysql-5/</guid><description>One of the questions I receive the most is: &amp;ldquo;What version of Plesk works with MySQL 5?&amp;rdquo; The minimum version of Plesk for MySQL 5 is 8.1.0. If you install MySQL 5 on a version prior to 8.1.0, you may be able to access then panel in the other 8.x versions, but your upgrades will fail miserably.
In case you&amp;rsquo;re curious about a slightly older system, full MySQL 4 support was available in Plesk 7.</description><content type="html"><![CDATA[<p>One of the questions I receive the most is: &ldquo;What version of Plesk works with MySQL 5?&rdquo; The minimum version of Plesk for MySQL 5 is <strong>8.1.0</strong>. If you install MySQL 5 on a version prior to 8.1.0, you may be able to access then panel in the other 8.x versions, but your upgrades will fail miserably.</p>
<p>In case you&rsquo;re curious about a slightly older system, full MySQL 4 support was available in Plesk <strong>7.5.3</strong>. However, MySQL 4 is supported on some distributions as far back as <strong>7.1</strong>:</p>
<p>Fedora Core 2</p>
<p>Mandrake 10</p>
<p>SuSE 9.0</p>
<p>FreeBSD 5.2.1</p>
<p>Check out SWSoft/Parallel&rsquo;s site for more information about MySQL <a href="http://kb.swsoft.com/en/305">4</a> and <a href="http://kb.swsoft.com/en/1792">5</a> support.</p>
]]></content></item><item><title>Limiting which commands are kept in the bash history file</title><link>https://major.io/2008/01/29/limiting-which-commands-are-kept-in-the-bash-history-file/</link><pubDate>Tue, 29 Jan 2008 18:33:55 +0000</pubDate><guid>https://major.io/2008/01/29/limiting-which-commands-are-kept-in-the-bash-history-file/</guid><description>By setting a certain bash environment variable, you can limit which commands are kept in the .bash_history file. The following options can be passed to the HISTCONTROL environmental variable:
ignorespace - omits commands beginning with a space
ignoredups - omits commands that match the previously run command
ignoreboth - combines ignorespace and ignoredups
erasedups - removes previous lines that match the line that was just run
To set it, simply run the following from the command line, or add it to the .</description><content type="html"><![CDATA[<p>By setting a certain bash environment variable, you can limit which commands are kept in the .bash_history file. The following options can be passed to the HISTCONTROL environmental variable:</p>
<p><strong>ignorespace</strong> - omits commands beginning with a space</p>
<p><strong>ignoredups</strong> - omits commands that match the previously run command</p>
<p><strong>ignoreboth</strong> - combines <strong>ignorespace</strong> and <strong>ignoredups</strong></p>
<p><strong>erasedups</strong> - removes previous lines that match the line that was just run</p>
<p>To set it, simply run the following from the command line, or add it to the .bashrc or a single user&rsquo;s .bash_profile:</p>
<p><code>export HISTCONTROL=ignorespace</code></p>
<p>If no value is set, then all commands will be saved regardless of their content.</p>
]]></content></item><item><title>Rebuilding the initial ram disk (initrd)</title><link>https://major.io/2008/01/28/rebuilding-the-initial-ram-disk-initrd/</link><pubDate>Mon, 28 Jan 2008 18:23:39 +0000</pubDate><guid>https://major.io/2008/01/28/rebuilding-the-initial-ram-disk-initrd/</guid><description>Installing new hardware may mean that new kernel need to be loaded when your server boots up. There&amp;rsquo;s a two step process to making a new initrd file:
First, add the appropriate line to your /etc/modules.conf or /etc/modprobe.conf which corresponds to your new kernel module.
Next, rebuild the initial ram disk after making a backup of the current one:
# cp /boot/initrd-`uname -r`.img /boot/initrd-`uname -r`.img.bak # mkinitrd -f initrd-`uname -r`.img `uname -r` Reboot the server now and make sure the new driver is loaded properly.</description><content type="html"><![CDATA[<p>Installing new hardware may mean that new kernel need to be loaded when your server boots up. There&rsquo;s a two step process to making a new initrd file:</p>
<p><strong>First,</strong> add the appropriate line to your /etc/modules.conf or /etc/modprobe.conf which corresponds to your new kernel module.</p>
<p><strong>Next,</strong> rebuild the initial ram disk after making a backup of the current one:</p>
<pre><code># cp /boot/initrd-`uname -r`.img /boot/initrd-`uname -r`.img.bak
# mkinitrd -f initrd-`uname -r`.img `uname -r`
</code></pre><p>Reboot the server now and make sure the new driver is loaded properly.</p>
]]></content></item><item><title>Can’t enable DNSBL/RBL in Plesk because it’s greyed out</title><link>https://major.io/2008/01/25/cant-enable-dnsblrbl-in-plesk-because-its-greyed-out/</link><pubDate>Fri, 25 Jan 2008 17:11:27 +0000</pubDate><guid>https://major.io/2008/01/25/cant-enable-dnsblrbl-in-plesk-because-its-greyed-out/</guid><description>If you have a new Plesk installation and the following option is greyed out in Server -&amp;gt; Mail:
Switch on spam protection based on DNS blackhole lists
Just install the following RPM from Plesk:
psa-qmail-rblsmtpd</description><content type="html"><![CDATA[<p>If you have a new Plesk installation and the following option is greyed out in Server -&gt; Mail:</p>
<p><em>Switch on spam protection based on DNS blackhole lists</em></p>
<p>Just install the following RPM from Plesk:</p>
<p><code>psa-qmail-rblsmtpd</code></p>
]]></content></item><item><title>MySQLTuner reaches 5,000 downloads!</title><link>https://major.io/2008/01/25/mysqltuner-reaches-5000-downloads/</link><pubDate>Fri, 25 Jan 2008 17:01:18 +0000</pubDate><guid>https://major.io/2008/01/25/mysqltuner-reaches-5000-downloads/</guid><description>I&amp;rsquo;ve just reviewed the stats for MySQLTuner, and it crossed the 5,000 download mark last week! Thanks to all of you who have helped me make it a successful tool for MySQL server optimization.
With that said - how can MySQLTuner be improved to help you? Please let me know, and I&amp;rsquo;ll get started on those features!</description><content type="html"><![CDATA[<p>I&rsquo;ve just reviewed the stats for MySQLTuner, and it crossed the 5,000 download mark last week! Thanks to all of you who have helped me make it a successful tool for MySQL server optimization.</p>
<p>With that said - how can MySQLTuner be improved to help you? Please let me know, and I&rsquo;ll get started on those features!</p>
]]></content></item><item><title>ip_conntrack: table full, dropping packet</title><link>https://major.io/2008/01/24/ip_conntrack-table-full-dropping-packet/</link><pubDate>Thu, 24 Jan 2008 18:26:40 +0000</pubDate><guid>https://major.io/2008/01/24/ip_conntrack-table-full-dropping-packet/</guid><description>Using Linux kernel 3.12 or later? See this updated post instead.
Last week, I found myself with a server under low load, but it couldn&amp;rsquo;t make or receive network connections. When I ran dmesg, I found the following line repeating over and over:
I&amp;rsquo;d seen this message before, but I headed over to Red Hat&amp;rsquo;s site for more details. It turns out that the server was running iptables, but it was under a very heavy load and also handling a high volume of network connections.</description><content type="html"><![CDATA[<p><strong>Using Linux kernel 3.12 or later?</strong> See <a href="https://major.io/2014/01/07/nf-conntrack-table-full-dropping-packet/">this updated post</a> instead.</p>
<hr>
<p>Last week, I found myself with a server under low load, but it couldn&rsquo;t make or receive network connections. When I ran <code>dmesg</code>, I found the following line repeating over and over:</p>
<!-- raw HTML omitted -->
<p>I&rsquo;d seen this message before, but I headed over to <a href="http://kbase.redhat.com/faq/FAQ_45_11238.shtm">Red Hat&rsquo;s site</a> for more details. It turns out that the server was running iptables, but it was under a very heavy load and also handling a high volume of network connections. Generally, the ip_conntrack_max is set to the total MB of RAM installed multiplied by 16. However, this server had 4GB of RAM, but ip_conntrack_max was set to 65536:</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>I logged into another server with 1GB of RAM (RHES 5, 32-bit) and another with 2GB of RAM (RHES 4, 64-bit), and both had ip_conntrack_max set to 65536. I&rsquo;m not sure if this is a known Red Hat issue, or if it&rsquo;s just set to a standard value out of the box.</p>
<p>If you want to check your server&rsquo;s current tracked connections, just run the following:</p>
<!-- raw HTML omitted -->
<p>If you want to adjust it (as I did), just run the following as root:</p>
<!-- raw HTML omitted -->
]]></content></item><item><title>qmail: This message is looping: it already has my Delivered-To line</title><link>https://major.io/2008/01/23/qmail-this-message-is-looping-it-already-has-my-delivered-to-line/</link><pubDate>Wed, 23 Jan 2008 18:20:27 +0000</pubDate><guid>https://major.io/2008/01/23/qmail-this-message-is-looping-it-already-has-my-delivered-to-line/</guid><description>I stumbled upon this peculiar bounce message recently while working on a server:
Hi. This is the qmail-send program at yourmailserver.com. I'm afraid I wasn't able to deliver your message to the following addresses. This is a permanent error; I've given up. Sorry it didn't work out. &amp;lt;user1@domain.com&amp;gt;: This message is looping: it already has my Delivered-To line. (#5.4.6) --- Below this line is a copy of the message.&amp;lt;/p&amp;gt; Return-Path: &amp;lt;remoteuser@otherdomain.</description><content type="html"><![CDATA[<p>I stumbled upon this peculiar bounce message recently while working on a server:</p>
<pre><code>Hi. This is the qmail-send program at yourmailserver.com.
I'm afraid I wasn't able to deliver your message to the following addresses.
This is a permanent error; I've given up. Sorry it didn't work out.
&lt;user1@domain.com&gt;:
This message is looping: it already has my Delivered-To line. (#5.4.6)
--- Below this line is a copy of the message.&lt;/p&gt;
Return-Path: &lt;remoteuser@otherdomain.com&gt;
Received: (qmail 14418 invoked by uid 110); 9 Jan 2008 13:04:33 -0600
Delivered-To: 54-user2@domain.com
Received: (qmail 14411 invoked by uid 110); 9 Jan 2008 13:04:33 -0600
Delivered-To: 53-user1@domain.com
Received: (qmail 14404 invoked from network); 9 Jan 2008 13:04:33 -0600
Received: from otherdomain.com (HELO otherdomain.com) (11.22.33.44)
by yourmailserver.com with SMTP; 9 Jan 2008 13:04:33 -0600
</code></pre><p>Basically, this is qmail&rsquo;s way of letting you know that your e-mails are stuck in a mail loop. One e-mail user is redirecting to another e-mail user, and that e-mail user is redirecting back to the first one. If q-mail already has a delivered to line which matches one that it already added, it bounces the e-mail and halts delivery.</p>
]]></content></item><item><title>Dovecot: libsepol.so.1: failed to map segment from shared object: Cannot allocate memory</title><link>https://major.io/2008/01/22/dovecot-libsepolso1-failed-to-map-segment-from-shared-object-cannot-allocate-memory/</link><pubDate>Tue, 22 Jan 2008 18:47:21 +0000</pubDate><guid>https://major.io/2008/01/22/dovecot-libsepolso1-failed-to-map-segment-from-shared-object-cannot-allocate-memory/</guid><description>You may catch this error when you attempt to start dovecot on a Red Hat Enterprise Linux 5.1 system with the 64-bit architecture:
dovecot: imap-login: imap-login: error while loading shared libraries: libsepol.so.1: failed to map segment from shared object: Cannot allocate memory dovecot: pop3-login: pop3-login: error while loading shared libraries: libsepol.so.1: failed to map segment from shared object: Cannot allocate memory If you start dovecot, the main dovecot daemon will run with one auth child process, but there will be no POP/IMAP processes started.</description><content type="html"><![CDATA[<p>You may catch this error when you attempt to start dovecot on a Red Hat Enterprise Linux 5.1 system with the 64-bit architecture:</p>
<pre><code>dovecot: imap-login: imap-login: error while loading shared libraries: libsepol.so.1: failed to map segment from shared object: Cannot allocate memory
dovecot: pop3-login: pop3-login: error while loading shared libraries: libsepol.so.1: failed to map segment from shared object: Cannot allocate memory
</code></pre><p>If you start dovecot, the main dovecot daemon will run with one auth child process, but there will be no POP/IMAP processes started. To fix the issue, open the /etc/dovecot.conf and adjust the following directive:</p>
<pre><code>login_process_size = 64
</code></pre><p>Restart dovecot after making the change:</p>
<pre><code># /etc/init.d/dovecot restart
</code></pre><p>This was tested on RHEL 5.1 x86_64.</p>
]]></content></item><item><title>Removing news feeds in Horde</title><link>https://major.io/2008/01/21/removing-news-feeds-in-horde/</link><pubDate>Mon, 21 Jan 2008 18:36:49 +0000</pubDate><guid>https://major.io/2008/01/21/removing-news-feeds-in-horde/</guid><description>If you&amp;rsquo;ve used newer versions of Horde with Plesk, you have probably noticed the news feed that runs down the left side of the screen. Depending on the types of e-mails you receive, you may get some pretty odd news popping up on the screen.
Luckily, you can remove the news feeds pretty easily. Open the following file in your favorite text editor:
/usr/share/psa-horde/templates/portal/sidebar.inc
Once the file is open, drop down to line 102 and comment out the entire if() statement (lines 102-117).</description><content type="html"><![CDATA[<p>If you&rsquo;ve used newer versions of Horde with Plesk, you have probably noticed the news feed that runs down the left side of the screen. Depending on the types of e-mails you receive, you may get some pretty odd news popping up on the screen.</p>
<p>Luckily, you can remove the news feeds pretty easily. Open the following file in your favorite text editor:</p>
<p><code>/usr/share/psa-horde/templates/portal/sidebar.inc</code></p>
<p>Once the file is open, drop down to line 102 and comment out the entire if() statement (lines 102-117).</p>
<p><strong>NOTE:</strong> If you upgrade Plesk, this change will most likely be reversed.</p>
]]></content></item><item><title>MySQLTuner Revision 29 is now available</title><link>https://major.io/2008/01/15/mysqltuner-revision-29-is-now-available/</link><pubDate>Wed, 16 Jan 2008 04:13:12 +0000</pubDate><guid>https://major.io/2008/01/15/mysqltuner-revision-29-is-now-available/</guid><description>A new version of MySQLTuner was released tonight to correct some bugs found within revision 26. As usual, you can get a new copy from the MySQLTuner site.
Here are the new features:
» Storage engine counts are shown
In addition to the actual size of the tables stored under each storage engine, there is a table count as well. The table count reflects the number of tables that exist on the server which use the specified storage engine.</description><content type="html"><![CDATA[<p>A new version of MySQLTuner was released tonight to correct some bugs found within revision 26. As usual, you can <a href="http://mysqltuner.com/mysqltuner.pl">get a new copy</a> from the <a href="http://mysqltuner.com/">MySQLTuner site</a>.</p>
<p>Here are the new features:</p>
<p><strong>» Storage engine counts are shown</strong></p>
<p>In addition to the actual size of the tables stored under each storage engine, there is a table count as well. The table count reflects the number of tables that exist on the server which use the specified storage engine.</p>
<p><strong>» Minor changes</strong></p>
<ul>
<li>Reduced overall code size</li>
<li>Optimized subroutines to use fewer system calls and math computations</li>
<li>Added storage engine disabling recommendations to the bottom of the output</li>
</ul>
<p><strong>» Bugs fixed</strong></p>
<ul>
<li>Fixed an issue that caused incorrect statistics for storage engines with MySQL 3.23</li>
<li>Corrected a logic bug that displayed odd storage engine statistics calculations</li>
</ul>
]]></content></item><item><title>MySQL Replication: Wrap-up</title><link>https://major.io/2008/01/15/mysql-replication-wrap-up/</link><pubDate>Tue, 15 Jan 2008 18:20:02 +0000</pubDate><guid>https://major.io/2008/01/15/mysql-replication-wrap-up/</guid><description>After a couple of weeks, my MySQL replication series has come to a close. Here&amp;rsquo;s links to all of the topics that I covered:
Performance
Redundancy
Backups and Data Integrity
Horizontal Data Partitioning
Delayed Slaves
Breakdown
Replication Across an External Network
Upgrading the MySQL Server
Slave Performance
If there&amp;rsquo;s any other questions that you have, please let me know and I&amp;rsquo;ll be happy to add some extra posts on your topic!</description><content type="html"><![CDATA[<p>After a couple of weeks, my MySQL replication series has come to a close. Here&rsquo;s links to all of the topics that I covered:</p>
<p><a href="http://rackerhacker.com/2008/01/02/mysql-replication-performance/">Performance</a></p>
<p><a href="http://rackerhacker.com/2008/01/03/mysql-replication-redundancy/">Redundancy</a></p>
<p><a href="http://rackerhacker.com/2008/01/04/mysql-replication-backups-data-integrity/">Backups and Data Integrity</a></p>
<p><a href="http://rackerhacker.com/2008/01/07/mysql-replication-horizontal-data-partitioning/">Horizontal Data Partitioning</a></p>
<p><a href="http://rackerhacker.com/2008/01/08/mysql-replication-delayed-slaves/">Delayed Slaves</a></p>
<p><a href="http://rackerhacker.com/2008/01/09/mysql-replication-breakdown/">Breakdown</a></p>
<p><a href="http://rackerhacker.com/2008/01/10/mysql-replication-across-an-external-network/">Replication Across an External Network</a></p>
<p><a href="http://rackerhacker.com/2008/01/11/mysql-replication-upgrading-the-mysql-server/">Upgrading the MySQL Server</a></p>
<p><a href="http://rackerhacker.com/2008/01/14/mysql-replication-slave-performance/">Slave Performance</a></p>
<p>If there&rsquo;s any other questions that you have, please let me know and I&rsquo;ll be happy to add some extra posts on your topic!</p>
]]></content></item><item><title>MySQLTuner Revision 26 is now available</title><link>https://major.io/2008/01/15/mysqltuner-revision-26-is-now-available/</link><pubDate>Tue, 15 Jan 2008 14:00:22 +0000</pubDate><guid>https://major.io/2008/01/15/mysqltuner-revision-26-is-now-available/</guid><description>As some subversion users may have noticed, revision 23 of MySQLTuner was released quietly on Sunday. Thanks to Mike Jackson, a few bugs from revision 23 were smashed today and revision 26 was released this morning.
To pick up the new script, visit the MySQLTuner site.
Here&amp;rsquo;s some of the new features:
» Shows banner of enabled and disabled storage engines
Near the top of the MySQLTuner output, you&amp;rsquo;ll find a line like this:</description><content type="html"><![CDATA[<p>As some subversion users may have noticed, revision 23 of MySQLTuner was released quietly on Sunday. Thanks to <a href="http://barking-dog.net/">Mike Jackson</a>, a few bugs from revision 23 were smashed today and revision 26 was released this morning.</p>
<p>To pick up the new script, <a href="http://mysqltuner.com/">visit the MySQLTuner site</a>.</p>
<p>Here&rsquo;s some of the new features:</p>
<p><strong>» Shows banner of enabled and disabled storage engines</strong></p>
<p>Near the top of the MySQLTuner output, you&rsquo;ll find a line like this:</p>
<p>`<!-- raw HTML omitted --></p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>Of course, this is shown in color within the terminal, but all enabled storage engines are shown in green with a plus sign at the front. Disabled storage engines are shown in red and are prepended with a minus sign.</p>
<p><strong>» Recommends disabling unused storage engines</strong></p>
<p>If MySQLTuner finds storage engines enabled that are not in use, it will recommend that they are disabled to save resources:</p>
<p>`<!-- raw HTML omitted --></p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p><strong>» Calculates total data corresponding to storage engines</strong></p>
<p>Any storage engines in use are shown with the total amount of data stored using those engines:</p>
<p>`<!-- raw HTML omitted --></p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p><strong>» Displays exact counts in addition to percentages</strong></p>
<p>For some results where percents are shown, exact counts are being displayed as well:</p>
<p>`<!-- raw HTML omitted --></p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p><strong>» Initial InnoDB support</strong></p>
<p>As an initial step towards InnoDB support, the innodb_buffer_pool_size is compared to the total amount of InnoDB data stored on the server:</p>
<p>`<!-- raw HTML omitted --></p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p><strong>» Other minor changes</strong></p>
<ul>
<li>Added additional section headers to further organize the output</li>
<li>Merged the total buffers lines into one for more compact output</li>
<li>Added MySQL 5.1 to the supported list to prepare for upcoming GA release</li>
<li>For filesize amounts less than 1024 bytes, the &ldquo;B&rdquo; letter is shown to represent bytes</li>
<li>InnoDB log file recommendation removed due to bad implementations based on recommendations</li>
<li>Switches from spaced indents to tabbed indents</li>
</ul>
]]></content></item><item><title>MySQL Replication: Slave Performance</title><link>https://major.io/2008/01/14/mysql-replication-slave-performance/</link><pubDate>Mon, 14 Jan 2008 18:26:40 +0000</pubDate><guid>https://major.io/2008/01/14/mysql-replication-slave-performance/</guid><description>There&amp;rsquo;s a few final configuration options that may help the performance of your slave MySQL servers. If you&amp;rsquo;re not using certain storage engines, like InnoDB or Berkeley, then by all means, remove them from your configuration. For those two specifically, just add the following to your my.cnf on the slave server:
skip-innodb&amp;lt;br /&amp;gt; skip-bdb
To reduce disk I/O on big MyISAM write operations, you can delay the flushing of indexes to the disk:</description><content type="html"><![CDATA[<p>There&rsquo;s a few final configuration options that may help the performance of your slave MySQL servers. If you&rsquo;re not using certain storage engines, like InnoDB or Berkeley, then by all means, remove them from your configuration. For those two specifically, just add the following to your my.cnf on the slave server:</p>
<p><code>skip-innodb&lt;br /&gt; skip-bdb</code></p>
<p>To reduce disk I/O on big MyISAM write operations, you can <a href="http://dev.mysql.com/doc/refman/5.0/en/server-options.html#option_mysqld_delay-key-write">delay the flushing of indexes</a> to the disk:</p>
<p><code>delay_key_write = ALL</code></p>
<p>You can also make all of your <a href="http://dev.mysql.com/doc/refman/5.0/en/server-options.html#option_mysqld_low-priority-updates">write queries take a backseat</a> to any reads:</p>
<p><code>low-priority-updates</code></p>
<p>Keep in mind, however, that the last two options will increase slave performance, but it <a href="http://rackerhacker.com/2008/01/08/mysql-replication-delayed-slaves/">may cause them to lag behind the master</a>. Depending on your application, this may not be acceptable.</p>
]]></content></item><item><title>MySQL Replication: Upgrading the MySQL server</title><link>https://major.io/2008/01/11/mysql-replication-upgrading-the-mysql-server/</link><pubDate>Fri, 11 Jan 2008 23:44:54 +0000</pubDate><guid>https://major.io/2008/01/11/mysql-replication-upgrading-the-mysql-server/</guid><description>If you want to make a DBA nervous, just let them know that they need to upgrade MySQL servers that are replicating in a production environment. There&amp;rsquo;s multiple ways to get the job done, but here is the safest route:
First, make sure you have dumped all of your databases properly. Verify that your backups are correct and intact, and that you have multiple copies of them.
Next, upgrade the slave servers individually to the newest version.</description><content type="html"><![CDATA[<p>If you want to make a DBA nervous, just let them know that they need to upgrade MySQL servers that are replicating in a production environment. There&rsquo;s multiple ways to get the job done, but here is the safest route:</p>
<p><strong>First,</strong> make sure you have dumped all of your databases properly. Verify that your backups are correct and intact, and that you have multiple copies of them.</p>
<p><strong>Next,</strong> upgrade the slave servers individually to the newest version. After upgrading the first one, make sure the slave server is operating properly. If it is working properly, then you can continue to upgrade the other slaves.</p>
<p><strong>Once all of the slaves have been upgraded,</strong> then you can upgrade the master. If a busy web application is sending write queries to the master, you may want to put up a temporary page that tells visitors that maintenance is being performed. Once all of the writes clear out, stop the master and upgrade it.</p>
<p>After the master starts up, be sure that the slaves reconnect, and you might want to perform a test write query. Verify that the write is performed on the slaves as it was done on the master.</p>
]]></content></item><item><title>MySQL Replication: Across an external network</title><link>https://major.io/2008/01/10/mysql-replication-across-an-external-network/</link><pubDate>Thu, 10 Jan 2008 18:51:39 +0000</pubDate><guid>https://major.io/2008/01/10/mysql-replication-across-an-external-network/</guid><description>While many people might find replicating over an external network to be an odd concept, it does have some uses. For example, if you need to replicate data for local access at certain locations, it may be helpful. Also, if you have a dedicated server, you can replicate to your home to run backups.
First off, you&amp;rsquo;re going to need security for the connection. This is easily done with SSL. On the master, simply add the following lines to the [mysqld] section and restart the master:</description><content type="html"><![CDATA[<p>While many people might find replicating over an external network to be an odd concept, it does have some uses. For example, if you need to replicate data for local access at certain locations, it may be helpful. Also, if you have a dedicated server, you can replicate to your home to run backups.</p>
<p>First off, you&rsquo;re going to need security for the connection. This is easily done with SSL. On the master, simply add the following lines to the <code>[mysqld]</code> section and restart the master:</p>
<p><code>ssl-ca=cacert.pem&lt;br /&gt; ssl-cert=server-cert.pem&lt;br /&gt; ssl-key=server-key.pem</code></p>
<p>To have the slaves use SSL connections to the master server, simply add on <code>MASTER_SSL=1</code> to the <code>CHANGE MASTER</code> statement on the slave.</p>
<p>Another aspect to consider is bandwidth usage. This may be a priority if your remote areas have slow downlinks, or if you are charged for your bandwidth usage. You can compress the MySQL traffic very easily. Simply add the following to the MySQL configuration file in the <code>[mysqld]</code> section:</p>
<p><code>slave_compressed_protocol = 1</code></p>
<p><strong>With both of these changes,</strong> keep in mind that there is a significant CPU overhead required to compress and/or encrypt data. Determine carefully what your application requires and test your configuration thoroughly.</p>
]]></content></item><item><title>MySQL Replication: Breakdown</title><link>https://major.io/2008/01/09/mysql-replication-breakdown/</link><pubDate>Wed, 09 Jan 2008 18:24:03 +0000</pubDate><guid>https://major.io/2008/01/09/mysql-replication-breakdown/</guid><description>On some occasions, MySQL replication can break down if an statement comes from the master that makes no sense to the slave. For example, if an UPDATE statement arrives from the master server, but the table referenced by the UPDATE no longer exists, then the slave will halt replication and throw an error when SHOW SLAVE STATUS; is run.
The obvious question here is: how can the master and the slave have different data after replication has started?</description><content type="html"><![CDATA[<p>On some occasions, MySQL replication can break down if an statement comes from the master that makes no sense to the slave. For example, if an UPDATE statement arrives from the master server, but the table referenced by the UPDATE no longer exists, then the slave will halt replication and throw an error when <code>SHOW SLAVE STATUS;</code> is run.</p>
<p>The obvious question here is: how can the master and the slave have different data after replication has started? After all, you make a dump file prior to starting replication, so both servers contain the same information. Stray updates can be thrown into the mix from application errors or plain user errors. These kinds of things happen, even though we all try to avoid it.</p>
<p>Don&rsquo;t worry - this is almost always an easy fix. You have two main options:</p>
<p><strong>Fix the problem yourself.</strong> If the master sent a query that the slave can&rsquo;t run, fix it manually. For example, if the master wants to run an INSERT on a table that doesn&rsquo;t exist, run a quick <code>SHOW CREATE TABLE;</code> on the master and create the table manually on the slave. When the table is there, run a <code>START SLAVE;</code> on the slave and you should be all set.</p>
<p><strong>Skip an unnecessary query.</strong> Let&rsquo;s say that the master sent over a <code>DROP TABLE</code> query but the table doesn&rsquo;t exist on the master. It&rsquo;s safe to say that the master won&rsquo;t be sending any write queries to that table in the future, so the query can be skipped. To skip it, run the following statement:</p>
<p><code>mysql&gt; SET GLOBAL SQL_SLAVE_SKIP_COUNTER = 1;&lt;br /&gt; mysql&gt; START SLAVE;</code></p>
<p>In short, you&rsquo;re telling MySQL to skip that unnecessary query and keep going with the ones after that. Of course, if you need to skip multiple queries, change the 1 to whatever number you need and then run <code>START SLAVE;</code>.</p>
]]></content></item><item><title>MySQL Replication: Delayed Slaves</title><link>https://major.io/2008/01/08/mysql-replication-delayed-slaves/</link><pubDate>Wed, 09 Jan 2008 03:11:11 +0000</pubDate><guid>https://major.io/2008/01/08/mysql-replication-delayed-slaves/</guid><description>In a perfect world, slaves will contain the same data as the master at all times. The events should be picked up and executed by the slaves in milliseconds. However, in real world scenarios, replication will be held up for different reasons. Whether it&amp;rsquo;s table locks, disk I/O, network saturation, or CPU usage, slaves might become several seconds, minutes or even hours behind the master.
In some situations, delays of less than 30 seconds may not be a big issue.</description><content type="html"><![CDATA[<p>In a perfect world, slaves will contain the same data as the master at all times. The events should be picked up and executed by the slaves in milliseconds. However, in real world scenarios, replication will be held up for different reasons. Whether it&rsquo;s table locks, disk I/O, network saturation, or CPU usage, slaves might become several seconds, minutes or even hours behind the master.</p>
<p>In some situations, delays of less than 30 seconds may not be a big issue. Some applications, like social networking applications, would need to have the data match at all times. Lags would not be acceptable.</p>
<p>For example, review this scenario. Let&rsquo;s say you go to a site and create an account. That would send a write query to the master. Once you&rsquo;ve finished the account creation, the page will depend on a read query. If the slave is behind the master, it won&rsquo;t have any data about your new account, and the application will probably tell you that you don&rsquo;t have an account. That would be pretty annoying for your application&rsquo;s users.</p>
<p>To check your current lag, simply run <a href="http://dev.mysql.com/doc/refman/5.0/en/show-slave-status.html"><code>SHOW SLAVE STATUS;</code></a> in MySQL, and review the number following <code>Seconds_Behind_Master</code>. If everything is running well, it should be followed by 0. If <code>NULL</code> is shown, then there is most likely an issue with replication, and you might want to check <code>Last_Error</code>.</p>
<p>So, how can replication lags be corrected? Try these methods:</p>
<p><strong>Review your queries.</strong> When queries keep running in MySQL, the slave may be unable to keep up. Make sure that your read queries are as <a href="http://dev.mysql.com/doc/refman/5.0/en/explain.html">optimized as possible</a> so they complete quickly.</p>
<p><strong>Optimize your MySQL server variables.</strong> Be sure to <a href="http://rackerhacker.com/mysqltuner/">thoroughly review your MySQL configuration</a> for any bottlenecks.</p>
<p><strong>Choose the right storage engines.</strong> If you&rsquo;re making a lot of updates to a table, <a href="http://rackerhacker.com/2007/11/06/when-to-use-myisam-or-innodb/">consider using InnoDB</a>. If your tables are not updated often, c<a href="http://rackerhacker.com/2007/11/06/when-to-use-myisam-or-innodb/">onsider using MyISAM tables</a> (or even <a href="http://dev.mysql.com/doc/refman/5.0/en/myisampack.html">compressed MyISAM tables</a>).</p>
<p><strong>Upgrade your hardware.</strong> Find your hardware bottleneck. If it&rsquo;s the CPU, consider upgrading to a multi-core CPU, or a CPU with a higher clock speed. For I/O bottlenecks, consider a RAID solution with SAS drives. If you&rsquo;re lucky enough to have a network bottleneck (lucky since it means you&rsquo;re doing well with CPU and I/O), use a dedicated switch or upgrade to gigabit (or faster) hardware.</p>
]]></content></item><item><title>MySQLTuner is now a Debian package</title><link>https://major.io/2008/01/08/mysqltuner-is-now-a-debian-package/</link><pubDate>Tue, 08 Jan 2008 16:37:32 +0000</pubDate><guid>https://major.io/2008/01/08/mysqltuner-is-now-a-debian-package/</guid><description>I just received a note yesterday that MySQLTuner was added to Debian&amp;rsquo;s package list for the sid release. I&amp;rsquo;d like to thank Marcela Tiznado for putting the package together and submitting it!
You can read more about the package.</description><content type="html"><![CDATA[<p>I just received a note yesterday that <a href="http://packages.debian.org/sid/main/mysqltuner">MySQLTuner</a> was added to Debian&rsquo;s <a href="http://packages.debian.org/sid/main/newpkg">package list</a> for the <em><a href="http://www.debian.org/releases/unstable/">sid</a></em> release. I&rsquo;d like to thank Marcela Tiznado for putting the package together and submitting it!</p>
<p>You can <a href="http://packages.debian.org/sid/main/mysqltuner">read more about the package</a>.</p>
]]></content></item><item><title>MySQL Replication: Horizontal Data Partitioning</title><link>https://major.io/2008/01/07/mysql-replication-horizontal-data-partitioning/</link><pubDate>Mon, 07 Jan 2008 17:57:56 +0000</pubDate><guid>https://major.io/2008/01/07/mysql-replication-horizontal-data-partitioning/</guid><description>If you have a master with multiple slaves, you can get some performance and save money on hardware by splitting data horizontally among your servers. For example, if you have one high traffic database and two lower traffic databases, you can selectively split them among the slaves. With five slaves, set up three of the slaves to replicate your high traffic database, and the two other slaves can handle one each out of the two low traffic databases.</description><content type="html"><![CDATA[<p>If you have a master with multiple slaves, you can get some performance and save money on hardware by splitting data horizontally among your servers. For example, if you have one high traffic database and two lower traffic databases, you can selectively split them among the slaves. With five slaves, set up three of the slaves to replicate your high traffic database, and the two other slaves can handle one each out of the two low traffic databases.</p>
<p>This allows you to expand when you&rsquo;re ready, and you can move your databases around to take advantage of idle servers. MySQL AB already has <a href="http://dev.mysql.com/doc/refman/5.0/en/replication-solutions-partitioning.html">documentation on how to make this possible</a>.</p>
]]></content></item><item><title>MySQL Replication: Backups &amp; Data Integrity</title><link>https://major.io/2008/01/04/mysql-replication-backups-data-integrity/</link><pubDate>Fri, 04 Jan 2008 19:39:12 +0000</pubDate><guid>https://major.io/2008/01/04/mysql-replication-backups-data-integrity/</guid><description>An often overlooked benefit of MySQL replication is the ability to make reliable backups without affecting the integrity of the MySQL data.
With one MySQL server, backups have a huge impact on the server. If file-based backups are performed, you have to stop MySQL completely while the files are copied (unless you purchase expensive utilities that accomplish this while MySQL is running). If dumps are made with mysqldump, table locking and I/O operations will crush the performance of the server.</description><content type="html"><![CDATA[<p>An often overlooked benefit of MySQL replication is the ability to make reliable backups without affecting the integrity of the MySQL data.</p>
<p>With one MySQL server, backups have a huge impact on the server. If file-based backups are performed, you have to stop MySQL completely while the files are copied (unless you purchase expensive utilities that accomplish this while MySQL is running). If dumps are made with mysqldump, table locking and I/O operations will crush the performance of the server.</p>
<p>You can get around these performance hits by running dumps in single transaction mode, or by restricting mysqldump to locking one table at a time. The performance gain comes at a price, however, as your backups are not a perfect snapshot. After one table is locked for a period of table, previously locked tables are actively changing and some tables might not match up.</p>
<p>By having a slave available, you can perform a snapshot backup and lock all of the tables during the process. This provides an exact point-in-time backup with a very low effect on your MySQL servers' performance.</p>
]]></content></item><item><title>MySQL Replication: Redundancy</title><link>https://major.io/2008/01/03/mysql-replication-redundancy/</link><pubDate>Fri, 04 Jan 2008 02:39:11 +0000</pubDate><guid>https://major.io/2008/01/03/mysql-replication-redundancy/</guid><description>Although performance is a much larger benefit of replication, it provides some redundancy for your application as well. Adding a slave server to a master allows you to perform read operations on either server, but you&amp;rsquo;re still bound to the master server for writes. In a group of multiple slaves with one master, you have your data available and online in multiple locations, which means that certain servers can fall out of replication without a large disaster.</description><content type="html"><![CDATA[<p>Although performance is a much larger benefit of replication, it provides some redundancy for your application as well. Adding a slave server to a master allows you to perform read operations on either server, but you&rsquo;re still bound to the master server for writes. In a group of multiple slaves with one master, you have your data available and online in multiple locations, which means that certain servers can fall out of replication without a large disaster.</p>
<p>When disaster does occur, use the following recommendations as a guide.</p>
<p><strong>If the master fails in a two-server replication environment,</strong> you will be dead in the water with regards to write queries. You will need to convert the slave into a master. This can be done relatively quickly by following these steps:</p>
<ol>
<li>Log into MySQL on the slave and run <code>STOP SLAVE; RESET SLAVE;</code></li>
<li>Add <code>log-bin</code> to the slave&rsquo;s /etc/my.cnf file and restart MySQL</li>
<li>The slave server will now be running as a master</li>
<li>Adjust your application to send reads and writes to the slave</li>
</ol>
<p>Once the original master comes back online, <a href="http://rackerhacker.com/2007/12/31/seven-step-mysql-replication/">set it up just like a new slave</a>. You can skip some steps, such as setting the server-id, since that still should correspond to your overall configuration.</p>
<p><strong>If the master fails in a multiple-server replication environment,</strong> you&rsquo;re still in bad shape for writes. Follow the steps shown above, and then adjust the other slaves (with <code>CHANGE MASTER</code>) so that they pull events from the new master instead.</p>
<p><strong>If a slave fails in any replication environment,</strong> adjust your application so that it no longer attempts to send reads to the failed slave. While you work to bring the failed slave back online, your queries will be distributed to the remaining servers.</p>
<p>You can automate many of these operations by using applications like <a href="http://www.linux-ha.org/">heartbeat</a>, or by using load balancers to automatically route database traffic.</p>
]]></content></item><item><title>MySQL Replication: Performance</title><link>https://major.io/2008/01/02/mysql-replication-performance/</link><pubDate>Wed, 02 Jan 2008 18:20:05 +0000</pubDate><guid>https://major.io/2008/01/02/mysql-replication-performance/</guid><description>MySQL replication can increase performance by allowing developers to spread queries over two servers. Queries that write data must be sent to the master at all times, but queries that read data can be sent to either server. This means that by adding a slave server to a database environment allows you to effectively double your read query performance.
However, there are some large caveats to consider here. The actual web site code itself will need to be written in such a way that read and write queries can be diverted to different destinations.</description><content type="html"><![CDATA[<p>MySQL replication can increase performance by allowing developers to spread queries over two servers. Queries that write data must be sent to the master at all times, but queries that read data can be sent to either server. This means that by adding a slave server to a database environment allows you to effectively double your read query performance.</p>
<p><strong>However, there are some large caveats to consider here.</strong> The actual web site code itself will need to be written in such a way that read and write queries can be diverted to different destinations. Depending on the size of the application and how it has been developed, the work requires to provide this functionality may be prohibitive for replication.</p>
<p>Some load balancers can balance MySQL query traffic, and this can help if the code cannot balance the load internally. Open source applications like MySQL Proxy and pound can be used as well.</p>
<p>Also, if the queries are not optimized, and the correct storage engines are not used, replication will not work well. If queries take an extended time to execute, the performance gains will be almost non existent. Also, if the wrong storage engines are used, and much of the rows or tables are locked, performance gains will be greatly limited. Some situations may actually cause replication to halt due to locking. When this occurs, the data on the slave becomes stale and SELECTs run against the master and slave will return different results.</p>
<p>In short:</p>
<ul>
<li>Replication can increase read performance</li>
<li>It cannot fix issues caused by bad queries/storage engines</li>
<li>Write queries can only be sent to the master</li>
</ul>
]]></content></item><item><title>MySQL Replication: A series of posts</title><link>https://major.io/2008/01/01/244/</link><pubDate>Tue, 01 Jan 2008 23:05:41 +0000</pubDate><guid>https://major.io/2008/01/01/244/</guid><description>One of the topics I receive the most questions about is MySQL replication. In a series of posts, I&amp;rsquo;ll explore these questions in-depth each day. If you have any extra questions that I don&amp;rsquo;t happen to answer, throw me a comment and I&amp;rsquo;ll be happy to review it for you.
The topics will cover MySQL 4.1 and 5.0, but I&amp;rsquo;ll leave out MySQL 5.1&amp;rsquo;s new replication abilities since it has not reached a General Availability status quite yet.</description><content type="html"><![CDATA[<p>One of the topics I receive the most questions about is MySQL replication. In a series of posts, I&rsquo;ll explore these questions in-depth each day. If you have any extra questions that I don&rsquo;t happen to answer, throw me a comment and I&rsquo;ll be happy to review it for you.</p>
<p>The topics will cover MySQL 4.1 and 5.0, but I&rsquo;ll leave out <a href="http://dev.mysql.com/doc/refman/5.1/en/replication-formats.html">MySQL 5.1&rsquo;s new replication abilities</a> since it has not reached a General Availability status quite yet.</p>
]]></content></item><item><title>Have a great 2008</title><link>https://major.io/2008/01/01/have-a-great-2008/</link><pubDate>Tue, 01 Jan 2008 18:00:41 +0000</pubDate><guid>https://major.io/2008/01/01/have-a-great-2008/</guid><description>I&amp;rsquo;d like to wish everyone a Happy New Year and I hope it is a great 2008 for you all!</description><content type="html">&lt;p>I&amp;rsquo;d like to wish everyone a Happy New Year and I hope it is a great 2008 for you all!&lt;/p></content></item><item><title>Seven Step MySQL Replication</title><link>https://major.io/2007/12/31/seven-step-mysql-replication/</link><pubDate>Mon, 31 Dec 2007 17:51:03 +0000</pubDate><guid>https://major.io/2007/12/31/seven-step-mysql-replication/</guid><description>MySQL replication may sound complicated, but it can be done easily. Here&amp;rsquo;s a quick 7-step guide:
Create a replication user on the master: GRANT REPLICATION SLAVE ON *.* TO 'repl'@'%' IDENTIFIED BY 'password'; On the master server, add the following to the [mysqld] section in my.cnf and restart MySQL: server-id = 1 relay_log=mysqldrelay log-bin expire_logs_days = 7 On the slave server, add the following to the [mysqld] sesion in my.</description><content type="html"><![CDATA[<p>MySQL replication may sound complicated, but it can be done easily. Here&rsquo;s a quick 7-step guide:</p>
<ol>
<li>Create a replication user on the master:</li>
</ol>
<pre><code> GRANT REPLICATION SLAVE ON *.* TO 'repl'@'%' IDENTIFIED BY 'password';
</code></pre><ol start="2">
<li>On the master server, add the following to the <code>[mysqld]</code> section in my.cnf and restart MySQL:</li>
</ol>
<pre><code>server-id = 1
relay_log=mysqldrelay
log-bin
expire_logs_days = 7
</code></pre><ol start="3">
<li>On the slave server, add the following to the <code>[mysqld]</code> sesion in my.cnf and restart MySQL:</li>
</ol>
<pre><code>server-id = 2
</code></pre><ol start="4">
<li>Create a mysqldump file on the master server which includes a global lock:</li>
</ol>
<pre><code> databases.sql
</code></pre><ol start="5">
<li>Configure the slave:</li>
</ol>
<pre><code># mysql -u user -ppassword
mysql&gt; CHANGE MASTER TO MASTER_HOST='master host name', MASTER_USER='repl', MASTER_PASSWORD='repl';
</code></pre><ol start="6">
<li>Move the dump to the slave server and import it:</li>
</ol>
<pre><code>mysql -u user -ppassword &lt; databases.sql
</code></pre><ol start="7">
<li>Start the slave:</li>
</ol>
<pre><code>mysql -u user -ppassword
mysql&gt; START SLAVE;
</code></pre>]]></content></item><item><title>Taking a little time off</title><link>https://major.io/2007/12/18/taking-a-little-time-off/</link><pubDate>Tue, 18 Dec 2007 18:02:42 +0000</pubDate><guid>https://major.io/2007/12/18/taking-a-little-time-off/</guid><description>I&amp;rsquo;m taking a little bit of time off for Christmas, but I&amp;rsquo;ll be adding new posts on December 31st. I wish all of you a happy holiday season, and if you celebrate Christmas, a Merry Christmas as well!</description><content type="html"><![CDATA[<p>I&rsquo;m taking a little bit of time off for Christmas, but I&rsquo;ll be adding new posts on December 31st. I wish all of you a happy holiday season, and if you celebrate Christmas, a Merry Christmas as well!</p>
]]></content></item><item><title>Why you should use caching for WordPress blogs</title><link>https://major.io/2007/12/06/why-you-should-use-caching-for-wordpress-blogs/</link><pubDate>Thu, 06 Dec 2007 18:09:42 +0000</pubDate><guid>https://major.io/2007/12/06/why-you-should-use-caching-for-wordpress-blogs/</guid><description>I had some time to do some testing of my blog&amp;rsquo;s performance today, and I discovered how much of a difference the WP-Cache plugin makes.
This blog runs on a server with dual Xeon Woodcrest CPU&amp;rsquo;s, 64-bit CentOS 4.5 and a 100mbit network connection. Here&amp;rsquo;s the first test with WP-Cache turned off:
$ http_load -parallel 10 -seconds 30 urltocheck.txt &amp;lt;strong&amp;gt;346&amp;lt;/strong&amp;gt; fetches, 10 max parallel, 1.78616e+07 bytes, in 30 seconds 51623.2 mean bytes/connection 11.</description><content type="html"><![CDATA[<p>I had some time to do some testing of my blog&rsquo;s performance today, and I discovered how much of a difference the <a href="http://mnm.uib.es/gallir/wp-cache-2/">WP-Cache</a> plugin makes.</p>
<p>This blog runs on a server with dual Xeon Woodcrest CPU&rsquo;s, 64-bit CentOS 4.5 and a 100mbit network connection. Here&rsquo;s the first test with WP-Cache turned off:</p>
<pre><code>$ http_load -parallel 10 -seconds 30 urltocheck.txt
&lt;strong&gt;346&lt;/strong&gt; fetches, 10 max parallel, 1.78616e+07 bytes, in 30 seconds
51623.2 mean bytes/connection
11.5333 fetches/sec, 595387 bytes/sec
msecs/connect: 15.1661 mean, 16.97 max, 14.922 min
msecs/first-response: 445.984 mean, 2328.82 max, 189.62 min
HTTP response codes:
  code 200 -- 346
</code></pre><p>346 fetches in 30 seconds is not a very exciting performance number for me. That&rsquo;s just over 10 fetches per second, and on a busy day, I sometimes reach that number. Also, while this test ran, the server&rsquo;s CPU usage was extremely high and over 80% of all four cores were in use. The iowait was about 20% across the board.</p>
<p>I decided to turn on WP-Cache and give it another go with the same test:</p>
<pre><code>$ http_load -parallel 10 -seconds 30 urltocheck.txt
&lt;strong&gt;3482&lt;/strong&gt; fetches, 10 max parallel, 1.79671e+08 bytes, in 30 seconds
51600 mean bytes/connection
116.067 fetches/sec, 5.98904e+06 bytes/sec
msecs/connect: 15.2259 mean, 18.257 max, 14.891 min
msecs/first-response: 20.7297 mean, 69.39 max, 18.861 min
HTTP response codes:
  code 200 -- 3482
</code></pre><p>Wow, that&rsquo;s a 10-fold improvement, and I can handle over 100 requests per second with 10 parallel requests. Also, the iowait dropped to 5%, and overall CPU usage remained under 8%.</p>
<p>I kicked it up to 20 parallel connections and tried again:</p>
<pre><code>$ http_load -parallel 20 -seconds 30 urltocheck.txt
&lt;strong&gt;5817&lt;/strong&gt; fetches, 20 max parallel, 3.02176e+08 bytes, in 30 seconds
51947 mean bytes/connection
193.9 fetches/sec, 1.00725e+07 bytes/sec
msecs/connect: 17.9175 mean, 30.831 max, 14.911 min
msecs/first-response: 24.5352 mean, 97.475 max, 18.978 min
HTTP response codes:
  code 200 -- 5817
</code></pre><p>Almost 194 connections served per second! Also, the CPU usage was only at about 14% during the duration of the test.</p>
<p>I decided to tempt fate and see if I could blow the roof off the test with 50 parallel connections:</p>
<pre><code>$ http_load -parallel 50 -seconds 30 urltocheck.txt
&lt;strong&gt;5794&lt;/strong&gt; fetches, 50 max parallel, 2.99718e+08 bytes, in 30 seconds
51729 mean bytes/connection
193.133 fetches/sec, 9.99059e+06 bytes/sec
msecs/connect: 43.286 mean, 63.878 max, 14.942 min
msecs/first-response: 68.967 mean, 202.854 max, 20.014 min
HTTP response codes:
  code 200 -- 5794
</code></pre><p>The performance suffered a bit, but the server was still pumping out almost 200 connections per second, and I&rsquo;m okay with that. Well, unless anyone has a spare Cisco 11501 laying around that I could have. :-) And, of course, one additional server.</p>
<p>Just as a sidenote, I installed <a href="http://www.zend.com/en/products/guard/optimizer/">Zend Optimizer v3.3</a> on the server and performance actually dropped by 1%-3% for each test. I found that a bit surprising.</p>
<p><em>I used <a href="http://www.acme.com/software/http_load/">http_load</a> to perform the benchmarks after I found it on <a href="http://calebgroom.com/archives/185">Caleb&rsquo;s blog</a>.</em></p>
]]></content></item><item><title>Red Hat Support EOL Dates</title><link>https://major.io/2007/12/05/red-hat-support-eol-dates/</link><pubDate>Wed, 05 Dec 2007 18:27:03 +0000</pubDate><guid>https://major.io/2007/12/05/red-hat-support-eol-dates/</guid><description>I spoke to a customer recently who was concerned about their Red Hat Enterprise Linux 2.1 server and its Red Hat support status. After some digging, I found these items on Red Hat&amp;rsquo;s security site:
Red Hat Enterprise Linux (version 5)
End of Full Support: Mar 31, 2010
End of Maintenance Support: Mar 31, 2014
Red Hat Enterprise Linux (version 4)
End of Full Support: May 15, 2008
End of Maintenance Phase: Feb 29, 2012</description><content type="html"><![CDATA[<p>I spoke to a customer recently who was concerned about their Red Hat Enterprise Linux 2.1 server and its Red Hat support status. After some digging, I found these items on Red Hat&rsquo;s security site:</p>
<p><strong>Red Hat Enterprise Linux (version 5)</strong></p>
<p>End of Full Support: Mar 31, 2010</p>
<p>End of Maintenance Support: Mar 31, 2014</p>
<p><strong>Red Hat Enterprise Linux (version 4)</strong></p>
<p>End of Full Support: May 15, 2008</p>
<p>End of Maintenance Phase: Feb 29, 2012</p>
<p><strong>Red Hat Enterprise Linux (version 3)</strong></p>
<p>Full Support (including hardware updates): April 30, 2006</p>
<p>Maintenance Support: Oct 31, 2010</p>
<p><strong>Red Hat Enterprise Linux (version 2.1)</strong></p>
<p>Full Support (including hardware updates): Nov 30, 2004</p>
<p>Maintenance Support: May 31, 2009</p>
<p>Here&rsquo;s the difference between the levels of support:</p>
<blockquote>
<p><strong>Full Support</strong></p>
</blockquote>
<blockquote>
<p>Start Date: General Availability</p>
</blockquote>
<blockquote>
<p>End Date: 3 Years from General Availability date</p>
</blockquote>
<blockquote>
<p>Description: During the Full Support phase, new hardware support will be provided at the discretion of Red Hat via Updates, Additionally, all available and qualified errata will be applied to the Enterprise products via Updates (or as required for Security level errata.) And finally, updated ISO images will only be provided during Phase 1: Full Support.</p>
<p><strong>Maintenance</strong></p>
</blockquote>
<blockquote>
<p>Start Date: 3.5 years from General Availability (end of Deployment)</p>
</blockquote>
<blockquote>
<p>End Date: 7 years from General Availability</p>
</blockquote>
<blockquote>
<p>Description: During the Maintenance phase, only Security errata and select mission critical bug fixes will be released for the Enterprise products.</p>
</blockquote>
<p><em>Data was gathered from <a href="http://www.redhat.com/security/updates/errata/">http://www.redhat.com/security/updates/errata/</a></em></p>
]]></content></item><item><title>Plesk and qmail: 421 temporary envelope failure (#4.3.0)</title><link>https://major.io/2007/12/04/plesk-and-qmail-421-temporary-envelope-failure-430/</link><pubDate>Tue, 04 Dec 2007 18:21:23 +0000</pubDate><guid>https://major.io/2007/12/04/plesk-and-qmail-421-temporary-envelope-failure-430/</guid><description>I stumbled upon a server running Plesk 8.2.1 where a certain user could not receive e-mail. I sent an e-mail to the user from my mail client, and I never saw it enter the user&amp;rsquo;s mailbox. It didn&amp;rsquo;t even appear in the logs.
After checking the usual suspects, like MX records, mail account configuration, and firewalls, I was unable to find out why it was occurring. Even after a run of mchk, the emails would not be delivered.</description><content type="html"><![CDATA[<p>I stumbled upon a server running Plesk 8.2.1 where a certain user could not receive e-mail. I sent an e-mail to the user from my mail client, and I never saw it enter the user&rsquo;s mailbox. It didn&rsquo;t even appear in the logs.</p>
<p>After checking the usual suspects, like MX records, mail account configuration, and firewalls, I was unable to find out why it was occurring. Even after a run of <code>mchk</code>, the emails would not be delivered.</p>
<p>I began testing with a telnet connection to the SMTP port:</p>
<pre><code>$ telnet 11.22.33.44 25
Trying 11.22.33.44...
Connected to 11.22.33.44.
Escape character is '^]'.
220 www.yourserver.com ESMTP
HELO domain.com
250 www.yourserver.com
MAIL FROM: test@test.com
250 ok
RCPT TO: someuser@somedomain.com
421 temporary envelope failure (#4.3.0)
QUIT
221 www.yourserver.com
Connection closed by foreign host.
</code></pre><p>Temporary envelope failure? I was still confused. After reviewing the logs, I found the following line whenever I tried to telnet to port 25 and send an e-mail:</p>
<pre><code>Dec  2 00:15:49 www relaylock: /var/qmail/bin/relaylock: mail from 44.33.22.11:17249 (yourdesktop.com)
</code></pre><p>It turns out that the customer was using greylisting in qmail with qmail-envelope-scanner. After a quick check of /tmp/greylist_dbg.txt, I found the entries from me (as well as a lot of other senders), and that ended up being the root of the problem.</p>
]]></content></item><item><title>MySQLTuner Revision 22 is now available</title><link>https://major.io/2007/12/03/mysqltuner-revision-22-is-now-available/</link><pubDate>Mon, 03 Dec 2007 18:09:32 +0000</pubDate><guid>https://major.io/2007/12/03/mysqltuner-revision-22-is-now-available/</guid><description>MySQLTuner revision 22 is available today. Here&amp;rsquo;s some of the notable fixes and changes:
» Changed how indexes are calculated on MySQL 5
Thanks to Jon Hinds, I found that when running the tuning script against MySQL 5, the following SQL statement caused MySQL to open all of the tables on the server, which of course caused the table cache hit rate to plummet each time the script is run:</description><content type="html"><![CDATA[<p>MySQLTuner revision 22 is available today. Here&rsquo;s some of the notable fixes and changes:</p>
<p><strong>» Changed how indexes are calculated on MySQL 5</strong></p>
<p>Thanks to Jon Hinds, I found that when running the tuning script against MySQL 5, the following SQL statement caused MySQL to open <strong>all</strong> of the tables on the server, which of course caused the table cache hit rate to plummet each time the script is run:</p>
<p><code>SELECT SUM(INDEX_LENGTH) from information_schema.TABLES where ENGINE='MyISAM'</code></p>
<p>The script now calculates index size using <code>du</code> operations for all MySQL versions.</p>
<p><strong>» Added checks for innodb_log_file_size</strong></p>
<p>I&rsquo;m working in some InnoDB support, and the script now checks to see whether the innodb_log_file_size is 25% (+/- 5%) of your innodb_buffer_pool_size.</p>
<p><strong>» Added checks for 32-bit and 64-bit architectures</strong></p>
<p>The script now determines if you have a 32-bit system with less than 2GB of RAM. You&rsquo;ll get a polite suggestion to move to a 64-bit OS so that MySQL can allocate more than 2GB of RAM safely. Also, if your maximum possible memory usage is over 2GB on a 32-bit system, you&rsquo;ll get a warning about stability issues. (Allocating more than 2GB on a 32-bit system can cause thread thrashing and a system crash.)</p>
<p><strong>» Fixed a bug in the recommendations for temporary tables</strong></p>
<p>I had a pretty ugly math error, and it&rsquo;s fixed now. You will see recommendations for increasing the size of the max_heap_table and tmp_table_size buffers as long as they are not at 256MB already.</p>
<p><strong>» Fixed thread cache recommendations and warnings</strong></p>
<p>If your thread cache is set to 0, you now receive a warning about a disabled thread cache. Also, a separate recommendation is made in that situation. If your thread cache is set too low, but still enabled, a separate recommendation will appear.</p>
<p><strong>» Internal changes</strong></p>
<p>Some of the commented lines are switched around a bit, and some of the arrays have been consolidated to speed up the script a bit more.</p>
<p><strong>Ready to download the script?</strong> Go to <a href="http://mysqltuner.com">http://mysqltuner.com/</a> and get it for free.</p>
]]></content></item><item><title>MySQLTuner is now a Mandriva Linux package!</title><link>https://major.io/2007/11/30/mysqltuner-is-now-a-mandriva-linux-package/</link><pubDate>Fri, 30 Nov 2007 18:49:32 +0000</pubDate><guid>https://major.io/2007/11/30/mysqltuner-is-now-a-mandriva-linux-package/</guid><description>Thanks to Oden Eriksson at Mandriva, MySQLTuner is now packaged into an RPM for easier installations on Mandriva Linux.
More data can be found in the Mandriva Linux Archives.
As an aside, I find it quite interesting that the first package for my script popped up on Mandriva. My first adventure into Linux was with Mandrake many years ago.</description><content type="html"><![CDATA[<p>Thanks to Oden Eriksson at Mandriva, MySQLTuner is now <a href="http://sophie.zarb.org/viewrpm/41102afc4b469a3d92abb9a147fc9271">packaged into an RPM</a> for easier installations on Mandriva Linux.</p>
<p>More data can be found in the <a href="http://archives.mandrivalinux.com/changelog/2007-11/msg00753.php">Mandriva Linux Archives</a>.</p>
<p><em>As an aside, I find it quite interesting that the first package for my script popped up on Mandriva. My first adventure into Linux was with Mandrake many years ago.</em></p>
]]></content></item><item><title>Table ‘mysql.proc’ doesn’t exist</title><link>https://major.io/2007/11/29/table-mysqlproc-doesnt-exist/</link><pubDate>Thu, 29 Nov 2007 18:37:55 +0000</pubDate><guid>https://major.io/2007/11/29/table-mysqlproc-doesnt-exist/</guid><description>After I was asked to create a stored procedure on a MySQL 5.0.45 installation last week, I received the following error:
ERROR 1146 at line 24: Table 'mysql.proc' doesn't exist
The server had the default MySQL 4.1.20 that comes with Red Hat Enterprise Linux 4, and it was upgraded to MySQL 5.0.45. After the upgrade, the mysql_upgrade script wasn&amp;rsquo;t run, so the privilege tables were wrong, and the special tables for procedures and triggers did not exist.</description><content type="html"><![CDATA[<p>After I was asked to create a stored procedure on a MySQL 5.0.45 installation last week, I received the following error:</p>
<p><code>ERROR 1146 at line 24: Table 'mysql.proc' doesn't exist</code></p>
<p>The server had the default MySQL 4.1.20 that comes with Red Hat Enterprise Linux 4, and it was upgraded to MySQL 5.0.45. After the upgrade, the <code>mysql_upgrade</code> script wasn&rsquo;t run, so the privilege tables were wrong, and the special tables for procedures and triggers did not exist.</p>
<p>To fix the problem, I ran:</p>
<p><code># /usr/bin/mysql_upgrade</code></p>
<p>After about 20 seconds, the script completed and I was able to add a stored procedure without a problem.</p>
]]></content></item><item><title>Fixing Horde problems in Plesk 8.1.x/8.2.x with PHP 5.2.5</title><link>https://major.io/2007/11/28/fixing-horde-problems-in-plesk-81x82x-with-php-525/</link><pubDate>Wed, 28 Nov 2007 18:33:37 +0000</pubDate><guid>https://major.io/2007/11/28/fixing-horde-problems-in-plesk-81x82x-with-php-525/</guid><description>There&amp;rsquo;s a few issues with PHP 5.2.5 and the version of Horde that is bundled with Plesk 8.1.x and 8.2.x. The PHP include paths that appear in the Apache configuration generated by Plesk conflict with the PHP installation, and that causes the Horde webmail interface to segmentation fault.
To fix the problem, create a file called /etc/httpd/conf.d/zz050a_horde_php_workaround.conf and put the following inside it:
&amp;lt;DirectoryMatch /usr/share/psa-horde&amp;gt; php_admin_value include_path &amp;#34;/usr/share/psa-horde/lib:/usr/share/psa-horde:/usr/share/psa-horde/pear:.&amp;#34; &amp;lt;/DirectoryMatch&amp;gt; Reload the Apache configuration and your Horde installation should work properly with PHP 5.</description><content type="html"><![CDATA[<p>There&rsquo;s a few issues with PHP 5.2.5 and the version of Horde that is bundled with Plesk 8.1.x and 8.2.x. The PHP include paths that appear in the Apache configuration generated by Plesk conflict with the PHP installation, and that causes the Horde webmail interface to segmentation fault.</p>
<p>To fix the problem, create a file called <code>/etc/httpd/conf.d/zz050a_horde_php_workaround.conf</code> and put the following inside it:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-apache" data-lang="apache"><span style="color:#268bd2">&lt;DirectoryMatch</span> <span style="color:#2aa198">/usr/share/psa-horde</span><span style="color:#268bd2">&gt;</span>
<span style="color:#b58900">php_admin_value</span> include_path <span style="color:#2aa198">&#34;/usr/share/psa-horde/lib:/usr/share/psa-horde:/usr/share/psa-horde/pear:.&#34;</span>
<span style="color:#268bd2">&lt;/DirectoryMatch&gt;</span>
</code></pre></div><p>Reload the Apache configuration and your Horde installation should work properly with PHP 5.2.5.</p>
<p><em>Credit for this fix goes to Kevin M.</em></p>
]]></content></item><item><title>Sort e-mail in Plesk with procmail</title><link>https://major.io/2007/11/27/sort-e-mail-in-plesk-with-procmail/</link><pubDate>Tue, 27 Nov 2007 18:27:26 +0000</pubDate><guid>https://major.io/2007/11/27/sort-e-mail-in-plesk-with-procmail/</guid><description>One of my biggest beefs with Plesk&amp;rsquo;s e-mail handling is the lack of server-side filtering. Plesk will only allow you to throw away e-mails marked as spam, but this won&amp;rsquo;t work for me since SpamAssassin marks some mails as spam that actually aren&amp;rsquo;t. If you set up filters in SquirrelMail or Horde, the filters will only work if you always log into the webmail interface to snag your e-mail.
Luckily, you can do some fancy work with procmail to have the filtering done server-side.</description><content type="html"><![CDATA[<p>One of my biggest beefs with Plesk&rsquo;s e-mail handling is the lack of server-side filtering. Plesk will only allow you to throw away e-mails marked as spam, but this won&rsquo;t work for me since SpamAssassin marks some mails as spam that actually aren&rsquo;t. If you set up filters in SquirrelMail or Horde, the filters will only work if you <strong>always</strong> log into the webmail interface to snag your e-mail.</p>
<p>Luckily, you can do some fancy work with procmail to have the filtering done server-side.</p>
<p>First, make sure procmail is installed on your server, and change to this directory:</p>
<p>/var/qmail/mailnames/yourdomain.com/yourusername/</p>
<p>Inside that directory, drop in a .procmailrc file which contains the following:</p>
<pre><code>MAILDIR=/var/qmail/mailnames/yourdomain.com/yourusername/Maildir
DEFAULT=${MAILDIR}/
SPAMDIR=${MAILDIR}/.Junk/
:0
* ^X-Spam-Status: Yes.*
${SPAMDIR}
</code></pre><p>Once that file is in place, move the .qmail file out of the way, and replace it with this:</p>
<pre><code>| /usr/local/psa/bin/psa-spamc accept
|preline /usr/bin/procmail -m -o .procmailrc
</code></pre><p>Please be aware that these changes will disappear if you make any adjustments to your mail configuration within Plesk. To get around this annoyance, just change the file attributes to immutable:</p>
<pre><code># chattr +i .qmail .procmailrc
</code></pre><p><em>Credit for this trick goes to <a href="http://www.russwittmann.com/2007/07/14/server-side-mail-filtering-using-qmailprocmail-under-plesk/">Russ Wittmann</a>.</em></p>
]]></content></item><item><title>Adding SSL encryption to vsftpd</title><link>https://major.io/2007/11/26/adding-ssl-encryption-to-vsftpd/</link><pubDate>Mon, 26 Nov 2007 18:21:54 +0000</pubDate><guid>https://major.io/2007/11/26/adding-ssl-encryption-to-vsftpd/</guid><description>There may be some situations where you want to encrypt FTP traffic with SSL certificates rather than using SFTP with SSH. Using vsftpd with SSL encryption is quite easy, and here&amp;rsquo;s how it&amp;rsquo;s done:
First, you&amp;rsquo;ll need to make a new self-signed SSL certificate (if you don&amp;rsquo;t have a key and certificate available already):
Once you have the key and certificate made, you&amp;rsquo;ll need to concatenate them into a PEM file:</description><content type="html"><![CDATA[<p>There may be some situations where you want to encrypt FTP traffic with SSL certificates rather than using SFTP with SSH. Using vsftpd with SSL encryption is quite easy, and here&rsquo;s how it&rsquo;s done:</p>
<p>First, you&rsquo;ll need to <a href="http://rackerhacker.com/2007/08/02/generate-self-signed-certificate-and-key-in-one-line/">make a new self-signed SSL certificate</a> (if you don&rsquo;t have a key and certificate available already):</p>
<!-- raw HTML omitted -->
<p>Once you have the key and certificate made, you&rsquo;ll need to concatenate them into a PEM file:</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>Now, simply adjust the vsftpd configuration file to enable SSL encryption:</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>Once that&rsquo;s complete, restart vsftpd and you will be able to connect to your FTP server using SSL/TLS encryption.</p>
<p>Further Reading:</p>
<p><a href="http://vsftpd.beasts.org/vsftpd_conf.html">Manpage of vsftpd.conf</a></p>
]]></content></item><item><title>Happy Thanksgiving</title><link>https://major.io/2007/11/22/happy-thanksgiving/</link><pubDate>Fri, 23 Nov 2007 04:20:36 +0000</pubDate><guid>https://major.io/2007/11/22/happy-thanksgiving/</guid><description>In the US, we celebrate Thanksgiving today. I&amp;rsquo;m taking off the rest of this week, but you can expect updates to begin again on Monday!</description><content type="html"><![CDATA[<p>In the US, we celebrate <a href="http://en.wikipedia.org/wiki/Thanksgiving">Thanksgiving</a> today. I&rsquo;m taking off the rest of this week, but you can expect updates to begin again on Monday!</p>
]]></content></item><item><title>EXT3-fs error (device hda3) in start_transaction: Journal has aborted</title><link>https://major.io/2007/11/20/ext3-fs-error-device-hda3-in-start_transaction-journal-has-aborted/</link><pubDate>Tue, 20 Nov 2007 18:23:40 +0000</pubDate><guid>https://major.io/2007/11/20/ext3-fs-error-device-hda3-in-start_transaction-journal-has-aborted/</guid><description>If your system abruptly loses power, or if a RAID card is beginning to fail, you might see an ominous message like this within your logs:
Basically, the system is telling you that it&amp;rsquo;s detected a filesystem/journal mismatch, and it can&amp;rsquo;t utilize the journal any longer. When this situation pops up, the filesystem gets mounted read-only almost immediately. To fix the situation, you can remount the partition as ext2 (if it isn&amp;rsquo;t your active root partition), or you can commence the repair operations.</description><content type="html"><![CDATA[<p>If your system abruptly loses power, or if a RAID card is beginning to fail, you might see an ominous message like this within your logs:</p>
<!-- raw HTML omitted -->
<p>Basically, the system is telling you that it&rsquo;s detected a filesystem/journal mismatch, and it can&rsquo;t utilize the journal any longer. When this situation pops up, the filesystem gets mounted read-only almost immediately. To fix the situation, you can remount the partition as ext2 (if it isn&rsquo;t your active root partition), or you can commence the repair operations.</p>
<p>If you&rsquo;re working with an active root partition, you will need to boot into some rescue media and perform these operations there. If this error occurs with an additional partition besides the root partition, simply unmount the broken filesystem and proceed with these operations.</p>
<p>Remove the journal from the filesystem (effectively turning it into ext2):</p>
<!-- raw HTML omitted -->
<p>Now, you will need to fsck it to correct any possible problems (throw in a -y flag to say yes to all repairs, -C for a progress bar):</p>
<!-- raw HTML omitted -->
<p>Once that&rsquo;s finished, make a new journal which effectively makes the partition an ext3 filesystem again:</p>
<!-- raw HTML omitted -->
<p>You should be able to mount the partition as an ext3 partition at this time:</p>
<!-- raw HTML omitted -->
<p>Be sure to check your dmesg output for any additional errors after you&rsquo;re finished!</p>
]]></content></item><item><title>Red Hat Perl Issues: unable to call function somefunction on undefined value</title><link>https://major.io/2007/11/19/red-hat-perl-issues-unable-to-call-function-somefunction-on-undefined-value/</link><pubDate>Mon, 19 Nov 2007 18:19:12 +0000</pubDate><guid>https://major.io/2007/11/19/red-hat-perl-issues-unable-to-call-function-somefunction-on-undefined-value/</guid><description>Apparently, a recent Red Hat Enterprise Linux update for ES3, 4 and 5 caused some Perl applications to throw errors like these:
unable to call function somefunction on undefined value Of course, replace somefunction with your function of choice. To correct the issue, you can force CPAN to bring back a more sane version of Scalar::Util:
# perl -MCPAN -e shell cpan&amp;gt; force install Scalar::Util</description><content type="html"><![CDATA[<p>Apparently, a recent Red Hat Enterprise Linux update for ES3, 4 and 5 caused some Perl applications to throw errors like these:</p>
<pre><code>unable to call function somefunction on undefined value
</code></pre><p>Of course, replace <code>somefunction</code> with your function of choice. To correct the issue, you can force CPAN to bring back a more sane version of Scalar::Util:</p>
<pre><code># perl -MCPAN -e shell
cpan&gt; force install Scalar::Util
</code></pre>]]></content></item><item><title>clamdscan: corrupt or unknown clamd scanner error or memory/resource/perms problem</title><link>https://major.io/2007/11/16/clamdscan-corrupt-or-unknown-clamd-scanner-error-or-memoryresourceperms-problem/</link><pubDate>Fri, 16 Nov 2007 18:11:05 +0000</pubDate><guid>https://major.io/2007/11/16/clamdscan-corrupt-or-unknown-clamd-scanner-error-or-memoryresourceperms-problem/</guid><description>A few days ago, I stumbled upon a server running qmail with qmail-scanner. The server was throwing out this error when a user on the server attempted to send an e-mail to someone else:
451 qq temporary problem (#4.3.0)
The one thing I love about qmail is its extremely descriptive error messages. Did I say descriptive? I meant cryptic.
Luckily, clamdscan was a bit more chatty in the general system logs:</description><content type="html"><![CDATA[<p>A few days ago, I stumbled upon a server running qmail with qmail-scanner. The server was throwing out this error when a user on the server attempted to send an e-mail to someone else:</p>
<p><code>451 qq temporary problem (#4.3.0)</code></p>
<p>The one thing I love about qmail is its extremely descriptive error messages. Did I say descriptive? I meant cryptic.</p>
<p>Luckily, clamdscan was a bit more chatty in the general system logs:</p>
<p><code>Nov 12 10:21:17 server X-Antivirus-MYDOMAIN-1.25-st-qms: server.somehost.com119488087677512190] clamdscan: corrupt or unknown clamd scanner error or memory/resource/perms problem - exit status 512/2</code></p>
<p>Okay, that helps a bit, but this one from /var/log/clamd.log was the big help:</p>
<p><code>Mon Nov 12 12:20:29 2007 -&gt; ERROR: Socket file /tmp/clamd.socket exists. Either remove it, or configure a different one.</code></p>
<p>I removed the /tmp/clamd.socket file and clamd began operating properly after a quick restart of the clamd service. This one was pretty easy, but it was not well documented (as I discovered from a little while of Google searching).</p>
]]></content></item><item><title>Change the default Apache character set</title><link>https://major.io/2007/11/15/change-the-default-apache-character-set/</link><pubDate>Thu, 15 Nov 2007 18:09:01 +0000</pubDate><guid>https://major.io/2007/11/15/change-the-default-apache-character-set/</guid><description>By default, Red Hat Enterprise Linux 4 sets the default character set in Apache to UTF-8. Your specific web application may need for the character set to be set to a different value, and the change can be made fairly easily. Here&amp;rsquo;s an example where the character set is changed to ISO-8859-1:
First, adjust the AddDefaultCharset directive in /etc/httpd/conf/httpd.conf:
#AddDefaultCharset UTF-8&amp;lt;br /&amp;gt; AddDefaultCharset ISO-8859-1 Then, reload Apache and check your headers:</description><content type="html"><![CDATA[<p>By default, Red Hat Enterprise Linux 4 sets the default character set in Apache to UTF-8. Your specific web application may need for the character set to be set to a different value, and the change can be made fairly easily. Here&rsquo;s an example where the character set is changed to ISO-8859-1:</p>
<p>First, adjust the AddDefaultCharset directive in /etc/httpd/conf/httpd.conf:</p>
<pre><code>#AddDefaultCharset UTF-8&lt;br /&gt;
AddDefaultCharset ISO-8859-1
</code></pre><p>Then, reload Apache and check your headers:</p>
<pre><code># /etc/init.d/httpd reload&lt;br /&gt;
# curl -I localhost&lt;br /&gt;
HTTP/1.1 403 Forbidden&lt;br /&gt;
Date: Thu, 08 Nov 2007 22:18:14 GMT&lt;br /&gt;
Server: Apache/2.0.52 (Red Hat)&lt;br /&gt;
Accept-Ranges: bytes&lt;br /&gt;
Content-Length: 3985&lt;br /&gt;
Connection: close&lt;br /&gt;
Content-Type: text/html; charset=ISO-8859-1
</code></pre><p><em>This was tested on Red Hat Enterprise Linux 4 Update 5</em></p>
]]></content></item><item><title>Plesk authorization failed: HTTP request error [7]</title><link>https://major.io/2007/11/14/plesk-authorization-failed-http-request-error-7/</link><pubDate>Wed, 14 Nov 2007 18:05:24 +0000</pubDate><guid>https://major.io/2007/11/14/plesk-authorization-failed-http-request-error-7/</guid><description>I found myself wrestling with a server where the Plesk interface suddenly became unavailable without any user intervention. An attempt to start the service was less than fruitful:
[root@server ~]# service psa start Key file: /opt/drweb/drweb32.key - Key file not found! A path to a valid license key file does not specified. Plesk authorization failed: HTTP request error [7] Error: Plesk Software not running. [FAILED] (Although I included the text from the drweb failure, I later found that it was not related to the issue.</description><content type="html"><![CDATA[<p>I found myself wrestling with a server where the Plesk interface suddenly became unavailable without any user intervention. An attempt to start the service was less than fruitful:</p>
<pre><code>[root@server ~]# service psa start
Key file: /opt/drweb/drweb32.key - Key file not found!
A path to a valid license key file does not specified.
Plesk authorization failed: HTTP request error [7]
Error: Plesk Software not running.
                                                           [FAILED]
</code></pre><p><em>(Although I included the text from the drweb failure, I later found that it was not related to the issue. However, since it might appear in your logs prior to the HTTP request error, I included it anyways.)</em></p>
<p>This was a perfectly working server that had no other issues besides this peculiar Plesk issue. Another technician had upgraded the license a few weeks prior, and it was verified at the the time to be working properly. After a bit of Google searching, I found that the solution was to completely stop Plesk and its related services and then start it all up again.</p>
<pre><code>[root@server ~]# service psa stopall
/usr/local/psa/admin/bin/httpsdctl stop: httpd stopped
Stopping Plesk:                                            [  OK  ]
Stopping named:                                            [  OK  ]
service psa startStopping MySQL:                           [  OK  ]
Stopping : Stopping Courier-IMAP server:
   Stopping imap                                           [  OK  ]
   Stopping imap-ssl                                       [  OK  ]
   Stopping pop3                                           [  OK  ]
   Stopping pop3-ssl                                       [  OK  ]

Stopping postgresql service:                               [  OK  ]
Shutting down psa-spamassassin service:                    [  OK  ]
Stopping httpd:                                            [  OK  ]

[root@server ~]# service psa start
Starting named:                                            [  OK  ]
Starting MySQL:                                            [  OK  ]
Starting qmail:                                            [  OK  ]
Starting Courier-IMAP server:
   Starting imapd                                          [  OK  ]
   Starting imap-ssl                                       [  OK  ]
   Starting pop3                                           [  OK  ]
   Starting pop3-ssl                                       [  OK  ]

Starting postgresql service:                               [  OK  ]
Starting psa-spamassassin service:                         [  OK  ]
Processing config directory: /usr/local/psa/admin/conf/httpsd.*.include
/usr/local/psa/admin/bin/httpsdctl start: httpd started
Starting Plesk:                                            [  OK  ]
Starting up drwebd:                                        [  OK  ]
</code></pre><p>I couldn&rsquo;t nail down anything within the Plesk log files that would explain the cause of the problem, but this solution corrected the issue instantly.</p>
<p><em>This issue occurred with Plesk 8.1.1 on Red Hat Enterprise Linux 4 Update 5</em></p>
]]></content></item><item><title>Rackspace Outage</title><link>https://major.io/2007/11/13/rackspace-outage/</link><pubDate>Wed, 14 Nov 2007 00:34:11 +0000</pubDate><guid>https://major.io/2007/11/13/rackspace-outage/</guid><description>I&amp;rsquo;ve received a lot of IM&amp;rsquo;s and e-mails from friends and readers of this blog about the Rackspace outages on November 11th and 12th. I work for a company that believes in full disclosure, so if you want the facts, they&amp;rsquo;re already available to the public:
Rackspace Information Center
TechCrunch - Quick, Plug The Internet Back In: Major Rackspace Outage
Laughing Squid - Massive Power Outage At Rackspace’s Dallas Data Center</description><content type="html"><![CDATA[<p>I&rsquo;ve received a lot of IM&rsquo;s and e-mails from friends and readers of this blog about the Rackspace outages on November 11th and 12th. I work for a company that believes in full disclosure, so if you want the facts, they&rsquo;re already available to the public:</p>
<p><a href="http://www.rackspace.com/information/announcements/datacenter.php">Rackspace Information Center</a></p>
<p><a href="http://www.techcrunch.com/2007/11/12/quick-plug-the-internet-back-in-major-rackspace-outage/">TechCrunch - Quick, Plug The Internet Back In: Major Rackspace Outage</a></p>
<p><a href="http://laughingsquid.wordpress.com/2007/11/12/massive-power-outage-at-rackspace-dallas-data-center/">Laughing Squid - Massive Power Outage At Rackspace’s Dallas Data Center</a></p>
<p><a href="http://37signals.blogs.com/products/2007/11/downtime-explan.html">37Signals - Downtime Explanation</a></p>
<p><a href="http://valleywag.com/tech/breakdowns/truck-driver-in-texas-kills-all-the-websites-you-really-use-321881.php">ValleyWag - Truck driver in Texas kills all the websites you really use</a></p>
<p>I don&rsquo;t know of any additional information besides what is contained within these articles and blog posts. However, I can tell you that I&rsquo;ve never worked for a company before that pulled together in such large numbers to get on the phones and respond to tickets long after shifts should have been over. Obviously, it was a horrible &ldquo;perfect storm&rdquo; type of situation, and no one would wish for it to happen to anyone.</p>
<p>Over the last five years, I&rsquo;ve had dedicated servers and VPS accounts with seven companies. Out of those seven, five have had major outages. After those outages, I can honestly say I received a timely and courteous response from one of the companies in that list. In one situation with a certain Texas hosting company, I had no network connectivity for almost 72 hours with no response to phone calls or trouble tickets.</p>
<p>After it&rsquo;s all said and done (and it&rsquo;s not done yet), I find myself to be very proud of the company for which I work. Server parts will eventually fail, as will networks, generators and power grids - it&rsquo;s inevitable. The important part is that the will of those who are providing the support never fails.</p>
<p>Our will is strong, and it continues to stay that way.</p>
]]></content></item><item><title>OpenSSL Tricks</title><link>https://major.io/2007/11/07/openssl-tricks/</link><pubDate>Wed, 07 Nov 2007 18:26:24 +0000</pubDate><guid>https://major.io/2007/11/07/openssl-tricks/</guid><description>Create a strong CSR and private key
openssl req -new -nodes -newkey rsa:2048 -out server.crt -keyout server.key
Parsing out the data within a certificate
openssl asn1parse -in server.crt
Checking a certificate/key modulus to see if they correspond
openssl rsa -in server.key -modulus -noout | openssl md5&amp;lt;br /&amp;gt; openssl x509 -in server.crt -modulus -noout | openssl md5
Convert a key from PEM -&amp;gt; DER
openssl rsa -inform PEM -in key.pem -outform DER -out keyout.</description><content type="html"><![CDATA[<p><strong>Create a strong CSR and private key</strong></p>
<p><code>openssl req -new -nodes -newkey rsa:2048 -out server.crt -keyout server.key</code></p>
<p><strong>Parsing out the data within a certificate</strong></p>
<p><code>openssl asn1parse -in server.crt</code></p>
<p><strong>Checking a certificate/key modulus to see if they correspond</strong></p>
<p><code>openssl rsa -in server.key -modulus -noout | openssl md5&lt;br /&gt; openssl x509 -in server.crt -modulus -noout | openssl md5</code></p>
<p><strong>Convert a key from PEM -&gt; DER</strong></p>
<p><code>openssl rsa -inform PEM -in key.pem -outform DER -out keyout.der</code></p>
<p><strong>Convert a key from DER -&gt; PEM</strong></p>
<p><code>openssl rsa -inform DER -in key.der -outform PEM -out keyout.pem</code></p>
<p><strong>Remove the password from an encrypted private key</strong></p>
<p><code>openssl rsa -in server.key -out server-nopass.key</code></p>
<p><strong>Reviewing a detailed SSL connection</strong></p>
<p><code>openssl s_client -connect 10.0.0.1:443</code></p>
]]></content></item><item><title>MySQLTuner Revision 20 is available</title><link>https://major.io/2007/11/06/mysqltuner-revision-20-is-available/</link><pubDate>Tue, 06 Nov 2007 23:27:00 +0000</pubDate><guid>https://major.io/2007/11/06/mysqltuner-revision-20-is-available/</guid><description>I&amp;rsquo;ve updated MySQLTuner to revision 20 tonight, and it has the following improvements:
Added NetBSD support (thanks to Dave Burgess)
Switched username/password prompts to STDERR so they won&amp;rsquo;t appear in printouts
Downloads are available, or you can do a new subversion checkout.</description><content type="html"><![CDATA[<p>I&rsquo;ve updated MySQLTuner to revision 20 tonight, and it has the following improvements:</p>
<ul>
<li>
<p>Added NetBSD support (thanks to Dave Burgess)</p>
</li>
<li>
<p>Switched username/password prompts to STDERR so they won&rsquo;t appear in printouts</p>
</li>
</ul>
<p><a href="http://rackerhacker.com/mysqltuner/">Downloads are available</a>, or you can do a new <a href="http://tools.assembla.com/mysqltuner/">subversion checkout</a>.</p>
]]></content></item><item><title>Attractive aterm/rxvt .Xdefaults configuration</title><link>https://major.io/2007/11/04/attractive-atermrxvt-xdefaults-configuration/</link><pubDate>Sun, 04 Nov 2007 18:04:27 +0000</pubDate><guid>https://major.io/2007/11/04/attractive-atermrxvt-xdefaults-configuration/</guid><description>I&amp;rsquo;ve struggled at times to get a decent-looking terminal on my desktop, and I believe I&amp;rsquo;ve found a good one. Toss this into your ~/.Xdefaults:
aterm*loginShell:true aterm*transparent:true aterm*shading:40 aterm*background:Black aterm*foreground:White aterm*scrollBar:true aterm*scrollBar_right:true aterm*transpscrollbar:true aterm*saveLines:32767 aterm*font:*-*-fixed-medium-r-normal--*-110-*-*-*-*-iso8859-1 aterm*boldFont:*-*-fixed-bold-r-normal--*-*-110-*-*-*-*-iso8859-1 Then load up the changes and start aterm:
$ xrdb -load .Xdefaults $ aterm Of course, if you like rxvt better for your Unicode needs, just use this configuration:
rxvt*loginShell:true rxvt*transparent:true rxvt*shading:40 rxvt*background:Black rxvt*foreground:White rxvt*scrollBar:true rxvt*scrollBar_right:true rxvt*transpscrollbar:true rxvt*saveLines:32767 rxvt*font:*-*-fixed-medium-r-normal--*-110-*-*-*-*-iso8859-1 rxvt*boldFont:*-*-fixed-bold-r-normal--*-*-110-*-*-*-*-iso8859-1</description><content type="html"><![CDATA[<p>I&rsquo;ve struggled at times to get a decent-looking terminal on my desktop, and I believe I&rsquo;ve found a good one. Toss this into your ~/.Xdefaults:</p>
<pre><code>aterm*loginShell:true
aterm*transparent:true
aterm*shading:40
aterm*background:Black
aterm*foreground:White
aterm*scrollBar:true
aterm*scrollBar_right:true
aterm*transpscrollbar:true
aterm*saveLines:32767
aterm*font:*-*-fixed-medium-r-normal--*-110-*-*-*-*-iso8859-1
aterm*boldFont:*-*-fixed-bold-r-normal--*-*-110-*-*-*-*-iso8859-1
</code></pre><p>Then load up the changes and start aterm:</p>
<pre><code>$ xrdb -load .Xdefaults
$ aterm
</code></pre><p>Of course, if you like rxvt better for your Unicode needs, just use this configuration:</p>
<pre><code>rxvt*loginShell:true
rxvt*transparent:true
rxvt*shading:40
rxvt*background:Black
rxvt*foreground:White
rxvt*scrollBar:true
rxvt*scrollBar_right:true
rxvt*transpscrollbar:true
rxvt*saveLines:32767
rxvt*font:*-*-fixed-medium-r-normal--*-110-*-*-*-*-iso8859-1
rxvt*boldFont:*-*-fixed-bold-r-normal--*-*-110-*-*-*-*-iso8859-1
</code></pre>]]></content></item><item><title>A short vacation</title><link>https://major.io/2007/11/02/a-short-vacation/</link><pubDate>Sat, 03 Nov 2007 05:21:58 +0000</pubDate><guid>https://major.io/2007/11/02/a-short-vacation/</guid><description>I&amp;rsquo;ve been on a bit of a vacation lately, and that&amp;rsquo;s why there&amp;rsquo;s been a lack of updates. On October 20th, I married the best woman on the planet, and we we&amp;rsquo;ve been on our honeymoon since then.
New updates should start again this Monday!</description><content type="html"><![CDATA[<p>I&rsquo;ve been on a bit of a vacation lately, and that&rsquo;s why there&rsquo;s been a lack of updates. On October 20th, I married the best woman on the planet, and we we&rsquo;ve been on our honeymoon since then.</p>
<p>New updates should start again this Monday!</p>
]]></content></item><item><title>Enforcing mode requested but no policy loaded. Halting now.</title><link>https://major.io/2007/10/17/enforcing-mode-requested-but-no-policy-loaded-halting-now/</link><pubDate>Wed, 17 Oct 2007 18:17:22 +0000</pubDate><guid>https://major.io/2007/10/17/enforcing-mode-requested-but-no-policy-loaded-halting-now/</guid><description>Here&amp;rsquo;s a pretty weird kernel panic that I came across the other day:
Enforcing mode requested but no policy loaded. Halting now. Kernel panic - not syncing: Attempted to kill init! This usually means that you&amp;rsquo;ve set SELINUX in enforcing mode within /etc/sysconfig/selinux or /etc/selinux/selinux.conf but you don&amp;rsquo;t have the appropriate SELINUX packages installed. To fix the issue, boot the server into the Red Hat rescue environment and disable SELINUX until you can install the proper packages that contain the SELINUX targeted configuration.</description><content type="html"><![CDATA[<p>Here&rsquo;s a pretty weird kernel panic that I came across the other day:</p>
<pre><code>Enforcing mode requested but no policy loaded. Halting now.
Kernel panic - not syncing: Attempted to kill init!
</code></pre><p>This usually means that you&rsquo;ve set SELINUX in enforcing mode within /etc/sysconfig/selinux or /etc/selinux/selinux.conf but you don&rsquo;t have the appropriate SELINUX packages installed. To fix the issue, boot the server into the Red Hat rescue environment and disable SELINUX until you can install the proper packages that contain the SELINUX targeted configuration.</p>
<p><em>This kernel panic appeared on a Red Hat Enterprise Linux 4 Update 5 server.</em></p>
]]></content></item><item><title>Installing package groups with up2date</title><link>https://major.io/2007/10/16/installing-package-groups-with-up2date/</link><pubDate>Wed, 17 Oct 2007 01:14:53 +0000</pubDate><guid>https://major.io/2007/10/16/installing-package-groups-with-up2date/</guid><description>A few days ago, I began to install a group of packages with up2date, and the person next to me was surprised that up2date even had this functionality. I use it regularly, but I realized that many users might not be familiar with it.
You can install package groups using an at-sign (@) in front of the group name:
# up2date -i &amp;quot;@X Window System&amp;quot; This will tell up2date to install all of the packages that are marked within the &amp;ldquo;X Window System&amp;rdquo; package group.</description><content type="html"><![CDATA[<p>A few days ago, I began to install a group of packages with up2date, and the person next to me was surprised that up2date even had this functionality. I use it regularly, but I realized that many users might not be familiar with it.</p>
<p>You can install package groups using an at-sign (@) in front of the group name:</p>
<pre><code># up2date -i &quot;@X Window System&quot;
</code></pre><p>This will tell up2date to install all of the packages that are marked within the &ldquo;X Window System&rdquo; package group. That would include X drivers, the X libraries/binaries, and twm (among many other packages). If you&rsquo;re not sure which groups are available, just pass the <code>--show-groups</code> flag and review the list:</p>
<pre><code># up2date --show-groups
Administration Tools
Arabic Support
Assamese Support
Authoring and Publishing
Base
Bengali Support
Brazilian Portuguese Support
British Support
Bulgarian Support
Catalan Support
Chinese Support
Compatibility Arch Development Support
Compatibility Arch Support
Core
Cyrillic Support
Czech Support
DNS Name Server
Danish Support
Development Libraries
Development Tools
Dialup Networking Support
Dutch Support
Editors
Emacs
Engineering and Scientific
Estonian Support
FTP Server
Finnish Support
French Support
GNOME
GNOME Desktop Environment
GNOME Software Development
Games and Entertainment
German Support
Graphical Internet
Graphics
Greek Support
Gujarati Support
Hebrew Support
Hindi Support
Hungarian Support
ISO8859-2 Support
ISO8859-9 Support
Icelandic Support
Italian Support
Japanese Support
KDE
KDE (K Desktop Environment)
KDE Software Development
Korean Support
Legacy Network Server
Legacy Software Development
Mail Server
Miscellaneous Included Packages
MySQL Database
Network Servers
News Server
Norwegian Support
Office/Productivity
Polish Support
Portuguese Support
PostgreSQL Database
Printing Support
Punjabi Support
Romanian Support
Ruby
Russian Support
Serbian Support
Server
Server Configuration Tools
Slovak Support
Slovenian Support
Sound and Video
Spanish Support
Swedish Support
System Tools
Tamil Support
Text-based Internet
Turkish Support
Ukrainian Support
Web Server
Welsh Support
Windows File Server
Workstation Common
X Software Development
X Window System
XEmacs
</code></pre>]]></content></item><item><title>MySQLTuner Revision 19 is available</title><link>https://major.io/2007/10/15/mysqltuner-revision-19-is-available/</link><pubDate>Tue, 16 Oct 2007 01:54:34 +0000</pubDate><guid>https://major.io/2007/10/15/mysqltuner-revision-19-is-available/</guid><description>I&amp;rsquo;ve revamped a few of the recommendations in MySQLTuner, and revision 19 is now available tonight! Here&amp;rsquo;s the main changes:
Adjusted infoprint to use asterisks (cosmetic)
Per-thread/global buffer counts are now displayed
Key buffer increases are only recommended if the buffer is smaller than total indexes and hit rate is &amp;lt; 95% * Dropped max_seeks_for_key checks * Temporary table size increases are not recommended over 256M * Aborted connection calculation and recommendation adjustments You can download the latest copy on the MySQLTuner page, and you can get diffs for the new version as well.</description><content type="html"><![CDATA[<p>I&rsquo;ve revamped a few of the recommendations in MySQLTuner, and revision 19 is now available tonight! Here&rsquo;s the main changes:</p>
<ul>
<li>
<p>Adjusted infoprint to use asterisks (cosmetic)</p>
</li>
<li>
<p>Per-thread/global buffer counts are now displayed</p>
</li>
<li>
<p>Key buffer increases are only recommended if the buffer is smaller than total indexes and hit rate is &lt; 95% * Dropped max_seeks_for_key checks * Temporary table size increases are not recommended over 256M * Aborted connection calculation and recommendation adjustments You can download the latest copy on the <a href="http://rackerhacker.com/mysqltuner/">MySQLTuner page</a>, and you can <a href="http://tools.assembla.com/mysqltuner/">get diffs for the new version</a> as well.</p>
</li>
</ul>
]]></content></item><item><title>Enabling Dr. Web virus scanning for new accounts</title><link>https://major.io/2007/10/12/enabling-dr-web-virus-scanning-for-new-accounts/</link><pubDate>Fri, 12 Oct 2007 18:35:06 +0000</pubDate><guid>https://major.io/2007/10/12/enabling-dr-web-virus-scanning-for-new-accounts/</guid><description>If you&amp;rsquo;re using Plesk 8.0 or later, you can set up Dr. Web to be enabled for all new mail accounts. To do this, you have to create an event handler.
Here&amp;rsquo;s the steps you will need:
» Log into Plesk
» Click &amp;ldquo;Server&amp;rdquo;
» Click &amp;ldquo;Event Manager&amp;rdquo;
» Choose &amp;ldquo;Mail Name Created&amp;rdquo; next to &amp;ldquo;Event&amp;rdquo;
» In the command area, enter /usr/local/psa/bin/mail.sh --update $NEW_MAILNAME -antivirus inout
» Click &amp;ldquo;OK&amp;rdquo;</description><content type="html"><![CDATA[<p>If you&rsquo;re using Plesk 8.0 or later, you can set up Dr. Web to be enabled for all new mail accounts. To do this, you have to create an event handler.</p>
<p>Here&rsquo;s the steps you will need:</p>
<p>» Log into Plesk</p>
<p>» Click &ldquo;Server&rdquo;</p>
<p>» Click &ldquo;Event Manager&rdquo;</p>
<p>» Choose &ldquo;Mail Name Created&rdquo; next to &ldquo;Event&rdquo;</p>
<p>» In the command area, enter <code>/usr/local/psa/bin/mail.sh --update $NEW_MAILNAME -antivirus inout</code></p>
<p>» Click &ldquo;OK&rdquo;</p>
]]></content></item><item><title>mysqldump: Got packet bigger than ‘max_allowed_packet’ bytes</title><link>https://major.io/2007/10/11/mysqldump-got-packet-bigger-than-max_allowed_packet-bytes/</link><pubDate>Fri, 12 Oct 2007 01:28:17 +0000</pubDate><guid>https://major.io/2007/10/11/mysqldump-got-packet-bigger-than-max_allowed_packet-bytes/</guid><description>When you dump table data from MySQL, you may end up pulling a large chunk of data and it may exceed the MySQL client&amp;rsquo;s max_allowed_packet variable. If that happens, you might catch an error like this:
mysqldump: Error 2020: Got packet bigger than 'max_allowed_packet' bytes when dumping table `tablename` at row: 1627 The default max_allowed_packet size is 25M, and you can adjust it for good within your my.cnf by setting the variable in a section for mysqldump:</description><content type="html"><![CDATA[<p>When you dump table data from MySQL, you may end up pulling a large chunk of data and it may exceed the MySQL client&rsquo;s max_allowed_packet variable. If that happens, you might catch an error like this:</p>
<pre><code>mysqldump: Error 2020: Got packet bigger than 'max_allowed_packet' bytes when dumping table `tablename` at row: 1627
</code></pre><p>The default max_allowed_packet size is 25M, and you can adjust it for good within your my.cnf by setting the variable in a section for mysqldump:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ini" data-lang="ini"><span style="color:#719e07">[mysqldump]</span>
max_allowed_packet <span style="color:#719e07">=</span> <span style="color:#2aa198">500M</span>
</code></pre></div>]]></content></item><item><title>BIND: ‘RRset exists (value dependent)’ prerequisite not satisfied (NXRRSET)</title><link>https://major.io/2007/10/10/bind-rrset-exists-value-dependent-prerequisite-not-satisfied-nxrrset/</link><pubDate>Wed, 10 Oct 2007 18:13:22 +0000</pubDate><guid>https://major.io/2007/10/10/bind-rrset-exists-value-dependent-prerequisite-not-satisfied-nxrrset/</guid><description>I was recently working on a server where a user on the server was concerned with these log messages:
Oct 7 20:59:33 web named[13698]: client 111.222.333.444#50389: updating zone 'domain.com/IN': update failed: 'RRset exists (value dependent)' prerequisite not satisfied (NXRRSET) Oct 7 20:59:34 web named[13698]: client 111.222.333.444#50392: update 'domain.com/IN' denied Oct 7 21:59:35 web named[13698]: client 111.222.333.444#50422: updating zone 'domain.com/IN': update failed: 'RRset exists (value dependent)' prerequisite not satisfied (NXRRSET) Oct 7 21:59:35 web named[13698]: client 111.</description><content type="html"><![CDATA[<p>I was recently working on a server where a user on the server was concerned with these log messages:</p>
<pre><code>Oct 7 20:59:33 web named[13698]: client 111.222.333.444#50389: updating zone 'domain.com/IN': update failed: 'RRset exists (value dependent)' prerequisite not satisfied (NXRRSET)
Oct 7 20:59:34 web named[13698]: client 111.222.333.444#50392: update 'domain.com/IN' denied
Oct 7 21:59:35 web named[13698]: client 111.222.333.444#50422: updating zone 'domain.com/IN': update failed: 'RRset exists (value dependent)' prerequisite not satisfied (NXRRSET)
Oct 7 21:59:35 web named[13698]: client 111.222.333.444#50425: update 'domain.com/IN' denied
Oct 7 22:59:20 web named[13698]: client 111.222.333.444#50458: updating zone 'domain.com/IN': update failed: 'RRset exists (value dependent)' prerequisite not satisfied (NXRRSET)
</code></pre><p>The messages here are actually showing that named is doing its job well. Some user was attempting to dynamically update a DNS zone repeatedly, but named was rejecting the updates since they were not coming from a valid sources.</p>
<p>Further reading:</p>
<p><a href="http://www.zytrax.com/books/dns/ch7/xfer.html#allow-update">Zytrax.com: DNS BIND Zone Transfers and Updates</a></p>
<p><a href="http://www.isc.org/sw/bind/arm95/BvARM-all.html#dynamic_update">Internet Systems Consortium: Dynamic Updates</a></p>
]]></content></item><item><title>Dovecot: mbox: Can’t create root IMAP folder</title><link>https://major.io/2007/10/09/dovecot-mbox-cant-create-root-imap-folder/</link><pubDate>Wed, 10 Oct 2007 01:05:52 +0000</pubDate><guid>https://major.io/2007/10/09/dovecot-mbox-cant-create-root-imap-folder/</guid><description>In some situations with dovecot running on your server, you may receive a message from your e-mail client stating that the &amp;ldquo;connection was interrupted with your mail server&amp;rdquo; or the &amp;ldquo;login process failed&amp;rdquo;. This may happen even if you&amp;rsquo;ve created the e-mail account, created the mail spool, and set a password for the user.
If you check your /var/log/maillog, you will generally find errors like these:
Oct 7 09:37:45 mailserver pop3-login: Login: newuser [111.</description><content type="html"><![CDATA[<p>In some situations with dovecot running on your server, you may receive a message from your e-mail client stating that the &ldquo;connection was interrupted with your mail server&rdquo; or the &ldquo;login process failed&rdquo;. This may happen even if you&rsquo;ve created the e-mail account, created the mail spool, and set a password for the user.</p>
<p>If you check your /var/log/maillog, you will generally find errors like these:</p>
<pre><code>Oct 7 09:37:45 mailserver pop3-login: Login: newuser [111.222.333.444]&lt;br /&gt;
Oct 7 09:37:45 mailserver pop3(newuser): mbox: Can't create root IMAP folder /home/newuser/mail: Permission denied&lt;br /&gt;
Oct 7 09:37:45 mailserver pop3(newuser): Failed to create storage with data: mbox:/var/spool/mail/newuser
</code></pre><p>Dovecot is telling you that it wants to store some mail-related data in the user&rsquo;s home directory, but it can&rsquo;t get access to the user&rsquo;s home directory. If the home directory doesn&rsquo;t exist, create it and set the permissions properly:</p>
<pre><code># mkdir /home/newuser&lt;br /&gt;
# chown newuser:newuser /home/newuser&lt;br /&gt;
# chmod 755 /home/newuser
</code></pre><p>If the directory is already there, double check the ownership and permissions on the directory. If filesystem acl&rsquo;s or filesystem quotas might be in play, be sure to check those as well.</p>
]]></content></item><item><title>Plesk: Error opening /var/lib/squirrelmail/prefs/default_pref</title><link>https://major.io/2007/10/08/plesk-error-opening-varlibsquirrelmailprefsdefault_pref/</link><pubDate>Tue, 09 Oct 2007 00:44:48 +0000</pubDate><guid>https://major.io/2007/10/08/plesk-error-opening-varlibsquirrelmailprefsdefault_pref/</guid><description>On brand new Plesk 8.2.x installations or on servers that have been upgraded to Plesk 8.2.x, you might run into this error when you attempt to log into squirrelmail after it was installed via RPM:
Error opening /var/lib/squirrelmail/prefs/default_pref Could not create initial preference file! /var/lib/squirrelmail/prefs/ should be writable by user apache Please contact your system administrator and report this error. No matter what you do to the /var/lib/squirrelmail/prefs/default_pref file, even if you chmod 777 the file, you will still get the error.</description><content type="html"><![CDATA[<p>On brand new Plesk 8.2.x installations or on servers that have been upgraded to Plesk 8.2.x, you might run into this error when you attempt to log into squirrelmail after it was installed via RPM:</p>
<pre><code>Error opening /var/lib/squirrelmail/prefs/default_pref
Could not create initial preference file!
/var/lib/squirrelmail/prefs/ should be writable by user apache
Please contact your system administrator and report this error.
</code></pre><p>No matter what you do to the /var/lib/squirrelmail/prefs/default_pref file, even if you chmod 777 the file, you will still get the error. If you check the /etc/php.ini, you will normally find <code>safe_mode</code> set to <strong>on</strong>.</p>
<pre><code>;
; Safe Mode
;
safe_mode = Off
</code></pre><p>Simply change <code>safe_mode</code> to <strong>off</strong> and reload Apache. If you try to log into squirrelmail again, it should complete successfully. I&rsquo;ve tested this on Red Hat Enterprise Linux 4:</p>
<pre><code># rpm -q squirrelmail
squirrelmail-1.4.8-4.0.1.el4
</code></pre>]]></content></item><item><title>Slow Horde login process with Plesk</title><link>https://major.io/2007/10/05/slow-horde-login-process-with-plesk/</link><pubDate>Fri, 05 Oct 2007 18:35:33 +0000</pubDate><guid>https://major.io/2007/10/05/slow-horde-login-process-with-plesk/</guid><description>I&amp;rsquo;ve seen quite a few situations where the Horde login process can take upwards of 45 minutes to log a user into the webmail interface. There&amp;rsquo;s a few issues that can cause these extended delays, and most of them can be fixed rather easily:
Too many filters / Giant whitelists and blacklists
This is the biggest cause that I&amp;rsquo;ve seen. Some users will create gigantic white and black lists (upwards of 5,000 is my record that I&amp;rsquo;ve seen) and this makes Horde compare each and every message in the inbox against these lists upon login.</description><content type="html"><![CDATA[<p>I&rsquo;ve seen quite a few situations where the Horde login process can take upwards of 45 minutes to log a user into the webmail interface. There&rsquo;s a few issues that can cause these extended delays, and most of them can be fixed rather easily:</p>
<p><strong>Too many filters / Giant whitelists and blacklists</strong></p>
<p>This is the biggest cause that I&rsquo;ve seen. Some users will create gigantic white and black lists (upwards of 5,000 is my record that I&rsquo;ve seen) and this makes Horde compare each and every message in the inbox against these lists upon login. This also applies to filters as Plesk does not use sieve/procmail for mail delivery. Horde is forced to do all of the filtering upon login (in some versions) and this can cause extreme delays.</p>
<p><strong>Mailbox is gigantic</strong></p>
<p>I&rsquo;ve seen Horde logins take quite a while in mailboxes that are over 500MB in size. If the size of your e-mails is large, and you have a large mailbox with fewer e-mails, Horde can normally work quickly. But, if your inbox is full of tiny e-mails, Horde takes a long time to fully index your mail and display the list (even though it only displays 25-30 at a time).</p>
<p><strong>Too many users logged into Horde simultaneously</strong></p>
<p>In my opinion, Horde&rsquo;s CPU and memory requirements are too large for a webmail application. I&rsquo;ve seen 30-40 simultaneous Horde sessions bring a dual-core box with 2-4GB of RAM and SCSI disks to its knees. Consider installing squirrelmail or roundcube webmail for some of your users and urge them to use it instead.</p>
<p><strong>IOwait caused by something else</strong></p>
<p>Sometimes the server can simply be bogged down with other requests from other daemons, and this slows Horde down. Make sure that your MySQL installation is tuned properly, and that users are not abusing scripts running through Apache.</p>
]]></content></item><item><title>Preventing Plesk 8.2.x from adding up2date sources</title><link>https://major.io/2007/10/04/preventing-plesk-82x-from-adding-up2date-sources/</link><pubDate>Thu, 04 Oct 2007 18:29:14 +0000</pubDate><guid>https://major.io/2007/10/04/preventing-plesk-82x-from-adding-up2date-sources/</guid><description>One of the most annoying (and explosive) changes in Plesk 8.2 is the automatic addition of up2date sources for its use. As of 8.2.0, the packages are not signed, and they generate errors with up2date. Also, Plesk often keeps adding the sources over and over to /etc/sysconfig/rhn/sources, and this causes additional errors and delays when you use up2date.
You can disable this behavior entirely by running the following:
# echo ALLOW_TO_USE_UP2DATE=no &amp;gt; /root/.</description><content type="html"><![CDATA[<p>One of the most annoying (and explosive) changes in Plesk 8.2 is the automatic addition of up2date sources for its use. As of 8.2.0, the packages are not signed, and they generate errors with up2date. Also, Plesk often keeps adding the sources over and over to /etc/sysconfig/rhn/sources, and this causes additional errors and delays when you use up2date.</p>
<p>You can disable this behavior entirely by running the following:</p>
<p><code># echo ALLOW_TO_USE_UP2DATE=no &gt; /root/.autoinstallerrc</code></p>
<p>This will instruct Plesk&rsquo;s autoinstaller to not add any sources to the up2date sources list.</p>
]]></content></item><item><title>Convert MyISAM tables to InnoDB</title><link>https://major.io/2007/10/03/convert-myisam-tables-to-innodb/</link><pubDate>Thu, 04 Oct 2007 03:29:13 +0000</pubDate><guid>https://major.io/2007/10/03/convert-myisam-tables-to-innodb/</guid><description>If you want to convert a MyISAM table to InnoDB, the process is fairly easy, but you can do something extra to speed things up. Before converting the table, adjust its order so that the primary key column is in order:
This will pre-arrange the table so that it can be converted quickly without a lot of re-arranging required in MySQL. Then, simply change the table engine: If your table is large, then it may take a while to convert it over.</description><content type="html"><![CDATA[<p>If you want to convert a MyISAM table to InnoDB, the process is fairly easy, but you can do something extra to speed things up. Before converting the table, adjust its order so that the primary key column is in order:</p>
<pre><code>

This will pre-arrange the table so that it can be converted quickly without a lot of re-arranging required in MySQL. Then, simply change the table engine:

</code></pre><p>If your table is large, then it may take a while to convert it over. There will probably be a fair amount of CPU usage and disk I/O in the process.</p>
<p>These statements are also safe in replicated environments. When you issue this statement to the master, it will begin the conversion process. Once it is complete on the master, the statement will roll down to the slaves, and they will begin the conversion as well. Keep in mind, however, that this can greatly reduce the performance of your configuration in the process.</p>
<p><em>Special thanks to Matthew Montgomery for the ORDER BY recommendation.</em></p>
]]></content></item><item><title>Plesk: There is incorrect combination of resource records in the zone</title><link>https://major.io/2007/10/02/plesk-there-is-incorrect-combination-of-resource-records-in-the-zone/</link><pubDate>Wed, 03 Oct 2007 03:21:19 +0000</pubDate><guid>https://major.io/2007/10/02/plesk-there-is-incorrect-combination-of-resource-records-in-the-zone/</guid><description>Yet another weird Plesk error with terrible grammar popped up on a server that I worked with this week:
Error: There is incorrect combination of resource records in the zone
As you can see, this error is not terribly informative. Here&amp;rsquo;s a little background on what I was doing before this alert appeared:
On Plesk 8.1.1, I needed to create an alias for a certain domain. Each time I&amp;rsquo;d try to create the alias, I&amp;rsquo;d receive the above error.</description><content type="html"><![CDATA[<p>Yet another weird Plesk error with terrible grammar popped up on a server that I worked with this week:</p>
<p><code>Error: There is incorrect combination of resource records in the zone</code></p>
<p>As you can see, this error is not terribly informative. Here&rsquo;s a little background on what I was doing before this alert appeared:</p>
<p>On Plesk 8.1.1, I needed to create an alias for a certain domain. Each time I&rsquo;d try to create the alias, I&rsquo;d receive the above error. I could even try junk domains like &lsquo;test.com&rsquo; and it would still fail with the error. I went to a different domain on the server, tried to add an alias there, and it failed as well. So, I went back to analyze the error further.</p>
<p>The only thing that tipped me off was the <strong>zone</strong> word, and I immediately began thinking of DNS. I checked the DNS configuration for a few of the domains, and they appeared to be pretty standard. There weren&rsquo;t any wild DNS records, and there were no problems with the named configuration nor the zone files themselves. I crawled through the dns_recs table in the psa database, and everything appears to be normal.</p>
<p>I admitted defeat and escalated the issue to SWSoft to get their help. The answer came back, and I was dumbfounded.</p>
<p>Apparently this record was present in the DNS configuration for all of the sites on the server:</p>
<p><code>mail.domain.com. CNAME domain.com.</code></p>
<p>This DNS record prevented Plesk from making an alias. <strong>Just</strong> this DNS record. In short, Plesk was unable to make the alias because of this lonely CNAME. The SWSoft developers claimed that it is an &lsquo;old-style&rsquo; notation and that it &lsquo;should not be used&rsquo;. However, during upgrades from 7.x to 8.x, they never thought it&rsquo;d be a good idea to check for this record and fix it accordingly.</p>
<p>Basically, the SWSoft developers recommended changing the DNS record manually for each domain to something like this:</p>
<p><code>mail.domain.com. A 111.222.333.444</code></p>
<p>I did that, and it worked flawlessly. Even though this fixes the issue, I still think that they should have considered this issue during the upgrade routines.</p>
]]></content></item><item><title>Plesk 7.5.4: Error: HTTPD_INCLUDE_D not defined</title><link>https://major.io/2007/10/01/plesk-754-error-httpd_include_d-not-defined/</link><pubDate>Tue, 02 Oct 2007 02:56:20 +0000</pubDate><guid>https://major.io/2007/10/01/plesk-754-error-httpd_include_d-not-defined/</guid><description>Normally, this error will pop up when you attempt to restart a Plesk-related service, like httpsd, psa-spamassassin or qmail:
Error: HTTPD_INCLUDE_D not defined This basically means that Plesk is unable to get some required configuration directives from the /etc/psa/psa.conf file. If you can&amp;rsquo;t find the directive in the file that Plesk is complaining about, check your Plesk RPM&amp;rsquo;s with rpm:
# rpm -q psa Most likely, you will find that there is a psa-7.</description><content type="html"><![CDATA[<p>Normally, this error will pop up when you attempt to restart a Plesk-related service, like httpsd, psa-spamassassin or qmail:</p>
<pre><code>Error: HTTPD_INCLUDE_D not defined
</code></pre><p>This basically means that Plesk is unable to get some required configuration directives from the /etc/psa/psa.conf file. If you can&rsquo;t find the directive in the file that Plesk is complaining about, check your Plesk RPM&rsquo;s with <code>rpm</code>:</p>
<pre><code># rpm -q psa
</code></pre><p>Most likely, you will find that there is a psa-7.5.4 RPM and a psa-8.1.0 or psa-8.1.1 RPM installed simultaneously. This generally appears because of a botched upgrade that was started within Plesk by the admin user.</p>
<p>To fix the issue, get the psa-7.5.4 RPM from <a href="http://autoinstall.plesk.com/">autoinstall.plesk.com</a>. Remove the psa-8.1.1 RPM and install the psa-7.5.4 RPM again rather forcefully:</p>
<pre><code># rpm -ev psa-8.1.1...
# rpm -Uvh --force --nodeps psa-7.5.4...
# /etc/init.d/psa restart
</code></pre><p>At this point, you can download the command line autoinstaller and try the Plesk upgrade again.</p>
<p>Further reading: <a href="http://forum.swsoft.com/showthread.php?threadid=32299">http://forum.swsoft.com/showthread.php?threadid=32299</a></p>
]]></content></item><item><title>Parsing HTML through PHP in Plesk</title><link>https://major.io/2007/09/28/parsing-html-through-php-in-plesk/</link><pubDate>Fri, 28 Sep 2007 18:17:00 +0000</pubDate><guid>https://major.io/2007/09/28/parsing-html-through-php-in-plesk/</guid><description>Some users will want to parse HTML through the PHP parser because one of their applications requires it, or because they think it&amp;rsquo;s a good idea. Parsing regular static content through PHP is not recommended as it will cause a performance hit on the server each time a static page is loaded.
Unfortunately, enabling this in conjunction with Plesk will cause problems with the Plesk web statistics. Since the PHP parsing is disabled for the /plesk-stat/ directories, Apache will mark the page as a PHP page and your browser will attempt to download it rather than display it.</description><content type="html"><![CDATA[<p>Some users will want to parse HTML through the PHP parser because one of their applications requires it, or because they think it&rsquo;s a good idea. Parsing regular static content through PHP is not recommended as it will cause a performance hit on the server each time a static page is loaded.</p>
<p>Unfortunately, enabling this in conjunction with Plesk will cause problems with the Plesk web statistics. Since the PHP parsing is disabled for the <code>/plesk-stat/</code> directories, Apache will mark the page as a PHP page and your browser will attempt to download it rather than display it.</p>
<p>To fix this issue, simply add the following LocationMatch to the bottom of your Apache configuration:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-apache" data-lang="apache"><span style="color:#b58900">AddType</span> application/x-httpd-php .php .html
<span style="color:#268bd2">&lt;LocationMatch</span> <span style="color:#2aa198">&#34;/plesk-stat/(.*)&#34;</span><span style="color:#268bd2">&gt;</span>
<span style="color:#b58900">AddType</span> text/html .html
<span style="color:#268bd2">&lt;/LocationMatch&gt;</span>
</code></pre></div><p>This will force Apache to serve HTML files from <code>/plesk-stat/</code> as text/html rather than application/x-http-php. Your web statistics will display in the browser rather than downloading as a PHP file.</p>
]]></content></item><item><title>Session problems with Horde in Plesk with AOL</title><link>https://major.io/2007/09/27/session-problems-with-horde-in-plesk-with-aol/</link><pubDate>Fri, 28 Sep 2007 02:06:52 +0000</pubDate><guid>https://major.io/2007/09/27/session-problems-with-horde-in-plesk-with-aol/</guid><description>Since AOL sends their users' traffic through proxy servers, this can cause problems with Horde&amp;rsquo;s session handling in Plesk. The problem arises when the user&amp;rsquo;s IP changes during the middle of the session.
You may see an error message in Horde that looks like this:
Your Internet Address has changed since the beginning of your Mail session. To protect your security, you must login again.
You&amp;rsquo;ll normally have this variable in /etc/psa-horde/horde/conf.</description><content type="html"><![CDATA[<p>Since AOL sends their users' traffic through proxy servers, this can cause problems with Horde&rsquo;s session handling in Plesk. The problem arises when the user&rsquo;s IP changes during the middle of the session.</p>
<p>You may see an error message in Horde that looks like this:</p>
<blockquote>
<p>Your Internet Address has changed since the beginning of your Mail session. To protect your security, you must login again.</p>
</blockquote>
<p>You&rsquo;ll normally have this variable in /etc/psa-horde/horde/conf.php:</p>
<p><code># $conf['auth']['checkip'] = true;</code></p>
<p>You can disable this ip check functionality which breaks sessions for AOL users by setting it to false:</p>
<p><code># $conf['auth']['checkip'] = false;</code></p>
]]></content></item><item><title>Counting open files per user</title><link>https://major.io/2007/09/26/counting-open-files-per-user/</link><pubDate>Wed, 26 Sep 2007 17:13:44 +0000</pubDate><guid>https://major.io/2007/09/26/counting-open-files-per-user/</guid><description>In the event that your system is running out of file descriptors, or you simply want to know what your users are doing, you can review their count of open files by running this command:
lsof | grep ' root ' | awk '{print $NF}' | sort | wc -l
Of course, if you want to drop the count and show the actual processes, you can run:
lsof | grep ' root '</description><content type="html"><![CDATA[<p>In the event that your system is running out of file descriptors, or you simply want to know what your users are doing, you can review their count of open files by running this command:</p>
<p><code>lsof | grep ' root ' | awk '{print $NF}' | sort | wc -l</code></p>
<p>Of course, if you want to drop the count and show the actual processes, you can run:</p>
<p><code>lsof | grep ' root '</code></p>
]]></content></item><item><title>A week’s vacation</title><link>https://major.io/2007/09/25/weeks-vacation/</link><pubDate>Wed, 26 Sep 2007 03:26:23 +0000</pubDate><guid>https://major.io/2007/09/25/weeks-vacation/</guid><description>So, you might be saying: &amp;ldquo;where the heck are the updates for RackerHacker lately?&amp;rdquo;
Don&amp;rsquo;t worry, I&amp;rsquo;m still alive! Some work and personal obligations kept me away from the blogging for a week, but I&amp;rsquo;ll be hard at work finding new things for you as soon as I can!</description><content type="html"><![CDATA[<p>So, you might be saying: &ldquo;where the heck are the updates for RackerHacker lately?&rdquo;</p>
<p>Don&rsquo;t worry, I&rsquo;m still alive! Some work and personal obligations kept me away from the blogging for a week, but I&rsquo;ll be hard at work finding new things for you as soon as I can!</p>
]]></content></item><item><title>Boxcheck.com improvements</title><link>https://major.io/2007/09/18/boxcheckcom-improvements/</link><pubDate>Tue, 18 Sep 2007 22:57:31 +0000</pubDate><guid>https://major.io/2007/09/18/boxcheckcom-improvements/</guid><description>I&amp;rsquo;ve been hard at work on the boxcheck.com site, and there&amp;rsquo;s plenty of improvements. The new interface is quicker, easier to use, and it uses ajax for a more Web 2.0 feel (can&amp;rsquo;t believe I just said Web 2.0).
Some beta testers are already giving it a whirl, and I hope to have it released to the public later next week!</description><content type="html"><![CDATA[<p>I&rsquo;ve been hard at work on the <a href="http://boxcheck.com">boxcheck.com</a> site, and there&rsquo;s plenty of improvements. The new interface is quicker, easier to use, and it uses ajax for a more Web 2.0 feel (can&rsquo;t believe I just said Web 2.0).</p>
<p>Some beta testers are already giving it a whirl, and I hope to have it released to the public later next week!</p>
]]></content></item><item><title>Adjusting postfix queue time / lifetime</title><link>https://major.io/2007/09/18/adjusting-postfix-queue-time-lifetime/</link><pubDate>Tue, 18 Sep 2007 22:55:38 +0000</pubDate><guid>https://major.io/2007/09/18/adjusting-postfix-queue-time-lifetime/</guid><description>If you want to adjust how long postfix will hold a piece of undeliverable mail in its queue, just adjust bounce_queue_lifetime. This variable is normally set to five days by default, but you can adjust it to any amount that you wish. You can set the value to zero, but that will cause e-mails that cannot be immediately sent to be rejected to their senders.
Postfix Configuration Parameters: bounce_queue_lifetime</description><content type="html"><![CDATA[<p>If you want to adjust how long postfix will hold a piece of undeliverable mail in its queue, just adjust <strong>bounce_queue_lifetime</strong>. This variable is normally set to five days by default, but you can adjust it to any amount that you wish. You can set the value to zero, but that will cause e-mails that cannot be immediately sent to be rejected to their senders.</p>
<p><a href="http://www.postfix.org/postconf.5.html#bounce_queue_lifetime">Postfix Configuration Parameters: bounce_queue_lifetime</a></p>
]]></content></item><item><title>Yum equivalents of up2date arguments</title><link>https://major.io/2007/09/17/yum-equivalents-of-up2date-arguments/</link><pubDate>Mon, 17 Sep 2007 22:50:01 +0000</pubDate><guid>https://major.io/2007/09/17/yum-equivalents-of-up2date-arguments/</guid><description>With RHEL 5 ditching up2date for yum, many Red Hat users might find themselves confused with the new command line flags. Red Hat has published a document detailing the new changes and their old counterparts.
Red Hat Knowledgebase: What are the yum equivalents of former up2date common tasks?</description><content type="html"><![CDATA[<p>With RHEL 5 ditching up2date for yum, many Red Hat users might find themselves confused with the new command line flags. Red Hat has published a document detailing the new changes and their old counterparts.</p>
<p><a href="http://kbase.redhat.com/faq/FAQ_80_11223.shtm">Red Hat Knowledgebase: What are the yum equivalents of former up2date common tasks?</a></p>
]]></content></item><item><title>Testing SpamAssassin with GTUBE</title><link>https://major.io/2007/09/15/testing-spamassassin-with-gtube/</link><pubDate>Sat, 15 Sep 2007 17:14:24 +0000</pubDate><guid>https://major.io/2007/09/15/testing-spamassassin-with-gtube/</guid><description>If you have SpamAssassin installed, but you want to make sure that it is marking or filtering your e-mails, simply send an e-mail which contains the special line provided here:
http://spamassassin.apache.org/gtube/gtube.txt
SpamAssassin will always mark e-mails that contain this special line as spam:
XJS*C4JDBQADN1.NSBN3*2IDNEN*GTUBE-STANDARD-ANTI-UBE-TEST-EMAIL*C.34X</description><content type="html"><![CDATA[<p>If you have SpamAssassin installed, but you want to make sure that it is marking or filtering your e-mails, simply send an e-mail which contains the special line provided here:</p>
<p><a href="http://spamassassin.apache.org/gtube/gtube.txt">http://spamassassin.apache.org/gtube/gtube.txt</a></p>
<p>SpamAssassin will always mark e-mails that contain this special line as spam:</p>
<p><code>XJS*C4JDBQADN1.NSBN3*2IDNEN*GTUBE-STANDARD-ANTI-UBE-TEST-EMAIL*C.34X</code></p>
]]></content></item><item><title>Check the modulus of an SSL certificate and key with openssl</title><link>https://major.io/2007/09/14/check-the-modulus-of-an-ssl-certificate-and-key-with-openssl/</link><pubDate>Fri, 14 Sep 2007 17:13:51 +0000</pubDate><guid>https://major.io/2007/09/14/check-the-modulus-of-an-ssl-certificate-and-key-with-openssl/</guid><description>When you create a CSR and private key to obtain an SSL certificate, the private key has some internal data called a modulus. This is integral to the security of your SSL encryption, but for this specific post, we will focus on one specific aspect.
If your private key and certificate do not contain the same modulus, then Apache will sometimes refuse to start or it may not respond properly to SSL requests.</description><content type="html"><![CDATA[<p>When you create a CSR and private key to obtain an SSL certificate, the private key has some internal data called a modulus. This is integral to the security of your SSL encryption, but for this specific post, we will focus on one specific aspect.</p>
<p>If your private key and certificate do not contain the same modulus, then Apache will sometimes refuse to start or it may not respond properly to SSL requests. You can check the modulus of your private key and SSL certificate with these commands:</p>
<pre><code># openssl rsa -noout -modulus -in server.key | openssl md5
# openssl x509 -noout -modulus -in server.crt | openssl md5
</code></pre><p>If the MD5 checksums match, then the certificate and key will work together. However, if they are different, then you cannot use them together. Generally, this means that you used the wrong CSR (that corresponded to some other private key) when you obtained/created your SSL certificate.</p>
]]></content></item><item><title>Slow IMAP and POP3 performance with large mailboxes on RHEL 2.1</title><link>https://major.io/2007/09/12/slow-imap-and-pop3-performance-with-large-mailboxes-on-rhel-21/</link><pubDate>Wed, 12 Sep 2007 13:00:32 +0000</pubDate><guid>https://major.io/2007/09/12/slow-imap-and-pop3-performance-with-large-mailboxes-on-rhel-21/</guid><description>By default, Red Hat Enterprise Linux 2.1 comes with UW-IMAP which runs from xinetd. This is fine for most users, but when mailbox sizes creep upwards of 500MB, you may notice odd performance degradations and undelivered mail.
This is because UW-IMAP only supports mbox files in RHEL 2.1. This means your e-mail ends up in one big file which has each e-mail listed one after another. This is a simple way to handle mail, but it scales in a horrible fashion.</description><content type="html"><![CDATA[<p>By default, Red Hat Enterprise Linux 2.1 comes with UW-IMAP which runs from xinetd. This is fine for most users, but when mailbox sizes creep upwards of 500MB, you may notice odd performance degradations and undelivered mail.</p>
<p>This is because UW-IMAP only supports <a href="http://en.wikipedia.org/wiki/Mbox">mbox</a> files in RHEL 2.1. This means your e-mail ends up in one big file which has each e-mail listed one after another. This is a simple way to handle mail, but it scales in a horrible fashion.</p>
<p><a href="http://en.wikipedia.org/wiki/Daniel_J._Bernstein">Daniel Bernstein</a>, the creator of <a href="http://en.wikipedia.org/wiki/Qmail">qmail</a>, created <a href="http://en.wikipedia.org/wiki/Maildir">maildir</a>, and (as much as I hate anything relating to qmail) it&rsquo;s the best method for storing mail that I&rsquo;ve seen so far.</p>
<p>Mbox files are slower because the entire file must be scanned when the POP or IMAP daemon receive a request for an e-mail held within it. That means that the daemon must scan through all of the e-mails until the one that it wants is found. If sendmail wants to drop off e-mail for the user, it has to wait since the mail spool is locked. If it can&rsquo;t deliver the e-mail, it may bounce it after a period of time.</p>
<p>This is especially awful if a user receives a fair amount of e-mail and checks their e-mail from a mobile device. This means that their computer and the mobile device are making the mail daemons scan the mbox file repeatedly when they check in. It causes sendmail to back up, disk I/O skyrockets, and the server performance as a whole can suffer.</p>
<p>The solution is to move to a newer version of RHEL, hopefully RHEL 4 or 5 where Postfix and maildir support are available. The only fix on RHEL 2.1 is to ask the user to clear out their mailbox to reduce the amount of disk I/O required to pick up e-mail.</p>
]]></content></item><item><title>RHEL limitations cheat sheet</title><link>https://major.io/2007/09/11/rhel-limitations-cheat-sheet/</link><pubDate>Wed, 12 Sep 2007 01:01:49 +0000</pubDate><guid>https://major.io/2007/09/11/rhel-limitations-cheat-sheet/</guid><description>When you find yourself in a pinch, and you don&amp;rsquo;t know the limits of a certain Red Hat Enterprise Linux version, you can find this information in one place. Whether you want to know RHEL&amp;rsquo;s CPU or memory limitations, you can find them here:
http://www.redhat.com/rhel/compare/</description><content type="html"><![CDATA[<p>When you find yourself in a pinch, and you don&rsquo;t know the limits of a certain Red Hat Enterprise Linux version, you can find this information in one place. Whether you want to know RHEL&rsquo;s CPU or memory limitations, you can find them here:</p>
<p><a href="http://www.redhat.com/rhel/compare/">http://www.redhat.com/rhel/compare/</a></p>
]]></content></item><item><title>Getting GrowlMail working with Apple Mail in Growl 1.1</title><link>https://major.io/2007/09/09/getting-growlmail-working-with-apple-mail-in-growl-11/</link><pubDate>Mon, 10 Sep 2007 04:44:10 +0000</pubDate><guid>https://major.io/2007/09/09/getting-growlmail-working-with-apple-mail-in-growl-11/</guid><description>So, this is not really related to the normal system administration topics discussed here, but it&amp;rsquo;s Sunday, so I feel like something different.
I downloaded the new Growl 1.1 tonight and I wanted to install GrowlMail to get mail notifications from Apple Mail. I went through the package installer, started Mail, and nothing happened. The preference pane didn&amp;rsquo;t exist either. After doing a bit of forum digging, I found these two commands to run in the terminal:</description><content type="html"><![CDATA[<p>So, this is not really related to the normal system administration topics discussed here, but it&rsquo;s Sunday, so I feel like something different.</p>
<p>I downloaded the new <a href="http://growl.info">Growl 1.1</a> tonight and I wanted to install GrowlMail to get mail notifications from Apple Mail. I went through the package installer, started Mail, and nothing happened. The preference pane didn&rsquo;t exist either. After doing a bit of <a href="http://forums.cocoaforge.com/viewtopic.php?p=90671#90671">forum digging</a>, I found these two commands to run in the terminal:</p>
<pre><code>defaults write com.apple.mail EnableBundles 1
defaults write com.apple.mail BundleCompatibilityVersion 2
</code></pre><p>It worked like a charm and I was all set. If you haven&rsquo;t tried it out yet, <a href="http://growl.info/downloads.php">download the new Growl 1.1</a> and install it. There&rsquo;s a ton of new features, and it&rsquo;s been worth the wait.</p>
]]></content></item><item><title>Hunting down annoying web spiders</title><link>https://major.io/2007/09/08/hunting-down-annoying-web-spiders/</link><pubDate>Sat, 08 Sep 2007 22:16:07 +0000</pubDate><guid>https://major.io/2007/09/08/hunting-down-annoying-web-spiders/</guid><description>We all enjoy having the GoogleBot and other search engine robots index our sites as it brings us higher on search engines, but it&amp;rsquo;s annoying when some user scrapes your site for their own benefit. This is especially bad on forum sites as they&amp;rsquo;re always a target, and it can severely impact server performance.
To hunt down these connections when the spidering is happening, simply run this command:
netstat -plan | grep :80 | awk '{print $5}' | sed 's/:.</description><content type="html"><![CDATA[<p>We all enjoy having the GoogleBot and other search engine robots index our sites as it brings us higher on search engines, but it&rsquo;s annoying when some user scrapes your site for their own benefit. This is especially bad on forum sites as they&rsquo;re always a target, and it can severely impact server performance.</p>
<p>To hunt down these connections when the spidering is happening, simply run this command:</p>
<p><code>netstat -plan | grep :80 | awk '{print $5}' | sed 's/:.*$//' | sort | uniq -c | sort -rn</code></p>
<p>The IP&rsquo;s that are making the most connections will appear at the top of the list, and from there, you can find out which unwelcome spider is scraping your site.</p>
]]></content></item><item><title>MySQL binary log rotation</title><link>https://major.io/2007/09/07/mysql-binary-log-rotation/</link><pubDate>Fri, 07 Sep 2007 22:07:17 +0000</pubDate><guid>https://major.io/2007/09/07/mysql-binary-log-rotation/</guid><description>If you&amp;rsquo;ve run MySQL in a replication environment, or if you&amp;rsquo;ve enabled binary logging for transactional integrity, you know that the binary logs can grow rather quickly. The only safe way to delete the logs is to use PURGE MASTER LOGS in MySQL, but if you want MySQL to automatically remove the logs after a certain period of time, add this in your my.cnf:
expire_logs_days = 14 5.11.3. The Binary Log</description><content type="html"><![CDATA[<p>If you&rsquo;ve run MySQL in a replication environment, or if you&rsquo;ve enabled binary logging for transactional integrity, you know that the binary logs can grow rather quickly. The only safe way to delete the logs is to use <a href="http://dev.mysql.com/doc/refman/5.0/en/purge-master-logs.html">PURGE MASTER LOGS</a> in MySQL, but if you want MySQL to automatically remove the logs after a certain period of time, add this in your my.cnf:</p>
<pre><code>expire_logs_days = 14
</code></pre><p><a href="http://dev.mysql.com/doc/refman/5.0/en/binary-log.html">5.11.3. The Binary Log</a></p>
]]></content></item><item><title>MySQLTuner minor update</title><link>https://major.io/2007/09/06/mysqltuner-minor-update/</link><pubDate>Fri, 07 Sep 2007 01:41:14 +0000</pubDate><guid>https://major.io/2007/09/06/mysqltuner-minor-update/</guid><description>I rolled out a new MySQLTuner update tonight and made the following changes:
Added aborted connection checks Added % reads/writes counts Adjusted recommendations for slow query logging, max seeks per key, and joins without indexes Added licensing data Added props for Matthew Montgomery Coming up soon is InnoDB support and additional per-thread buffer checks.</description><content type="html"><![CDATA[<p>I rolled out a new <a href="http://rackerhacker.com/mysqltuner/">MySQLTuner</a> update tonight and made the following changes:</p>
<ul>
<li>Added aborted connection checks</li>
<li>Added % reads/writes counts</li>
<li>Adjusted recommendations for slow query logging, max seeks per key, and joins without indexes</li>
<li>Added licensing data</li>
<li>Added props for Matthew Montgomery</li>
</ul>
<p>Coming up soon is InnoDB support and additional per-thread buffer checks.</p>
]]></content></item><item><title>Low priority Plesk backups</title><link>https://major.io/2007/09/05/low-priority-plesk-backups/</link><pubDate>Thu, 06 Sep 2007 03:27:09 +0000</pubDate><guid>https://major.io/2007/09/05/low-priority-plesk-backups/</guid><description>I hear a lot of complaints about Plesk&amp;rsquo;s backup routines and how they can bring a server to its knees. You can reduce the load (except for mysqldumps) by renicing pleskbackup. If you want something really handy, use this Perl scriptlet that I wrote:
#!/usr/bin/perl @domains = `ls /var/www/vhosts/ | egrep -v '^default\$|^chroot\$'`; $today = `date +%m%d%y`; foreach $domain (@domains) { chomp($domain); $cmd = &amp;quot;nice -n 19 /usr/local/psa/bin/pleskbackup -vv domains $domain --skip-logs - | ssh someuser\@somehost -i /home/username/.</description><content type="html"><![CDATA[<p>I hear a lot of complaints about Plesk&rsquo;s backup routines and how they can bring a server to its knees. You can reduce the load (except for mysqldumps) by renicing pleskbackup. If you want something really handy, use this Perl scriptlet that I wrote:</p>
<pre><code>#!/usr/bin/perl
@domains = `ls /var/www/vhosts/ | egrep -v '^default\$|^chroot\$'`;
$today = `date +%m%d%y`;
foreach $domain (@domains) {
	chomp($domain);
	$cmd = &quot;nice -n 19 /usr/local/psa/bin/pleskbackup -vv domains $domain --skip-logs - | ssh someuser\@somehost -i /home/username/.ssh/id_rsa \&quot;dd of=/home/username/pleskbackups/$domain-$today.dump\&quot;&quot;;
	`$cmd`;
}
</code></pre><p>It will transmit your backups to another server via SSH, and it will reduce the priority to the lowest available. This combination will reduce CPU usage and disk I/O throughout the backup.</p>
]]></content></item><item><title>Happy Labor Day</title><link>https://major.io/2007/09/03/happy-labor-day/</link><pubDate>Tue, 04 Sep 2007 03:49:58 +0000</pubDate><guid>https://major.io/2007/09/03/happy-labor-day/</guid><description>In the spirit of Labor Day, there will be no new items today. Plus, I worked today, and I&amp;rsquo;m tired. :-)</description><content type="html"><![CDATA[<p>In the spirit of <a href="http://en.wikipedia.org/wiki/Labor_day">Labor Day</a>, there will be no new items today. Plus, I worked today, and I&rsquo;m tired. :-)</p>
]]></content></item><item><title>MySQL and InnoDB: Orphaned .frm files</title><link>https://major.io/2007/09/01/mysql-and-innodb-orphaned-frm-files/</link><pubDate>Sun, 02 Sep 2007 01:52:00 +0000</pubDate><guid>https://major.io/2007/09/01/mysql-and-innodb-orphaned-frm-files/</guid><description>If an .frm file that corresponds to an InnoDB table gets deleted without using DROP TABLE, MySQL won&amp;rsquo;t let you create a new table with the same name. You&amp;rsquo;ll find this in the error log:
InnoDB: Error: table test/parent already exists in InnoDB internal InnoDB: data dictionary. Have you deleted the .frm file InnoDB: and not used DROP TABLE? Have you used DROP DATABASE InnoDB: for InnoDB tables in MySQL version &amp;lt;= 3.</description><content type="html"><![CDATA[<p>If an .frm file that corresponds to an InnoDB table gets deleted without using DROP TABLE, MySQL won&rsquo;t let you create a new table with the same name. You&rsquo;ll find this in the error log:</p>
<pre><code>InnoDB: Error: table test/parent already exists in InnoDB internal
InnoDB: data dictionary. Have you deleted the .frm file
InnoDB: and not used DROP TABLE? Have you used DROP DATABASE
InnoDB: for InnoDB tables in MySQL version &lt;= 3.23.43?
InnoDB: See the Restrictions section of the InnoDB manual.
InnoDB: You can drop the orphaned table inside InnoDB by
InnoDB: creating an InnoDB table with the same name in another
InnoDB: database and moving the .frm file to the current database.
InnoDB: Then MySQL thinks the table exists, and DROP TABLE will
InnoDB: succeed.
</code></pre><p>Luckily, the error tells you exactly how to fix the problem! Simply make a new database and create a table that matches your old .frm file. Stop MySQL, move the .frm file from the new database&rsquo;s directory back to the old database&rsquo;s directory. Start MySQL, and then run DROP TABLE like normal.</p>
<p>This will remove the table from the ibdata tablespace file and allow you to create a new table with the same name.</p>
<p>Further reading:</p>
<p><a href="http://dev.mysql.com/doc/refman/5.0/en/innodb-troubleshooting-datadict.html">13.2.17.1. Troubleshooting InnoDB Data Dictionary Operations</a></p>
]]></content></item><item><title>Postfix: 554 Relay access denied</title><link>https://major.io/2007/08/30/postfix-554-relay-access-denied/</link><pubDate>Fri, 31 Aug 2007 01:06:02 +0000</pubDate><guid>https://major.io/2007/08/30/postfix-554-relay-access-denied/</guid><description>Let&amp;rsquo;s say you have a user who can&amp;rsquo;t receive e-mail. Each time they send a message to the server, this pops up in the mail logs:
postfix/smtpd[23897]: NOQUEUE: reject: RCPT from remotemailserver.com[10.0.0.2]: 554 &amp;lt;user@domain.com&amp;gt;: Relay access denied; from=&amp;lt;user@otherdomain.com&amp;gt; to=&amp;lt;user@domain.com&amp;gt; proto=ESMTP helo=&amp;lt;remotemailserver.com&amp;gt; This is happening because Postfix is receiving e-mail for a domain for which it doesn&amp;rsquo;t expect to handle mail. Add the domains to the mydestination parameter in /etc/postfix/main.cf:
mydestination = domain.</description><content type="html"><![CDATA[<p>Let&rsquo;s say you have a user who can&rsquo;t receive e-mail. Each time they send a message to the server, this pops up in the mail logs:</p>
<pre><code>postfix/smtpd[23897]: NOQUEUE: reject: RCPT from remotemailserver.com[10.0.0.2]: 554 &lt;user@domain.com&gt;: Relay access denied; from=&lt;user@otherdomain.com&gt; to=&lt;user@domain.com&gt; proto=ESMTP helo=&lt;remotemailserver.com&gt;
</code></pre><p>This is happening because Postfix is receiving e-mail for a domain for which it doesn&rsquo;t expect to handle mail. Add the domains to the <code>mydestination</code> parameter in /etc/postfix/main.cf:</p>
<pre><code>mydestination = domain.com, domain2.com, domain3.com
</code></pre><p>If you have a lot of domains to add, create a mydomains hash file and change the <code>mydestination</code> parameter:</p>
<pre><code>mydestination = hash:/etc/postfix/mydomains
</code></pre><p>Create /etc/postfix/mydomains:</p>
<pre><code>localhost               OK
localmailserver.com     OK
domain.com              OK
</code></pre><p>Then run:</p>
<pre><code># postmap /etc/postfix/mydomains
</code></pre><p>This will create the hash file (mydomains.db) within /etc/postfix. If you&rsquo;ve just added the directive to the main.cf, run <code>postfix reload</code>. However, if the directive was already there, but you just adjusted the mydomains and ran postmap, then there is nothing left to do.</p>
]]></content></item><item><title>Apache: Disable TRACE and TRACK methods</title><link>https://major.io/2007/08/28/apache-disable-trace-and-track-methods/</link><pubDate>Wed, 29 Aug 2007 00:27:59 +0000</pubDate><guid>https://major.io/2007/08/28/apache-disable-trace-and-track-methods/</guid><description>Lots of PCI Compliance and vulnerability scan vendors will complain about TRACE and TRACK methods being enabled on your server. Since most providers run Nessus, you&amp;rsquo;ll see this fairly often. Here&amp;rsquo;s the rewrite rules to add:
RewriteEngine on RewriteCond %{REQUEST_METHOD} ^(TRACE|TRACK) RewriteRule .* - [F] These directives will need to be added to each VirtualHost.
Further reading:
Apache Debugging Guide</description><content type="html"><![CDATA[<p>Lots of PCI Compliance and vulnerability scan vendors will complain about TRACE and TRACK methods being enabled on your server. Since most providers run Nessus, you&rsquo;ll see this fairly often. Here&rsquo;s the rewrite rules to add:</p>
<pre><code>RewriteEngine on
RewriteCond %{REQUEST_METHOD} ^(TRACE|TRACK)
RewriteRule .* - [F]
</code></pre><p>These directives will need to be added to each VirtualHost.</p>
<p>Further reading:</p>
<p><a href="http://httpd.apache.org/dev/debugging.html">Apache Debugging Guide</a></p>
]]></content></item><item><title>Use a different IP for sending mail</title><link>https://major.io/2007/08/27/use-a-different-ip-for-sending-mail/</link><pubDate>Tue, 28 Aug 2007 03:13:21 +0000</pubDate><guid>https://major.io/2007/08/27/use-a-different-ip-for-sending-mail/</guid><description>If you find yourself in a pinch and you need a temporary fix when your primary IP is blacklisted, use the following iptables rule:
/sbin/iptables -t nat -A POSTROUTING -p tcp --dport 25 -j SNAT --to-source [desired outgoing ip] Keep in mind, however, that you will need to adjust any applicable SPF records for your domains since your e-mail will appear to be leaving via one of the secondary IP&amp;rsquo;s on your server.</description><content type="html"><![CDATA[<p>If you find yourself in a pinch and you need a temporary fix when your primary IP is blacklisted, use the following iptables rule:</p>
<pre><code>/sbin/iptables -t nat -A POSTROUTING -p tcp --dport 25 -j SNAT --to-source [desired outgoing ip]
</code></pre><p>Keep in mind, however, that you will need to adjust any applicable SPF records for your domains since your e-mail will appear to be leaving via one of the secondary IP&rsquo;s on your server. Also, remember that this is only a temporary fix - you should find out why you were blacklisted and eliminate that problem as soon as possible. :-)</p>
]]></content></item><item><title>DB function failed with error number 1033</title><link>https://major.io/2007/08/25/182/</link><pubDate>Sat, 25 Aug 2007 23:18:37 +0000</pubDate><guid>https://major.io/2007/08/25/182/</guid><description>One of these errors might appear on your website without warning:
Warning: DB function failed with error number 1033 Incorrect information in file: './database_name/table_name.frm' SQL=SELECT col1, col2 FROM table_name WHERE col3 = 'some_value' ORDER BY col1 ASC MySQL is telling you that the table structure it has within data files doesn&amp;rsquo;t match the structure in the .frm file that&amp;rsquo;s on the disk. There&amp;rsquo;s only a few scenarios where this can happen:</description><content type="html"><![CDATA[<p>One of these errors might appear on your website without warning:</p>
<pre><code>Warning:  DB function failed with error number 1033
Incorrect information in file: './database_name/table_name.frm' SQL=SELECT col1, col2 FROM table_name WHERE col3 = 'some_value' ORDER BY col1 ASC
</code></pre><p>MySQL is telling you that the table structure it has within data files doesn&rsquo;t match the structure in the <code>.frm</code> file that&rsquo;s on the disk. There&rsquo;s only a few scenarios where this can happen:</p>
<p><strong>Different version of the .frm files</strong></p>
<p>If the .frm files from an older or later version of the table are placed in MySQL&rsquo;s data directory, MySQL will become confused and it won&rsquo;t be able to determine the proper database structure.</p>
<p><strong>Pending table alteration</strong></p>
<p>A pending database operation that ran an <code>ALTER TABLE</code> may not have written changes to the disk. MySQL may have stopped running abruptly or the entire server may have crashed. The normal operation for MySQL is to make changes in memory first and then perform disk operations.</p>
<p><strong>Complete wierdness</strong></p>
<p>I cannot explain it, and I can&rsquo;t figure out the logic that would allow it to happen, but some web application vulnerabilities can cause this problem. I&rsquo;ve seen it happen with Joomla! sites running on fairly secure servers, and there was no Apache privilege escalation used to modify the .frm files directly.</p>
<p>How is it fixed? The only way to repair it is to import the table again from a mysqldump backup, find the correct .frm file and restore it on the server, or run an <code>ALTER TABLE</code> to bring the table back to its original state.</p>
]]></content></item><item><title>Apache: No space left on device: Couldn’t create accept lock</title><link>https://major.io/2007/08/24/apache-no-space-left-on-device-couldnt-create-accept-lock/</link><pubDate>Fri, 24 Aug 2007 21:55:30 +0000</pubDate><guid>https://major.io/2007/08/24/apache-no-space-left-on-device-couldnt-create-accept-lock/</guid><description>This error completely stumped me a couple of weeks ago. Apparently someone was adjusting the Apache configuration, then they checked their syntax and attempted to restart Apache. It went down without a problem, but it refused to start properly, and didn&amp;rsquo;t bind to any ports.
Within the Apache error logs, this message appeared over and over:
Apache is basically saying “I want to start, but I need to write some things down before I can start, and I have nowhere to write them!</description><content type="html"><![CDATA[<p>This error completely stumped me a couple of weeks ago. Apparently someone was adjusting the Apache configuration, then they checked their syntax and attempted to restart Apache. It went down without a problem, but it refused to start properly, and didn&rsquo;t bind to any ports.</p>
<p>Within the Apache error logs, this message appeared over and over:</p>
<!-- raw HTML omitted -->
<p>Apache is basically saying “I want to start, but I need to write some things down before I can start, and I have nowhere to write them!” If this happens to you, check these items in order:</p>
<p><strong>1. Check your disk space</strong></p>
<p>This comes first because it&rsquo;s the easiest to check, and sometimes the quickest to fix. If you&rsquo;re out of disk space, then you need to fix that problem. :-)</p>
<p><strong>2. Review filesystem quotas</strong></p>
<p>If your filesystem uses quotas, you might be reaching a quota limit rather than a disk space limit. Use <code>repquota /</code> to review your quotas on the root partition. If you&rsquo;re at the limit, raise your quota or clear up some disk space. Apache logs are usually the culprit in these situations.</p>
<p><strong>3. Clear out your active semaphores</strong></p>
<p>Semaphores? What the heck is a semaphore? Well, it&rsquo;s actually an <a href="http://en.wikipedia.org/wiki/Semaphore">apparatus for conveying information by means of visual signals</a>. But, when it comes to programming, <a href="http://en.wikipedia.org/wiki/Semaphore_%28programming%29">semaphores are used for communicating between the active processes of a certain application</a>. In the case of Apache, they&rsquo;re used to communicate between the parent and child processes. If Apache can&rsquo;t write these things down, then it can&rsquo;t communicate properly with all of the processes it starts.</p>
<p>I&rsquo;d assume if you&rsquo;re reading this article, Apache has stopped running. Run this command as root:</p>
<!-- raw HTML omitted -->
<p>If you see a list of semaphores, Apache has not cleaned up after itself, and some semaphores are stuck. Clear them out with this command:</p>
<!-- raw HTML omitted -->
<p>Now, in almost all cases, Apache should start properly. If it doesn&rsquo;t, you may just be completely out of available semaphores. You may want to increase your available semaphores, and you&rsquo;ll need to tickle your kernel to do so. Add this to /etc/sysctl.conf:</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>And then run <code>sysctl -p</code> to pick up the new changes.</p>
<p>Further reading:</p>
<p><a href="http://en.wikipedia.org/wiki/Semaphore_%28programming%29">Wikipedia: Semaphore (Programming)</a></p>
<p><a href="http://www.webpipe.net/howto/Apache_accept_lock_fix">Apache accept lock fix</a></p>
]]></content></item><item><title>MySQL couldn’t find log file</title><link>https://major.io/2007/08/23/mysql-couldnt-find-log-file/</link><pubDate>Fri, 24 Aug 2007 00:24:28 +0000</pubDate><guid>https://major.io/2007/08/23/mysql-couldnt-find-log-file/</guid><description>This error will pop up when binary logging is enabled, and someone thought it was a good idea to remove binary logs from the filesystem:
/usr/sbin/mysqld: File './mysql_bin.000025' not found (Errcode: 2) [ERROR] Failed to open log (file './9531_mysql_bin.000025', errno 2) [ERROR] Could not open log file [ERROR] Can't init tc log [ERROR] Aborting InnoDB: Starting shutdown... InnoDB: Shutdown completed; log sequence number 0 2423986213 [Note] /usr/sbin/mysqld: Shutdown complete Basically, MySQL is looking in the mysql-bin.</description><content type="html"><![CDATA[<p>This error will pop up when binary logging is enabled, and someone thought it was a good idea to remove binary logs from the filesystem:</p>
<pre><code>/usr/sbin/mysqld: File './mysql_bin.000025' not found (Errcode: 2)
[ERROR] Failed to open log (file './9531_mysql_bin.000025', errno 2)
[ERROR] Could not open log file
[ERROR] Can't init tc log
[ERROR] Aborting
</code></pre><pre><code>InnoDB: Starting shutdown...
InnoDB: Shutdown completed; log sequence number 0 2423986213
[Note] /usr/sbin/mysqld: Shutdown complete
</code></pre><p>Basically, MySQL is looking in the <strong>mysql-bin.index</strong> file and it cannot find the log files that are listed within the index. This will keep MySQL from starting, but the fix is quick and easy. You have two options:</p>
<p><strong>Edit the index file</strong></p>
<p>You can edit the mysql-bin.index file in a text editor of your choice and remove the references to any logs which don&rsquo;t exist on the filesystem any longer. Once you&rsquo;re done, save the index file and start MySQL.</p>
<p><strong>Take away the index file</strong></p>
<p>Move or delete the index file and start MySQL. This will cause MySQL to reset its binary log numbering scheme, so if this is important to you, you may want to choose the previous option.</p>
<p>So how do you prevent this from happening? Use the <code>PURGE MASTER LOGS</code> statement and allow MySQL to delete its logs on its own terms. If you&rsquo;re concerned about log files piling up, adjust the <code>expire_logs_days</code> variable in your /etc/my.cnf.</p>
<p>Further reading:</p>
<p><a href="http://dev.mysql.com/doc/refman/5.0/en/purge-master-logs.html">12.6.1.1. PURGE MASTER LOGS Syntax</a></p>
<p><a href="http://dev.mysql.com/doc/refman/5.0/en/server-system-variables.html">5.2.3 System Variables</a></p>
]]></content></item><item><title>POP3 server disconnects immediately after login</title><link>https://major.io/2007/08/22/pop3-server-disconnects-immediately-after-login/</link><pubDate>Thu, 23 Aug 2007 03:54:52 +0000</pubDate><guid>https://major.io/2007/08/22/pop3-server-disconnects-immediately-after-login/</guid><description>When connecting to your server&amp;rsquo;s POP3 service, your client might provide this error just after authentication:
The connection to the server was interrupted.
Your best bet is to check the mail log and see exactly what the problem is:
web pop3-login: Login: john [192.168.0.5] pop3(john): Invalid mbox file /var/spool/mail/john: No such file or directory pop3(john): Failed to create storage with data: mbox:/var/spool/mail/john dovecot: child 29864 (pop3) returned error 89 In this case, the mbox file has become corrupt (possible from malformed &amp;lsquo;From&amp;rsquo; headers).</description><content type="html"><![CDATA[<p>When connecting to your server&rsquo;s POP3 service, your client might provide this error just after authentication:</p>
<p><code>The connection to the server was interrupted.</code></p>
<p>Your best bet is to check the mail log and see exactly what the problem is:</p>
<pre><code>web pop3-login: Login: john [192.168.0.5]
pop3(john): Invalid mbox file /var/spool/mail/john: No such file or directory
pop3(john): Failed to create storage with data: mbox:/var/spool/mail/john
dovecot: child 29864 (pop3) returned error 89
</code></pre><p>In this case, the mbox file has become corrupt (possible from malformed &lsquo;From&rsquo; headers). You have the option of repairing the issues within the file, or you can simply create a new mail spool for the user.</p>
]]></content></item><item><title>Qmail-smtpd spawns many processes and uses 100% of CPU</title><link>https://major.io/2007/08/21/qmail-smtpd-spawns-many-processes-and-uses-100-of-cpu/</link><pubDate>Wed, 22 Aug 2007 02:47:18 +0000</pubDate><guid>https://major.io/2007/08/21/qmail-smtpd-spawns-many-processes-and-uses-100-of-cpu/</guid><description>It&amp;rsquo;s not abnormal for qmail act oddly at times with Plesk, and sometimes it can use 100% of the CPU. However, if you find qmail&amp;rsquo;s load to be higher than usual with a small volume of mail, there may be a fix that you need.
First off, check for two files in /var/qmail/control called dh512.pem and dh1024.pem. If they are present, well, then this article won&amp;rsquo;t be able to help you.</description><content type="html"><![CDATA[<p>It&rsquo;s not abnormal for qmail act oddly at times with Plesk, and sometimes it can use 100% of the CPU. However, if you find qmail&rsquo;s load to be higher than usual with a small volume of mail, there may be a fix that you need.</p>
<p>First off, check for two files in <strong>/var/qmail/control</strong> called <strong>dh512.pem</strong> and <strong>dh1024.pem</strong>. If they are present, well, then this article won&rsquo;t be able to help you. You have a different issue that is causing increased CPU load (check for swap usage and upgrade your disk&rsquo;s speed).</p>
<p>If the files aren&rsquo;t there, do the following:</p>
<pre><code># cd /var/qmail/control
# cp dhparam512.pem dh512.pem
# cp dhparam1024.pem dh1024.pem
# /etc/init.d/qmail restart
# /etc/init.d/xinetd restart
</code></pre><p>At this point, your CPU load should be reduced once the currently running processes for qmail clear out.</p>
<p>So why is this fix required? Without dh512.pem and dh1024.pem, qmail has to create certificate and key pairs when other mail servers or mail users connect to qmail via TLS. If qmail is forced to generate them on the fly, you will get a big performance hit, and your load will be much higher than it could be. By copying the dhparam files over, you will pre-populate the SSL key and certificate for qmail to use, and all it has to do is pick it up off the file system rather than regenerating it each time.</p>
<p>Further reading:</p>
<p><a href="http://forum.swsoft.com/printthread.php?threadid=40173">SWsoft Forums: Qmail-smtpd spawning many processes, using full cpu</a></p>
]]></content></item><item><title>Change Plesk back to short mail names</title><link>https://major.io/2007/08/20/change-plesk-back-to-short-mail-names/</link><pubDate>Tue, 21 Aug 2007 00:46:50 +0000</pubDate><guid>https://major.io/2007/08/20/change-plesk-back-to-short-mail-names/</guid><description>If you have to use short e-mail usernames in Plesk (which is a bad idea), and someone accidentally sets the server to use full usernames, you can force Plesk to go back. You can&amp;rsquo;t do this in the interface, however. Plesk realizes that duplicate mail names exist, and it wont allow the change.
Plesk will say something like:
Unable to allow the use of short mail names for POP3/IMAP accounts. There are mail names matching the encrypted passwords.</description><content type="html"><![CDATA[<p>If you have to use short e-mail usernames in Plesk (which is a bad idea), and someone accidentally sets the server to use full usernames, you can force Plesk to go back. You can&rsquo;t do this in the interface, however. Plesk realizes that duplicate mail names exist, and it wont allow the change.</p>
<p>Plesk will say something like:</p>
<p><em>Unable to allow the use of short mail names for POP3/IMAP accounts. There are mail names matching the encrypted passwords.</em></p>
<p>Forcing it back is easy with one SQL statement:</p>
<pre><code># mysql -u admin -p`cat /etc/psa/.psa.shadow` psa
mysql&gt; UPDATE misc set val='enabled' where param='allow_short_pop3_names';
</code></pre><p>Keep in mind that users logging in with shortnames will get into the same mailbox if they have the same username and password.</p>
<p>Additional reading:</p>
<p><a href="http://kb.swsoft.com/en/888?st=advc">How can I change back the option &ldquo;Use of short and full POP3/IMAP mail account names is allowed&rdquo; forcedly?</a></p>
]]></content></item><item><title>MySQL: Errcode: 24 when using LOCK TABLES</title><link>https://major.io/2007/08/19/mysql-errcode-24-when-using-lock-tables/</link><pubDate>Mon, 20 Aug 2007 03:07:30 +0000</pubDate><guid>https://major.io/2007/08/19/mysql-errcode-24-when-using-lock-tables/</guid><description>While running into MySQL&amp;rsquo;s open files limit will manifest itself into various error messages, this is the standard one that you&amp;rsquo;ll receive during a mysqldump:
mysqldump: Got error: 29: File './databasename/tablename.MYD' not found (Errcode: 24) when using LOCK TABLES&amp;lt;/pre&amp;gt; The best way to get to the bottom of the error is to find out what it means: $ perror 24 OS error code 24: Too many open filesThere&amp;rsquo;s two ways to fix the problem.</description><content type="html"><![CDATA[<p>While running into MySQL&rsquo;s open files limit will manifest itself into various error messages, this is the standard one that you&rsquo;ll receive during a mysqldump:</p>
<pre><code>mysqldump: Got error: 29: File './databasename/tablename.MYD' not found
(Errcode: 24) when using LOCK TABLES&lt;/pre&gt;

The best way to get to the bottom of the error is to find out what it means:

</code></pre><p>$ perror 24
OS error code  24:  Too many open files<!-- raw HTML omitted --></p>
<p>There&rsquo;s two ways to fix the problem. First, if you find that you only hit the limit during mysqldumps and never during normal database operation, just add <code>--single-transaction</code> to your mysqldump command line options. This will cause mysql to keep only one table open at a time.</p>
<p>However, if this happens while backups aren&rsquo;t running, you may want to increase the <code>open_files_limit</code> in your MySQL configuration file. By default, the variable is set to 1,024 open files.</p>
<p>For further reading:</p>
<p><a href="http://dev.mysql.com/doc/refman/5.0/en/server-system-variables.html#option_mysqld_open_files_limit">5.2.3. System Variables</a></p>
<p><a href="http://dev.mysql.com/doc/refman/5.0/en/mysqldump.html">7.13. mysqldump - A Database Backup Program</a></p>
]]></content></item><item><title>Issues with mysqldump and views in Plesk</title><link>https://major.io/2007/08/18/issues-with-mysqldump-and-views-in-plesk/</link><pubDate>Sat, 18 Aug 2007 17:40:00 +0000</pubDate><guid>https://major.io/2007/08/18/issues-with-mysqldump-and-views-in-plesk/</guid><description>By default, views in MySQL 5.x are created with a security definer set to the root user. However, Plesk drops the root user from MySQL and replaces it with the admin user. When this happens, your views cannot by dumped by mysqldump since the root user (the security definer for the view) doesn&amp;rsquo;t exist in the mysql.user table.
You receive an error similar to the following:
mysqldump: Couldn't execute 'SHOW FIELDS FROM `some_tablename`': There is no 'root'@'localhost' registered (1449) Usually, if you run a SHOW CREATE VIEW tablename, you&amp;rsquo;ll see something like this:</description><content type="html"><![CDATA[<p>By default, views in MySQL 5.x are created with a security definer set to the root user. However, Plesk drops the root user from MySQL and replaces it with the admin user. When this happens, your views cannot by dumped by mysqldump since the root user (the security definer for the view) doesn&rsquo;t exist in the mysql.user table.</p>
<p>You receive an error similar to the following:</p>
<pre><code>mysqldump: Couldn't execute 'SHOW FIELDS FROM `some_tablename`': There is no 'root'@'localhost' registered (1449)
</code></pre><p>Usually, if you run a <code>SHOW CREATE VIEW tablename</code>, you&rsquo;ll see something like this:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sql" data-lang="sql"><span style="color:#719e07">CREATE</span> ALGORITHM<span style="color:#719e07">=</span>UNDEFINED <span style="color:#719e07">DEFINER</span><span style="color:#719e07">=`</span>root<span style="color:#719e07">`@`</span>localhost<span style="color:#719e07">`</span> <span style="color:#719e07">SQL</span> <span style="color:#719e07">SECURITY</span> <span style="color:#719e07">DEFINER</span> <span style="color:#719e07">VIEW</span> <span style="color:#719e07">`</span>some_tablename<span style="color:#719e07">`</span> <span style="color:#719e07">AS</span> <span style="color:#719e07">select</span> <span style="color:#719e07">distinct</span> <span style="color:#719e07">`</span>some_database<span style="color:#719e07">`</span>.<span style="color:#719e07">`</span>some_tablename<span style="color:#719e07">`</span>.<span style="color:#719e07">`</span>some_column<span style="color:#719e07">`</span> <span style="color:#719e07">AS</span> <span style="color:#719e07">`</span><span style="color:#719e07">alias</span><span style="color:#719e07">`</span> <span style="color:#719e07">from</span> <span style="color:#719e07">`</span>some_tablename<span style="color:#719e07">`</span>
</code></pre></div><p>You have two options in this situation:</p>
<ul>
<li>Change the security definer for each of your views to &lsquo;admin&rsquo;@&lsquo;localhost&rsquo;. Any new views you create will need to be adjusted as well.</li>
<li>Create a root user in MySQL with the same privileges as the admin user and use the root user&rsquo;s login to run mysqldump.</li>
</ul>
]]></content></item><item><title>MySQL unauthenticated login pile-up</title><link>https://major.io/2007/08/16/mysql-unauthenticated-login-pile-up/</link><pubDate>Thu, 16 Aug 2007 12:14:21 +0000</pubDate><guid>https://major.io/2007/08/16/mysql-unauthenticated-login-pile-up/</guid><description>Sometimes MySQL&amp;rsquo;s process list will fill with unauthenticated login entries that look like this:
| 971 | unauthenticated user | xxx.xxx.xxx.xxx:35406 | NULL | Connect | NULL | login | NULL | Generally, this means one of two things are happening. First, this could be a brute force attack against your server from an external attacker. Be sure to firewall off access to port 3306 from the outside world or run MySQL with skip-networking in the /etc/my.</description><content type="html"><![CDATA[<p>Sometimes MySQL&rsquo;s process list will fill with unauthenticated login entries that look like this:</p>
<pre><code>|  971 | unauthenticated user | xxx.xxx.xxx.xxx:35406 | NULL        | Connect | NULL | login | NULL             |
</code></pre><p>Generally, this means one of two things are happening. First, this could be a brute force attack against your server from an external attacker. Be sure to firewall off access to port 3306 from the outside world or run MySQL with <code>skip-networking</code> in the /etc/my.cnf file, and that should curtail those login attempts quickly.</p>
<p>However, MySQL could be attempting to resolve the reverse DNS for each connection, and this definitely isn&rsquo;t necessary if your grant statements refer to remote machines' IP addresses rather than hostnames (as they should). In this case, add <code>skip-name-resolve</code> to your /etc/my.cnf and restart MySQL. These connection attempts should authenticate much faster, and they shouldn&rsquo;t pile up in the queue any longer.</p>
<p><strong>Note:</strong> Connections via sockets aren&rsquo;t affected by DNS resolution since sockets don&rsquo;t involve any networking access at all. If your web applications use &lsquo;localhost&rsquo; for their connection string, then MySQL won&rsquo;t bring DNS resolution into play whatsoever.</p>
<p>Recommended reading: <a href="http://dev.mysql.com/doc/refman/5.0/en/dns.html">6.5.9. How MySQL Uses DNS</a></p>
]]></content></item><item><title>It’s on Digg: Automated MySQL Performance Tuning Script</title><link>https://major.io/2007/08/14/automated-mysql-performance-tuning-script/</link><pubDate>Wed, 15 Aug 2007 03:22:29 +0000</pubDate><guid>https://major.io/2007/08/14/automated-mysql-performance-tuning-script/</guid><description>Help me out! Digg my MySQLTuner script on digg.com!
Don&amp;rsquo;t have the money or time for a DBA? Use this free MySQL tuning script to review your server&amp;rsquo;s variables and statistics. It will suggest specific variable changes and point to configuration errors that exist on your server.
read more | digg story</description><content type="html"><![CDATA[<p>Help me out! <a href="http://digg.com/linux_unix/Automated_MySQL_Performance_Tuning_Script">Digg my MySQLTuner script on digg.com!</a></p>
<p>Don&rsquo;t have the money or time for a DBA? Use this free MySQL tuning script to review your server&rsquo;s variables and statistics. It will suggest specific variable changes and point to configuration errors that exist on your server.</p>
<p><a href="http://mysqltuner.com/">read more</a> | <a href="http://digg.com/linux_unix/Automated_MySQL_Performance_Tuning_Script">digg story</a></p>
]]></content></item><item><title>Huge MySQLTuner overhaul</title><link>https://major.io/2007/08/12/huge-mysqltuner-overhaul/</link><pubDate>Sun, 12 Aug 2007 23:51:05 +0000</pubDate><guid>https://major.io/2007/08/12/huge-mysqltuner-overhaul/</guid><description>I&amp;rsquo;ve been flooded with requests for MySQLTuner and I&amp;rsquo;ve answered them this weekend. Here&amp;rsquo;s the changes that were made:
Specific variable recommendations are made with suggested values as well Odd recommendations have been reduced Some math errors were corrected More configuration items are supported, like table locks, thread caching, table caching and open file limits. To find out more and to download the script, head on over to mysqltuner.</description><content type="html"><![CDATA[<p>I&rsquo;ve been flooded with requests for MySQLTuner and I&rsquo;ve answered them this weekend. Here&rsquo;s the changes that were made:</p>
<ul>
<li>Specific variable recommendations are made with suggested values as well</li>
<li>Odd recommendations have been reduced</li>
<li>Some math errors were corrected</li>
<li>More configuration items are supported, like table locks, thread caching, table caching and open file limits.</li>
</ul>
<p>To find out more and to download the script, head on over to <a href="http://mysqltuner.com/">mysqltuner.com</a>.</p>
]]></content></item><item><title>Using wildcard subdomains in Plesk</title><link>https://major.io/2007/08/10/using-wildcard-subdomains-in-plesk/</link><pubDate>Sat, 11 Aug 2007 02:19:09 +0000</pubDate><guid>https://major.io/2007/08/10/using-wildcard-subdomains-in-plesk/</guid><description>In some situations, you may want to have domain.com as well as *.domain.com point to the same site in Plesk. Plesk will automatically set up hosting for domain.com and www.domain.com within the Apache configuration, but you can direct all subdomains for a particular domain to a certain virtual host fairly easily.
DNS
Add a CNAME or A record for *.domain.com which points to domain.com (for a CNAME), or the domain&amp;rsquo;s IP (for an A record.</description><content type="html"><![CDATA[<p>In some situations, you may want to have domain.com as well as *.domain.com point to the same site in Plesk. Plesk will automatically set up hosting for domain.com and <a href="http://www.domain.com">www.domain.com</a> within the Apache configuration, but you can direct all subdomains for a particular domain to a certain virtual host fairly easily.</p>
<p><strong>DNS</strong></p>
<p>Add a CNAME or A record for *.domain.com which points to domain.com (for a CNAME), or the domain&rsquo;s IP (for an A record.</p>
<p><strong>Apache Configuration</strong></p>
<p>Edit the /var/www/vhosts/domain.com/conf/vhost.conf or /home/httpd/vhosts/domain.com/conf/vhost.conf file and enter this information:</p>
<p><code>ServerAlias *.domain.com</code></p>
<p>If the vhost.conf didn&rsquo;t exist before, you will need to run:</p>
<p><code># /usr/local/psa/admin/bin/websrvmng -av</code></p>
<p>Whether the vhost.conf was new or not, you will need to reload the Apache configuration:</p>
<p><code># /etc/init.d/httpd reload</code></p>
<blockquote>
<p>Credit for this fix goes to <a href="http://kb.swsoft.com/en/955">SWSoft&rsquo;s KB #955</a></p>
</blockquote>
]]></content></item><item><title>Correcting Horde problems after upgrading to PHP 5 on Plesk 7.5.x</title><link>https://major.io/2007/08/10/correcting-horde-problems-after-upgrading-to-php-5-on-plesk-75x/</link><pubDate>Sat, 11 Aug 2007 02:08:49 +0000</pubDate><guid>https://major.io/2007/08/10/correcting-horde-problems-after-upgrading-to-php-5-on-plesk-75x/</guid><description>With Plesk 7.5.x, a PHP upgrade to version 5 will cause some issues with Horde. These issues stem from problems with the pear scripts that Horde depends on.
To fix it, run these commands:
# pear upgrade DB # cp -a /usr/share/pear/DB.php /usr/share/pear/DB/ /usr/share/psa-horde/pear/ Credit for this fix goes to Mike J.</description><content type="html"><![CDATA[<p>With Plesk 7.5.x, a PHP upgrade to version 5 will cause some issues with Horde. These issues stem from problems with the pear scripts that Horde depends on.</p>
<p>To fix it, run these commands:</p>
<pre><code># pear upgrade DB
# cp -a /usr/share/pear/DB.php /usr/share/pear/DB/ /usr/share/psa-horde/pear/
</code></pre><blockquote>
<p>Credit for this fix goes to Mike J.</p>
</blockquote>
]]></content></item><item><title>Urchin: Unable to open database for writing since it has been archived</title><link>https://major.io/2007/08/09/urchin-unable-to-open-database-for-writing-since-it-has-been-archived/</link><pubDate>Fri, 10 Aug 2007 01:43:55 +0000</pubDate><guid>https://major.io/2007/08/09/urchin-unable-to-open-database-for-writing-since-it-has-been-archived/</guid><description>Urchin sometimes takes it upon itself to do some weird things, and this is one of those times. If Urchin has archived a month of data, and then you ask Urchin to parse a log that contains accesses from that archived month, you&amp;rsquo;ll receive this ugly error:
Unable to open database for writing since it has been archived
To fix it, cd into /usr/local/urchin/data/reports/[profile name]/ and unzip the YYYYMM-archive.zip files, then move the zip files out of the way.</description><content type="html"><![CDATA[<p>Urchin sometimes takes it upon itself to do some weird things, and this is one of those times. If Urchin has archived a month of data, and then you ask Urchin to parse a log that contains accesses from that archived month, you&rsquo;ll receive this ugly error:</p>
<p><code>Unable to open database for writing since it has been archived</code></p>
<p>To fix it, cd into /usr/local/urchin/data/reports/[profile name]/ and unzip the YYYYMM-archive.zip files, then move the zip files out of the way. Make sure that the unzipped files are owned by the Urchin user and group. You should then be able to re-run your stats without a problem.</p>
<blockquote>
<p>Credit for this fix goes to <a href="http://www.google.com/support/urchin45/bin/answer.py?answer=28527&amp;topic=7393">Google</a></p>
</blockquote>
]]></content></item><item><title>MySQL’s query cache explained</title><link>https://major.io/2007/08/08/mysqls-query-cache-explained/</link><pubDate>Thu, 09 Aug 2007 01:42:58 +0000</pubDate><guid>https://major.io/2007/08/08/mysqls-query-cache-explained/</guid><description>An often misused and misunderstood aspect of MySQL is the query cache. I&amp;rsquo;ve seen blog post after blog post online talking about query caching as the most integral and important feature in MySQL. Many of these same posts advocate cranking the variables to the max to give you &amp;ldquo;ultimate performance.&amp;rdquo; One of the worst things you can do to a MySQL server is crank your variables up and hope for the best.</description><content type="html"><![CDATA[<p>An often misused and misunderstood aspect of MySQL is the query cache. I&rsquo;ve seen blog post after blog post online talking about query caching as the most integral and important feature in MySQL. Many of these same posts advocate cranking the variables to the max to give you &ldquo;ultimate performance.&rdquo; One of the worst things you can do to a MySQL server is crank your variables up and hope for the best. I&rsquo;ll try to clear some things up here.</p>
<p>The MySQL query cache is available in MySQL 4.0, 4.1, 5.0, 5.1, and 6.0 (3.23 has no query cache). The goal of the query cache is to hold result sets that are retrieved repeatedly. Since the data is held in memory, MySQL only feeds the data from memory (which is fast) into your application without digging into the tables themselves (which is slow). The result set from the query you&rsquo;re running and the query in the query cache must be completely identical, or MySQL will pull the data as it traditionally does from the tables.</p>
<p>Queries and result sets must meet certain criteria to make it into the query cache:</p>
<ul>
<li>Must not be prepared statements (See 12.7. <a href="http://dev.mysql.com/doc/refman/5.0/en/sqlps.html">SQL Syntax for Prepared Statements</a>)</li>
<li>Subqueries are not cached, only the outer query is cached</li>
<li>Queries that are run from stored procedures, functions, or triggers are not cached (applies to versions 5.0+ only)</li>
<li>The result set must be equal to or smaller than the query_cache_limit (more on this below)</li>
<li>The query cannot refer to the mysql database</li>
<li>Queries cannot use user variables, user-defined functions, temporary tables or tables with column-level privileges</li>
</ul>
<p>Besides these rules, all other queries are approved to enter the query cache. This includes wild things such as views, joins, and queries with subqueries.</p>
<p>The MySQL query cache is controlled by several variables:</p>
<ul>
<li><strong>query_alloc_block_size</strong> (defaults to 8192): the actual size of the memory blocks created for result sets in the query cache (don&rsquo;t adjust)</li>
<li><strong>query_cache_limit</strong> (defaults to 1048576): queries with result sets larger than this won&rsquo;t make it into the query cache</li>
<li><strong>query_cache_min_res_unit</strong> (defaults to 4096): the smallest size (in bytes) for blocks in the query cache (don&rsquo;t adjust)</li>
<li><strong>query_cache_size</strong> (defaults to 0): the total size of the query cache (disables query cache if equal to 0)</li>
<li><strong>query_cache_type</strong> (defaults to 1): 0 means don&rsquo;t cache, 1 means cache everything, 2 means only cache result sets on demand</li>
<li><strong>query_cache_wlock_invalidate</strong> (defaults to FALSE): allows SELECTS to run from query cache even though the MyISAM table is locked for writing</li>
</ul>
<p>Explaining the query_cache_type is a little rough. If the query_cache_type is 0:</p>
<ul>
<li>and the query_cache_size is 0: no memory is allocated and the cache is disabled</li>
<li>and the query_cache_size is greater than 0: the memory <strong>is</strong> allocated but the cache is disabled</li>
</ul>
<p>If the query_cache_type is 1:</p>
<ul>
<li>and the query_cache_size is 0: no memory is allocated and the cache is disabled</li>
<li>and the query_cache_size is greater than 0: the cache is enabled and all queries that don&rsquo;t use <a href="http://dev.mysql.com/doc/refman/5.0/en/query-cache-in-select.html">SQL_NO_CACHE</a> will be cached automatically</li>
</ul>
<p>If the query_cache_type is 2:</p>
<ul>
<li>and the query_cache_size is 0: no memory is allocated and the cache is disabled</li>
<li>and the query_cache_size is greater than 0: the cache is enabled and queries must use <a href="http://dev.mysql.com/doc/refman/5.0/en/query-cache-in-select.html">SQL_CACHE</a> to be cached</li>
</ul>
<p>Now that we have the variables behind us, how can we tell if we&rsquo;re using the query cache appropriately? Each time a query runs against the query cache, the server will increment the Qcache_hits status variable instead of Com_select (which is incremented when a normal SELECT runs). If the table changes for any reason, its data is rendered invalid and is dropped from the query cache.</p>
<p>It&rsquo;s vital to understand the performance implications of the query cache:</p>
<p><strong>Purging the cache</strong></p>
<p>If the query cache fills completely, it will be flushed entirely - this is a significant performance hit as many memory addresses will have to be adjusted. Check your Qcache_lowmem_prunes in your status variables and increase the query_cache_size if you find yourself pruning the query cache more than a few times per hour.</p>
<p><strong>Query cache utilization</strong></p>
<p>There&rsquo;s a simple formula to calculate your query cache efficiency in percentage form:</p>
<p><code>Qcache_hits / (Com_select + Qcache_hits) x 100</code></p>
<p>A query cache efficiency percentage of 20% or less points to a performance problem. You may want to shrink your result sets by building more restrictive queries. If that isn&rsquo;t possible, then you can increase your query_cache_limit so that more of your larger result sets actually make it into the cache. Keep in mind, however, that this will increase your prunes (see the previous paragraph) and can reduce performance. Increasing the query_cache_limit by small amounts and then recalculating your efficiency is a good idea.</p>
<p><strong>Fighting fragmentation</strong></p>
<p>As queries move in and out of the query cache, the memory may become fragmented. This is normally signified by an increase in slow queries, but your query cache efficiency percentage still remains high. In this situation, run <code>FLUSH QUERY CACHE</code> from the MySQL client and keep monitoring your efficiency. If this doesn&rsquo;t help, you may be better off flushing the cache entirely with <code>RESET QUERY CACHE</code>.</p>
<blockquote>
<p>I&rsquo;ve tried to piece quite a bit of documentation and DBA knowledge into this article, but you may benefit from reviewing the following documentation sections on MySQL.com: <a href="http://dev.mysql.com/doc/refman/5.0/en/server-system-variables.html">5.2.3. System Variables</a>, <a href="http://dev.mysql.com/doc/refman/5.0/en/server-status-variables.html">5.2.5 Status Variables</a>, and <a href="http://dev.mysql.com/doc/refman/5.0/en/query-cache.html">6.5.4. The MySQL Query Cache</a>.</p>
</blockquote>
]]></content></item><item><title>Reset the Urchin admin password</title><link>https://major.io/2007/08/08/reset-the-urchin-admin-password/</link><pubDate>Thu, 09 Aug 2007 00:50:45 +0000</pubDate><guid>https://major.io/2007/08/08/reset-the-urchin-admin-password/</guid><description>Should you find yourself in the situation where you&amp;rsquo;ve forgotten the Urchin admin password, don&amp;rsquo;t worry. It&amp;rsquo;s easily reset with the following command:
cd util ./uconf-driver action=set_parameter table=user name=&amp;quot;(admin)&amp;quot; ct_password=urchin This will set the password to &amp;lsquo;urchin&amp;rsquo;, and then you can log into Urchin&amp;rsquo;s web interface and change it to a secure password. The credit for this fix goes to Urchin&amp;rsquo;s site.</description><content type="html"><![CDATA[<p>Should you find yourself in the situation where you&rsquo;ve forgotten the Urchin admin password, don&rsquo;t worry. It&rsquo;s easily reset with the following command:</p>
<pre><code>cd util
./uconf-driver action=set_parameter  table=user name=&quot;(admin)&quot;  ct_password=urchin
</code></pre><p>This will set the password to &lsquo;urchin&rsquo;, and then you can log into Urchin&rsquo;s web interface and change it to a secure password. The credit for this fix goes to <a href="http://www.google.com/support/urchin45/bin/answer.py?answer=28531&amp;topic=7392">Urchin&rsquo;s site</a>.</p>
]]></content></item><item><title>Urchin: Warning! Task scheduler disabled.</title><link>https://major.io/2007/08/08/urchin-warning-task-scheduler-disabled/</link><pubDate>Thu, 09 Aug 2007 00:48:20 +0000</pubDate><guid>https://major.io/2007/08/08/urchin-warning-task-scheduler-disabled/</guid><description>When Urchin&amp;rsquo;s task scheduler fails, you&amp;rsquo;ll notice big gaps in your data within Urchin. If your logs rotate out before someone catches the problem, then your data is gone, and unless you have it backed up, you&amp;rsquo;re out of luck. I&amp;rsquo;ve scoured the internet (and Urchin gurus) and I&amp;rsquo;ve yet to find a complete explanation for the occasional death of Urchin&amp;rsquo;s task scheduler.
You&amp;rsquo;ll see the &amp;ldquo;Warning! Task scheduler disabled.&amp;rdquo; error in bright red print in Urchin&amp;rsquo;s configuration menu when you click the &amp;ldquo;Run/Schedule&amp;rdquo; tab.</description><content type="html"><![CDATA[<p>When Urchin&rsquo;s task scheduler fails, you&rsquo;ll notice big gaps in your data within Urchin. If your logs rotate out before someone catches the problem, then your data is gone, and unless you have it backed up, you&rsquo;re out of luck. I&rsquo;ve scoured the internet (and Urchin gurus) and I&rsquo;ve yet to find a complete explanation for the occasional death of Urchin&rsquo;s task scheduler.</p>
<p>You&rsquo;ll see the &ldquo;Warning! Task scheduler disabled.&rdquo; error in bright red print in Urchin&rsquo;s configuration menu when you click the &ldquo;Run/Schedule&rdquo; tab. It appears right below the gleaming &ldquo;Run Now&rdquo; button. If you click &ldquo;Run Now&rdquo;, Urchin will tell you again that the task scheduler is disabled.</p>
<p>To correct the problem, completely stop Urchin as root:</p>
<pre><code># /etc/init.d/urchin stop
-- OR --
# /usr/local/urchin/bin/urchinctl stop
</code></pre><p>Now, change to the /usr/local/urchin/bin directory and run:</p>
<pre><code># ./urchinctl status
</code></pre><p>If the Urchin webserver is running, but the task scheduler isn&rsquo;t (which is the most likely situation), run:</p>
<pre><code># ./urchinctl -s start
# ./urchinctl status
Urchin webserver is running
Urchin scheduler is running
</code></pre><p>You should be all set. Credit for this fix goes to <a href="http://www.google.com/support/urchin45/bin/answer.py?answer=28337&amp;topic=7401">Urchin&rsquo;s site</a>.</p>
]]></content></item><item><title>Adding IP aliases in FreeBSD</title><link>https://major.io/2007/08/08/adding-ip-aliases-in-freebsd/</link><pubDate>Thu, 09 Aug 2007 00:35:44 +0000</pubDate><guid>https://major.io/2007/08/08/adding-ip-aliases-in-freebsd/</guid><description>One question I hear quite often is &amp;ldquo;how do I add IP aliases in FreeBSD?&amp;rdquo; It&amp;rsquo;s not terribly intuitive, but you can follow these steps:
Example:
Server&amp;rsquo;s primary IP: 192.168.1.5
Additional IP&amp;rsquo;s to add: 192.168.1.10, 192.168.1.15, and 192.168.1.20
Boot-time configuration:
Add it to /etc/rc.conf first (so you don&amp;rsquo;t forget). In this example, we have a Realtek card called rl0:
ifconfig_rl0=&amp;quot;inet 192.168.1.5 netmask 255.255.255.0&amp;quot; ifconfig_rl0_alias0=&amp;quot;inet 192.168.1.10 netmask 255.255.255.0&amp;quot; ifconfig_rl0_alias1=&amp;quot;inet 192.168.1.15 netmask 255.</description><content type="html"><![CDATA[<p>One question I hear quite often is &ldquo;how do I add IP aliases in FreeBSD?&rdquo; It&rsquo;s not terribly intuitive, but you can follow these steps:</p>
<p><strong>Example:</strong></p>
<p>Server&rsquo;s primary IP: 192.168.1.5</p>
<p>Additional IP&rsquo;s to add: 192.168.1.10, 192.168.1.15, and 192.168.1.20</p>
<p><strong>Boot-time configuration:</strong></p>
<p>Add it to /etc/rc.conf first (so you don&rsquo;t forget). In this example, we have a Realtek card called rl0:</p>
<pre><code>ifconfig_rl0=&quot;inet 192.168.1.5 netmask 255.255.255.0&quot;
ifconfig_rl0_alias0=&quot;inet 192.168.1.10 netmask 255.255.255.0&quot;
ifconfig_rl0_alias1=&quot;inet 192.168.1.15 netmask 255.255.255.0&quot;
ifconfig_rl0_alias2=&quot;inet 192.168.1.20 netmask 255.255.255.0&quot;
</code></pre><p><strong>UBER-IMPORTANT NOTE:</strong> Start with the number 0 (zero) any time that you make IP alias configurations in /etc/rc.conf.</p>
<p>This is <strong>BAD</strong> form:</p>
<pre><code>ifconfig_rl0=&quot;inet 192.168.1.5 netmask 255.255.255.0&quot;
ifconfig_rl0_alias1=&quot;inet 192.168.1.10 netmask 255.255.255.0&quot;
ifconfig_rl0_alias2=&quot;inet 192.168.1.15 netmask 255.255.255.0&quot;
ifconfig_rl0_alias3=&quot;inet 192.168.1.20 netmask 255.255.255.0&quot;
</code></pre><p>If you do it the wrong way (which means starting alias with anything but alias0), only the primary comes up. Keep that in mind.</p>
<p><strong>Bringing up the new IP&rsquo;s:</strong></p>
<p>You can do things the extraordinarily dangerous way:</p>
<pre><code># /etc/rc.network restart
</code></pre><p>Or, you can follow the recommended steps:</p>
<pre><code># ifconfig rl0 alias 192.168.1.10 netmask 255.255.255.0
# ifconfig rl0 alias 192.168.1.15 netmask 255.255.255.0
# ifconfig rl0 alias 192.168.1.20 netmask 255.255.255.0
</code></pre><p><strong>Test your work:</strong></p>
<p>Any good system administrator knows to test things once their configured. Make sure to ping your new IP&rsquo;s from a source on your network and outside your network (if possible/applicable).</p>
]]></content></item><item><title>MySQL: Missing *.ibd files</title><link>https://major.io/2007/08/08/mysql-missing-ibd-files/</link><pubDate>Thu, 09 Aug 2007 00:22:40 +0000</pubDate><guid>https://major.io/2007/08/08/mysql-missing-ibd-files/</guid><description>Using the InnoDB engine can be tricky due to the ibdata files' rather untraditional behavior. Instead of storing data in MYI and MYD files for each table, InnoDB stores everything in one (or several) large files starting with ibdata1. Of course, MySQL nerds know that you can adjust this behavior slightly with innodb_file_per_table, but you can read up on this at your leisure.
If you&amp;rsquo;ve restored the ibdata files from a previous backup, or if you just toss the .</description><content type="html"><![CDATA[<p>Using the InnoDB engine can be tricky due to the ibdata files' rather untraditional behavior. Instead of storing data in MYI and MYD files for each table, InnoDB stores everything in one (or several) large files starting with ibdata1. Of course, MySQL nerds know that you can adjust this behavior slightly with innodb_file_per_table, but you can <a href="http://dev.mysql.com/doc/refman/5.0/en/multiple-tablespaces.html">read up</a> on this at <a href="http://bignerdranch.com/">your leisure</a>.</p>
<p>If you&rsquo;ve restored the ibdata files from a previous backup, or if you just toss the .frm files into a database directory, you might find this when you start MySQL:</p>
<pre><code>ERROR 1016 (HY000): Can't open file: 'files.ibd' (errno: 1)
</code></pre><p>Any good MySQL DBA will find out what error #1 means:</p>
<pre><code># perror 1
OS error code   1:  Operation not permitted
</code></pre><p>This error sure sounds like a permission error. Go ahead and check your permissions in <code>/var/lib/mysql</code>, but you&rsquo;ll probably find that they&rsquo;re properly set.</p>
<p><strong>So, why is the operation not permitted?</strong></p>
<p>MySQL is actually hiding the actual problem behind an incorrect error. The actual issue is that the tables described in your .frm files are not present in the InnoDB tablespace (the ibdata files). This may occur if you restore the .frm files, but you don&rsquo;t restore the correct ibdata files.</p>
<p><strong>What&rsquo;s the solution?</strong></p>
<p>The easiest fix is to obtain a mysqldump backup of your original data. When you import it, MySQL will create your .frm files and populate the ibdata files for you without any fuss. You&rsquo;ll be up and running in no time.</p>
<p>If you don&rsquo;t have mysqldump backups, then you&rsquo;ve just realized how important it is to have a flatfile backup of your databases. :-) If you can restore your original ibdata file that you backed up with your .frm&rsquo;s, you should be able to stop MySQL, put the old ibdata file and transaction logs back, and start MySQL. However, if multiple databases have InnoDB tables, you&rsquo;re going to be reverting them to their previous state. This could cause BIG problems if you&rsquo;re not careful. You will want to begin running this on a regular basis:</p>
<pre><code>mysqldump -Q --opt -A --single-transaction -u username -p &gt; mysqldump.sql
</code></pre><p>As a sidenote, this error utterly stumped this DBA. I&rsquo;ve never run into this issue before, and I assumed that the server was supposed to have tablespaces per table, but I couldn&rsquo;t find any mention in the /etc/my.cnf file. I found the <a href="http://forums.mysql.com/read.php?22,68927,69008#msg-69008">solution on MySQL&rsquo;s site</a> after some intense Google action.</p>
]]></content></item><item><title>WordPress 2.2.2</title><link>https://major.io/2007/08/08/wordpress-222/</link><pubDate>Thu, 09 Aug 2007 00:04:22 +0000</pubDate><guid>https://major.io/2007/08/08/wordpress-222/</guid><description>Rackerhacker has just been upgraded to WordPress 2.2.2. If you haven&amp;rsquo;t updated it your own blog yet, you will want to download the new version and upgrade it soon.
If you want to know why, you can review the fixes and code changes.</description><content type="html"><![CDATA[<p>Rackerhacker has just been upgraded to WordPress 2.2.2. If you haven&rsquo;t updated it your own blog yet, you will want to download the <a href="http://wordpress.org/download/">new version</a> and <a href="http://codex.wordpress.org/Upgrading_WordPress_Extended">upgrade</a> it soon.</p>
<p>If you want to know why, you can review the <a href="http://trac.wordpress.org/query?status=closed&amp;milestone=2.2.2">fixes</a> and <a href="http://trac.wordpress.org/changeset?new=branches%2F2.2%405849&amp;old=branches%2F2.2%405725">code changes</a>.</p>
]]></content></item><item><title>Obscure MySQL variable explained: max_seeks_for_key</title><link>https://major.io/2007/08/03/obscure-mysql-variable-explained-max_seeks_for_key/</link><pubDate>Fri, 03 Aug 2007 22:01:33 +0000</pubDate><guid>https://major.io/2007/08/03/obscure-mysql-variable-explained-max_seeks_for_key/</guid><description>MySQL documentation can be awfully flaky - extremely verbose on issues that don&amp;rsquo;t require such verbosity, and then extremely terse on issues that need a lot of explanation. The documentation for max_seeks_for_key matches the latter.
This is MySQL&amp;rsquo;s own documentation:
7.2.16. How to Avoid Table Scans
Start mysqld with the -max-seeks-for-key=1000 option or use SET max_seeks_for_key=1000 to tell the optimizer to assume that no key scan causes more than 1,000 key seeks.</description><content type="html"><![CDATA[<p>MySQL documentation can be awfully flaky - extremely verbose on issues that don&rsquo;t require such verbosity, and then extremely terse on issues that need a lot of explanation. The documentation for <strong>max_seeks_for_key</strong> matches the latter.</p>
<p>This is MySQL&rsquo;s own documentation:</p>
<blockquote>
<p><a href="http://dev.mysql.com/doc/refman/5.0/en/how-to-avoid-table-scan.html">7.2.16. How to Avoid Table Scans</a></p>
</blockquote>
<blockquote>
<p>Start mysqld with the -max-seeks-for-key=1000 option or use SET max_seeks_for_key=1000 to tell the optimizer to assume that no key scan causes more than 1,000 key seeks. <a href="http://dev.mysql.com/doc/refman/5.0/en/server-system-variables.html">See Section 5.2.3, â€œSystem Variablesâ€</a>.</p>
</blockquote>
<blockquote>
<p><a href="http://dev.mysql.com/doc/refman/5.0/en/server-system-variables.html">5.2.3. System Variables</a></p>
</blockquote>
<blockquote>
<p>Limit the assumed maximum number of seeks when looking up rows based on a key. The MySQL optimizer assumes that no more than this number of key seeks are required when searching for matching rows in a table by scanning an index, regardless of the actual cardinality of the index (see <a href="http://dev.mysql.com/doc/refman/5.0/en/show-index.html">Section 13.5.4.13, &ldquo;SHOW INDEX Syntax&rdquo;</a>). By setting this to a low value (say, 100), you can force MySQL to prefer indexes instead of table scans.</p>
</blockquote>
<p>Just in case you need a quick refresher on cardinality, here you go:</p>
<blockquote>
<p><a href="http://dev.mysql.com/doc/refman/5.0/en/show-index.html">13.5.4.13. SHOW INDEX Syntax</a></p>
</blockquote>
<blockquote>
<p><strong>Cardinality</strong></p>
</blockquote>
<blockquote>
<p>An estimate of the number of unique values in the index. This is updated by running ANALYZE TABLE or myisamchk -a. Cardinality is counted based on statistics stored as integers, so the value is not necessarily exact even for small tables. The higher the cardinality, the greater the chance that MySQL uses the index when doing joins.</p>
</blockquote>
<p>Are you confused yet? If you&rsquo;re not confused, you are a tremedously awesome DBA (or you&rsquo;re a MySQL developer). Here&rsquo;s the break down:</p>
<p>Cardinality is the count of how many items in the index are unique. So, if you have 10 values in an indexed column, and the same two values are reused throughout, then the cardinality would be relatively low. A good example of this would be if you have country or state names in a database table. You&rsquo;re going to have repeats, so this means that your cardinality is low. A good example of high cardinality is when you have a column that is a primary key (or unique). In this case, every single row has a unique key in the column, and the cardinality should equal the number of rows.</p>
<p>How does this come into play with max_seeks_for_key? It&rsquo;s higly confusing based on the documentation, but lowering this variable actually makes MySQL prefer to use indexes - even if your cardinality is low - rather than using table scans. This can reduce total query time, iowait, and CPU usage. I&rsquo;m not completely sure why MySQL doesn&rsquo;t default to this behavior since it&rsquo;s easy to see the performance gains.</p>
<p>By default, this variable is set to the largest number your system can handle. On 32-bit systems, this is 4,294,967,296. On 64-bit systems, this is 18,446,744,073,709,551,616. Some linux variants, like Gentoo Linux, are setting this value to 1,000 in the default configuration files. Reducing max_seeks_for_key to 1,000 is like telling MySQL that you want it to use indexes when the cardinality of the index is over 1,000. I&rsquo;ve seen this variable reduced to as low as 1 on some servers without any issues.</p>
<p>I&rsquo;m still utterly confused at why this variable is set so high by default. If anyone has any ideas, please send them my way!</p>
]]></content></item><item><title>Add custom rules to the Plesk firewall</title><link>https://major.io/2007/08/02/add-custom-rules-to-the-plesk-firewall/</link><pubDate>Fri, 03 Aug 2007 02:54:01 +0000</pubDate><guid>https://major.io/2007/08/02/add-custom-rules-to-the-plesk-firewall/</guid><description>Plesk has a (somewhat annoying) default firewall configuration that you can adjust from within the Plesk interface. However, if you want to add additional rules, you may find that you can&amp;rsquo;t add the rules you want from the interface. If you add them from the command line, Plesk will overwrite them when it feels the urge, even if you run service iptables save as you&amp;rsquo;re supposed to.
You can override this by making /etc/sysconfig/iptables immutable with chattr.</description><content type="html"><![CDATA[<p>Plesk has a (somewhat annoying) default firewall configuration that you can adjust from within the Plesk interface. However, if you want to add additional rules, you may find that you can&rsquo;t add the rules you want from the interface. If you add them from the command line, Plesk will overwrite them when it feels the urge, even if you run <code>service iptables save</code> as you&rsquo;re supposed to.</p>
<p>You can override this by making <code>/etc/sysconfig/iptables</code> immutable with chattr. Just run the following:</p>
<p><code># chattr +i /etc/sysconfig/iptables</code></p>
<p>Now, Plesk can&rsquo;t adjust your iptables rules without your intervention. Well, that is until SWSoft figures out how to run chattr when Plesk can&rsquo;t edit certain configuration files. :-)</p>
]]></content></item><item><title>Generate self-signed certificate and key in one line</title><link>https://major.io/2007/08/02/generate-self-signed-certificate-and-key-in-one-line/</link><pubDate>Fri, 03 Aug 2007 02:48:25 +0000</pubDate><guid>https://major.io/2007/08/02/generate-self-signed-certificate-and-key-in-one-line/</guid><description>If you need a quick self-signed certificate, you can generate the key/certificate pair, then sign it, all with one openssl line:
openssl req -new -newkey rsa:2048 -days 365 -nodes -x509 -keyout server.key -out server.crt</description><content type="html"><![CDATA[<p>If you need a quick self-signed certificate, you can generate the key/certificate pair, then sign it, all with one openssl line:</p>
<pre><code>openssl req -new -newkey rsa:2048 -days 365 -nodes -x509 -keyout server.key -out server.crt
</code></pre>]]></content></item><item><title>Plesk: Unable to make action: Unable to manage service by dnsmng: dnsmng: Service named failed to start</title><link>https://major.io/2007/08/02/plesk-unable-to-make-action-unable-to-manage-service-by-dnsmng-dnsmng-service-named-failed-to-start/</link><pubDate>Fri, 03 Aug 2007 02:43:35 +0000</pubDate><guid>https://major.io/2007/08/02/plesk-unable-to-make-action-unable-to-manage-service-by-dnsmng-dnsmng-service-named-failed-to-start/</guid><description>This error means that Plesk attempted to make a DNS change and reload named, but it failed. The problem generally lies within some seemingly innocent RPM&amp;rsquo;s that are causing problems with Plesk&amp;rsquo;s installation of bind.
Check your /var/log/messages for lines like these:
named[xxx]: could not configure root hints from 'named.root': file not found named[xxx]: loading configuration: file not found named[xxx]: exiting (due to fatal error) named: named startup failed In this case, do a quick check for these RPM&amp;rsquo;s and remove them if they are on the system:</description><content type="html"><![CDATA[<p>This error means that Plesk attempted to make a DNS change and reload named, but it failed. The problem generally lies within some seemingly innocent RPM&rsquo;s that are causing problems with Plesk&rsquo;s installation of bind.</p>
<p>Check your /var/log/messages for lines like these:</p>
<pre><code>named[xxx]: could not configure root hints from 'named.root': file not found
named[xxx]: loading configuration: file not found
named[xxx]: exiting (due to fatal error)
named: named startup failed
</code></pre><p>In this case, do a quick check for these RPM&rsquo;s and remove them if they are on the system:</p>
<ul>
<li>bind-chroot</li>
<li>caching-nameserver</li>
</ul>
<pre><code># rpm -ev bind-chroot
# rpm -ev caching-nameserver
</code></pre>]]></content></item><item><title>Freeing up file descriptors in Plesk 8.2 with piped Apache logs</title><link>https://major.io/2007/08/02/freeing-up-file-descriptors-in-plesk-82-with-piped-apache-logs/</link><pubDate>Fri, 03 Aug 2007 02:39:45 +0000</pubDate><guid>https://major.io/2007/08/02/freeing-up-file-descriptors-in-plesk-82-with-piped-apache-logs/</guid><description>If you&amp;rsquo;ve used Plesk with a large amount of domains, you know what a pain running out of file descriptors can be. Web pages begin acting oddly, Horde throws wild errors, and even squirrelmail rolls over onto itself. Luckily, Plesk introduced piped Apache logs (along with lots of bugs!) in Plesk 8.2, and you can enable piped logs with the following commands:
# mysql -uadmin -p`cat /etc/psa/.psa.shadow` psa -e &amp;quot;replace into misc (param,val) values ('apache_pipelog', 'true');&amp;quot; # /usr/local/psa/admin/sbin/websrvmng -v -a Technically, these changes will allow Plesk to host about 900 sites, but this is still a little extreme in my opinion, even on the best hardware money can buy.</description><content type="html"><![CDATA[<p>If you&rsquo;ve used Plesk with a large amount of domains, you know what a pain running out of file descriptors can be. Web pages begin acting oddly, Horde throws wild errors, and even squirrelmail rolls over onto itself. Luckily, Plesk introduced piped Apache logs (along with lots of bugs!) in Plesk 8.2, and you can enable piped logs with the following commands:</p>
<pre><code># mysql -uadmin -p`cat /etc/psa/.psa.shadow` psa -e &quot;replace into misc (param,val) values ('apache_pipelog', 'true');&quot;
# /usr/local/psa/admin/sbin/websrvmng -v -a
</code></pre><p>Technically, these changes will allow Plesk to host about 900 sites, but this is still a little extreme in my opinion, even on the best hardware money can buy. If you find yourself passing the 900 mark, then you should probably follow this <a href="http://kb.swsoft.com/en/260">SWSoft KB</a> article, adjust your FD_SETSIZE and recompile.</p>
<p>More information about configuring piped logs can be found on <a href="http://kb.swsoft.com/en/2066">SWSoft&rsquo;s site</a>. <em>Thanks, Jon!</em></p>
]]></content></item><item><title>Big updates for mysqltuner</title><link>https://major.io/2007/08/02/big-updates-for-mysqltuner/</link><pubDate>Fri, 03 Aug 2007 02:35:16 +0000</pubDate><guid>https://major.io/2007/08/02/big-updates-for-mysqltuner/</guid><description>If you haven&amp;rsquo;t checked out my automated mysqltuner.pl script, head on over to mysqltuner.com and give it a try. The script is written in Perl, and it&amp;rsquo;s an automated way to optimize your MySQL variables for the best performance. It&amp;rsquo;s like being able to ask a DBA to fix your variables, except the script is free, it doesn&amp;rsquo;t require coffee to function (only Perl), and it gives immediate results.
Changes for tonight:</description><content type="html"><![CDATA[<p>If you haven&rsquo;t checked out my automated mysqltuner.pl script, head on over to <a href="http://mysqltuner.com">mysqltuner.com</a> and give it a try. The script is written in Perl, and it&rsquo;s an automated way to optimize your MySQL variables for the best performance. It&rsquo;s like being able to ask a DBA to fix your variables, except the script is free, it doesn&rsquo;t require coffee to function (only Perl), and it gives immediate results.</p>
<p>Changes for tonight:</p>
<ul>
<li>Fixed some of the logic for key buffer calculations</li>
<li>Added a warning if MySQL hasn&rsquo;t been running very long</li>
<li>Additional detailed explanations</li>
<li>Checks for max_seeks_for_key variable</li>
</ul>
]]></content></item><item><title>Kernel Panic: Unable to find volume group “VolGroup00”; Kernel panic – not syncing: Attempted to kill init!</title><link>https://major.io/2007/08/01/kernel-panic-unable-to-find-volume-group-volgroup00-kernel-panic-not-syncing-attempted-to-kill-init/</link><pubDate>Wed, 01 Aug 2007 16:18:26 +0000</pubDate><guid>https://major.io/2007/08/01/kernel-panic-unable-to-find-volume-group-volgroup00-kernel-panic-not-syncing-attempted-to-kill-init/</guid><description>Operating System:
Red Hat Enterprise Linux 4
What happened:
The kernel was upgraded and these messages appeared upon reboot.
Work done:
The modules for the RAID card had to be re-compiled and installed for the new kernel.
End result:
Once the drivers were installed, the server came up just fine after a reboot.</description><content type="html"><![CDATA[<p><a href="http://cdn.cloudfiles.mosso.com/c8031/kernelpanic-missingdriver.png" title="kernelpanic-missingdriver.png"><img src="http://cdn.cloudfiles.mosso.com/c8031/kernelpanic-missingdriver.thumbnail.png" alt="kernelpanic-missingdriver.png"></a></p>
<p><strong>Operating System:</strong></p>
<p>Red Hat Enterprise Linux 4</p>
<p><strong>What happened:</strong></p>
<p>The kernel was upgraded and these messages appeared upon reboot.</p>
<p><strong>Work done:</strong></p>
<p>The modules for the RAID card had to be re-compiled and installed for the new kernel.</p>
<p><strong>End result:</strong></p>
<p>Once the drivers were installed, the server came up just fine after a reboot.</p>
]]></content></item><item><title>Kernel Panic: Unexpected soft update inconsistency; run fsck manually</title><link>https://major.io/2007/08/01/kernel-panic-unexpected-soft-update-inconsistency-run-fsck-manually/</link><pubDate>Wed, 01 Aug 2007 16:15:54 +0000</pubDate><guid>https://major.io/2007/08/01/kernel-panic-unexpected-soft-update-inconsistency-run-fsck-manually/</guid><description>Operating System:
FreeBSD 5.4
What happened:
Server went unresponsive at the console and came up with the error upon power cycling.
Work done:
Ran a fsck from single user mode and then rebooted.
End result:
The fsck was successful and the server rebooted without an issue.</description><content type="html"><![CDATA[<p><a href="http://cdn.cloudfiles.mosso.com/c8031/kernelpanic-bsdfs54.png" title="kernelpanic-bsdfs54.png"><img src="http://cdn.cloudfiles.mosso.com/c8031/kernelpanic-bsdfs54.thumbnail.png" alt="kernelpanic-bsdfs54.png"></a></p>
<p><strong>Operating System:</strong></p>
<p>FreeBSD 5.4</p>
<p><strong>What happened:</strong></p>
<p>Server went unresponsive at the console and came up with the error upon power cycling.</p>
<p><strong>Work done:</strong></p>
<p>Ran a fsck from single user mode and then rebooted.</p>
<p><strong>End result:</strong></p>
<p>The fsck was successful and the server rebooted without an issue.</p>
]]></content></item><item><title>EXT3-fs error: ext3_check_descriptors / group descriptors corrupted</title><link>https://major.io/2007/08/01/ext3-fs-error-ext3_check_descriptors-group-descriptors-corrupted/</link><pubDate>Wed, 01 Aug 2007 12:28:42 +0000</pubDate><guid>https://major.io/2007/08/01/ext3-fs-error-ext3_check_descriptors-group-descriptors-corrupted/</guid><description>Operating System:
Red Hat Enterprise Linux 4 Update 5
What happened:
The server was abruptly powered down, disassembled, and re-assembled.
Work done:
Ran a fsck from the rescue environment and it eventually completed (after much corruption was found), but the server would not boot properly.
End result:
The damage to the filesystem could not be adequately repaired as the errors were very extensive. The RAID array had to be rebuilt and a new OS was installed.</description><content type="html"><![CDATA[<p><a href="http://cdn.cloudfiles.mosso.com/c8031/kernelpanic-badfs.png" title="kernelpanic-badfs.png"><img src="http://cdn.cloudfiles.mosso.com/c8031/kernelpanic-badfs.thumbnail.png" alt="kernelpanic-badfs.png"></a></p>
<p><strong>Operating System:</strong></p>
<p>Red Hat Enterprise Linux 4 Update 5</p>
<p><strong>What happened:</strong></p>
<p>The server was abruptly powered down, disassembled, and re-assembled.</p>
<p><strong>Work done:</strong></p>
<p>Ran a fsck from the rescue environment and it eventually completed (after much corruption was found), but the server would not boot properly.</p>
<p><strong>End result:</strong></p>
<p>The damage to the filesystem could not be adequately repaired as the errors were very extensive. The RAID array had to be rebuilt and a new OS was installed.</p>
]]></content></item><item><title>Kernel panic: No init found. Try passing init= option to kernel</title><link>https://major.io/2007/08/01/kernel-panic-no-init-found-try-passing-init-option-to-kernel/</link><pubDate>Wed, 01 Aug 2007 12:26:14 +0000</pubDate><guid>https://major.io/2007/08/01/kernel-panic-no-init-found-try-passing-init-option-to-kernel/</guid><description>Operating System:
Red Hat Enterprise Linux 2.1
What happened:
The server was powered down gracefully, moved to a new rack, and then powered on.
Work done:
Ran a fsck from the rescue environment and it eventually completed, but the server would not boot properly.
End result:
The hard drive was in the process of failing, so it was replaced and the operating system was installed onto the new disk. The old disk was stable enough to be mounted read-only, and much of the data was salvaged.</description><content type="html"><![CDATA[<p><a href="http://cdn.cloudfiles.mosso.com/c8031/kernelpanic-noinit.png" title="kernelpanic-noinit.png"><img src="http://cdn.cloudfiles.mosso.com/c8031/kernelpanic-noinit.thumbnail.png" alt="kernelpanic-noinit.png"></a></p>
<p><strong>Operating System:</strong></p>
<p>Red Hat Enterprise Linux 2.1</p>
<p><strong>What happened:</strong></p>
<p>The server was powered down gracefully, moved to a new rack, and then powered on.</p>
<p><strong>Work done:</strong></p>
<p>Ran a fsck from the rescue environment and it eventually completed, but the server would not boot properly.</p>
<p><strong>End result:</strong></p>
<p>The hard drive was in the process of failing, so it was replaced and the operating system was installed onto the new disk. The old disk was stable enough to be mounted read-only, and much of the data was salvaged.</p>
]]></content></item><item><title>Add spam filtering for all users in Plesk</title><link>https://major.io/2007/07/29/add-spam-filtering-for-all-users-in-plesk/</link><pubDate>Mon, 30 Jul 2007 02:46:10 +0000</pubDate><guid>https://major.io/2007/07/29/add-spam-filtering-for-all-users-in-plesk/</guid><description>These two commands will enable SpamAssassin for all users on a Plesk 8 server:
# mysql -u admin -p`cat /etc/psa/.psa.shadow` psa mysql&amp;gt; update mail set spamfilter = 'true' where postbox = 'true'; # /usr/local/psa/admin/bin/mchk --with-spam Thanks to Sean R. for this one!</description><content type="html"><![CDATA[<p>These two commands will enable SpamAssassin for all users on a Plesk 8 server:</p>
<pre><code># mysql -u admin -p`cat /etc/psa/.psa.shadow` psa
mysql&gt; update mail set spamfilter = 'true' where postbox = 'true';
# /usr/local/psa/admin/bin/mchk --with-spam
</code></pre><p>Thanks to Sean R. for this one!</p>
]]></content></item><item><title>Disable X support in FreeBSD</title><link>https://major.io/2007/07/18/disable-x-support-in-freebsd/</link><pubDate>Wed, 18 Jul 2007 15:12:40 +0000</pubDate><guid>https://major.io/2007/07/18/disable-x-support-in-freebsd/</guid><description>Add to /etc/make.conf:
WITHOUT_X11=yes&amp;lt;br /&amp;gt; USE_NONDEFAULT_X11BASE=yes</description><content type="html"><![CDATA[<p>Add to /etc/make.conf:</p>
<p><code>WITHOUT_X11=yes&lt;br /&gt; USE_NONDEFAULT_X11BASE=yes</code></p>
]]></content></item><item><title>Installing Lighttpd + PHP + FastCGI on FreeBSD</title><link>https://major.io/2007/07/18/installing-lighttpd-php-fastcgi-on-freebsd/</link><pubDate>Wed, 18 Jul 2007 15:12:06 +0000</pubDate><guid>https://major.io/2007/07/18/installing-lighttpd-php-fastcgi-on-freebsd/</guid><description>With portinstall:
# portinstall lighttpd fcgi php5 Without portinstall:
# make -C /usr/ports/www/lighttpd all install clean # make -C /usr/ports/www/fcgi all install clean # make -C /usr/ports/lang/php5 all install clean Add lighttpd_enable=&amp;quot;YES&amp;quot; to /etc/rc.conf, and uncomment the usual items in /usr/local/etc/lighttpd.conf to enable fastcgi.</description><content type="html"><![CDATA[<p>With portinstall:</p>
<pre><code># portinstall lighttpd fcgi php5
</code></pre><p>Without portinstall:</p>
<pre><code># make -C /usr/ports/www/lighttpd all install clean
# make -C /usr/ports/www/fcgi all install clean
# make -C /usr/ports/lang/php5 all install clean
</code></pre><p>Add <code>lighttpd_enable=&quot;YES&quot;</code> to /etc/rc.conf, and uncomment the usual items in /usr/local/etc/lighttpd.conf to enable fastcgi.</p>
]]></content></item><item><title>Upgrading FreeBSD remotely</title><link>https://major.io/2007/07/18/upgrading-freebsd-remotely/</link><pubDate>Wed, 18 Jul 2007 15:10:59 +0000</pubDate><guid>https://major.io/2007/07/18/upgrading-freebsd-remotely/</guid><description>It can be best to upgrade FreeBSD in an offline state, but if you do it online, you can do it like this:
# csup -g -L 2 -h cvsup5.us.freebsd.org /usr/share/examples/cvsup/standard-supfile # cd /usr/src # make buildworld # make buildkernel # make installkernel # make installworld # shutdown -r now</description><content type="html"><![CDATA[<p>It can be best to upgrade FreeBSD in an offline state, but if you do it online, you can do it like this:</p>
<pre><code># csup -g -L 2 -h cvsup5.us.freebsd.org /usr/share/examples/cvsup/standard-supfile
# cd /usr/src
# make buildworld
# make buildkernel
# make installkernel
# make installworld
# shutdown -r now
</code></pre>]]></content></item><item><title>Importing existing keys and certificates into java keystore files</title><link>https://major.io/2007/07/18/importing-existing-keys-and-certificates-into-java-keystore-files/</link><pubDate>Wed, 18 Jul 2007 15:05:37 +0000</pubDate><guid>https://major.io/2007/07/18/importing-existing-keys-and-certificates-into-java-keystore-files/</guid><description>Making Java keystores at the same time as you create a CSR and key is pretty easy, but if you have a pre-made private key that you want to throw into a keystore, it can be difficult. However, follow these steps and you&amp;rsquo;ll ber done quickly!
Save the new certificate to server.crt and the new key to server.key. If intermediate certificates are necessary, then place all of the certificates into a file called cacert.</description><content type="html"><![CDATA[<p>Making Java keystores at the same time as you create a CSR and key is pretty easy, but if you have a pre-made private key that you want to throw into a keystore, it can be difficult. However, follow these steps and you&rsquo;ll ber done quickly!</p>
<p>Save the new certificate to server.crt and the new key to server.key. If intermediate certificates are necessary, then place all of the certificates into a file called cacert.crt. Now, you will have to make a PKCS #12 file:</p>
<pre><code>openssl pkcs12 -export -inkey server.key -in server.crt \
    -name tomcat-domain.com -certfile cacert.crt -out domain.com.p12
</code></pre><p>To perform the rest of the work, you will need a copy of the <a href="/wp-content/ktg.tgz">KeyTool GUI</a>. In the GUI, make a new keystore in JKS format. Import the PKCS #12 key pair, and save the keystore as a JKS. Upload the keystore to the server and then configure the keystore within Tomcat/JBoss.</p>
]]></content></item><item><title>/bin/tar: Argument list too long</title><link>https://major.io/2007/07/05/bintar-argument-list-too-long/</link><pubDate>Fri, 06 Jul 2007 03:11:43 +0000</pubDate><guid>https://major.io/2007/07/05/bintar-argument-list-too-long/</guid><description>If you find yourself stuck with over 30,000 files in a directory (text files in this example), packing them into a tar file can be tricky. You can get around it with this:
find . -name '*.txt' -print &amp;gt;/tmp/test.manifest tar -cvzf textfiles.tar.gz --files-from /tmp/test.manifest find . -name '*.txt' | xargs rm -v</description><content type="html"><![CDATA[<p>If you find yourself stuck with over 30,000 files in a directory (text files in this example), packing them into a tar file can be tricky. You can get around it with this:</p>
<pre><code>find . -name '*.txt' -print &gt;/tmp/test.manifest
tar -cvzf textfiles.tar.gz --files-from /tmp/test.manifest
find . -name '*.txt' | xargs rm -v
</code></pre>]]></content></item><item><title>Automatic Plesk login</title><link>https://major.io/2007/07/05/automatic-plesk-login/</link><pubDate>Fri, 06 Jul 2007 03:09:27 +0000</pubDate><guid>https://major.io/2007/07/05/automatic-plesk-login/</guid><description>If you want to make a quick bookmark that will automatically log yourself into Plesk, make this bookmark:
https://yourserver.com:8443/login_up.php3?login_name=admin&amp;amp;passwd=yourpassword</description><content type="html"><![CDATA[<p>If you want to make a quick bookmark that will automatically log yourself into Plesk, make this bookmark:</p>
<p><strong><a href="https://yourserver.com">https://yourserver.com</a>:8443/login_up.php3?login_name=admin&amp;passwd=yourpassword</strong></p>
]]></content></item><item><title>Enable submission port 587 in Postfix</title><link>https://major.io/2007/07/04/enable-submission-port-587-in-postfix/</link><pubDate>Thu, 05 Jul 2007 00:29:36 +0000</pubDate><guid>https://major.io/2007/07/04/enable-submission-port-587-in-postfix/</guid><description>Enabling submission port support for Postfix is really easy. To have postfix listen on both 25 and 587, be sure that the line starting with submission is uncommented in /etc/postfix/master.cf:
smtp inet n - n - - smtpd submission inet n - n - - smtpd</description><content type="html"><![CDATA[<p>Enabling submission port support for Postfix is really easy. To have postfix listen on both 25 and 587, be sure that the line starting with <strong>submission</strong> is uncommented in /etc/postfix/master.cf:</p>
<pre><code>smtp      inet  n       -       n       -       -       smtpd
submission inet n      -       n       -       -       smtpd
</code></pre>]]></content></item><item><title>Check available entropy in Linux</title><link>https://major.io/2007/07/01/check-available-entropy-in-linux/</link><pubDate>Sun, 01 Jul 2007 16:46:11 +0000</pubDate><guid>https://major.io/2007/07/01/check-available-entropy-in-linux/</guid><description>Sometimes servers just have the weirdest SSL problems ever. In some of these situations, the entropy has been drained. Entropy is the measure of the random numbers available from /dev/urandom, and if you run out, you can&amp;rsquo;t make SSL connections. To check the status of your server&amp;rsquo;s entropy, just run the following:
# cat /proc/sys/kernel/random/entropy_avail If it returns anything less than 100-200, you have a problem. Try installing rng-tools, or generating I/O, like large find operations.</description><content type="html"><![CDATA[<p>Sometimes servers just have the weirdest SSL problems ever. In some of these situations, the entropy has been drained. Entropy is the measure of the random numbers available from /dev/urandom, and if you run out, you can&rsquo;t make SSL connections. To check the status of your server&rsquo;s entropy, just run the following:</p>
<pre><code># cat /proc/sys/kernel/random/entropy_avail
</code></pre><p>If it returns anything less than 100-200, you have a problem. Try installing rng-tools, or generating I/O, like large find operations. Linux normally uses keyboard and mouse input to generate entropy on systems without random number generators, and this isn&rsquo;t very handy for dedicated servers.</p>
]]></content></item><item><title>Active FTP connections through iptables</title><link>https://major.io/2007/07/01/active-ftp-connections-through-iptables/</link><pubDate>Sun, 01 Jul 2007 16:42:01 +0000</pubDate><guid>https://major.io/2007/07/01/active-ftp-connections-through-iptables/</guid><description>One of the main reasons people like passive FTP is that it&amp;rsquo;s easier to get through firewalls with it. However, some users might now know that they need to enable passive FTP, or they may have incapable clients. To get active FTP through firewalls, start by adding these rules:
Allowing established and related connections is generally a good idea:
iptables -A INPUT -m state --state RELATED,ESTABLISHED -j ACCEPT Inbound connections on port 21 are required:</description><content type="html"><![CDATA[<p>One of the main reasons people like passive FTP is that it&rsquo;s easier to get through firewalls with it. However, some users might now know that they need to enable passive FTP, or they may have incapable clients. To get active FTP through firewalls, start by adding these rules:</p>
<p>Allowing established and related connections is generally a good idea:</p>
<pre><code>iptables -A INPUT -m state --state RELATED,ESTABLISHED -j ACCEPT
</code></pre><p>Inbound connections on port 21 are required:</p>
<pre><code>iptables -A INPUT -p tcp --dport 21 -j ACCEPT
</code></pre><p>Just to cover our bases, add in a rule to allow established and related traffic leaving port 20 on the client&rsquo;s machine:</p>
<pre><code>iptables -A INPUT -p tcp --sport 20 -m state --state ESTABLISHED,RELATED -j ACCEPT
</code></pre><p>Now, you have everything you need to allow the connections, but iptables will need to be able to mark and track these connections to allow them to pass properly. That is done with the ip_conntrack_ftp kernel module. To test things out, run this:</p>
<pre><code>modprobe ip_conntrack_ftp
</code></pre><p>At this point, you should be able to connect without a problem. However, to keep this module loaded whenever iptables is running, you will need to add it to /etc/sysconfig/iptables-config:</p>
<pre><code>IPTABLES_MODULES=&quot;ip_conntrack_ftp&quot;
</code></pre>]]></content></item><item><title>Redirect e-mails in postfix based on subject line</title><link>https://major.io/2007/07/01/redirect-e-mails-in-postfix-based-on-subject-line/</link><pubDate>Sun, 01 Jul 2007 16:37:31 +0000</pubDate><guid>https://major.io/2007/07/01/redirect-e-mails-in-postfix-based-on-subject-line/</guid><description>Depending on your situation, it may be handy to redirect e-mails that have a certain subject line before it even reaches a user&amp;rsquo;s inbox. Let&amp;rsquo;s say you&amp;rsquo;re tired of getting e-mails that start with the word &amp;ldquo;Cialis&amp;rdquo;. Just follow these steps to redirect those e-mails.
First, enable header checks in /etc/postfix/main.cf:
header_checks = regexp:/etc/postfix/header_checks Then, create /etc/postfix/header_checks and add the following:
/^Subject: Cialis*/ REDIRECT someotheruser@domain.com For a lot more information about header checks in postfix, review the documentation here:</description><content type="html"><![CDATA[<p>Depending on your situation, it may be handy to redirect e-mails that have a certain subject line before it even reaches a user&rsquo;s inbox. Let&rsquo;s say you&rsquo;re tired of getting e-mails that start with the word &ldquo;Cialis&rdquo;. Just follow these steps to redirect those e-mails.</p>
<p>First, enable header checks in /etc/postfix/main.cf:</p>
<pre><code>header_checks = regexp:/etc/postfix/header_checks
</code></pre><p>Then, create /etc/postfix/header_checks and add the following:</p>
<pre><code>/^Subject: Cialis*/
REDIRECT someotheruser@domain.com
</code></pre><p>For a lot more information about header checks in postfix, review the documentation here:</p>
<ul>
<li><a href="http://www.postfix.org/header_checks.5.html">http://www.postfix.org/header_checks.5.html</a></li>
</ul>
]]></content></item><item><title>Repair auto_increment in MySQL</title><link>https://major.io/2007/07/01/repair-auto_increment-in-mysql/</link><pubDate>Sun, 01 Jul 2007 16:34:03 +0000</pubDate><guid>https://major.io/2007/07/01/repair-auto_increment-in-mysql/</guid><description>Table corruption in MySQL can often wreak havoc on the auto_increment fields. I&amp;rsquo;m still unsure why it happens, but if you find a table tries to count from 0 after a table corruption, just find the highest key in the column and add 1 to it (in this example, I&amp;rsquo;ll say the highest key is 9500).
Just run this one SQL statement on the table:
ALTER TABLE brokentablename AUTO_INCREMENT=9501;
If you run a quick insert and then run SELECT last_insert_id(), the correct key number should be returned (9501 in this case).</description><content type="html"><![CDATA[<p>Table corruption in MySQL can often wreak havoc on the auto_increment fields. I&rsquo;m still unsure why it happens, but if you find a table tries to count from 0 after a table corruption, just find the highest key in the column and add 1 to it (in this example, I&rsquo;ll say the highest key is 9500).</p>
<p>Just run this one SQL statement on the table:</p>
<p><code>ALTER TABLE brokentablename AUTO_INCREMENT=9501;</code></p>
<p>If you run a quick insert and then run <code>SELECT last_insert_id()</code>, the correct key number should be returned (9501 in this case).</p>
]]></content></item><item><title>Enable SSL support in Postfix</title><link>https://major.io/2007/07/01/enable-ssl-support-in-postfix/</link><pubDate>Sun, 01 Jul 2007 16:31:01 +0000</pubDate><guid>https://major.io/2007/07/01/enable-ssl-support-in-postfix/</guid><description>If you have postfix installed with OpenSSL support compiled in, you can enable SSL connections by editing two configuration files. First, add the following to /etc/postfix/main.cf:
smtpd_use_tls = yes #smtpd_tls_auth_only = yes smtpd_tls_key_file = /etc/postfix/newkey.pem smtpd_tls_cert_file = /etc/postfix/newcert.pem smtpd_tls_CAfile = /etc/postfix/cacert.pem smtpd_tls_loglevel = 3 smtpd_tls_received_header = yes smtpd_tls_session_cache_timeout = 3600s tls_random_source = dev:/dev/urandom Then, simply uncomment this line in /etc/postfix/master.cf:
smtps inet n - n - - smtpd Make sure to keep tabs between the elements in the master.</description><content type="html"><![CDATA[<p>If you have postfix installed with OpenSSL support compiled in, you can enable SSL connections by editing two configuration files. First, add the following to /etc/postfix/main.cf:</p>
<pre><code>smtpd_use_tls = yes
#smtpd_tls_auth_only = yes
smtpd_tls_key_file = /etc/postfix/newkey.pem
smtpd_tls_cert_file = /etc/postfix/newcert.pem
smtpd_tls_CAfile = /etc/postfix/cacert.pem
smtpd_tls_loglevel = 3
smtpd_tls_received_header = yes
smtpd_tls_session_cache_timeout = 3600s
tls_random_source = dev:/dev/urandom
</code></pre><p>Then, simply uncomment this line in /etc/postfix/master.cf:</p>
<pre><code>smtps     inet  n       -       n       -       -       smtpd
</code></pre><p>Make sure to keep tabs between the elements in the master.cf file.</p>
]]></content></item><item><title>MySQL time zone different from system time zone</title><link>https://major.io/2007/07/01/mysql-time-zone-different-from-system-time-zone/</link><pubDate>Sun, 01 Jul 2007 16:29:11 +0000</pubDate><guid>https://major.io/2007/07/01/mysql-time-zone-different-from-system-time-zone/</guid><description>In some situations, the system time zone will be different than the one in MySQL, even though MySQL is set to use the system time zone. This normally means that a user has changed the system time zone, but they haven&amp;rsquo;t started MySQL to cause it to change as well.
If you find yourself in this situation, just restart MySQL and the situation should be fixed:</description><content type="html"><![CDATA[<p>In some situations, the system time zone will be different than the one in MySQL, even though MySQL is set to use the system time zone. This normally means that a user has changed the system time zone, but they haven&rsquo;t started MySQL to cause it to change as well.</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>If you find yourself in this situation, just restart MySQL and the situation should be fixed:</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
]]></content></item><item><title>Plesk and PHPMyAdmin: Non-static method PMA_Config::isHttps() should not be called statically</title><link>https://major.io/2007/07/01/plesk-and-phpmyadmin-non-static-method-pma_configishttps-should-not-be-called-statically/</link><pubDate>Sun, 01 Jul 2007 16:27:21 +0000</pubDate><guid>https://major.io/2007/07/01/plesk-and-phpmyadmin-non-static-method-pma_configishttps-should-not-be-called-statically/</guid><description>If this situation pops up in Plesk, it means that a user has changed their MySQL password outside of Plesk. The password in Plesk&amp;rsquo;s own database does not match, so the auto-creation of the phpMyAdmin settings fails. You&amp;rsquo;ll end up seeing this after clicking &amp;ldquo;DB WebAdmin&amp;rdquo;:
MySQL said: Non-static method PMA_Config::isHttps() should not be called statically
The funny thing is that MySQL doesn&amp;rsquo;t actually say this. It&amp;rsquo;s a PHP error. To correct the problem, you can manually change the password within Plesk&amp;rsquo;s database, or you can follow an easier method:</description><content type="html"><![CDATA[<p>If this situation pops up in Plesk, it means that a user has changed their MySQL password outside of Plesk. The password in Plesk&rsquo;s own database does not match, so the auto-creation of the phpMyAdmin settings fails. You&rsquo;ll end up seeing this after clicking &ldquo;DB WebAdmin&rdquo;:</p>
<p><strong>MySQL said: Non-static method PMA_Config::isHttps() should not be called statically</strong></p>
<p>The funny thing is that MySQL doesn&rsquo;t actually say this. It&rsquo;s a PHP error. To correct the problem, you can manually change the password within Plesk&rsquo;s database, or you can follow an easier method:</p>
<p>Click Databases</p>
<p>Click Database Users</p>
<p>Click the user that has a password change</p>
<p>In the password fields, enter the new password that they&rsquo;re using with MySQL</p>
<p>This will force Plesk to change its password in its own database, and it will run the query to change the password in MySQL (but since it&rsquo;s the same password, no change will be made).</p>
]]></content></item><item><title>Remove all open_basedir restrictions in Plesk</title><link>https://major.io/2007/06/30/remove-all-open_basedir-restrictions-in-plesk/</link><pubDate>Sat, 30 Jun 2007 15:54:49 +0000</pubDate><guid>https://major.io/2007/06/30/remove-all-open_basedir-restrictions-in-plesk/</guid><description>If you want to remove all of the open_basedir restrictions for all sites in Plesk, simply create a file called /etc/httpd/conf.d/zzz_openbasedir_removal.conf and add this text within it:
&amp;lt;DirectoryMatch /var/www/vhosts/(.*)/httpdocs/&amp;gt; php_admin_value open_basedir none &amp;lt;/DirectoryMatch&amp;gt; Just a note, this isn&amp;rsquo;t a terribly great idea from a security standpoint. :-)</description><content type="html"><![CDATA[<p>If you want to remove all of the open_basedir restrictions for all sites in Plesk, simply create a file called /etc/httpd/conf.d/zzz_openbasedir_removal.conf and add this text within it:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-apache" data-lang="apache"><span style="color:#268bd2">&lt;DirectoryMatch</span> <span style="color:#2aa198">/var/www/vhosts/(.*)/httpdocs/</span><span style="color:#268bd2">&gt;</span>
        <span style="color:#b58900">php_admin_value</span> open_basedir <span style="color:#719e07">none</span>
<span style="color:#268bd2">&lt;/DirectoryMatch&gt;</span>
</code></pre></div><p>Just a note, this isn&rsquo;t a terribly great idea from a security standpoint. :-)</p>
]]></content></item><item><title>Basic SNMP Configuration</title><link>https://major.io/2007/06/27/basic-snmp-configuration/</link><pubDate>Wed, 27 Jun 2007 23:06:21 +0000</pubDate><guid>https://major.io/2007/06/27/basic-snmp-configuration/</guid><description>If you want to get a really basic, wide-open for localhost setup for SNMP, just toss the following into /etc/snmp/snmpd.conf:
com2sec local 127.0.0.1/32 public group MyROGroup v1 local group MyROGroup v2c local group MyROGroup usm local view all included .1 80 access MyROGroup &amp;quot;&amp;quot; any noauth exact all none none syslocation MyLocation syscontact Me &amp;lt;me@somewhere.org&amp;gt;</description><content type="html"><![CDATA[<p>If you want to get a really basic, wide-open for localhost setup for SNMP, just toss the following into /etc/snmp/snmpd.conf:</p>
<pre><code>com2sec local     127.0.0.1/32    public

group MyROGroup v1         local
group MyROGroup v2c        local
group MyROGroup usm        local

view all    included  .1                               80

access MyROGroup &quot;&quot;      any       noauth    exact  all    none   none

syslocation MyLocation
syscontact Me &lt;me@somewhere.org&gt;
</code></pre>]]></content></item><item><title>Sub-par DNS stuff at DNSStuff</title><link>https://major.io/2007/06/18/sub-par-dns-stuff-at-dnsstuff/</link><pubDate>Tue, 19 Jun 2007 02:56:06 +0000</pubDate><guid>https://major.io/2007/06/18/sub-par-dns-stuff-at-dnsstuff/</guid><description>If you&amp;rsquo;re like me, you&amp;rsquo;re frustrated with DNSStuff&amp;rsquo;s actions lately. You only get 4 lookups without making a login, and even when you make a login, you have to log back in each time you access the site. Even a dim-witted web developer would know how to use cookies to automate this process and make things easier on the users.
Also, why should you pay for a web front-end for tools that people have on their servers already?</description><content type="html"><![CDATA[<p>If you&rsquo;re like me, you&rsquo;re frustrated with DNSStuff&rsquo;s actions lately. You only get 4 lookups without making a login, and even when you make a login, you have to log back in each time you access the site. Even a dim-witted web developer would know how to use cookies to automate this process and make things easier on the users.</p>
<p>Also, why should you pay for a web front-end for tools that people have on their servers already?</p>
<p>Check out <a href="http://boxcheck.com/">Boxcheck.com</a> and let me know what you think. I became so fed up, I decided to make my own site that is faster and guaranteed free forever. Period.</p>
]]></content></item><item><title>Corrupt /dev/null</title><link>https://major.io/2007/06/18/corrupt-devnull/</link><pubDate>Tue, 19 Jun 2007 02:49:56 +0000</pubDate><guid>https://major.io/2007/06/18/corrupt-devnull/</guid><description>If you find that /dev/null is no longer a block device, and it causes issues during init on Red Hat boxes, you will need to follow these steps to return things to normal:
Reboot the server When grub appears, edit your kernel line to include init=/bin/bash at the end Allow the server to boot into the emergency shell Run the following three commands # rm -rf /dev/null # mknod /dev/null c 1 3 # chmod 666 /dev/null You should be back to normal.</description><content type="html"><![CDATA[<p>If you find that /dev/null is no longer a block device, and it causes issues during init on Red Hat boxes, you will need to follow these steps to return things to normal:</p>
<ul>
<li>Reboot the server</li>
<li>When grub appears, edit your kernel line to include <code>init=/bin/bash</code> at the end</li>
<li>Allow the server to boot into the emergency shell</li>
<li>Run the following three commands</li>
</ul>
<pre><code># rm -rf /dev/null
# mknod /dev/null c 1 3
# chmod 666 /dev/null
</code></pre><p>You should be back to normal. Make sure that the root users on your server don&rsquo;t use <strong>cp</strong> or <strong>mv</strong> with /dev/null as this will cause some pretty ugly issues.</p>
]]></content></item><item><title>500 OOPS error from vsftpd</title><link>https://major.io/2007/06/14/500-oops-error-from-vsftpd/</link><pubDate>Thu, 14 Jun 2007 23:14:51 +0000</pubDate><guid>https://major.io/2007/06/14/500-oops-error-from-vsftpd/</guid><description>If you find yourself with the ever-so-peculiar 500 OOPS error from vsftpd when you attempt to login over SSH, there could be a few different things at play. Generally, this is the type of error you will get:
500 OOPS: cannot change directory:/home/someuser 500 OOPS: child died You can search for a solution in this order:
Home Directory
Does the user&amp;rsquo;s home directory even exist? Check /etc/passwd for the current home directory for the user and see what&amp;rsquo;s set:</description><content type="html"><![CDATA[<p>If you find yourself with the ever-so-peculiar <strong>500 OOPS</strong> error from vsftpd when you attempt to login over SSH, there could be a few different things at play. Generally, this is the type of error you will get:</p>
<pre><code>500 OOPS: cannot change directory:/home/someuser
500 OOPS: child died
</code></pre><p>You can search for a solution in this order:</p>
<p><strong>Home Directory</strong></p>
<p>Does the user&rsquo;s home directory even exist? Check <code>/etc/passwd</code> for the current home directory for the user and see what&rsquo;s set:</p>
<pre><code># grep someuser /etc/passwd
someuser❌10001:2524::/var/www/someuser:/bin/bash
</code></pre><p>In this case, does <code>/var/www/someuser</code> exist? If it doesn&rsquo;t, fix that and then move onto the next solution if you&rsquo;re still having problems.</p>
<p><strong>File/Directory Permissions</strong></p>
<p>Be sure that the user that you are logging in as actually has permissions to be in the directory. This affects users that have home directories of <code>/var/www/html</code> because the execute bit normally isn&rsquo;t set for the world on <code>/var/www</code> or <code>/var/www/html</code>. Make sure that the appropriate permissions and ownerships are set, and this should help eliminate the issue.</p>
<p><strong>SELINUX</strong></p>
<p>If SELINUX is rearing its ugly head on the server, this can be a problem. Check your current SELINUX status and disable it if necessary:</p>
<pre><code># setenforce
Enforcing
# setenforce 0
</code></pre><p>Try to login over FTP again and you should have a success. If you want to turn off SELINUX entirely, adjust <code>/etc/sysconfig/selinux</code> (RHEL4) or <code>/etc/selinux/config</code> (RHEL5).</p>
]]></content></item><item><title>Adjusting qmail queue time / lifetime</title><link>https://major.io/2007/06/14/adjusting-qmail-queue-time-lifetime/</link><pubDate>Thu, 14 Jun 2007 22:58:01 +0000</pubDate><guid>https://major.io/2007/06/14/adjusting-qmail-queue-time-lifetime/</guid><description>If you want to adjust how long e-mails will spend in the qmail queue before they&amp;rsquo;re bounced, simple set the queuelifetime:
# echo &amp;quot;432000&amp;quot; &amp;gt; /var/qmail/control/queuelifetime # /etc/init.d/qmail restart The above example is for 5 days (qmail needs the time length in seconds). Just take the days and multiply by 86,400 seconds to get your result.</description><content type="html"><![CDATA[<p>If you want to adjust how long e-mails will spend in the qmail queue before they&rsquo;re bounced, simple set the queuelifetime:</p>
<pre><code># echo &quot;432000&quot; &gt; /var/qmail/control/queuelifetime
# /etc/init.d/qmail restart
</code></pre><p>The above example is for 5 days (qmail needs the time length in seconds). Just take the days and multiply by 86,400 seconds to get your result.</p>
]]></content></item><item><title>Adjusting sendmail queue time / lifetime</title><link>https://major.io/2007/06/14/adjusting-sendmail-queue-time-lifetime/</link><pubDate>Thu, 14 Jun 2007 22:56:27 +0000</pubDate><guid>https://major.io/2007/06/14/adjusting-sendmail-queue-time-lifetime/</guid><description>By default, sendmail will keep items in the queue for up to 5 days. If you want to make this something shorter, like 3 days, you can adjust the following in /etc/mail/sendmail.mc:
define(`confTO_QUEUERETURN', `3d')dnl If you want to get super fancy, you can adjust the queue lifetime for messages with certain priorities:
define(`confTO_QUEUERETURN_NORMAL', `3d')dnl define(`confTO_QUEUERETURN_URGENT', `5d')dnl define(`confTO_QUEUERETURN_NONURGENT', `1d')dnl</description><content type="html"><![CDATA[<p>By default, sendmail will keep items in the queue for up to 5 days. If you want to make this something shorter, like 3 days, you can adjust the following in /etc/mail/sendmail.mc:</p>
<pre><code>define(`confTO_QUEUERETURN', `3d')dnl
</code></pre><p>If you want to get super fancy, you can adjust the queue lifetime for messages with certain priorities:</p>
<pre><code>define(`confTO_QUEUERETURN_NORMAL', `3d')dnl
define(`confTO_QUEUERETURN_URGENT', `5d')dnl
define(`confTO_QUEUERETURN_NONURGENT', `1d')dnl
</code></pre>]]></content></item><item><title>PHP CLI memory limit is different between users and root</title><link>https://major.io/2007/06/14/php-cli-memory-limit-is-different-between-users-and-root/</link><pubDate>Thu, 14 Jun 2007 22:53:33 +0000</pubDate><guid>https://major.io/2007/06/14/php-cli-memory-limit-is-different-between-users-and-root/</guid><description>If you find that memory limits differ between root and other users when PHP scripts are run from the command line, there may be an issue with your php.ini or your script. To verify that it isn&amp;rsquo;t your script, try this:
$ echo &amp;quot;&amp;lt;? var_dump(ini_get('memory_limit')); ?&amp;gt;&amp;quot; &amp;gt;&amp;gt; memtest.php $ php -f memtest.php string(3) &amp;quot;8M&amp;quot; $ su - # php -f memtest.php string(3) &amp;quot;64M&amp;quot; If you get the same two values from both users, there&amp;rsquo;s probably a problem with your script.</description><content type="html"><![CDATA[<p>If you find that memory limits differ between root and other users when PHP scripts are run from the command line, there may be an issue with your php.ini or your script. To verify that it isn&rsquo;t your script, try this:</p>
<pre><code>$ echo &quot;&lt;? var_dump(ini_get('memory_limit')); ?&gt;&quot; &gt;&gt; memtest.php
$ php -f memtest.php
string(3) &quot;8M&quot;
$ su -
# php -f memtest.php
string(3) &quot;64M&quot;
</code></pre><p>If you get the same two values from both users, there&rsquo;s probably a problem with your script. Make sure that there&rsquo;s no <code>ini_set()</code> functions in your script that are overriding the php.ini file.</p>
<p>However, if you get results like the ones above, check the permissions on /etc/php.ini:</p>
<pre><code># ls -al /etc/php.ini
-rw-------  1 root root 27 Jun  6 18:39 /etc/php.ini
</code></pre><p>As you can see, php.ini is only readable to root, which prevents PHP&rsquo;s command line parser from accessing your custom memory_limit directive in the php.ini. PHP&rsquo;s general default is 8M for a memory limit if nothing is specified anywhere else, and that&rsquo;s why normal users cannot get the higher memory limit that&rsquo;s set in your php.ini file.</p>
<p>Simply set the permissions on the file to 644 and you should be set to go:</p>
<pre><code># chmod 644 /etc/php.ini
# ls -al /etc/php.ini
-rw-r--r--  1 root root 45022 Jun  6 23:00 /etc/php.ini
</code></pre>]]></content></item><item><title>Send Plesk e-mail to /dev/null or blackhole</title><link>https://major.io/2007/06/14/send-plesk-e-mail-to-devnull-or-blackhole/</link><pubDate>Thu, 14 Jun 2007 22:42:22 +0000</pubDate><guid>https://major.io/2007/06/14/send-plesk-e-mail-to-devnull-or-blackhole/</guid><description>Should you find yourself needing to send e-mail destined for a certain account to a blackhole or to /dev/null, you&amp;rsquo;ll find very little information from Google. The actual solution is not terribly intuitive, and not well documented:
Click Domains Click the domain you want to modify Click Mail If the account hasn&amp;rsquo;t been created, click &amp;ldquo;Add New Mail Name&amp;rdquo; and create the account as usual. Then simply uncheck the mailbox option near the bottom.</description><content type="html"><![CDATA[<p>Should you find yourself needing to send e-mail destined for a certain account to a blackhole or to /dev/null, you&rsquo;ll find very little information from Google. The actual solution is not terribly intuitive, and not well documented:</p>
<ul>
<li>Click <strong>Domains</strong></li>
<li>Click the domain you want to modify</li>
<li>Click <strong>Mail</strong></li>
</ul>
<p><strong>If the account hasn&rsquo;t been created</strong>, click &ldquo;Add New Mail Name&rdquo; and create the account as usual. Then simply <strong>uncheck the mailbox</strong> option near the bottom. This will create a mail account, but any inbound e-mail for the user is thrown out.</p>
<p><strong>If the e-mail account has already been created</strong>, but you want to blackhole any future e-mails, just click the <strong>Mailbox</strong> icon and uncheck the <strong>Mailbox</strong> checkbox on the next page. Click <strong>OK</strong> and any future e-mails are thrown out.</p>
]]></content></item><item><title>Rebuild RPM file permissions and ownerships</title><link>https://major.io/2007/06/14/rebuild-rpm-file-permissions-and-ownerships/</link><pubDate>Thu, 14 Jun 2007 22:36:47 +0000</pubDate><guid>https://major.io/2007/06/14/rebuild-rpm-file-permissions-and-ownerships/</guid><description>If you find that someone has done a recursive chmod or chown on a server, don&amp;rsquo;t fret. You can set almost everything back to its original permissions and ownership by doing the following:
rpm -qa | xargs rpm --setperms --setugids Depending on how many packages are installed as well as the speed of your disk I/O, this may take a while to complete.</description><content type="html"><![CDATA[<p>If you find that someone has done a recursive chmod or chown on a server, don&rsquo;t fret. You can set almost everything back to its original permissions and ownership by doing the following:</p>
<pre><code>rpm -qa | xargs rpm --setperms --setugids
</code></pre><p>Depending on how many packages are installed as well as the speed of your disk I/O, this may take a while to complete.</p>
]]></content></item><item><title>Replace Urchin license key / serial number</title><link>https://major.io/2007/06/06/replace-urchin-license-key-serial-number/</link><pubDate>Thu, 07 Jun 2007 04:47:52 +0000</pubDate><guid>https://major.io/2007/06/06/replace-urchin-license-key-serial-number/</guid><description>If something horrible happened to your Urchin license key or you need to replace it with something else, just run this command to change the key:
cd /usr/local/urchin/util ./uconf-driver action=set_parameter recnum=1 ct_serial=[NEW SERIAL] uconf-driver action=set_parameter recnum=1 ct_license=0 For some reason, this blows up on some Urchin versions. If it doesn&amp;rsquo;t work, then the command will actually remove your license entirely. Don&amp;rsquo;t worry! You can log into Urchin&amp;rsquo;s web interface and put in the new key without a problem.</description><content type="html"><![CDATA[<p>If something horrible happened to your Urchin license key or you need to replace it with something else, just run this command to change the key:</p>
<pre><code>cd /usr/local/urchin/util
./uconf-driver action=set_parameter recnum=1 ct_serial=[NEW SERIAL] uconf-driver action=set_parameter recnum=1 ct_license=0
</code></pre><p>For some reason, this blows up on some Urchin versions. If it doesn&rsquo;t work, then the command will actually remove your license entirely. Don&rsquo;t worry! You can log into Urchin&rsquo;s web interface and put in the new key without a problem.</p>
]]></content></item><item><title>Postgres process listing</title><link>https://major.io/2007/06/06/postgres-process-listing/</link><pubDate>Thu, 07 Jun 2007 04:44:39 +0000</pubDate><guid>https://major.io/2007/06/06/postgres-process-listing/</guid><description>If you&amp;rsquo;re used to SHOW PROCESSLIST; or mysqladmin processlist in MySQL, you might be searching for this same functionality in postgresql. Here&amp;rsquo;s the quick way to get a process list in postgresql:
Switch to the postgres user:
# su - postgres
Get into the postgres shell:
# psql
Then run a quick query:
select * from pg_stat_activity;
NOTE: To actually see the queries being run, you will need logging enabled (it&amp;rsquo;s disabled by default).</description><content type="html"><![CDATA[<p>If you&rsquo;re used to <code>SHOW PROCESSLIST;</code> or <code>mysqladmin processlist</code> in MySQL, you might be searching for this same functionality in postgresql. Here&rsquo;s the quick way to get a process list in postgresql:</p>
<p>Switch to the postgres user:</p>
<p><code># su - postgres</code></p>
<p>Get into the postgres shell:</p>
<p><code># psql</code></p>
<p>Then run a quick query:</p>
<p><code>select * from pg_stat_activity;</code></p>
<p><strong>NOTE:</strong> To actually see the queries being run, you will need logging enabled (it&rsquo;s disabled by default). I don&rsquo;t know how to turn it on yet, so this post will be left open until I find out!</p>
]]></content></item><item><title>FreeBSD: Limiting closed port RST response</title><link>https://major.io/2007/06/06/freebsd-limiting-closed-port-rst-response/</link><pubDate>Thu, 07 Jun 2007 04:42:13 +0000</pubDate><guid>https://major.io/2007/06/06/freebsd-limiting-closed-port-rst-response/</guid><description>One of the nifty things about FreeBSD&amp;rsquo;s kernel is that it will limit closed port RST responses, which, in layman&amp;rsquo;s terms, just means that if someone repeatedly hits a port that&amp;rsquo;s closed, the kernel won&amp;rsquo;t respond to all of the requests.
You generally get something like this in the system log:
kernel: Limiting closed port RST response from 211 to 200 packets/sec kernel: Limiting closed port RST response from203 to 200 packets/sec In certain situations, this functionality might be undesirable.</description><content type="html"><![CDATA[<p>One of the nifty things about FreeBSD&rsquo;s kernel is that it will limit closed port RST responses, which, in layman&rsquo;s terms, just means that if someone repeatedly hits a port that&rsquo;s closed, the kernel won&rsquo;t respond to all of the requests.</p>
<p>You generally get something like this in the system log:</p>
<pre><code>kernel: Limiting closed port RST response from 211 to 200 packets/sec
kernel: Limiting closed port RST response from203 to 200 packets/sec
</code></pre><p>In certain situations, this functionality might be undesirable. For example, if you&rsquo;re running an IDS like snort or a vulnerability scanner like nessus, these responses might be helpful. If you want to disable this functionality, just add the following to <code>/etc/sysctl.conf</code>:</p>
<pre><code>net.inet.tcp.blackhole=2
net.inet.udp.blackhole=1
</code></pre>]]></content></item><item><title>Arrow keys in iTerm not working in vi/vim</title><link>https://major.io/2007/05/31/arrow-keys-in-iterm-not-working-in-vivim/</link><pubDate>Fri, 01 Jun 2007 03:28:58 +0000</pubDate><guid>https://major.io/2007/05/31/arrow-keys-in-iterm-not-working-in-vivim/</guid><description>I found myself pretty darned frustrated when my arrow keys didn&amp;rsquo;t work in iTerm in vi/vim or other ncurses-based applications. However, give this a shot in an iTerm if you find yourself in the same predicament:
export TERM=linux
Then open something in vi/vim or run an ncurses application. It should let your arrow keys work normally now. To make the setting stick, just do this:
echo &amp;quot;TERM=linux&amp;quot; &amp;gt;&amp;gt; ~/.profile</description><content type="html"><![CDATA[<p>I found myself pretty darned frustrated when my arrow keys didn&rsquo;t work in iTerm in vi/vim or other ncurses-based applications. However, give this a shot in an iTerm if you find yourself in the same predicament:</p>
<p><code>export TERM=linux</code></p>
<p>Then open something in vi/vim or run an ncurses application. It should let your arrow keys work normally now. To make the setting stick, just do this:</p>
<p><code>echo &quot;TERM=linux&quot; &gt;&gt; ~/.profile</code></p>
]]></content></item><item><title>Cisco PIX: Cannot select private key</title><link>https://major.io/2007/05/27/cisco-pix-cannot-select-private-key/</link><pubDate>Mon, 28 May 2007 02:10:13 +0000</pubDate><guid>https://major.io/2007/05/27/cisco-pix-cannot-select-private-key/</guid><description>If you receive the following error, your PIX does not have a key set up for use with SSH:
Type help or '?' for a list of available commands. pix&amp;gt; Cannot select private key Regenerating the key can be done by executing the following:
conf t ca zeroize rsa ca generate rsa key 1024 ca save all write mem reload</description><content type="html"><![CDATA[<p>If you receive the following error, your PIX does not have a key set up for use with SSH:</p>
<pre><code>Type help or '?' for a list of available commands.
pix&gt;
Cannot select private key
</code></pre><p>Regenerating the key can be done by executing the following:</p>
<pre><code>conf t
ca zeroize rsa
ca generate rsa key 1024
ca save all
write mem
reload
</code></pre>]]></content></item><item><title>Install snort and BASE on FreeBSD</title><link>https://major.io/2007/05/27/install-snort-and-base-on-freebsd/</link><pubDate>Sun, 27 May 2007 22:23:17 +0000</pubDate><guid>https://major.io/2007/05/27/install-snort-and-base-on-freebsd/</guid><description>Installing snort from ports on FreeBSD is pretty straightforward, but there are some &amp;lsquo;gotchas&amp;rsquo; that you need to be aware of. Here&amp;rsquo;s a step by step:
Compile snort form the ports tree:
# portinstall snort -- OR -- # make -C /usr/ports/security/snort install all You will be asked about which support you want to add to snort, so be sure to choose MySQL (unless you&amp;rsquo;re not going to use MySQL). When the build is complete, you&amp;rsquo;ll need oinkmaster as well to update your snort rules:</description><content type="html"><![CDATA[<p>Installing snort from ports on FreeBSD is pretty straightforward, but there are some &lsquo;gotchas&rsquo; that you need to be aware of. Here&rsquo;s a step by step:</p>
<p>Compile snort form the ports tree:</p>
<pre><code># portinstall snort
-- OR --
# make -C /usr/ports/security/snort install all
</code></pre><p>You will be asked about which support you want to add to snort, so be sure to choose MySQL (unless you&rsquo;re not going to use MySQL). When the build is complete, you&rsquo;ll need oinkmaster as well to update your snort rules:</p>
<pre><code># portinstall oinkmaster
-- OR --
# make -C /usr/ports/security/oinkmaster install all
</code></pre><p>Oinkmaster needs a snort download code/hash to be able to get your rules for you. Go to <a href="http://snort.org">http://snort.org</a> and register for an account. You&rsquo;ll be given a hash (looks SHA-1-ish) at the bottom of your main account page. Copy /usr/local/etc/oinkmaster.conf.sample to /usr/local/etc/oinkmaster.conf:</p>
<pre><code># cp /usr/local/etc/oinkmaster.conf.sample /usr/local/etc/oinkmaster.conf
</code></pre><p>Replace <strong><!-- raw HTML omitted --></strong> with the hash you received from snort.org in /usr/local/etc/oinkmaster.conf and uncomment the line:</p>
<pre><code># Example for Snort-current (&quot;current&quot; means cvs snapshots).
url = http://www.snort.org/pub-bin/oinkmaster.cgi/&lt;oinkcode&gt;/snortrules-snapshot-CURRENT.tar.gz
</code></pre><p>Now that oinkmaster is set up, you can update your snort rules using this command:</p>
<pre><code># oinkmaster -o /usr/local/etc/snort/rules/
Loading /usr/local/etc/oinkmaster.conf
Downloading file from http://www.snort.org/pub-bin/oinkmaster.cgi/*oinkcode*/snortrules-snapshot-CURRENT.tar.gz... done.
Archive successfully downloaded, unpacking... done.
Setting up rules structures... done.
Processing downloaded rules... disabled 0, enabled 0, modified 0, total=9942
Setting up rules structures... done.
Comparing new files to the old ones... done.
Updating local rules files... done.
</code></pre><p>Create the snort database and user:</p>
<pre><code># mysql -u root -ppassword
mysql&gt; CREATE DATABASE `snort`;
mysql&gt; GRANT ALL PRIVILEGES ON snort.* TO 'snort'@'localhost' IDENTIFIED BY 'snortpassword';`
</code></pre><p>There&rsquo;s a script that is pre-packaged with snort to set up the tables for you:</p>
<pre><code># mysql -u snort -psnortpassword snort &lt; /usr/local/share/examples/snort/create_mysql
</code></pre><p>Now it&rsquo;s time to make changes in the snort.conf:</p>
<pre><code># nano -w /usr/local/etc/snort/snort.conf
</code></pre><p>Uncomment and configure these lines:</p>
<pre><code># config detection: search-method lowmem
# output alert_syslog: LOG_AUTH LOG_ALERT
# output database: log, mysql, user=root password=test dbname=db host=localhost
</code></pre><p>Uncomment all of the <code>include $RULE_PATH/*.rules</code> lines except for this one:</p>
<pre><code># include $RULE_PATH/local.rules [comment this line out]
</code></pre><p>Now, enable snort in the /etc/rc.conf and start it up:</p>
<pre><code># echo &quot;snort_enable=\&quot;YES\&quot;&quot; &gt;&gt; /etc/rc.conf
# /usr/local/etc/rc.d/snort start
Starting snort.
</code></pre><p>If you run <code>tail /var/log/messages</code>, you should get some output like this:</p>
<pre><code>snort[12558]: Initializing daemon mode
kernel: fxp0: promiscuous mode enabled
snort[12559]: PID path stat checked out ok, PID path set to /var/run/
snort[12559]: Writing PID &quot;12559&quot; to file &quot;/var/run//snort_fxp0.pid&quot;
snort[12559]: Daemon initialized, signaled parent pid: 12558
snort[12558]: Daemon parent exiting
snort[12559]: Snort initialization completed successfully (pid=12559)
</code></pre><p>If you see an error like this, don&rsquo;t worry, nothing&rsquo;s wrong:</p>
<pre><code>snort[12559]: Not Using PCAP_FRAMES
</code></pre><p>To test snort, run a ping against your server from an outside source, and you should see something in your syslog like this:</p>
<pre><code>snort[12559]: [1:368:6] ICMP PING BSDtype [Classification: Misc activity] [Priority: 3]: {ICMP} xxx.xxx.xxx.xxx -&gt; xxx.xxx.xxx.xxx
snort[12559]: [1:366:7] ICMP PING *NIX [Classification: Misc activity] [Priority: 3]: {ICMP} xxx.xxx.xxx.xxx -&gt; xxx.xxx.xxx.xxx
snort[12559]: [1:384:5] ICMP PING [Classification: Misc activity] [Priority: 3]: {ICMP} xxx.xxx.xxx.xxx -&gt; xxx.xxx.xxx.xxx
</code></pre><p>Installing BASE is pretty simple. You&rsquo;ll need the adodb port plus the BASE tarball from SourceForge:</p>
<pre><code># portinstall adodb
-- OR --
# make -C /usr/ports/databases/adodb install clean
</code></pre><p>After you expand the tarball, go to your BASE install&rsquo;s URL in a browser. It will ask for the path to adodb, which is /usr/local/share/adodb. Provide the snort database information on the third screen and then just finish out the wizard. You will then be all set!</p>
]]></content></item><item><title>Install mysql-server from ports on FreeBSD</title><link>https://major.io/2007/05/27/install-mysql-server-from-ports-on-freebsd/</link><pubDate>Sun, 27 May 2007 21:47:13 +0000</pubDate><guid>https://major.io/2007/05/27/install-mysql-server-from-ports-on-freebsd/</guid><description>Installing mysql on FreeBSD from ports is one of the oddest installations I&amp;rsquo;ve ever completed. Here&amp;rsquo;s the step by step:
Get it compiled:
# portinstall mysql50-server -- OR -- # make -C /usr/ports/databases/mysql50-server install clean Once it&amp;rsquo;s installed, copy my-small.cnf, my-medium.cnf or my-huge.cnf to /usr/local/etc/my.cnf:
# cp /usr/local/share/mysql/my-small.cnf /usr/local/etc/my.cnf
Enable mysql in the rc.conf:
# echo &amp;quot;mysql_enable=\&amp;quot;YES\&amp;quot;&amp;quot; &amp;gt;&amp;gt; /etc/rc.conf
Install the authentication tables:
# mysql_install_db
Last, change the ownership on MySQL&amp;rsquo;s data directory:</description><content type="html"><![CDATA[<p>Installing mysql on FreeBSD from ports is one of the oddest installations I&rsquo;ve ever completed. Here&rsquo;s the step by step:</p>
<p>Get it compiled:</p>
<pre><code># portinstall mysql50-server
-- OR --
# make -C /usr/ports/databases/mysql50-server install clean
</code></pre><p>Once it&rsquo;s installed, copy my-small.cnf, my-medium.cnf or my-huge.cnf to /usr/local/etc/my.cnf:</p>
<p><code># cp /usr/local/share/mysql/my-small.cnf /usr/local/etc/my.cnf</code></p>
<p>Enable mysql in the rc.conf:</p>
<p><code># echo &quot;mysql_enable=\&quot;YES\&quot;&quot; &gt;&gt; /etc/rc.conf</code></p>
<p>Install the authentication tables:</p>
<p><code># mysql_install_db</code></p>
<p>Last, change the ownership on MySQL&rsquo;s data directory:</p>
<p><code># chown -R mysql:mysql /var/db/mysql</code></p>
<p>If you miss the last step, you&rsquo;ll get something ugly like this:</p>
<pre><code>mysqld started
[ERROR] /usr/local/libexec/mysqld: Can't find file: './mysql/host.frm' (errno: 13)
[ERROR] /usr/local/libexec/mysqld: Can't find file: './mysql/host.frm' (errno: 13)
[ERROR] Fatal error: Can't open and lock privilege tables: Can't find file: './mysql/host.frm' (errno: 13)
mysqld ended
</code></pre>]]></content></item><item><title>Errors with ifup regarding MAC addresses</title><link>https://major.io/2007/05/27/errors-with-ifup-regarding-mac-addresses/</link><pubDate>Sun, 27 May 2007 16:44:55 +0000</pubDate><guid>https://major.io/2007/05/27/errors-with-ifup-regarding-mac-addresses/</guid><description>If Redhat, CentOS, Fedora, or any other similar OS provides the following error:
# ifup eth1 Device eth1 has different MAC address than expected, ignoring. Check that someone didn&amp;rsquo;t put an IP in as a hardware address:
DEVICE=eth1 HWADDR=10.240.11.100 NETMASK=255.255.224.0 ONBOOT=yes TYPE=Ethernet If they did, then fix it with the correct configuration directive:
DEVICE=eth1 IPADDR=10.240.11.100 NETMASK=255.255.224.0 ONBOOT=yes TYPE=Ethernet</description><content type="html"><![CDATA[<p>If Redhat, CentOS, Fedora, or any other similar OS provides the following error:</p>
<pre><code># ifup eth1
Device eth1 has different MAC address than expected, ignoring.
</code></pre><p>Check that someone didn&rsquo;t put an IP in as a hardware address:</p>
<pre><code>DEVICE=eth1
HWADDR=10.240.11.100
NETMASK=255.255.224.0
ONBOOT=yes
TYPE=Ethernet
</code></pre><p>If they did, then fix it with the correct configuration directive:</p>
<pre><code>DEVICE=eth1
IPADDR=10.240.11.100
NETMASK=255.255.224.0
ONBOOT=yes
TYPE=Ethernet
</code></pre>]]></content></item><item><title>Forward e-mail sent to non-existent users in Postfix</title><link>https://major.io/2007/05/27/forward-e-mail-sent-to-non-existent-users-in-postfix/</link><pubDate>Sun, 27 May 2007 16:43:08 +0000</pubDate><guid>https://major.io/2007/05/27/forward-e-mail-sent-to-non-existent-users-in-postfix/</guid><description>Normally, Postfix will reject e-mail sent to non-existent users if a catchall isn&amp;rsquo;t present for the specific domain that is receiving mail. However, you can make a super catchall to catch any and all e-mail that Postfix receives for the domains in its mydestination list:
Add the following to /etc/postfix/main.cf:
luser_relay = root local_recipient_maps = Then reload the Postfix configuration:
# postfix reload For more information: http://www.postfix.org/rewrite.html#luser_relay</description><content type="html"><![CDATA[<p>Normally, Postfix will reject e-mail sent to non-existent users if a catchall isn&rsquo;t present for the specific domain that is receiving mail. However, you can make a super catchall to catch any and all e-mail that Postfix receives for the domains in its mydestination list:</p>
<p>Add the following to /etc/postfix/main.cf:</p>
<pre><code>luser_relay = root
local_recipient_maps =
</code></pre><p>Then reload the Postfix configuration:</p>
<pre><code># postfix reload
</code></pre><p>For more information: <a href="http://www.postfix.org/rewrite.html#luser_relay">http://www.postfix.org/rewrite.html#luser_relay</a></p>
]]></content></item><item><title>rpmdb: Lock table is out of available locker entries</title><link>https://major.io/2007/05/27/rpmdb-lock-table-is-out-of-available-locker-entries/</link><pubDate>Sun, 27 May 2007 16:38:32 +0000</pubDate><guid>https://major.io/2007/05/27/rpmdb-lock-table-is-out-of-available-locker-entries/</guid><description>If up2date throws some horrible Python errors and rpm says “rpmdb: Lock table is out of available locker entries”, you can restore your system to normality with the following:
The errors:
rpmdb: Lock table is out of available locker entries error: db4 error(22) from db-&amp;gt;close: Invalid argument error: cannot open Packages index using db3 - Cannot allocate memory (12) error: cannot open Packages database in /var/lib/rpm Make a backup of /var/lib/rpm in case you break something:</description><content type="html"><![CDATA[<p>If up2date throws some horrible Python errors and rpm says “rpmdb: Lock table is out of available locker entries”, you can restore your system to normality with the following:</p>
<p>The errors:</p>
<pre><code>rpmdb: Lock table is out of available locker entries
error: db4 error(22) from db-&gt;close: Invalid argument
error: cannot open Packages index using db3 - Cannot allocate memory (12)
error: cannot open Packages database in /var/lib/rpm
</code></pre><p>Make a backup of /var/lib/rpm in case you break something:</p>
<pre><code>tar cvzf rpmdb-backup.tar.gz /var/lib/rpm
</code></pre><p>Remove the Berkeley databases that rpm uses:</p>
<pre><code>rm /var/lib/rpm/__db.00*
</code></pre><p>Make rpm rebuild the databases from scratch (may take a short while):</p>
<pre><code>rpm --rebuilddb
</code></pre><p>Now, check rpm to make sure everything is okay:</p>
<pre><code>rpm -qa | sort
</code></pre><p><strong>Why does this happen?</strong></p>
<p>When rpm accesses the Berkeley database files, it makes temporary locker entries within the tables while it searches for data. If you control-c your rpm processes often, this issue will occur much sooner because the locks are never cleared.</p>
]]></content></item><item><title>Remove PHP’s open_basedir restriction in Plesk</title><link>https://major.io/2007/05/23/remove-php-open_basedir-restriction-in-plesk/</link><pubDate>Wed, 23 May 2007 17:21:58 +0000</pubDate><guid>https://major.io/2007/05/23/remove-php-open_basedir-restriction-in-plesk/</guid><description>If you have an open_basedir restriction that is causing issues with a domain, you can remove the restriction easily. First, put the following text in /home/httpd/vhosts/[domain]/conf/vhost.conf:
&amp;lt;Directory /home/httpd/vhosts/[domain]/httpdocs&amp;gt; php_admin_value open_basedir none &amp;lt;/Directory&amp;gt; If there was already a vhost.conf in the directory, then just reload Apache. Otherwise, run the magic wand:
/usr/local/psa/admin/bin/websrvmng -av Then reload Apache:
/etc/init.d/httpd reload</description><content type="html"><![CDATA[<p>If you have an open_basedir restriction that is causing issues with a domain, you can remove the restriction easily. First, put the following text in /home/httpd/vhosts/[domain]/conf/vhost.conf:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-apache" data-lang="apache"><span style="color:#268bd2">&lt;Directory</span> <span style="color:#2aa198">/home/httpd/vhosts/[domain]/httpdocs</span><span style="color:#268bd2">&gt;</span>
<span style="color:#b58900">php_admin_value</span> open_basedir <span style="color:#719e07">none</span>
<span style="color:#268bd2">&lt;/Directory&gt;</span>
</code></pre></div><p>If there was already a vhost.conf in the directory, then just reload Apache. Otherwise, run the magic wand:</p>
<pre><code>/usr/local/psa/admin/bin/websrvmng -av
</code></pre><p>Then reload Apache:</p>
<pre><code>/etc/init.d/httpd reload
</code></pre>]]></content></item><item><title>Changing the default SSL certificate in Plesk</title><link>https://major.io/2007/05/21/changing-the-default-ssl-certificate-in-plesk/</link><pubDate>Tue, 22 May 2007 02:16:57 +0000</pubDate><guid>https://major.io/2007/05/21/changing-the-default-ssl-certificate-in-plesk/</guid><description>When Plesk is installed, the default certificate for the Plesk interface itself is a self-signed certificate that is generated during the installation. This can be easily changed within the Server options page.
If your SSL certificate is installed at the domain level:
Click Domains &amp;gt; domain.com &amp;gt; Certificates &amp;gt; certificate name. Copy the CSR, key and CA certificates to a text application temporarily, and then click Server &amp;gt; Certificates. Once you&amp;rsquo;re there, click Add Certificate and paste in the CSR, key and CA certificate.</description><content type="html"><![CDATA[<p>When Plesk is installed, the default certificate for the Plesk interface itself is a self-signed certificate that is generated during the installation. This can be easily changed within the Server options page.</p>
<p><strong>If your SSL certificate is installed at the domain level:</strong></p>
<p>Click Domains &gt; domain.com &gt; Certificates &gt; certificate name. Copy the CSR, key and CA certificates to a text application temporarily, and then click Server &gt; Certificates. Once you&rsquo;re there, click Add Certificate and paste in the CSR, key and CA certificate. You will need to select a new name for the certificate that is different from the one you use at the domain level. Once you&rsquo;re done inserting that information, click OK and follow the instructions below.</p>
<p><strong>If your SSL certificate is installed at the server level</strong></p>
<p>Click Server &gt; Certificates. Click the checkbox next to the certificate which needs to be installed as the default, then click Setup just above the certificate listing. Plesk will install the certificate and reload itself (which generally takes 5-15 seconds). Depending on your browser, you may need to log out of Plesk and log back in to see the new certificate.</p>
<p>When everything is complete, verify that the correct certificate is used when you access the Plesk interface, and also be sure that the intermediate certificates are installed correctly as well.</p>
]]></content></item><item><title>Enable submission port 587 in Sendmail</title><link>https://major.io/2007/05/21/enable-submission-port-587-in-sendmail/</link><pubDate>Mon, 21 May 2007 16:08:34 +0000</pubDate><guid>https://major.io/2007/05/21/enable-submission-port-587-in-sendmail/</guid><description>To enable submission access on port 587 in sendmail, add the following to the sendmail.mc:
DAEMON_OPTIONS(`Port=submission, Name=MSA, M=Ea')dnl
Rebuild the sendmail.cf file and restart sendmail.</description><content type="html"><![CDATA[<p>To enable submission access on port 587 in sendmail, add the following to the sendmail.mc:</p>
<p><code>DAEMON_OPTIONS(`Port=submission, Name=MSA, M=Ea')dnl</code></p>
<p>Rebuild the sendmail.cf file and restart sendmail.</p>
]]></content></item><item><title>Postgresql not listening on network</title><link>https://major.io/2007/05/21/postgresql-not-listening-on-network/</link><pubDate>Mon, 21 May 2007 15:04:12 +0000</pubDate><guid>https://major.io/2007/05/21/postgresql-not-listening-on-network/</guid><description>On some operating systems, postgresql is not configured to listen on the network. To enable the TCP/IP connections, edit the /var/lib/pgsql/data/postgresql.conf and change the following:
tcpip_socket = true port = 5432 Restart postgresql and you should be all set:
/etc/init.d/postgresql restart</description><content type="html"><![CDATA[<p>On some operating systems, postgresql is not configured to listen on the network. To enable the TCP/IP connections, edit the /var/lib/pgsql/data/postgresql.conf and change the following:</p>
<pre><code>tcpip_socket = true
port = 5432
</code></pre><p>Restart postgresql and you should be all set:</p>
<pre><code>/etc/init.d/postgresql restart
</code></pre>]]></content></item><item><title>Speeding up MySQL</title><link>https://major.io/2007/05/20/speeding-up-mysql/</link><pubDate>Mon, 21 May 2007 03:44:33 +0000</pubDate><guid>https://major.io/2007/05/20/speeding-up-mysql/</guid><description>If there&amp;rsquo;s one question I get a lot, it would be &amp;ldquo;Hey, how can I speed up MySQL?&amp;rdquo; There&amp;rsquo;s absolutely no end-all, be-all answer to this question. Instead a combination of many factors contribute to the overall performance of any SQL server. However, here&amp;rsquo;s a list of my recommendations for great MySQL performance. They&amp;rsquo;re arranged from the biggest gains to smallest gains:
Query Optimization
I know - you were hoping I&amp;rsquo;d talk about hardware to start this thing off, but optimizing queries is the #1 way to get a MySQL server in gear.</description><content type="html"><![CDATA[<p>If there&rsquo;s one question I get a lot, it would be &ldquo;Hey, how can I speed up MySQL?&rdquo; There&rsquo;s absolutely no end-all, be-all answer to this question. Instead a combination of many factors contribute to the overall performance of any SQL server. However, here&rsquo;s a list of my recommendations for great MySQL performance. They&rsquo;re arranged from the biggest gains to smallest gains:</p>
<p><strong>Query Optimization</strong></p>
<p>I know - you were hoping I&rsquo;d talk about hardware to start this thing off, but optimizing queries is the #1 way to get a MySQL server in gear. MySQL gives you great tools, like the slow query log, multiple status variables, and the EXPLAIN statement. Put these three things together and your queries will be on their way to a more optimized state. I&rsquo;ll go into great detail about query optimization in a later post.</p>
<p><strong>Memory / System Architecture</strong></p>
<p>We all know MySQL likes RAM, and the more you give it (to a point) the better the performance will be. If you consider the alternative to memory, which is swapping on disk, it&rsquo;s obvious to see the gains.</p>
<p>So why did I add system architecture to this section? Well, if you have 32-bit Redhat, you can only allocate 2GB per process with the standard kernel. If you jump up to the SMP or hugemem kernel (in ES 2.1, you need the hugemem kernel for this to work), you can allocate 3GB per process. There is a caveat - MySQL can only use 2GB per buffer in 32-bit land. In a 64-bit OS with an appropriate Redhat kernel, you can allocate much larger buffers, and this can be tremendously helpful to tables which use the InnoDB engine. The memory allocation abilities are a great benefit, but also keep in mind that you will also get a boost in math performance within MySQL due to the 64-bit architecture. It&rsquo;s a win-win!</p>
<p><strong>Disk Performance</strong></p>
<p>Running a critical database on IDE or SATA drives just doesn&rsquo;t cut it any more. A SCSI or SAS drive is required for the best performance. Although you hope that MySQL doesn&rsquo;t touch the disk much, it&rsquo;s important to remember that you need to make backups often, and you may need to restore data. Also, if your site is write-intensive, the disk performance is much more important than you think. It will reduce the time that tables are locked, and it will also reduce the time for backups and restores.</p>
<p><strong>CPU</strong></p>
<p>Although CPU comes last, don&rsquo;t forget how important it can be. If you run a high number of complex queries and perform a lot of mathematical operations, you&rsquo;re going to need a CPU that can handle this load. Dual CPU&rsquo;s or dual core CPU&rsquo;s will help out even more, since MySQL can use multiple CPU cores to perform simultaneous operations. Keep in mind that 64-bit will outperform 32-bit in MySQL, and also allow for greater memory allocations (look in the Memory section above).</p>
<p><strong><em>Final Note:</em></strong></p>
<p>Keep in mind that these are <strong>general</strong> suggestions, and these suggestions may not apply to all users. For example, on sites that are heavily read-intensive, you may find that CPU speed is more important than disk speed. Also, if you&rsquo;re not using all of the available memory on your server, but your performance is still sagging, adding more memory won&rsquo;t help. Consult with a DBA and find out where your server&rsquo;s slowdowns are, then make a change with your queries or with your hardware. Remember, throwing more hardware at the problem will not always solve it.</p>
]]></content></item><item><title>MySQL connections in sleep state</title><link>https://major.io/2007/05/20/mysql-connections-in-sleep-state/</link><pubDate>Mon, 21 May 2007 03:26:11 +0000</pubDate><guid>https://major.io/2007/05/20/mysql-connections-in-sleep-state/</guid><description>On some servers, you may notice that MySQL is consuming CPU and memory resources when it&amp;rsquo;s not processing any queries. During these times, running a mysqladmin processlist will show many processes in the &amp;lsquo;sleep&amp;rsquo; state for many minutes.
These issues occur because of code that uses a persistent connection to the database. In PHP, this is done with mysql_pconnect. This causes PHP to connect to the database, execute queries, remove the authentication for the connection, and then leave the connection open.</description><content type="html"><![CDATA[<p>On some servers, you may notice that MySQL is consuming CPU and memory resources when it&rsquo;s not processing any queries. During these times, running a <code>mysqladmin processlist</code> will show many processes in the &lsquo;sleep&rsquo; state for many minutes.</p>
<p>These issues occur because of code that uses a persistent connection to the database. In PHP, this is done with mysql_pconnect. This causes PHP to connect to the database, execute queries, remove the authentication for the connection, and then leave the connection open. Any per-thread buffers will be kept in memory until the thread dies (which is 28,800 seconds in MySQL by default). There&rsquo;s three ways to handle this type of issue:</p>
<p><strong>Fix the code</strong></p>
<p>This is the #1 most effective way to correct the problem. Persistent connections are <strong>rarely</strong> needed. The only time when they would be even mildly useful is if your MySQL server has a huge latency. For example, if your web server takes &gt; 250ms to make contact with your MySQL server, this setting might save you fractions of a second. Then again, if your web server and MySQL server are so far apart to where latency is even a consideration, you have more problems than I can help you with.</p>
<p><strong>Restrict the connections</strong></p>
<p>If push comes to shove, and you have users on a server who are abusing their MySQL privileges with mysql_pconnect, then you can pull the plug on their shenanigans with GRANT. You can reduce the maximum simultaneous connections for their database user, and they&rsquo;ll find themselves wanting to make code changes pretty quickly. MySQL doesn&rsquo;t queue extra connections for users who have passed their maximum, so they get a really nice error stating that they have exceeded their max connections. To set up this grant, just do something like the following:</p>
<p><code>GRANT ALL PRIVILEGES ON database.* TO 'someuser'@'localhost' WITH MAX_USER_CONNECTIONS = 20;</code></p>
<p><strong>Reduce the timeouts</strong></p>
<p>If changing the code isn&rsquo;t an option, and you don&rsquo;t feel mean enough to restrict your users (however, if they were causing a denial of service on my MySQL server, I&rsquo;d have no problem restricting them), you can reduce the <em>wait_timeout</em> and <em>interactive_timeout</em> variables. The wait_timeout affects non-interactive connections (like TCP/IP and Unix socket) and interactive_timeout affects interactive connections (if you don&rsquo;t know what these are, you&rsquo;re not alone). The defaults of these are fairly high (usually 480 minutes) and you can drop them to something more reasonable, like 30-60 seconds. Web visitors shouldn&rsquo;t notice the difference - it will just cause the next page load to start a new connection to the database server.</p>
]]></content></item><item><title>Joomla and Plesk permissions</title><link>https://major.io/2007/05/20/joomla-and-plesk-permissions/</link><pubDate>Mon, 21 May 2007 03:13:23 +0000</pubDate><guid>https://major.io/2007/05/20/joomla-and-plesk-permissions/</guid><description>Thanks to a highly awesome technician on my team, we&amp;rsquo;ve discovered the perfect permissions setup for Joomla and Plesk:
Change the umask in &amp;lsquo;/etc/proftpd.conf&amp;rsquo; to &amp;lsquo;002&amp;rsquo; and add the &amp;lsquo;apache&amp;rsquo; user to the &amp;lsquo;psacln&amp;rsquo; group. Then, update the directory permissions:
cd /home/httpd/vhosts/[domain.com] chown -R [username]:psacln httpdocs chmod -R g+w httpdocs find httpdocs -type d -exec chmod g+s {} \; Joomla also complains about some PHP settings, sometimes including not being able to write to &amp;lsquo;/var/lib/php/session&amp;rsquo;.</description><content type="html"><![CDATA[<p>Thanks to a highly awesome technician on my team, we&rsquo;ve discovered the perfect permissions setup for Joomla and Plesk:</p>
<p>Change the umask in &lsquo;/etc/proftpd.conf&rsquo; to &lsquo;002&rsquo; and add the &lsquo;apache&rsquo; user to the &lsquo;psacln&rsquo; group. Then, update the directory permissions:</p>
<pre><code>cd /home/httpd/vhosts/[domain.com]
chown -R [username]:psacln httpdocs
chmod -R g+w httpdocs
find httpdocs -type d -exec chmod g+s {} \;
</code></pre><p>Joomla also complains about some PHP settings, sometimes including not being able to write to &lsquo;/var/lib/php/session&rsquo;. To fix the issues, make some adjustments to the vhost.conf for the domain:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-apache" data-lang="apache"><span style="color:#268bd2">&lt;Directory</span> <span style="color:#2aa198">/home/httpd/vhosts/[domain]/httpdocs</span><span style="color:#268bd2">&gt;</span>
<span style="color:#b58900">php_admin_flag</span> magic_quotes_gpc <span style="color:#719e07">on</span>
<span style="color:#b58900">php_admin_flag</span> display_errors <span style="color:#719e07">on</span>
<span style="color:#b58900">php_admin_value</span> session.save_path <span style="color:#2aa198">/tmp</span>
<span style="color:#268bd2">&lt;/Directory&gt;</span>
</code></pre></div><p>If the vhost.conf is brand new, then run:</p>
<pre><code>/usr/local/psa/admin/bin/websrvmng -av
</code></pre><p>Make sure Apache runs with your new configuration:</p>
<pre><code># httpd -t (check your work)
# /etc/init.d/httpd reload
</code></pre><p>Credit for this goes to Bryan T.</p>
]]></content></item><item><title>Remove query strings from URL’s with mod_rewrite</title><link>https://major.io/2007/05/18/remove-query-strings-from-urls-with-mod_rewrite/</link><pubDate>Fri, 18 May 2007 14:07:37 +0000</pubDate><guid>https://major.io/2007/05/18/remove-query-strings-from-urls-with-mod_rewrite/</guid><description>If you need to strip query strings from a URL with mod_rewrite, you can use a rewrite syntax such as the following:
RewriteEngine on RewriteCond %{QUERY_STRING} &amp;quot;action=register&amp;quot; [NC] RewriteRule ^/. http://www.domain.com/registerpage.html? [R,L]</description><content type="html"><![CDATA[<p>If you need to strip query strings from a URL with mod_rewrite, you can use a rewrite syntax such as the following:</p>
<pre><code class="language-apache2" data-lang="apache2">RewriteEngine on
RewriteCond %{QUERY_STRING} &quot;action=register&quot; [NC]
RewriteRule ^/. http://www.domain.com/registerpage.html? [R,L]
</code></pre>]]></content></item><item><title>Relay access denied</title><link>https://major.io/2007/05/17/relay-access-denied/</link><pubDate>Fri, 18 May 2007 03:16:06 +0000</pubDate><guid>https://major.io/2007/05/17/relay-access-denied/</guid><description>If you&amp;rsquo;re checking through your mail logs, or you catch a bounced e-mail with &amp;ldquo;554 relay access denied&amp;rdquo; in the bounce, the issue can be related to a few different things:
If your server bounces with this message when people send e-mail to you:
Check to make sure that your mail server is configured to receive mail for your domain Postfix: /etc/postfix/mydomains (on some systems) Sendmail: /etc/mail/local-host-names Qmail: /var/qmail/control/rcpthosts Verify that your MX records are pointing to your server, and not someone else&amp;rsquo;s (very important during server migrations) If you recently made changes in Postfix, make sure to run postmap on your domains file and run postfix reload If you get this message when you try to send e-mail to other people through your server:</description><content type="html"><![CDATA[<p>If you&rsquo;re checking through your mail logs, or you catch a bounced e-mail with &ldquo;554 relay access denied&rdquo; in the bounce, the issue can be related to a few different things:</p>
<p>If <strong>your</strong> server bounces with this message when people send e-mail to <strong>you</strong>:</p>
<ul>
<li>Check to make sure that your mail server is configured to receive mail for your domain
<ul>
<li>Postfix: /etc/postfix/mydomains (on some systems)</li>
<li>Sendmail: /etc/mail/local-host-names</li>
<li>Qmail: /var/qmail/control/rcpthosts</li>
</ul>
</li>
<li>Verify that your MX records are pointing to your server, and not someone else&rsquo;s (very important during server migrations)</li>
<li>If you recently made changes in Postfix, make sure to run <code>postmap</code> on your domains file and run <code>postfix reload</code></li>
</ul>
<p>If you get this message when you try to send e-mail to other people through your server:</p>
<ul>
<li>Enable SMTP authentication in your e-mail client</li>
<li>If SMTP authentication is on in your client, check your server&rsquo;s authentication daemons to be sure they&rsquo;re operating properly</li>
</ul>
]]></content></item><item><title>Show hidden dot files in proftpd</title><link>https://major.io/2007/05/16/show-hidden-dot-files-in-proftpd/</link><pubDate>Thu, 17 May 2007 01:46:59 +0000</pubDate><guid>https://major.io/2007/05/16/show-hidden-dot-files-in-proftpd/</guid><description>If you can&amp;rsquo;t see hidden files in proftpd (the files beginning with a dot, like .htaccess), you can enable the option in your client. However, you can force the files to be displayed in almost all clients with a server wide variable in your proftpd.conf:
ListOptions -a
Make sure to restart proftpd afterwards and re-connect to the FTP server to see the changes.</description><content type="html"><![CDATA[<p>If you can&rsquo;t see hidden files in proftpd (the files beginning with a dot, like .htaccess), you can enable the option in your client. However, you can force the files to be displayed in almost all clients with a server wide variable in your proftpd.conf:</p>
<p><code>ListOptions -a</code></p>
<p>Make sure to restart proftpd afterwards and re-connect to the FTP server to see the changes.</p>
]]></content></item><item><title>Add SSL/TLS support to proftpd</title><link>https://major.io/2007/05/16/add-ssltls-support-to-proftpd/</link><pubDate>Thu, 17 May 2007 01:45:37 +0000</pubDate><guid>https://major.io/2007/05/16/add-ssltls-support-to-proftpd/</guid><description>To enable SSL/TLS support in proftpd, add the following to the proftpd.conf file:
&amp;lt;IfModule mod_tls.c&amp;gt; TLSEngine on TLSLog /var/ftpd/tls.log TLSRequired off TLSRSACertificateFile /usr/share/ssl/certs/server.crt TLSRSACertificateKeyFile /usr/share/ssl/private/server.key TLSCACertificateFile /usr/share/ssl/certs/cacert.crt TLSVerifyClient off TLSRenegotiate required off &amp;lt;/IfModule&amp;gt; To require SSL/TLS on all connections, change TLSRequired to on. Of course, replace the certificate, key, and CA certificate (if applicable) to the correct files on your system.
Once you&amp;rsquo;re all done, close your FTP connection and make a new one.</description><content type="html"><![CDATA[<p>To enable SSL/TLS support in proftpd, add the following to the proftpd.conf file:</p>
<pre><code>&lt;IfModule mod_tls.c&gt;
    TLSEngine on
    TLSLog /var/ftpd/tls.log
    TLSRequired off
    TLSRSACertificateFile /usr/share/ssl/certs/server.crt
    TLSRSACertificateKeyFile /usr/share/ssl/private/server.key
    TLSCACertificateFile /usr/share/ssl/certs/cacert.crt
    TLSVerifyClient off
    TLSRenegotiate required off
&lt;/IfModule&gt;
</code></pre><p>To <strong>require</strong> SSL/TLS on all connections, change <code>TLSRequired</code> to <strong>on</strong>. Of course, replace the certificate, key, and CA certificate (if applicable) to the correct files on your system.</p>
<p>Once you&rsquo;re all done, close your FTP connection and make a new one. There is no need to restart xinetd.</p>
]]></content></item><item><title>Plesk submission port (587) for outbound mail</title><link>https://major.io/2007/05/15/plesk-submission-port-587-for-outbound-mail/</link><pubDate>Tue, 15 May 2007 14:28:17 +0000</pubDate><guid>https://major.io/2007/05/15/plesk-submission-port-587-for-outbound-mail/</guid><description>If you can&amp;rsquo;t send mail via port 25 due to blocks imposed by your ISP, you can enable the submission port within Plesk pretty easily. There&amp;rsquo;s two methods:
The iptables way:
iptables -t nat -A PREROUTING -p tcp --dport 587 -i eth0 -j REDIRECT --to-ports 25 The xinetd way (recommended):
# cd /etc/xinetd.d # cp smtp_psa smtp_additional # vi smtp_additional Make the first line say &amp;ldquo;service submission&amp;rdquo; and save the file.</description><content type="html"><![CDATA[<p>If you can&rsquo;t send mail via port 25 due to blocks imposed by your ISP, you can enable the submission port within Plesk pretty easily. There&rsquo;s two methods:</p>
<p>The iptables way:</p>
<pre><code>iptables -t nat -A PREROUTING -p tcp --dport 587 -i eth0 -j REDIRECT --to-ports 25
</code></pre><p>The xinetd way (recommended):</p>
<pre><code># cd /etc/xinetd.d
# cp smtp_psa smtp_additional
# vi smtp_additional
</code></pre><p>Make the first line say &ldquo;service submission&rdquo; and save the file. Then restart xinetd:</p>
<pre><code>/etc/rc.d/init.d/xinetd restart
</code></pre><p><strong>This is no longer needed in Plesk 8.4. To enable the submission port in Plesk 8.4, log into the Plesk interface as the Administrator, click Server and click Mail.</strong></p>
]]></content></item><item><title>Horde refreshes when logging in</title><link>https://major.io/2007/05/07/horde-refreshes-when-logging-in/</link><pubDate>Mon, 07 May 2007 21:20:51 +0000</pubDate><guid>https://major.io/2007/05/07/horde-refreshes-when-logging-in/</guid><description>If you find that Horde (with Plesk) keeps refreshing when you attempt to log in, and there are no errors logged on the screen or in Apache&amp;rsquo;s logs, check the session.auto_start variable in /etc/php.ini.
If session.auto_start is set to 1, set it to 0 and Horde will miraculously start working again.</description><content type="html"><![CDATA[<p>If you find that Horde (with Plesk) keeps refreshing when you attempt to log in, and there are no errors logged on the screen or in Apache&rsquo;s logs, check the <code>session.auto_start</code> variable in /etc/php.ini.</p>
<p>If <code>session.auto_start</code> is set to 1, set it to 0 and Horde will miraculously start working again.</p>
]]></content></item><item><title>Plesk SQL Statements</title><link>https://major.io/2007/04/27/plesk-sql-statements/</link><pubDate>Fri, 27 Apr 2007 15:52:17 +0000</pubDate><guid>https://major.io/2007/04/27/plesk-sql-statements/</guid><description>When you need to find information about anything in Plesk, here&amp;rsquo;s some SQL statements that you can use:
Start out with:
# mysql -u admin -p`cat /etc/psa/.psa.shadow` mysql&amp;gt; use psa; Find all e-mail passwords:
select concat_ws(&amp;#39;@&amp;#39;,mail.mail_name,domains.name),accounts.password from domains,mail,accounts where domains.id=mail.dom_id and accounts.id=mail.account_id order by domains.name ASC,mail.mail_name ASC; Find e-mail passwords made out of only letters:
select concat_ws(&amp;#39;@&amp;#39;,mail.mail_name,domains.name),accounts.password from domains,mail,accounts where domains.id=mail.dom_id and accounts.id=mail.account_id and accounts.password rlike binary &amp;#39;^[a-z]+$&amp;#39;; Find e-mail passwords made out of only numbers:</description><content type="html"><![CDATA[<p>When you need to find information about anything in Plesk, here&rsquo;s some SQL statements that you can use:</p>
<p>Start out with:</p>
<pre><code># mysql -u admin -p`cat /etc/psa/.psa.shadow`
mysql&gt; use psa;
</code></pre><p>Find all e-mail passwords:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sql" data-lang="sql"><span style="color:#719e07">select</span> concat_ws(<span style="color:#2aa198">&#39;@&#39;</span>,mail.mail_name,domains.name),accounts.password <span style="color:#719e07">from</span> domains,mail,accounts <span style="color:#719e07">where</span> domains.id<span style="color:#719e07">=</span>mail.dom_id <span style="color:#719e07">and</span> accounts.id<span style="color:#719e07">=</span>mail.account_id <span style="color:#719e07">order</span> <span style="color:#719e07">by</span> domains.name <span style="color:#719e07">ASC</span>,mail.mail_name <span style="color:#719e07">ASC</span>;
</code></pre></div><p>Find e-mail passwords made out of only letters:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sql" data-lang="sql"><span style="color:#719e07">select</span> concat_ws(<span style="color:#2aa198">&#39;@&#39;</span>,mail.mail_name,domains.name),accounts.password <span style="color:#719e07">from</span> domains,mail,accounts <span style="color:#719e07">where</span> domains.id<span style="color:#719e07">=</span>mail.dom_id <span style="color:#719e07">and</span> accounts.id<span style="color:#719e07">=</span>mail.account_id <span style="color:#719e07">and</span> accounts.password rlike <span style="color:#b58900">binary</span> <span style="color:#2aa198">&#39;^[a-z]+$&#39;</span>;
</code></pre></div><p>Find e-mail passwords made out of only numbers:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sql" data-lang="sql"><span style="color:#719e07">select</span> concat_ws(<span style="color:#2aa198">&#39;@&#39;</span>,mail.mail_name,domains.name),accounts.password <span style="color:#719e07">from</span> domains,mail,accounts <span style="color:#719e07">where</span> domains.id<span style="color:#719e07">=</span>mail.dom_id <span style="color:#719e07">and</span> accounts.id<span style="color:#719e07">=</span>mail.account_id <span style="color:#719e07">and</span> accounts.password rlike <span style="color:#2aa198">&#39;^[0-9]+$&#39;</span>;
</code></pre></div><p>Find which domains aren&rsquo;t bouncing/rejecting e-mails to unknown recipients:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sql" data-lang="sql"><span style="color:#719e07">select</span> d.name <span style="color:#719e07">as</span> <span style="color:#719e07">domain</span>, p.value <span style="color:#719e07">as</span> catchall_address <span style="color:#719e07">from</span> <span style="color:#719e07">Parameters</span> p, DomainServices ds, domains d <span style="color:#719e07">where</span> d.id <span style="color:#719e07">=</span> ds.dom_id <span style="color:#719e07">and</span> ds.parameters_id <span style="color:#719e07">=</span> p.id <span style="color:#719e07">and</span> p.<span style="color:#719e07">parameter</span> <span style="color:#719e07">=</span> <span style="color:#2aa198">&#39;catch_addr&#39;</span> <span style="color:#719e07">order</span> <span style="color:#719e07">by</span> d.name
</code></pre></div>]]></content></item><item><title>Adding chrooted FTP users outside of Plesk</title><link>https://major.io/2007/04/27/adding-chrooted-ftp-users-outside-of-plesk/</link><pubDate>Fri, 27 Apr 2007 15:51:59 +0000</pubDate><guid>https://major.io/2007/04/27/adding-chrooted-ftp-users-outside-of-plesk/</guid><description>To add a chrooted FTP user outside of Plesk properly, you need to:
Create the user with the home directory as the root of what they can access Give the user a password Make their primary group psacln Add them to the psaserv group as well # useradd username -d /var/www/html/website/slideshow/ # echo &amp;quot;password&amp;quot; | passwd username --stdin Changing password for user username. passwd: all authentication tokens updated successfully.</description><content type="html"><![CDATA[<p>To add a chrooted FTP user outside of Plesk properly, you need to:</p>
<ul>
<li>Create the user with the home directory as the root of what they can access</li>
<li>Give the user a password</li>
<li>Make their primary group psacln</li>
<li>Add them to the psaserv group as well</li>
</ul>
<pre><code># useradd username -d /var/www/html/website/slideshow/
# echo &quot;password&quot; | passwd username --stdin
Changing password for user username.
passwd: all authentication tokens updated successfully.
# usermod -g psacln username
# usermod -G psaserv username
# lftp username:password@localhost
lftp username@localhost:/&gt; cd ..
lftp username@localhost:/&gt;
</code></pre>]]></content></item><item><title>Install PayFlowPro for PHP on RHEL</title><link>https://major.io/2007/04/26/install-payflowpro-for-php-on-rhel/</link><pubDate>Thu, 26 Apr 2007 22:06:35 +0000</pubDate><guid>https://major.io/2007/04/26/install-payflowpro-for-php-on-rhel/</guid><description>To install PayFlowPro, you will need a few things:
The PHP source code for version of PHP installed (go here) The SDK from Verisign/PayPal (this comes from the portal, login required) The gcc and automake packages Take the Verisign SDK and copy the following:
Copy pfpro.h to /usr/include Copy the .so file to /usr/lib Untar the PHP source code and cd into php-[version]/ext/pfpro. Run phpize and make sure it finishes successfully.</description><content type="html"><![CDATA[<p>To install PayFlowPro, you will need a few things:</p>
<ul>
<li>The PHP source code for version of PHP installed (<a href="http://museum.php.net/">go here</a>)</li>
<li>The SDK from Verisign/PayPal (this comes from the portal, login required)</li>
<li>The gcc and automake packages</li>
</ul>
<p>Take the Verisign SDK and copy the following:</p>
<ul>
<li>Copy pfpro.h to /usr/include</li>
<li>Copy the .so file to /usr/lib</li>
</ul>
<p>Untar the PHP source code and cd into php-[version]/ext/pfpro. Run <code>phpize</code> and make sure it finishes successfully. Now run:</p>
<p><code>./configure --prefix=/usr --enable-shared</code></p>
<p>Then run <code>make</code> and <code>make install</code>. Now, go to the php.ini and add:</p>
<p><code>extension=pfpro.so</code></p>
<p>Run <code>php -i | grep pfpro</code> to make sure the module was successfully built. Restart Apache and you&rsquo;re all set!</p>
<p><em>The pfpro module is now available via pecl in PHP 5.1+. Thanks to Chris R. for pointing that out.</em></p>
]]></content></item><item><title>Add SPF records to all domains in Plesk</title><link>https://major.io/2007/04/24/add-spf-records-to-all-domains-in-plesk/</link><pubDate>Tue, 24 Apr 2007 16:28:31 +0000</pubDate><guid>https://major.io/2007/04/24/add-spf-records-to-all-domains-in-plesk/</guid><description>If you find yourself in the situation where you need to bulk add SPF records to every domain in Plesk, you can use this huge one-liner:
mysql -u admin -p`cat /etc/psa/.psa.shadow` psa -e &amp;#34;select dns_zone_id,displayHost from dns_recs GROUP BY dns_zone_id ORDER BY dns_zone_id ASC;&amp;#34; | awk &amp;#39;{print &amp;#34;INSERT INTO dns_recs (type,host,val,time_stamp,dns_zone_id,displayHost,displayVal) VALUES (&amp;#39;\&amp;#39;&amp;#39;TXT&amp;#39;\&amp;#39;&amp;#39;,&amp;#39;\&amp;#39;&amp;#39;&amp;#34;$2&amp;#34;&amp;#39;\&amp;#39;&amp;#39;,&amp;#39;\&amp;#39;&amp;#39;v=spf1 a mx ~all&amp;#39;\&amp;#39;&amp;#39;,NOW(),&amp;#34;$1&amp;#34;,&amp;#39;\&amp;#39;&amp;#39;&amp;#34;$2&amp;#34;&amp;#39;\&amp;#39;&amp;#39;,&amp;#39;\&amp;#39;&amp;#39;v=spf1 a mx ~all&amp;#39;\&amp;#39;&amp;#39;);&amp;#34;}&amp;#39; | mysql -u admin -p`cat /etc/psa/.psa.shadow` psa` Then you&amp;rsquo;ll need to make Plesk write these changes to the zone files:</description><content type="html"><![CDATA[<p>If you find yourself in the situation where you need to bulk add SPF records to every domain in Plesk, you can use this huge one-liner:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sql" data-lang="sql">mysql <span style="color:#719e07">-</span>u <span style="color:#719e07">admin</span> <span style="color:#719e07">-</span>p<span style="color:#719e07">`</span>cat <span style="color:#719e07">/</span>etc<span style="color:#719e07">/</span>psa<span style="color:#719e07">/</span>.psa.shadow<span style="color:#719e07">`</span> psa <span style="color:#719e07">-</span>e <span style="color:#2aa198">&#34;select dns_zone_id,displayHost from dns_recs GROUP BY dns_zone_id ORDER BY dns_zone_id ASC;&#34;</span> <span style="color:#719e07">|</span> awk <span style="color:#2aa198">&#39;{print &#34;INSERT INTO dns_recs (type,host,val,time_stamp,dns_zone_id,displayHost,displayVal) VALUES (&#39;</span>\<span style="color:#2aa198">&#39;&#39;</span>TXT<span style="color:#2aa198">&#39;\&#39;&#39;,&#39;</span>\<span style="color:#2aa198">&#39;&#39;</span><span style="color:#2aa198">&#34;$2&#34;</span><span style="color:#2aa198">&#39;\&#39;&#39;,&#39;</span>\<span style="color:#2aa198">&#39;&#39;</span>v<span style="color:#719e07">=</span>spf1 a mx <span style="color:#719e07">~</span><span style="color:#719e07">all</span><span style="color:#2aa198">&#39;\&#39;&#39;,NOW(),&#34;$1&#34;,&#39;</span>\<span style="color:#2aa198">&#39;&#39;</span><span style="color:#2aa198">&#34;$2&#34;</span><span style="color:#2aa198">&#39;\&#39;&#39;,&#39;</span>\<span style="color:#2aa198">&#39;&#39;</span>v<span style="color:#719e07">=</span>spf1 a mx <span style="color:#719e07">~</span><span style="color:#719e07">all</span><span style="color:#2aa198">&#39;\&#39;&#39;);&#34;}&#39;</span> <span style="color:#719e07">|</span> mysql <span style="color:#719e07">-</span>u <span style="color:#719e07">admin</span> <span style="color:#719e07">-</span>p<span style="color:#719e07">`</span>cat <span style="color:#719e07">/</span>etc<span style="color:#719e07">/</span>psa<span style="color:#719e07">/</span>.psa.shadow<span style="color:#719e07">`</span> psa<span style="color:#719e07">`</span>
</code></pre></div><p>Then you&rsquo;ll need to make Plesk write these changes to the zone files:</p>
<pre><code># mysql -Ns -uadmin -p`cat /etc/psa/.psa.shadow` -D psa -e 'select name from domains' | awk '{print &quot;/usr/local/psa/admin/sbin/dnsmng update &quot; $1 }' | sh
</code></pre><p>You can check your work by viewing the new entries you made:</p>
<pre><code>mysql -u admin -p`cat /etc/psa/.psa.shadow` psa -e &quot;SELECT * FROM dns_recs WHERE type='TXT';&quot;
</code></pre>]]></content></item><item><title>Can’t find file: ‘horde_sessionhandler.MYI’</title><link>https://major.io/2007/04/19/cant-find-file-horde_sessionhandlermyi/</link><pubDate>Thu, 19 Apr 2007 16:38:49 +0000</pubDate><guid>https://major.io/2007/04/19/cant-find-file-horde_sessionhandlermyi/</guid><description>If you get this error, you&amp;rsquo;ve most likely done a file-based MySQL backup restore, and the InnoDB files are hosed. The horde_sessionhandler table isn&amp;rsquo;t a MyISAM table at all - it&amp;rsquo;s actually an InnoDB table. The easiest way to fix the issue is to stop MySQL and trash the .frm:
# /etc/init.d/mysqld stop # rm /var/lib/mysql/horde/horde_sessionhandler.frm Now start MySQL and re-create the table:
# /etc/init.d/mysqld start # mysql -u admin -p`cat /etc/psa/.</description><content type="html"><![CDATA[<p>If you get this error, you&rsquo;ve most likely done a file-based MySQL backup restore, and the InnoDB files are hosed. The horde_sessionhandler table isn&rsquo;t a MyISAM table at all - it&rsquo;s actually an InnoDB table. The easiest way to fix the issue is to stop MySQL and trash the .frm:</p>
<pre><code># /etc/init.d/mysqld stop
# rm /var/lib/mysql/horde/horde_sessionhandler.frm
</code></pre><p>Now start MySQL and re-create the table:</p>
<pre><code># /etc/init.d/mysqld start
# mysql -u admin -p`cat /etc/psa/.psa.shadow`
</code></pre><p>Here&rsquo;s the SQL statements to run:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sql" data-lang="sql"><span style="color:#719e07">CREATE</span> <span style="color:#719e07">TABLE</span> horde_sessionhandler (session_id <span style="color:#b58900">VARCHAR</span>(<span style="color:#2aa198">32</span>) <span style="color:#719e07">NOT</span> <span style="color:#719e07">NULL</span>, session_lastmodified <span style="color:#b58900">INT</span> <span style="color:#719e07">NOT</span> <span style="color:#719e07">NULL</span>, session_data LONGBLOB, <span style="color:#719e07">PRIMARY</span> <span style="color:#719e07">KEY</span> (session_id)) ENGINE <span style="color:#719e07">=</span> InnoDB;
<span style="color:#719e07">GRANT</span> <span style="color:#719e07">SELECT</span>, <span style="color:#719e07">INSERT</span>, <span style="color:#719e07">UPDATE</span>, <span style="color:#719e07">DELETE</span> <span style="color:#719e07">ON</span> horde_sessionhandler <span style="color:#719e07">TO</span> horde<span style="color:#719e07">@</span>localhost;
</code></pre></div><p>You&rsquo;re good to go!</p>
]]></content></item><item><title>Too many languages – can’t upgrade Plesk license</title><link>https://major.io/2007/04/19/too-many-languages-cant-upgrade-plesk-license/</link><pubDate>Thu, 19 Apr 2007 13:40:12 +0000</pubDate><guid>https://major.io/2007/04/19/too-many-languages-cant-upgrade-plesk-license/</guid><description>If Plesk throws an error that it can&amp;rsquo;t upgrade your license key because of languages, you need to remove the extra locales:
# rpm -qa | grep psa-locale | grep -v base psa-locale-el-GR-8.1-build81061127.19 psa-locale-fr-FR-8.1-build81061127.19 psa-locale-lt-LT-8.1-build81061127.19 psa-locale-pt-BR-8.1-build81061127.19 psa-locale-sv-SE-8.1-build81061127.19 psa-locale-ca-ES-8.1-build81061127.19 psa-locale-de-DE-8.1-build81061127.19 psa-locale-es-ES-8.1-build81061127.19 psa-locale-fi-FI-8.1-build81061127.19 psa-locale-hu-HU-8.1-build81061127.19 psa-locale-ja-JP-8.1-build81061127.19 psa-locale-nl-BE-8.1-build81061127.19 psa-locale-pl-PL-8.1-build81061127.19 psa-locale-pt-PT-8.1-build81061127.19 psa-locale-ru-RU-8.1-build81061127.19 psa-locale-tr-TR-8.1-build81061127.19 psa-locale-zh-TW-8.1-build81061127.19 psa-locale-cs-CZ-8.1-build81061127.19 psa-locale-es-MX-8.1-build81061127.19 psa-locale-it-IT-8.1-build81061127.19 psa-locale-nl-NL-8.1-build81061127.19 psa-locale-ro-RO-8.1-build81061127.19 psa-locale-zh-CN-8.1-build81061127.19 # rpm -ev `rpm -qa | grep psa-locale | grep -v base`</description><content type="html"><![CDATA[<p>If Plesk throws an error that it can&rsquo;t upgrade your license key because of languages, you need to remove the extra locales:</p>
<pre><code># rpm -qa | grep psa-locale | grep -v base
psa-locale-el-GR-8.1-build81061127.19
psa-locale-fr-FR-8.1-build81061127.19
psa-locale-lt-LT-8.1-build81061127.19
psa-locale-pt-BR-8.1-build81061127.19
psa-locale-sv-SE-8.1-build81061127.19
psa-locale-ca-ES-8.1-build81061127.19
psa-locale-de-DE-8.1-build81061127.19
psa-locale-es-ES-8.1-build81061127.19
psa-locale-fi-FI-8.1-build81061127.19
psa-locale-hu-HU-8.1-build81061127.19
psa-locale-ja-JP-8.1-build81061127.19
psa-locale-nl-BE-8.1-build81061127.19
psa-locale-pl-PL-8.1-build81061127.19
psa-locale-pt-PT-8.1-build81061127.19
psa-locale-ru-RU-8.1-build81061127.19
psa-locale-tr-TR-8.1-build81061127.19
psa-locale-zh-TW-8.1-build81061127.19
psa-locale-cs-CZ-8.1-build81061127.19
psa-locale-es-MX-8.1-build81061127.19
psa-locale-it-IT-8.1-build81061127.19
psa-locale-nl-NL-8.1-build81061127.19
psa-locale-ro-RO-8.1-build81061127.19
psa-locale-zh-CN-8.1-build81061127.19
# rpm -ev `rpm -qa | grep psa-locale | grep -v base`
</code></pre>]]></content></item><item><title>Telnet POP3 Commands</title><link>https://major.io/2007/04/17/telnet-pop3-commands/</link><pubDate>Tue, 17 Apr 2007 22:28:19 +0000</pubDate><guid>https://major.io/2007/04/17/telnet-pop3-commands/</guid><description>If you ever need to communicate with a POP3 server via telnet to test it, here&amp;rsquo;s some commands you can use:
USER userid PASS password STAT LIST RETR msg# TOP msg# #lines DELE msg# RSET QUIT</description><content type="html"><![CDATA[<p>If you ever need to communicate with a POP3 server via telnet to test it, here&rsquo;s some commands you can use:</p>
<pre><code>USER userid
PASS password
STAT
LIST
RETR msg#
TOP msg# #lines
DELE msg#
RSET
QUIT
</code></pre>]]></content></item><item><title>SSL connection to a non-secure port</title><link>https://major.io/2007/04/17/ssl-connection-to-a-non-secure-port/</link><pubDate>Tue, 17 Apr 2007 22:27:12 +0000</pubDate><guid>https://major.io/2007/04/17/ssl-connection-to-a-non-secure-port/</guid><description>If you have weird SSL errors and this one appears, you are trying to speak SSL to a daemon that doesn&amp;rsquo;t understand it:
$ openssl s_client -connect 222.222.222.222:443 CONNECTED(00000003) 5057:error:140770FC:SSL routines:SSL23_GET_SERVER_HELLO:unknown protocol:s23_clnt.c:567: If you get this with Apache, be sure that you have SSLEngine On in the applicable VirtualHost and be sure that mod_ssl is being loaded.</description><content type="html"><![CDATA[<p>If you have weird SSL errors and this one appears, you are trying to speak SSL to a daemon that doesn&rsquo;t understand it:</p>
<pre><code>$ openssl s_client -connect 222.222.222.222:443
CONNECTED(00000003)
5057:error:140770FC:SSL routines:SSL23_GET_SERVER_HELLO:unknown protocol:s23_clnt.c:567:
</code></pre><p>If you get this with Apache, be sure that you have <code>SSLEngine On</code> in the applicable VirtualHost and be sure that mod_ssl is being loaded.</p>
]]></content></item><item><title>Disable SSH timeouts</title><link>https://major.io/2007/04/12/disable-ssh-timeouts/</link><pubDate>Thu, 12 Apr 2007 16:15:02 +0000</pubDate><guid>https://major.io/2007/04/12/disable-ssh-timeouts/</guid><description>To pretty much completely disable SSH timeouts, simply adjust the following directives in /etc/ssh/sshd_config:
TCPKeepAlive yes ClientAliveInterval 30 ClientAliveCountMax 99999 EDIT: Once that&amp;rsquo;s changed, be sure to restart your ssh daemon.
SECURITY WARNING:If you remove users from your system, but they&amp;rsquo;re still connected via ssh, their connection may remain open indefinitely. Be sure to check all active ssh sessions after adjusting a user&amp;rsquo;s access.</description><content type="html"><![CDATA[<p>To pretty much completely disable SSH timeouts, simply adjust the following directives in /etc/ssh/sshd_config:</p>
<pre><code>TCPKeepAlive yes
ClientAliveInterval 30
ClientAliveCountMax 99999
</code></pre><p><strong>EDIT:</strong> Once that&rsquo;s changed, be sure to restart your ssh daemon.</p>
<p><!-- raw HTML omitted -->SECURITY WARNING:<!-- raw HTML omitted --> If you remove users from your system, but they&rsquo;re still connected via ssh, their connection may remain open indefinitely. Be sure to check all active ssh sessions after adjusting a user&rsquo;s access.</p>
]]></content></item><item><title>Pre-upgrade Plesk Backup</title><link>https://major.io/2007/04/10/pre-upgrade-plesk-backup/</link><pubDate>Tue, 10 Apr 2007 18:55:06 +0000</pubDate><guid>https://major.io/2007/04/10/pre-upgrade-plesk-backup/</guid><description>Before you upgrade Plesk, it&amp;rsquo;s always a good idea to make a backup and also make your ip and shell maps:
/usr/local/psa/bin/psadump -f /path/to/psa.dump --nostop --nostop-domain /usr/local/psa/bin/psarestore -t -f /path/to/psa.dump -m ip_map -s shell_map If you need to restore data, just drop the -t on the psarestore command.</description><content type="html"><![CDATA[<p>Before you upgrade Plesk, it&rsquo;s always a good idea to make a backup and also make your ip and shell maps:</p>
<pre><code>/usr/local/psa/bin/psadump -f /path/to/psa.dump --nostop --nostop-domain
/usr/local/psa/bin/psarestore -t -f /path/to/psa.dump -m ip_map -s shell_map
</code></pre><p>If you need to restore data, just drop the <code>-t</code> on the <code>psarestore</code> command.</p>
]]></content></item><item><title>Disable SSLv2 in Lighttpd</title><link>https://major.io/2007/04/08/disable-sslv2-in-lighttpd/</link><pubDate>Sun, 08 Apr 2007 23:26:07 +0000</pubDate><guid>https://major.io/2007/04/08/disable-sslv2-in-lighttpd/</guid><description>As with most things, turning off SSLv2 in Lighttpd is much easier than in Apache. Toss the following line in your lighttpd.conf and you&amp;rsquo;re good to go:
ssl.use-sslv2 = &amp;quot;disable&amp;quot;</description><content type="html"><![CDATA[<p>As with most things, turning off SSLv2 in Lighttpd is much easier than in Apache. Toss the following line in your lighttpd.conf and you&rsquo;re good to go:</p>
<pre><code>ssl.use-sslv2 = &quot;disable&quot;
</code></pre>]]></content></item><item><title>WordPress permalinks in Lighttpd</title><link>https://major.io/2007/04/08/wordpress-permalinks-in-lighttpd/</link><pubDate>Sun, 08 Apr 2007 23:21:17 +0000</pubDate><guid>https://major.io/2007/04/08/wordpress-permalinks-in-lighttpd/</guid><description>WordPress uses .htaccess files to process its permalinks structure, but Lighttpd won&amp;rsquo;t obey .htaccess files (yet). So, instead of banging your head against the wall, just use something like the following:
server.error-handler-404 = &amp;quot;/index.php?error=404&amp;quot; For example, the virtual host for this very website is:
$HTTP[&amp;quot;host&amp;quot;] =~ &amp;quot;rackerhacker\\.com&amp;quot; { server.document-root = basedir+&amp;quot;rackerhacker.com/&amp;quot; server.error-handler-404 = &amp;quot;/index.php?error=404&amp;quot; }</description><content type="html"><![CDATA[<p>WordPress uses .htaccess files to process its permalinks structure, but Lighttpd won&rsquo;t obey .htaccess files (yet). So, instead of banging your head against the wall, just use something like the following:</p>
<pre><code>server.error-handler-404 = &quot;/index.php?error=404&quot;
</code></pre><p>For example, the virtual host for this very website is:</p>
<pre><code>$HTTP[&quot;host&quot;] =~ &quot;rackerhacker\\.com&quot; {
        server.document-root = basedir+&quot;rackerhacker.com/&quot;
        server.error-handler-404 = &quot;/index.php?error=404&quot;
}
</code></pre>]]></content></item><item><title>Plesk Database Schematic</title><link>https://major.io/2007/04/07/plesk-database-schematic/</link><pubDate>Sat, 07 Apr 2007 15:00:03 +0000</pubDate><guid>https://major.io/2007/04/07/plesk-database-schematic/</guid><description>I&amp;rsquo;ve heard a lot of people are interested in something like this, so I decided to create it. I didn&amp;rsquo;t try to make it look confusing, it just came out that way.
plesk-db-diagram.pdf [99KB]</description><content type="html"><![CDATA[<p>I&rsquo;ve heard a lot of people are interested in something like this, so I decided to create it. I didn&rsquo;t try to make it look confusing, it just came out that way.</p>
<p><a href="http://cdn.cloudfiles.mosso.com/c8031/plesk-db-diagram.pdf"><!-- raw HTML omitted -->
plesk-db-diagram.pdf [99KB]</a></p>
]]></content></item><item><title>Lighttpd proxy to Tomcat</title><link>https://major.io/2007/04/05/lighttpd-proxy-to-tomcat/</link><pubDate>Fri, 06 Apr 2007 04:41:05 +0000</pubDate><guid>https://major.io/2007/04/05/lighttpd-proxy-to-tomcat/</guid><description>It seems like lighttpd and Tomcat are at the forefront of what is ‘hot' these days. If you don&amp;rsquo;t need the completeness of Apache on your server, you can use lighttpd to proxy to Tomcat, and it&amp;rsquo;s pretty simple. This how-to will show you how to install lighttpd, Tomcat, and the Java JRE. Once they&amp;rsquo;re installed it will also show you how to get lighttpd to use mod_proxy to connect to your Tomcat installation.</description><content type="html"><![CDATA[<p>It seems like lighttpd and Tomcat are at the forefront of what is ‘hot' these days. If you don&rsquo;t need the completeness of Apache on your server, you can use lighttpd to proxy to Tomcat, and it&rsquo;s pretty simple. This how-to will show you how to install lighttpd, Tomcat, and the Java JRE. Once they&rsquo;re installed it will also show you how to get lighttpd to use mod_proxy to connect to your Tomcat installation.</p>
<p>First, some downloading has to be done. Grab the latest lighttpd RPM from rpmfind.net for your distribution. You will also need to pick up the latest version of <a href="http://tomcat.apache.org/">Tomcat</a> and the <a href="http://java.sun.com/j2se">Java JRE</a>.</p>
<p>Once all three of those are on the server, get them installed:</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>Before you can do much else, you will need to set up your JAVA_HOME and add JAVA_HOME/bin to your path. Open up /etc/profile and add the following before the <code>export</code> statement:</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>To make this change actually take effect, you will need to log out and become root again. Now, check that your JAVA_HOME is set:</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>If the JAVA_HOME is not set up, check your /etc/profile again. If it&rsquo;s set up, try starting Tomcat – there&rsquo;s no need to set the $CATALINA_HOME, because Tomcat can figure it out on its own:</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>Try to connect to the server now on port 8080 and you should see a Tomcat default page. Now, go add a manager user to the $CATALINA_HOME/conf/tomcat-users.xml:</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>Restart Tomcat for the changes to take effect:</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>Tomcat is ready to go, so it&rsquo;s time to configure lighttpd. Open the /etc/lighttpd/lighttpd.conf and activate mod_proxy by uncommenting it:</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>Drop to the bottom of the configuration file and add something like this, replacing your information as necessary:</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>Replace the IP address with a hostname or the correct IP for your server. This proxy directive makes lighttpd connect to Tomcat on the localhost on port 8080 whenever a request comes in on port 80 to lighttpd on the IP 10.10.10.56. Start lighttpd now and try it yourself!</p>
<!-- raw HTML omitted -->
]]></content></item><item><title>Disable reverse lookups with qmail in Plesk</title><link>https://major.io/2007/04/05/disable-reverse-lookups-with-qmail-in-plesk/</link><pubDate>Thu, 05 Apr 2007 16:00:53 +0000</pubDate><guid>https://major.io/2007/04/05/disable-reverse-lookups-with-qmail-in-plesk/</guid><description>To disable reverse lookups in qmail with Plesk, simply add -Rt0 to the server_args line in /etc/xinetd.d/smtp_psa
service smtp { socket_type = stream protocol = tcp wait = no disable = no user = root instances = UNLIMITED server = /var/qmail/bin/tcp-env server_args = &amp;lt;strong&amp;gt;-Rt0&amp;lt;/strong&amp;gt; /usr/sbin/rblsmtpd -r sbl-xbl.spamhaus.org /var/qmail/bin/relaylock /var/qmail/bin/qmail-smtpd /var/qmail/bin/smtp_auth /var/qmail/bin/true /var/qmail/bin/cmd5checkpw /var/qmail/bin/true } Once that&amp;rsquo;s been saved, simply restart xinetd:
# /etc/init.d/xinetd restart WATCH OUT! This change will be overwritten if you change certain mail settings in Plesk, like MAPS protection.</description><content type="html"><![CDATA[<p>To disable reverse lookups in qmail with Plesk, simply add -Rt0 to the <code>server_args</code> line in /etc/xinetd.d/smtp_psa</p>
<pre><code>
service smtp
{
        socket_type     = stream
        protocol        = tcp
        wait            = no
        disable         = no
        user            = root
        instances       = UNLIMITED
        server          = /var/qmail/bin/tcp-env
        server_args     = &lt;strong&gt;-Rt0&lt;/strong&gt; /usr/sbin/rblsmtpd  -r sbl-xbl.spamhaus.org /var/qmail/bin/relaylock /var/qmail/bin/qmail-smtpd /var/qmail/bin/smtp_auth /var/qmail/bin/true /var/qmail/bin/cmd5checkpw /var/qmail/bin/true
}
</code></pre><p>Once that&rsquo;s been saved, simply restart xinetd:</p>
<pre><code># /etc/init.d/xinetd restart
</code></pre><p><strong>WATCH OUT!</strong> This change will be overwritten if you change certain mail settings in Plesk, like MAPS protection.</p>
]]></content></item><item><title>451 Could not complete sender verify callout</title><link>https://major.io/2007/03/29/451-could-not-complete-sender-verify-callout/</link><pubDate>Thu, 29 Mar 2007 14:17:06 +0000</pubDate><guid>https://major.io/2007/03/29/451-could-not-complete-sender-verify-callout/</guid><description>This is one of Exim&amp;rsquo;s more cryptic errors:
Mar 29 11:22:52 114075-web1 postfix/smtp[20589]: 9E0142FC589: to=&amp;lt;orders@somehost.com&amp;gt;, relay=somehost.com[11.11.11.11], delay=147966, status=deferred (host somehost.com[11.11.11.11] said: 451 Could not complete sender verify callout (in reply to RCPT TO command)) When you send e-mail to an Exim server with a sender verify callout enabled, the Exim server will connect back into your server and verify that your server accepts mail for the sender&amp;rsquo;s e-mail address. For example, if you send e-mail from orders@somehost.</description><content type="html"><![CDATA[<p>This is one of Exim&rsquo;s more cryptic errors:</p>
<pre><code>Mar 29 11:22:52 114075-web1 postfix/smtp[20589]: 9E0142FC589: to=&lt;orders@somehost.com&gt;, relay=somehost.com[11.11.11.11], delay=147966, status=deferred (host somehost.com[11.11.11.11] said: 451 Could not complete sender verify callout (in reply to RCPT TO command))
</code></pre><p>When you send e-mail to an Exim server with a sender verify callout enabled, the Exim server will connect back into your server and verify that your server accepts mail for the sender&rsquo;s e-mail address. For example, if you send e-mail from <a href="mailto:orders@somehost.com">orders@somehost.com</a>, the Exim server will connect back into your server and do this:</p>
<pre><code>HELO someotherhost.com
250 somehost.com
MAIL FROM: test@someotherhost.com
250 2.1.0 Ok
RCPT TO: orders@somehost.com
250 2.1.5 Ok
</code></pre><p>Exim will make sure that it gets a 250 success code before it will allow the e-mail to come into its server. Some situations that cause problems with this process are:</p>
<ul>
<li>Port 25 is blocked inbound on the sender&rsquo;s server</li>
<li>Something else is filtering port 25 inbound on the sender&rsquo;s server</li>
<li>The sender&rsquo;s server uses blacklists which delay the responses to Exim&rsquo;s commands</li>
</ul>
]]></content></item><item><title>Setting the hostname in Sendmail</title><link>https://major.io/2007/03/27/setting-the-hostname-in-sendmail/</link><pubDate>Tue, 27 Mar 2007 21:01:26 +0000</pubDate><guid>https://major.io/2007/03/27/setting-the-hostname-in-sendmail/</guid><description>If you need to change the hostname that Sendmail announces itself as, just add the following to sendmail.mc:
define(`confDOMAIN_NAME', `mail.yourdomain.com')dnl
And, to add additional stuff onto the end of the line:
define(`confSMTP_LOGIN_MSG',`mailer ready')dnl</description><content type="html"><![CDATA[<p>If you need to change the hostname that Sendmail announces itself as, just add the following to sendmail.mc:</p>
<p><code>define(`confDOMAIN_NAME', `mail.yourdomain.com')dnl</code></p>
<p>And, to add additional stuff onto the end of the line:</p>
<p><code>define(`confSMTP_LOGIN_MSG',`mailer ready')dnl</code></p>
]]></content></item><item><title>/bin/rm: Argument list too long</title><link>https://major.io/2007/03/26/binrm-argument-list-too-long/</link><pubDate>Mon, 26 Mar 2007 17:16:55 +0000</pubDate><guid>https://major.io/2007/03/26/binrm-argument-list-too-long/</guid><description>If you have too many files to remove, try this trick:
find . -name '*' | xargs rm -v</description><content type="html"><![CDATA[<p>If you have too many files to remove, try this trick:</p>
<pre><code>find . -name '*' | xargs rm -v
</code></pre>]]></content></item><item><title>Reset MySQL root password</title><link>https://major.io/2007/03/25/reset-mysql-root-password/</link><pubDate>Mon, 26 Mar 2007 03:27:33 +0000</pubDate><guid>https://major.io/2007/03/25/reset-mysql-root-password/</guid><description>If you&amp;rsquo;ve forgotten the root password for a MySQL server, but you know the system root, you can reset the MySQL root password pretty easily. Just remember to work quickly since the server is wide open until you finish working.
First, add skip-grant-tables to the [mysqld] section of /etc/my.cnf and restart the MySQL server.
Next, run mysql from the command line and use the following SQL statement:
UPDATE mysql.user SET Password=PASSWORD(&amp;#39;newpwd&amp;#39;) WHERE User=&amp;#39;root&amp;#39;;&amp;lt;br /&amp;gt; FLUSH PRIVILEGES; Remove the skip-grant-tables from /etc/my.</description><content type="html"><![CDATA[<p>If you&rsquo;ve forgotten the root password for a MySQL server, but you know the system root, you can reset the MySQL root password pretty easily. Just remember to work quickly since the server is wide open until you finish working.</p>
<p>First, add <code>skip-grant-tables</code> to the <code>[mysqld]</code> section of /etc/my.cnf and restart the MySQL server.</p>
<p>Next, run <code>mysql</code> from the command line and use the following SQL statement:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sql" data-lang="sql"><span style="color:#719e07">UPDATE</span> mysql.<span style="color:#719e07">user</span> <span style="color:#719e07">SET</span> Password<span style="color:#719e07">=</span>PASSWORD(<span style="color:#2aa198">&#39;newpwd&#39;</span>) <span style="color:#719e07">WHERE</span> <span style="color:#719e07">User</span><span style="color:#719e07">=</span><span style="color:#2aa198">&#39;root&#39;</span>;<span style="color:#719e07">&lt;</span>br <span style="color:#719e07">/&gt;</span>
FLUSH <span style="color:#719e07">PRIVILEGES</span>;
</code></pre></div><p>Remove the <code>skip-grant-tables</code> from /etc/my.cnf and leave the server running. There&rsquo;s no need to restart it.</p>
]]></content></item><item><title>Apache’s mysterious trailing slash</title><link>https://major.io/2007/03/23/apaches-mysterious-trailing-slash/</link><pubDate>Fri, 23 Mar 2007 13:20:27 +0000</pubDate><guid>https://major.io/2007/03/23/apaches-mysterious-trailing-slash/</guid><description>You may find that some sites do not work well if you omit a trailing slash on the URL. For example, if you have a directory on domain.com called &amp;ldquo;news&amp;rdquo;, the following two URL&amp;rsquo;s should take you to the same place:
http://domain.com/news
http://domain.com/news/
If you find that they do not take you to the same place, be sure that the mod_dir (Apache 1 or Apache 2) module is being loaded in Apache.</description><content type="html"><![CDATA[<p>You may find that some sites do not work well if you omit a trailing slash on the URL. For example, if you have a directory on domain.com called &ldquo;news&rdquo;, the following two URL&rsquo;s should take you to the same place:</p>
<p><a href="http://domain.com/news">http://domain.com/news</a></p>
<p><a href="http://domain.com/news/">http://domain.com/news/</a></p>
<p>If you find that they do not take you to the same place, be sure that the mod_dir (<a href="http://httpd.apache.org/docs/1.3/mod/mod_dir.html">Apache 1</a> or <a href="http://httpd.apache.org/docs/2.0/mod/mod_dir.html">Apache 2</a>) module is being loaded in Apache. If that module is being loaded, and you&rsquo;re still having problems, make sure mod_rewrite is loaded as well.</p>
<p>If none of that works, make sure that there is no ErrorDocument 301 or ErrorDocument 302. Should either of those exist, promptly slap the developer/sysadmin that enabled those options. Apache will do a 301 redirect when the trailing slash is missing so that the user will be directed to the correct location, and if there is an ErrorDocument 301, this error document will always be presented rather than the proper redirection to the directory on your site.</p>
]]></content></item><item><title>Adjust max_execution_time for Horde in Plesk</title><link>https://major.io/2007/03/23/adjust-max_execution_time-for-horde-in-plesk/</link><pubDate>Fri, 23 Mar 2007 13:15:41 +0000</pubDate><guid>https://major.io/2007/03/23/adjust-max_execution_time-for-horde-in-plesk/</guid><description>Often times, the wonderful webmail application known as Horde will spin out of control and cause unnecessary resource usage and often cause defunct Apache processes to appear. You may wonder how this can happen, especially if you set the max_execution_time variable in php.ini. Well, the Horde developers took it upon themselves to overwrite your settings in their own configuration file in /usr/share/psa-horde/config/conf.xml:
&amp;lt;configinteger name=&amp;#34;max_exec_time&amp;#34; desc=&amp;#34;If we need to perform a long operation, what should we set max_execution_time to (in seconds)?</description><content type="html"><![CDATA[<p>Often times, the wonderful webmail application known as Horde will spin out of control and cause unnecessary resource usage and often cause defunct Apache processes to appear. You may wonder how this can happen, especially if you set the max_execution_time variable in php.ini. Well, the Horde developers took it upon themselves to overwrite your settings in their own configuration file in <code>/usr/share/psa-horde/config/conf.xml</code>:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-xml" data-lang="xml"><span style="color:#268bd2">&lt;configinteger</span> name=<span style="color:#2aa198">&#34;max_exec_time&#34;</span> desc=<span style="color:#2aa198">&#34;If we need to perform a long operation, what should we set max_execution_time to (in seconds)? 0 means no limit; however, a value of 0 will cause a warning if you are running in safe mode. See http://www.php.net/manual/function.set-time-limit.php for more information.&#34;</span><span style="color:#268bd2">&gt;</span>0<span style="color:#268bd2">&lt;/configinteger&gt;</span>
</code></pre></div><p>It&rsquo;s set to forever by default in Horde. However, if you do turn on safe_mode, Horde will have some problems setting its time limit variable. You can change the zero to something more reasonable, such as 30 or 60 by editing the conf.xml and reloading Apache.</p>
]]></content></item><item><title>Exporting SSL certificates from Windows to Linux</title><link>https://major.io/2007/03/23/exporting-ssl-certificates-from-windows-to-linux/</link><pubDate>Fri, 23 Mar 2007 13:11:16 +0000</pubDate><guid>https://major.io/2007/03/23/exporting-ssl-certificates-from-windows-to-linux/</guid><description>First, you have to get the certificate and key out of Windows in a pfx (PKCS #12) format.
Click Start, Run, then type &amp;ldquo;mmc&amp;rdquo; and hit enter. In the leftmost menu, choose &amp;ldquo;Add/Remove Snap In&amp;rdquo;. Click &amp;ldquo;Add&amp;rdquo;, then click &amp;ldquo;Certificates&amp;rdquo;, then OK. When the wizard starts, choose &amp;ldquo;Computer Account&amp;rdquo;, &amp;ldquo;Local Computer&amp;rdquo; and finish out the wizard. Once you&amp;rsquo;re finished, get back to the MMC and expand the &amp;ldquo;Certificates&amp;rdquo; node, then the &amp;ldquo;Personal&amp;rdquo; node.</description><content type="html"><![CDATA[<p>First, you have to get the certificate and key out of Windows in a pfx (PKCS #12) format.</p>
<ul>
<li>Click Start, Run, then type &ldquo;mmc&rdquo; and hit enter.</li>
<li>In the leftmost menu, choose &ldquo;Add/Remove Snap In&rdquo;.</li>
<li>Click &ldquo;Add&rdquo;, then click &ldquo;Certificates&rdquo;, then OK.</li>
<li>When the wizard starts, choose &ldquo;Computer Account&rdquo;, &ldquo;Local Computer&rdquo; and finish out the wizard.</li>
<li>Once you&rsquo;re finished, get back to the MMC and expand the &ldquo;Certificates&rdquo; node, then the &ldquo;Personal&rdquo; node.</li>
<li>Click on the &ldquo;Certificates&rdquo; node under &ldquo;Personal&rdquo; and find your certificate in the right pane.</li>
<li>Right click on the certificate and choose &ldquo;All Tasks&rdquo;, then &ldquo;Export&rdquo;.</li>
<li>When the wizard starts, choose &ldquo;Yes&rdquo; for exporting the private key, then select <strong>ONLY</strong> &ldquo;Strong Private Key Protection&rdquo; from the PFX section. You will also need to set a password and specify a location for the PFX file.</li>
<li>Once the PFX file has been saved, close out the MMC (don&rsquo;t save the snap-in if it asks).</li>
<li>Get the PFX over to the Linux server somehow.</li>
</ul>
<p>Once the PFX makes it over to the Linux server, you have to decrypt the PFX into a plaintext PEM file (PFX&rsquo;s are binary files, and can&rsquo;t be viewed in a text editor):</p>
<pre><code>openssl pkcs12 -in file.pfx -out file.pem
</code></pre><p>You will be asked for the password for the PFX (which is the one you set in the Windows wizard). Once you enter that, you will be asked for a new password. This new password is used to encrypt the private key. You cannot proceed until you enter a password that is 4 characters or longer. <strong>REMEMBER</strong> this password!</p>
<p>When this step is complete, you should have a PEM file that you can read in a text editor. Open the file in a text editor and copy the private key and certificate to different files. Remember to keep the dashed lines intact when you copy the certificates - this is important. There is some additional text above the key, and also between the key and certificate - this text should be ignored and should not be included in the certificate and key files.</p>
<p>Now that you have the key and certificate separated, you need to decrypt the private key (or face the wrath of Apache every time you restart the server). You can decrypt the private key like this:</p>
<pre><code>openssl rsa -in file.key -out file.key
</code></pre><p>Yes, provide the same file name twice and it will decrypt the key onto itself, keeping everything in one file. OpenSSL will ask for a password to decrypt the key, and this is the password you set when you decrypted the PFX. If you forgot the password, you will need to start over from when you brought it over from the Windows box.</p>
<p>After this entire process, you will have four files, a PFX, PEM, KEY, and CRT. Throw away the PFX and PEM, and you can use the key and certificate files to install into Apache. In case you forget the syntax, here&rsquo;s what goes in the Apache configuration:</p>
<pre><code>SSLEngine On
SSLCertificateFile /path/to/your/certificate
SSLCertificateKeyFile /path/to/your/privatekey
</code></pre>]]></content></item><item><title>Forcing HTTPS (SSL) with mod_rewrite</title><link>https://major.io/2007/03/21/forcing-https-with-mod_rewrite/</link><pubDate>Wed, 21 Mar 2007 15:00:15 +0000</pubDate><guid>https://major.io/2007/03/21/forcing-https-with-mod_rewrite/</guid><description>If you can&amp;rsquo;t use PHP to force HTTPS, you can use mod_rewrite instead. Toss this in an .htaccess file in the web root of your site:
RewriteEngine On RewriteCond %{SERVER_PORT} 80 RewriteRule ^(.*)$ https://www.domain.com/$1 [R,L] Or, if it needs to be forced only for a certain folder:
RewriteEngine On RewriteCond %{SERVER_PORT} 80 RewriteCond %{REQUEST_URI} somefolder RewriteRule ^(.*)$ https://www.domain.com/somefolder/$1 [R,L]</description><content type="html"><![CDATA[<p>If you can&rsquo;t use PHP to force HTTPS, you can use mod_rewrite instead. Toss this in an .htaccess file in the web root of your site:</p>
<pre><code>RewriteEngine On
RewriteCond %{SERVER_PORT} 80
RewriteRule ^(.*)$ https://www.domain.com/$1 [R,L]
</code></pre><p>Or, if it needs to be forced only for a certain folder:</p>
<pre><code>RewriteEngine On
RewriteCond %{SERVER_PORT} 80
RewriteCond %{REQUEST_URI} somefolder
RewriteRule ^(.*)$ https://www.domain.com/somefolder/$1 [R,L]
</code></pre>]]></content></item><item><title>Forcing HTTPS with PHP</title><link>https://major.io/2007/03/21/forcing-https-with-php/</link><pubDate>Wed, 21 Mar 2007 14:27:35 +0000</pubDate><guid>https://major.io/2007/03/21/forcing-https-with-php/</guid><description>To force HTTPS with a PHP script, just put this snippet near the top:
if ($_SERVER[&amp;#39;SERVER_PORT&amp;#39;] != 443) { header(&amp;#34;Location: https://&amp;#34;.$_SERVER[&amp;#39;HTTP_HOST&amp;#39;].$_SERVER[&amp;#39;REQUEST_URI&amp;#39;]); }</description><content type="html"><![CDATA[<p>To force HTTPS with a PHP script, just put this snippet near the top:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-php" data-lang="php"><span style="color:#719e07">if</span> (<span style="color:#268bd2">$_SERVER</span>[<span style="color:#2aa198">&#39;SERVER_PORT&#39;</span>] <span style="color:#719e07">!=</span> <span style="color:#2aa198">443</span>) {
header(<span style="color:#2aa198">&#34;Location: https://&#34;</span><span style="color:#719e07">.</span><span style="color:#268bd2">$_SERVER</span>[<span style="color:#2aa198">&#39;HTTP_HOST&#39;</span>]<span style="color:#719e07">.</span><span style="color:#268bd2">$_SERVER</span>[<span style="color:#2aa198">&#39;REQUEST_URI&#39;</span>]);
}
</code></pre></div>]]></content></item><item><title>AWStats icons don’t appear in Plesk 8.1</title><link>https://major.io/2007/03/18/awstats-icons-dont-appear-in-plesk-81/</link><pubDate>Sun, 18 Mar 2007 19:17:26 +0000</pubDate><guid>https://major.io/2007/03/18/awstats-icons-dont-appear-in-plesk-81/</guid><description>The AWStats package in RHEL4/Centos4 and Plesk 8.1 uses an alias directory for the icons called /awstats-icon, but when the AWStats contents is generated, the icon directory is different (/icon). To fix this issue, change this file:
/usr/share/awstats/awstats_buildstaticpages.pl:
my $DirIcons=&amp;#39;/awstats-icon&amp;#39;;</description><content type="html"><![CDATA[<p>The AWStats package in RHEL4/Centos4 and Plesk 8.1 uses an alias directory for the icons called /awstats-icon, but when the AWStats contents is generated, the icon directory is different (/icon). To fix this issue, change this file:</p>
<p><code>/usr/share/awstats/awstats_buildstaticpages.pl</code>:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-perl" data-lang="perl"><span style="color:#719e07">my</span> <span style="color:#268bd2">$DirIcons</span><span style="color:#719e07">=</span><span style="color:#2aa198">&#39;/awstats-icon&#39;</span>;
</code></pre></div>]]></content></item><item><title>Redhat ES DST Links</title><link>https://major.io/2007/03/12/redhat-es-dst-links/</link><pubDate>Mon, 12 Mar 2007 15:22:12 +0000</pubDate><guid>https://major.io/2007/03/12/redhat-es-dst-links/</guid><description>If you need Redhat DST change information, you can get it here.</description><content type="html">&lt;p>If you need Redhat DST change information, you can &lt;a href="http://kbase.redhat.com/faq/FAQ_80_7909.shtm">get it here&lt;/a>.&lt;/p></content></item><item><title>Quick and fancy mail blacklist checking</title><link>https://major.io/2007/03/11/quick-and-fancy-mail-blacklist-checking/</link><pubDate>Mon, 12 Mar 2007 00:53:57 +0000</pubDate><guid>https://major.io/2007/03/11/quick-and-fancy-mail-blacklist-checking/</guid><description>I rarely try to toot my own horn, but I&amp;rsquo;ve created a pretty handy site. Check out Blacklist Watch if you get the chance. You can immediately test a server against the most commonly used spam blacklists available online. Soon enough, I&amp;rsquo;ll have an automated notification service so that you can be notified when your IP&amp;rsquo;s appear on a blacklist.
Here&amp;rsquo;s a quick example of the blacklist checking.</description><content type="html"><![CDATA[<p>I rarely try to toot my own horn, but I&rsquo;ve created a pretty handy site. Check out <a href="http://blwatch.com/">Blacklist Watch</a> if you get the chance. You can immediately test a server against the most commonly used spam blacklists available online. Soon enough, I&rsquo;ll have an automated notification service so that you can be notified when your IP&rsquo;s appear on a blacklist.</p>
<p><a href="http://blwatch.com/doquickcheck.php?host=rackerhacker.com">Here&rsquo;s a quick example of the blacklist checking</a>.</p>
]]></content></item><item><title>Finding compromised scripts</title><link>https://major.io/2007/03/06/finding-compromised-scripts/</link><pubDate>Tue, 06 Mar 2007 18:41:21 +0000</pubDate><guid>https://major.io/2007/03/06/finding-compromised-scripts/</guid><description>If your server is sending out spam because of some bad scripts, hunt that stuff down:
grep POST /var/log/httpd/access_log | awk '{ print $7 }' | sort | uniq -c | sort -rn Or on Plesk:
grep POST /home/httpd/vhosts/*/statistics/logs/access_log | awk '{ print $7 }' | sort | uniq -c | sort -rn</description><content type="html"><![CDATA[<p>If your server is sending out spam because of some bad scripts, hunt that stuff down:</p>
<pre><code>grep POST /var/log/httpd/access_log | awk '{ print $7 }' | sort | uniq -c | sort -rn
</code></pre><p>Or on Plesk:</p>
<pre><code>grep POST /home/httpd/vhosts/*/statistics/logs/access_log | awk '{ print $7 }' | sort | uniq -c | sort -rn
</code></pre>]]></content></item><item><title>Stopping Double Bounces in Plesk</title><link>https://major.io/2007/03/05/stopping-double-bounces-in-plesk/</link><pubDate>Mon, 05 Mar 2007 22:50:41 +0000</pubDate><guid>https://major.io/2007/03/05/stopping-double-bounces-in-plesk/</guid><description>To stop those evil double bounce e-mails in Plesk, just do:
echo &amp;quot;#&amp;quot; &amp;gt; /var/qmail/control/doublebounceto</description><content type="html"><![CDATA[<p>To stop those evil double bounce e-mails in Plesk, just do:</p>
<p><code>echo &quot;#&quot; &gt; /var/qmail/control/doublebounceto</code></p>
]]></content></item><item><title>Change Primary IP Address in Plesk</title><link>https://major.io/2007/02/28/change-primary-ip-address-in-plesk/</link><pubDate>Wed, 28 Feb 2007 15:36:19 +0000</pubDate><guid>https://major.io/2007/02/28/change-primary-ip-address-in-plesk/</guid><description>If you need to change to a different primary IP in Plesk, here&amp;rsquo;s the easiest way:
In Plesk 7 there is no concept of the Primary IP address for the server. From the Control panels point of view all IP addresses are equal. The only difference between the main IP address and aliases is that the main IP address can not be deleted from the control panel.
To change the main IP address you need to first remove this address from all IP pools.</description><content type="html"><![CDATA[<p>If you need to change to a different primary IP in Plesk, here&rsquo;s the easiest way:</p>
<p>In Plesk 7 there is no concept of the Primary IP address for the server. From the Control panels point of view all IP addresses are equal. The only difference between the main IP address and aliases is that the main IP address can not be deleted from the control panel.</p>
<p>To change the main IP address you need to first remove this address from all IP pools. Then stop Plesk and manually change the IP address on the server from the backend as root. Then start Plesk again and restore the list of IP addresses through SERVER -&gt; IP Aliasing and click on Re-read button.</p>
]]></content></item><item><title>Disabling SSLv2 in Plesk</title><link>https://major.io/2007/02/27/disabling-sslv2-in-plesk/</link><pubDate>Tue, 27 Feb 2007 18:17:02 +0000</pubDate><guid>https://major.io/2007/02/27/disabling-sslv2-in-plesk/</guid><description>To disable SSLv2 server-wide on a Plesk server, add this in your /etc/httpd/conf.d/ssl.conf:
SSLCipherSuite ALL:!ADH:!LOW:!SSLv2:!EXP:+HIGH:+MEDIUM SSLProtocol all -SSLv2 Put the directive very high in the file, outside the VirtualHost directive, preferably right below the Listen directive. This will work for all SSL VirtualHosts.
How can I ensure that Apache does not allow SSL 2.0 protocol that has known weaknesses?</description><content type="html"><![CDATA[<p>To disable SSLv2 server-wide on a Plesk server, add this in your /etc/httpd/conf.d/ssl.conf:</p>
<pre><code>SSLCipherSuite ALL:!ADH:!LOW:!SSLv2:!EXP:+HIGH:+MEDIUM
SSLProtocol all -SSLv2
</code></pre><p>Put the directive very high in the file, outside the VirtualHost directive, preferably right below the Listen directive. This will work for all SSL VirtualHosts.</p>
<p><a href="http://kb.swsoft.com/en/1763">How can I ensure that Apache does not allow SSL 2.0 protocol that has known weaknesses?</a></p>
]]></content></item><item><title>Disable Dr. Web Notifications in Plesk</title><link>https://major.io/2007/02/27/disable-dr-web-notifications-plesk/</link><pubDate>Tue, 27 Feb 2007 16:10:56 +0000</pubDate><guid>https://major.io/2007/02/27/disable-dr-web-notifications-plesk/</guid><description>You can edit /etc/drweb/drweb_qmail.conf to eliminate receiving notification messages when Dr. Web has an issue:
[VirusNotifications] SenderNotify = no AdminNotify = no RcptsNotify = no Then just restart Dr. Web with:
/etc/init.d/drwebd restart Plesk has a KB article about this issue as well.</description><content type="html"><![CDATA[<p>You can edit /etc/drweb/drweb_qmail.conf to eliminate receiving notification messages when Dr. Web has an issue:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ini" data-lang="ini"><span style="color:#719e07">[VirusNotifications]</span>
SenderNotify <span style="color:#719e07">=</span> <span style="color:#2aa198">no</span>
AdminNotify <span style="color:#719e07">=</span> <span style="color:#2aa198">no</span>
RcptsNotify <span style="color:#719e07">=</span> <span style="color:#2aa198">no</span>
</code></pre></div><p>Then just restart Dr. Web with:</p>
<pre><code>/etc/init.d/drwebd restart
</code></pre><p>Plesk has a <a href="http://kb.swsoft.com/article_122_1685_en.html">KB article</a> about this issue as well.</p>
]]></content></item><item><title>Hide Apache Version</title><link>https://major.io/2007/02/23/hide-apache-version/</link><pubDate>Fri, 23 Feb 2007 21:07:48 +0000</pubDate><guid>https://major.io/2007/02/23/hide-apache-version/</guid><description>If you want to hide the current version of Apache and your OS, just replace
ServerTokens OS
with
ServerTokens Prod
and restart Apache.</description><content type="html"><![CDATA[<p>If you want to hide the current version of Apache and your OS, just replace</p>
<p><code>ServerTokens OS</code></p>
<p>with</p>
<p><code>ServerTokens Prod</code></p>
<p>and restart Apache.</p>
]]></content></item><item><title>New DST changes for 2007</title><link>https://major.io/2007/02/22/new-dst-changes-for-2007/</link><pubDate>Fri, 23 Feb 2007 00:54:25 +0000</pubDate><guid>https://major.io/2007/02/22/new-dst-changes-for-2007/</guid><description>As you may or may not know, there&amp;rsquo;s new Daylight Savings Time changes in 2007. It starts earlier this year on March 11th. There&amp;rsquo;s already new packages available for Redhat, Gentoo (more), and Debian (more).
For a tubload of other OSes, check here.</description><content type="html"><![CDATA[<p>As you may or may not know, there&rsquo;s new Daylight Savings Time changes in 2007. It starts earlier this year on March 11th. There&rsquo;s already new packages available for <a href="http://kbase.redhat.com/faq/FAQ_79_9950.shtm">Redhat</a>, <a href="http://www.gentoo.org/news/en/gwn/20070212-newsletter.xml">Gentoo</a> (<a href="http://forums.gentoo.org/viewtopic-t-534993-highlight-daylight+savings.html">more</a>), and <a href="http://people.debian.org/~terpstra/message/20061005.205220.2e89b18e.en.html">Debian</a> (<a href="http://www.linuxquestions.org/questions/showthread.php?s=6987b66841b23d781a3a6f6ffad96c7e&amp;p=2584279#post2584279">more</a>).</p>
<p>For a tubload of other OSes, <a href="http://www.edgeblog.net/2007/daylight-saving-time-the-year-2007-problem/">check here</a>.</p>
]]></content></item><item><title>ProFTPD shows incorrect GMT time with Plesk</title><link>https://major.io/2007/02/21/gmt-ftp-timestamps-in-plesk/</link><pubDate>Wed, 21 Feb 2007 19:13:01 +0000</pubDate><guid>https://major.io/2007/02/21/gmt-ftp-timestamps-in-plesk/</guid><description>A really really strange issue randomly appears with ProFTPD and Plesk occasionally. On the filesystem, a file will have a correct creation/modification date, but then when you view it over FTP, it&amp;rsquo;s always off by the amount of hours you differ from GMT.
For example, if the server is on Central Time, all of the files will seem to be created 6 hours after they were really created. The filesystem will show something like 10AM, but the FTP client will say 4PM.</description><content type="html"><![CDATA[<p>A really really strange issue randomly appears with ProFTPD and Plesk occasionally. On the filesystem, a file will have a correct creation/modification date, but then when you view it over FTP, it&rsquo;s always off by the amount of hours you differ from GMT.</p>
<p>For example, if the server is on Central Time, all of the files will seem to be created 6 hours after they were really created. The filesystem will show something like 10AM, but the FTP client will say 4PM. Luckily, <strong>there is a fix!</strong></p>
<p>Add the following to your /etc/proftpd.conf file and you should be good to go:</p>
<pre><code>TimesGMT off
SetEnv TZ :/etc/localtime
</code></pre>]]></content></item><item><title>Chmod and the mysterious first octet</title><link>https://major.io/2007/02/13/chmod-and-the-mysterious-first-octet/</link><pubDate>Wed, 14 Feb 2007 04:00:52 +0000</pubDate><guid>https://major.io/2007/02/13/chmod-and-the-mysterious-first-octet/</guid><description>If you&amp;rsquo;ve ever worked on a linux system, chances are that you&amp;rsquo;ve used chmod many times. However, the quickest way to stump many linux users is to ask how many octets a full permissions set has. Many people think of this and say three:
chmod 777 file
But what you&amp;rsquo;re actually saying:
chmod 0777 file
The first octet works the same way as the other three as it has 3 possible values that add to make the octet (for the letter-lovers, i&amp;rsquo;ve included those too):</description><content type="html"><![CDATA[<p>If you&rsquo;ve ever worked on a linux system, chances are that you&rsquo;ve used chmod many times. However, the quickest way to stump many linux users is to ask how many octets a full permissions set has. Many people think of this and say three:</p>
<p><code>chmod 777 file</code></p>
<p>But what you&rsquo;re actually saying:</p>
<p><code>chmod 0777 file</code></p>
<p>The first octet works the same way as the other three as it has 3 possible values that add to make the octet (for the letter-lovers, i&rsquo;ve included those too):</p>
<p><code>4 - setuid (letter-style: s)&lt;br /&gt; 2 - setgid (letter-style: s)&lt;br /&gt; 1 - sticky bit (letter-style: t)</code></p>
<p>Remember - your first octet will <strong>always be reset to 0</strong> when using chown or chgrp on files.</p>
<p><strong>Setuid</strong></p>
<p>If you setuid on a binary, you&rsquo;re telling the operating system that you want this binary to always be executed as the user owner of the binary. So, let&rsquo;s say the permissions on a binary are set like so:</p>
<p>`# chmod 4750 some_binary<!-- raw HTML omitted --></p>
<h1 id="ls--al-some_binarybr-">ls -al some_binary<!-- raw HTML omitted --></h1>
<p>-rwsr-x&mdash; 1 root users 0 Feb 13 21:43 some_binary`</p>
<p>You&rsquo;ll notice the small &rsquo;s' in the user permissions blocks - this means that if a user on the system executes this binary, it will run as root with full root permissions. Obviously, anyone in the users group can run this binary since the execute bit is set for the group, but when the binary runs, it will run with root permissions. <strong>Be smart with setuid!</strong> Anything higher than 4750 can be very dangerous as it allows the world to run the binary as the root user. Also, if you allow full access plus setgid, you will be opening yourself up for something mighty nasty:</p>
<p>`# chmod 4777 some_binary<!-- raw HTML omitted --></p>
<h1 id="ls--al-some_binarybr--1">ls -al some_binary<!-- raw HTML omitted --></h1>
<p>-rwsrwxrwx 1 root users 0 Feb 13 21:43 some_binary`</p>
<p>Not only can every user on the system execute this binary, but they can <strong>edit it</strong> before it runs at <strong>root</strong>! It goes without saying, but this could be used to beat up your system pretty badly. If you neglect to allow enough user permissions for execution, linux laughs at you by throwing the uppercase &lsquo;S&rsquo; into your terminal:</p>
<p>`# chmod 4400 some_binary<!-- raw HTML omitted --></p>
<h1 id="ls--al-some_binarybr--2">ls -al some_binary<!-- raw HTML omitted --></h1>
<p>-r-S&mdash;&mdash; 1 root users 0 Feb 13 21:43 some_binary`</p>
<p>Since no one can execute this script anyways (except root), you get the big capital &lsquo;S&rsquo; for &lsquo;Silly&rsquo;. (It probably doesn&rsquo;t stand for silly, but whatever.)</p>
<p><strong>Setgid</strong></p>
<p>Setgid is pretty much the exact same as setuid, but the binary runs with the privileges of the owner group rather than the user&rsquo;s primary group privileges. This isn&rsquo;t quite so useful in my opinion, but in case you need it, here&rsquo;s how the permissions come out:</p>
<p>`# chmod 2750 some_binary<!-- raw HTML omitted --></p>
<h1 id="ls--al-some_binarybr--3">ls -al some_binary<!-- raw HTML omitted --></h1>
<p>-rwxr-s&mdash; 1 root users 0 Feb 13 21:43 some_binary`</p>
<p>And if you enjoy being made fun of:</p>
<p>`# chmod 2400 some_binary<!-- raw HTML omitted --></p>
<h1 id="ls--al-some_binarybr--4">ls -al some_binary<!-- raw HTML omitted --></h1>
<p>-r&mdash;-S&mdash; 1 root users 0 Feb 13 21:43 some_binary`</p>
<p><strong>Sticky Bit</strong></p>
<p>This is such a giggly term for a linux file permission, but it&rsquo;s rather important, and it best applies to your tmp directory (or any other world writable location). Since world writable locations allow users to go hog-wild with creating, editing, appending, and deleting files, this can get annoying if certain users share a common directory.</p>
<p>Let&rsquo;s say users work in an office and they work on files in a world writeable directory. One user gets mad because another user got a raise, so they trash all of the files that belong to that recently promoted user. Obviously, this could lead to a touchy situation. If you apply the sticky bit on the directory, the users can do anything they want to files they create, but they can&rsquo;t write to or delete files which they didn&rsquo;t create. Pretty slick, er, sticky, right? Here&rsquo;s how to set the sticky bit:</p>
<p>`#chmod 1777 /tmp<!-- raw HTML omitted --></p>
<h1 id="ls--ld-tmpbr-">ls -ld /tmp<!-- raw HTML omitted --></h1>
<p>drwxrwxrwt 3 root root 4096 Feb 13 21:57 /tmp`</p>
<p>And again, linux will laugh at you for setting sticky bits on non-world writable directories, but this time it does so with a capital &lsquo;T&rsquo;:</p>
<p>`#chmod 1744 /tmp<!-- raw HTML omitted --></p>
<h1 id="ls--ld-tmpbr--1">ls -ld /tmp<!-- raw HTML omitted --></h1>
<p>drw-r&ndash;r-T 3 root root 4096 Feb 13 21:57 /tmp`</p>
<p><strong>Setuid + Setgid on Directories</strong></p>
<p>Setting the setgid bit on a directory means any files created in that directory will be owned by the group who owns the directory. No matter what your primary group is, any files you make will be owned by the group who owns the directory.</p>
<p>Setting the setuid bit on a directory has no effect in almost all Linux variants. However, in FreeBSD, it acts the same as the setgid (except it changes the ownership of new files as the user who owns the folder).</p>
]]></content></item><item><title>Understanding LVM</title><link>https://major.io/2007/02/13/understanding-lvm/</link><pubDate>Wed, 14 Feb 2007 03:34:06 +0000</pubDate><guid>https://major.io/2007/02/13/understanding-lvm/</guid><description>LVM is handy when you want additional flexibility to grow or shrink your storage space safely without impacting filesystems negatively. It&amp;rsquo;s key to remember that LVM provides flexibility - not redundancy. The best way to understand LVM is to understand four terms: physical volumes, physical extents, volume groups and logical volumes.
Physical volumes are probably the easiest to understand for most users. The stuff you deal with all day, /dev/hda2, /dev/sd3 - these are physical volumes.</description><content type="html"><![CDATA[<p>LVM is handy when you want additional flexibility to grow or shrink your storage space safely without impacting filesystems negatively. It&rsquo;s key to remember that LVM provides flexibility - not redundancy. The best way to understand LVM is to understand four terms: physical volumes, physical extents, volume groups and logical volumes.</p>
<p><strong>Physical volumes</strong> are probably the easiest to understand for most users. The stuff you deal with all day, /dev/hda2, /dev/sd3 - these are physical volumes. They&rsquo;re real hard drive partitions which are finitely defined. LVM comes along and chops those physical volumes up into little pieces called <strong>physical extents</strong>. Extents are simply just pieces of a regular system partition, and the size of the extent is determined by the OS.</p>
<p>So what happens with these extents? You can pool a group of extents together to form a <strong>volume group</strong>. From there, you can carve out chunks of the extents from the volume group to make <strong>logical volumes</strong>.</p>
<p>Confused? You should be! Let&rsquo;s try an example:</p>
<p>You have two system partitions: /dev/sda2 and /dev/sda3. Let&rsquo;s say that /dev/sda2 has 1,000 extents and /dev/sda3 has 2,000 extents. The first thing you&rsquo;ll want to do is initialize the physical volumes, which basically tells LVM you want to chop them up into pieces so you can use them later:</p>
<pre><code># pvcreate /dev/sda2
# pvcreate /dev/sda3`
</code></pre><p>Graphically, here&rsquo;s what&rsquo;s happened so far:</p>
<pre><code>+-----[ Physical Volume ]------+
| PE | PE | PE | PE | PE | PE  |
+------------------------------+
</code></pre><p>Now, LVM has split these physical volumes (partitions) into small pieces called extents. So, we should have 3,000 extents total once we create the physical volumes with LVM (1,000 for sda2 and 2,000 for sda3). Now, we need to take all of these extents and put them into a group, called the volume group:</p>
<pre><code>vgcreate test /dev/sda2 /dev/sda3
</code></pre><p>Again, here&rsquo;s what we&rsquo;ve done:</p>
<pre><code>+------[ Volume Group ]-----------------+
|  +--[PV]--------+  +--[PV]---------+  |
|  | PE | PE | PE |  | PE | PE | PE  |  |
|  +--------------+  +---------------+  |
+---------------------------------------+
</code></pre><p>So what&rsquo;s happened so far? The physical volumes (partitions) are unchanged, but LVM has split them into extents, and we&rsquo;ve now told LVM that we want to include the extents from both physical volumes in a volume group called test. The volume group test is basically a big bucket holding all of our extents from both physical volumes. To move on, you need to find out how many extents we have in our volume group now:</p>
<pre><code>vgdisplay -v test
</code></pre><p>We should see that <strong>Total PE</strong> in the output shows 3,000, with a <strong>Free PE</strong> of 3,000 since we haven&rsquo;t done anything with our extents yet. Now we can take all these extents in the volume group and lump them together into a 1,500 extent partition:</p>
<pre><code>lvcreate -l 1500 -n FIRST test
</code></pre><p>What did we just do? We made a real linux volume called /dev/test/FIRST that has 1,500 extents. Toss a filesystem onto that new volume and you&rsquo;re good to go:</p>
<pre><code>mke2fs -j /dev/test/FIRST
</code></pre><p>So, this new logical volume contains 1,500 extents, which means we have 1,500 left over. Might as well make a second volume out of the remaining extents in our volume group:</p>
<pre><code>lvcreate -l 1500 -n SECOND test
mke2fs -j /dev/test/SECOND
</code></pre><p>Now you have two equal sized logical volumes whereas you had one small one (sda2) and one large one (sda3) before. The two logical volumes use extents from both physical volumes that are both held within the same volume group. You end up with something like this:</p>
<pre><code>+------[ Volume Group ]-----------------+
|  +--[PV]--------+  +--[PV]---------+  |
|  | PE | PE | PE |  | PE | PE | PE  |  |
|  +--+---+---+---+  +-+----+----+---+  |
|     |   |   | +-----/     |    |      |
|     |   |   | |           |    |      |
|   +-+---+---+-+      +----+----+--+   |
|   |  Logical  |      |  Logical   |   |
|   |  Volume   |      |   Volume   |   |
|   |           |      |            |   |
|   |  /FIRST   |      |   /SECOND  |   |
|   +-----------+      +------------+   |
+---------------------------------------+
</code></pre>]]></content></item><item><title>Measuring raw shell bandwidth</title><link>https://major.io/2007/02/11/measuring-raw-shell-bandwidth/</link><pubDate>Mon, 12 Feb 2007 04:20:06 +0000</pubDate><guid>https://major.io/2007/02/11/measuring-raw-shell-bandwidth/</guid><description>Okay, so we know it&amp;rsquo;s easy to measure web, ftp and mail traffic, right? You can just parse the logs, sum it all up, and move on with your day. However, what do you do about users with SFTP or RSYNC privileges? This can create a problem when the bandwidth on your server keeps cranking up, but your web/ftp/mail traffic stats don&amp;rsquo;t show an increase.
Need a solution? Enjoy:
First, create an OUTPUT rule for your user, which in this case will be root.</description><content type="html"><![CDATA[<p>Okay, so we know it&rsquo;s easy to measure web, ftp and mail traffic, right? You can just parse the logs, sum it all up, and move on with your day. However, what do you do about users with SFTP or RSYNC privileges? This can create a problem when the bandwidth on your server keeps cranking up, but your web/ftp/mail traffic stats don&rsquo;t show an increase.</p>
<p>Need a solution? Enjoy:</p>
<p>First, create an OUTPUT rule for your user, which in this case will be root. Why no INPUT rule? Many hosts don&rsquo;t charge for incoming bandwidth, so why bother?</p>
<pre><code># iptables -A INPUT -j ACCEPT -m owner --uid-owner=root
</code></pre><p>Now check this out:</p>
<pre><code># /sbin/iptables -v -xL -Z
Chain OUTPUT (policy ACCEPT 1287 packets, 221983 bytes)
pkts      bytes target     prot opt in     out     source        destination
437    59684 ACCEPT     all  --  any    any     anywhere      anywhere  OWNER UID match root`
</code></pre><p>The number in the &lsquo;bytes&rsquo; column is the count of bytes that this user sent out of your server since the last time you ran that iptables command. If you don&rsquo;t want to zero out the bytes each time you run the command, just drop the Z flag from the iptables command.</p>
<p>You can go wild with awk if you desire:</p>
<pre><code># /sbin/iptables -v -xL | grep root | awk '{ print $2 }'
59684
</code></pre>]]></content></item><item><title>Bulk IP update in Plesk</title><link>https://major.io/2007/02/11/bulk-ip-update-in-plesk/</link><pubDate>Mon, 12 Feb 2007 01:41:49 +0000</pubDate><guid>https://major.io/2007/02/11/bulk-ip-update-in-plesk/</guid><description>There&amp;rsquo;s lots of situations where you&amp;rsquo;d want to use a bulk IP change in Plesk:
Server is moving and needs to change IP&amp;rsquo;s An IP is the destination for some type of DDOS attack An IP needs to be removed from the server So how do you shift tons of domains from one IP to another without spending hours in Plesk clicking and clicking? Do the following instead:</description><content type="html"><![CDATA[<p>There&rsquo;s lots of situations where you&rsquo;d want to use a bulk IP change in Plesk:</p>
<ul>
<li>Server is moving and needs to change IP&rsquo;s</li>
<li>An IP is the destination for some type of DDOS attack</li>
<li>An IP needs to be removed from the server</li>
</ul>
<p>So how do you shift tons of domains from one IP to another without spending hours in Plesk clicking and clicking? Do the following instead:</p>
<p>Get into MySQL and find out which IP you&rsquo;re moving from and to:</p>
<pre><code>mysql -u admin -p`cat /etc/psa/.psa.shadow`
mysql&gt; select * from IP_Addresses;
</code></pre><p>You should see a printout of all of the available IP&rsquo;s on the server. Make a note of the &ldquo;id&rdquo; of the IP you&rsquo;re moving from and to. In this example, here&rsquo;s what we&rsquo;re doing:</p>
<ul>
<li>Moving FROM &ldquo;192.168.1.192&rdquo; (id = 2)</li>
<li>Moving TO &ldquo;192.168.1.209&rdquo; (id =3)</li>
</ul>
<p>Now we can start shifting the physically hosted domains over in the database:</p>
<pre><code>mysql&gt; update hosting set ip_address_id=3 where ip_address_id=2;
</code></pre><p>We also need to change the domains that are set up for standard or frame forwarding:</p>
<pre><code>mysql&gt; update forwarding set ip_address_id=3 where ip_address_id=2;
</code></pre><p>Now we&rsquo;re stuck with the arduous task of updating DNS records. Plesk is kind enough to store this data in four different ways:</p>
<pre><code>mysql&gt; update dns_recs set displayHost='192.168.1.209' where displayHost='192.168.1.192';
mysql&gt; update dns_recs set host='192.168.1.209' where host='192.168.1.192';
mysql&gt; update dns_recs set displayVal='192.168.1.209' where displayVal='192.168.1.192';
mysql&gt; update dns_recs set val='192.168.1.209' where val='192.168.1.192';
</code></pre><p>Everything domain related is now moved, but the clients that the domains belong to might not have this new IP address in their IP pool. First, we need to find out our component ID&rsquo;s from the repository table (which generally should be the same as the IP_Addresses.id column, but not always)</p>
<pre><code>mysql&gt; SELECT clients.login, IP_Addresses.ip_address,Repository.* FROM clients LEFT JOIN Repository ON clients.pool_id = Repository.rep_id LEFT JOIN IP_Addresses ON Repository.component_id = IP_Addresses.id;
</code></pre><p>For this example, we&rsquo;ll pretend that the output consists of 2&rsquo;s for these clients. We can flip the IP&rsquo;s in the clients' IP pools by running the following:</p>
<pre><code>mysql&gt; update Repository set component_id=3 where component_id=2;
</code></pre><p>Now that everything is changed in Plesk&rsquo;s database, it&rsquo;s time to change up the Apache and BIND configuration files. Luckily, this can be done pretty easily with Plesk&rsquo;s command line tools:</p>
<pre><code># /usr/local/psa/admin/bin/websrvmng -av
# mysql -Ns -uadmin -p`cat /etc/psa/.psa.shadow` -D psa -e 'select name from domains' | awk '{print &quot;/usr/local/psa/admin/sbin/dnsmng update &quot; $1 }' | sh
</code></pre><p>All that is left is to force Apache and BIND to pick up the new configuration:</p>
<pre><code># /etc/init.d/httpd reload
# /etc/init.d/named reload
</code></pre><p>Just wait for the DNS records to propagate and you should be all set! The instructions are cumbersome, I know, but it&rsquo;s easier than clicking for-ev-er.</p>
]]></content></item><item><title>Move domain between clients in Plesk</title><link>https://major.io/2007/02/11/move-domain-between-clients-in-plesk/</link><pubDate>Mon, 12 Feb 2007 01:29:18 +0000</pubDate><guid>https://major.io/2007/02/11/move-domain-between-clients-in-plesk/</guid><description>Moving domains from client to client in Plesk is pretty quick from the command line. Just replace DOMAIN with the domain name you want to move and CLIENTLOGIN with the client&amp;rsquo;s username:
/usr/local/psa/bin/domain.sh --update DOMAIN -clogin CLIENTLOGIN</description><content type="html"><![CDATA[<p>Moving domains from client to client in Plesk is pretty quick from the command line. Just replace DOMAIN with the domain name you want to move and CLIENTLOGIN with the client&rsquo;s username:</p>
<p><code>/usr/local/psa/bin/domain.sh --update DOMAIN -clogin CLIENTLOGIN</code></p>
]]></content></item><item><title>Finding compromised mail accounts in Plesk</title><link>https://major.io/2007/02/10/finding-compromised-mail-accounts-in-plesk/</link><pubDate>Sat, 10 Feb 2007 16:35:23 +0000</pubDate><guid>https://major.io/2007/02/10/finding-compromised-mail-accounts-in-plesk/</guid><description>If odd bounced e-mails are coming back to the server or the server is listed in a blacklist, some accounts may be compromised on the server. Here&amp;rsquo;s how to diagnose the issue:
Read the queue and look for messages with funky senders or lots of recipients.
10 Feb 2007 07:31:08 GMT #476884 10716 &amp;lt;service@paypal.com&amp;gt; remote debbarger@earthlink.net remote debbiabbis@hotmail.com remote debbiak@aol.com *** lots more recicpients below *** This is a phishing e-mail being sent out to imitate PayPal.</description><content type="html"><![CDATA[<p>If odd bounced e-mails are coming back to the server or the server is listed in a blacklist, some accounts may be compromised on the server. Here&rsquo;s how to diagnose the issue:</p>
<p>Read the queue and look for messages with funky senders or lots of recipients.</p>
<pre><code class="language-#" data-lang="#">10 Feb 2007 07:31:08 GMT  #476884  10716  &lt;service@paypal.com&gt;
        remote  debbarger@earthlink.net
        remote  debbiabbis@hotmail.com
        remote  debbiak@aol.com
        *** lots more recicpients below ***
</code></pre><p>This is a phishing e-mail being sent out to imitate PayPal. Now you need to find which IP is sending this e-mail, so grab the message ID and pass it to qmHandle:</p>
<pre><code># qmHandle -m476884 | less
Received: (qmail 20390 invoked from network); 10 Feb 2007 07:31:08 -0600
Received: from unknown (HELO User) (207.219.92.194)
</code></pre><p>In this case, the offender is from 207.219.92.194. Now we can dig for the login in /var/log/messages:</p>
<pre><code># grep -i 207.219.92.194 /var/log/messages
Feb 10 10:19:33 s60418 smtp_auth: SMTP connect from unknown@ [207.219.92.194]
Feb 10 10:19:33 s60418 smtp_auth: smtp_auth: SMTP user [USER] : /var/qmail/mailnames/[DOMAIN]/[USER] logged in from unknown@ [207.219.92.194]
</code></pre><p>Just for giggles, let&rsquo;s find out what their password is:</p>
<pre><code># mysql -u admin -p`cat /etc/psa/.psa.shadow`
mysql&gt; use psa;
mysql&gt; select CONCAT(mail_name,&quot;@&quot;,name) as email_address,accounts.password
from mail left join domains on domains.id=mail.dom_id left join accounts on
accounts.id=mail.account_id where mail_name like '[USER]';
+---------------------------+----------+
| email_address             | password |
+---------------------------+----------+
| [USER]@[DOMAIN]           | password |
+---------------------------+----------+
1 row in set (0.00 sec)
</code></pre><p>Well, &lsquo;password&rsquo; isn&rsquo;t a great password. Log into Plesk and change this password ASAP. To verify your work, tail /var/log/messages and you should see this:</p>
<pre><code># tail -f /var/log/messages
Feb 10 10:27:08 s60418 smtp_auth: SMTP connect from unknown@ [207.219.92.194]
Feb 10 10:27:08 s60418 smtp_auth: smtp_auth: FAILED: [USER] - password incorrect  from unknown@ [207.219.92.194]
</code></pre><p>Big thanks goes to Jon B. and Mike J. for this.</p>
]]></content></item><item><title>Delete single iptables rules</title><link>https://major.io/2007/02/09/delete-single-iptables-rules/</link><pubDate>Fri, 09 Feb 2007 19:03:18 +0000</pubDate><guid>https://major.io/2007/02/09/delete-single-iptables-rules/</guid><description>You can delete them based on what they&amp;rsquo;re doing:
iptables -D INPUT -s 127.0.0.1 -p tcp --dport 111 -j ACCEPT Or you can delete them based on their number and chain name:
iptables -D INPUT 4</description><content type="html"><![CDATA[<p>You can delete them based on what they&rsquo;re doing:</p>
<pre><code>iptables -D INPUT -s 127.0.0.1 -p tcp --dport 111 -j ACCEPT
</code></pre><p>Or you can delete them based on their number and chain name:</p>
<pre><code>iptables -D INPUT 4
</code></pre>]]></content></item><item><title>Enabling SSL in ProFTPD</title><link>https://major.io/2007/02/08/enabling-ssl-in-proftpd/</link><pubDate>Thu, 08 Feb 2007 23:28:18 +0000</pubDate><guid>https://major.io/2007/02/08/enabling-ssl-in-proftpd/</guid><description>If you need to enable SSL in ProFTPD, try this out:
&amp;lt;IfModule mod_tls.c&amp;gt; TLSEngine on TLSRequired off TLSRSACertificateFile /etc/httpd/conf/ssl.crt/server.crt TLSRSACertificateKeyFile /etc/httpd/conf/ssl.key/server.key TLSVerifyClient off &amp;lt;/IfModule&amp;gt;</description><content type="html"><![CDATA[<p>If you need to enable SSL in ProFTPD, try this out:</p>
<pre><code>&lt;IfModule mod_tls.c&gt;
    TLSEngine on
    TLSRequired off
    TLSRSACertificateFile /etc/httpd/conf/ssl.crt/server.crt
    TLSRSACertificateKeyFile /etc/httpd/conf/ssl.key/server.key
    TLSVerifyClient off
&lt;/IfModule&gt;
</code></pre>]]></content></item><item><title>Rewrite for certain IP addresses</title><link>https://major.io/2007/02/08/rewrite-for-certain-ip-addresses/</link><pubDate>Thu, 08 Feb 2007 18:26:45 +0000</pubDate><guid>https://major.io/2007/02/08/rewrite-for-certain-ip-addresses/</guid><description>Need to redirect all users except for yourself to another site until yours is live?
RewriteCond %{REMOTE_ADDR} !&amp;quot;^64\.39\.0\.38&amp;quot; RewriteRule .* http://othersite.com/</description><content type="html"><![CDATA[<p>Need to redirect all users except for yourself to another site until yours is live?</p>
<pre><code>RewriteCond %{REMOTE_ADDR} !&quot;^64\.39\.0\.38&quot;
RewriteRule .* http://othersite.com/
</code></pre>]]></content></item><item><title>Fighting DDOS attacks in Linux</title><link>https://major.io/2007/02/07/fighting-ddos-attacks-in-linux/</link><pubDate>Wed, 07 Feb 2007 13:45:43 +0000</pubDate><guid>https://major.io/2007/02/07/fighting-ddos-attacks-in-linux/</guid><description>Check for a SYN flood:
# netstat -alnp | grep :80 | grep SYN_RECV -c 1024 Adjust network variables accordingly:
echo 1 &amp;gt; /proc/sys/net/ipv4/tcp_syncookies echo 30 &amp;gt; /proc/sys/net/ipv4/tcp_fin_timeout echo 1800 &amp;gt;/proc/sys/net/ipv4/tcp_keepalive_time echo 0 &amp;gt;/proc/sys/net/ipv4/tcp_window_scaling echo 0 &amp;gt;/proc/sys/net/ipv4/tcp_sack echo 0 &amp;gt;/proc/sys/net/ipv4/tcp_timestamps</description><content type="html"><![CDATA[<p>Check for a SYN flood:</p>
<pre><code># netstat -alnp | grep :80 | grep SYN_RECV -c 1024
</code></pre><p>Adjust network variables accordingly:</p>
<pre><code>echo 1 &gt; /proc/sys/net/ipv4/tcp_syncookies
echo 30 &gt; /proc/sys/net/ipv4/tcp_fin_timeout
echo 1800 &gt;/proc/sys/net/ipv4/tcp_keepalive_time
echo 0 &gt;/proc/sys/net/ipv4/tcp_window_scaling
echo 0 &gt;/proc/sys/net/ipv4/tcp_sack
echo 0 &gt;/proc/sys/net/ipv4/tcp_timestamps
</code></pre>]]></content></item><item><title>Getting the SMTP Auth ID with Plesk</title><link>https://major.io/2007/02/07/getting-the-smtp-auth-id-with-plesk/</link><pubDate>Wed, 07 Feb 2007 12:54:29 +0000</pubDate><guid>https://major.io/2007/02/07/getting-the-smtp-auth-id-with-plesk/</guid><description>If you think an e-mail account has been hacked in Plesk, use this to hunt down which one it could be:
cat /var/log/messages | grep -i smtp_auth | grep &amp;quot;logged in&amp;quot; | awk {' print $11 '} | awk -F / {' print $6&amp;quot;@&amp;quot;$5 '} | sort | uniq -c | sort -n | tail</description><content type="html"><![CDATA[<p>If you think an e-mail account has been hacked in Plesk, use this to hunt down which one it could be:</p>
<pre><code>cat /var/log/messages | grep -i smtp_auth | grep &quot;logged in&quot; | awk {' print $11 '} | awk -F / {' print $6&quot;@&quot;$5 '} | sort | uniq -c | sort -n | tail
</code></pre>]]></content></item><item><title>Cisco Logging to RHEL</title><link>https://major.io/2007/02/06/cisco-logging-to-rhel/</link><pubDate>Tue, 06 Feb 2007 21:48:54 +0000</pubDate><guid>https://major.io/2007/02/06/cisco-logging-to-rhel/</guid><description>If you have a Cisco device logging to RHEL, here&amp;rsquo;s all that&amp;rsquo;s necessary:
# vi /etc/sysconfig/syslog SYSLOGD_OPTIONS=&amp;quot;-m 0 -r&amp;quot; Check the facility listed in the Cisco configuration, and convert it into the linux syslog facility levels found on Cisco&amp;rsquo;s syslog configuration documentation:
For example, Cisco&amp;rsquo;s facility 19 is the same as linux&amp;rsquo;s facility 3.
# vi /etc/syslog.conf *.info;mail.none;authpriv.none;cron.none;local3.none; /var/log/messages local3.* /var/log/cisco.log Add local3.none; to the /var/log/messages line and add the local3.</description><content type="html"><![CDATA[<p>If you have a Cisco device logging to RHEL, here&rsquo;s all that&rsquo;s necessary:</p>
<pre><code># vi /etc/sysconfig/syslog
SYSLOGD_OPTIONS=&quot;-m 0 -r&quot;
</code></pre><p>Check the facility listed in the Cisco configuration, and convert it into the linux syslog facility levels found on <a href="http://www.cisco.com/en/US/products/hw/vpndevc/ps2030/products_tech_note09186a0080094030.shtml">Cisco&rsquo;s syslog configuration documentation</a>:</p>
<p>For example, Cisco&rsquo;s facility 19 is the same as linux&rsquo;s facility 3.</p>
<pre><code># vi /etc/syslog.conf
*.info;mail.none;authpriv.none;cron.none;local3.none;   /var/log/messages
local3.*                                                /var/log/cisco.log
</code></pre><p>Add <code>local3.none;</code> to the <code>/var/log/messages</code> line and add the <code>local3.*</code> line at the bottom of the file.</p>
<p>Restart syslog with <code>/etc/init.d/syslog restart</code>. Verify that the syslog server is listening on port 514 and then tail your new <code>/var/log/cisco.log</code>:</p>
<pre><code># netstat -plan | grep 514
udp        0      0 0.0.0.0:514                 0.0.0.0:*          3770/syslogd
</code></pre>]]></content></item><item><title>Get Plesk e-mail addresses and passwords</title><link>https://major.io/2007/02/01/get-plesk-e-mail-addresses-and-passwords/</link><pubDate>Thu, 01 Feb 2007 14:33:59 +0000</pubDate><guid>https://major.io/2007/02/01/get-plesk-e-mail-addresses-and-passwords/</guid><description>Need a handy way to list all the email accounts and their passwords?
select CONCAT(mail_name,&amp;#34;@&amp;#34;,name) as email_address,accounts.password from mail left join domains on domains.id=mail.dom_id left join accounts on accounts.id=mail.account_id;</description><content type="html"><![CDATA[<p>Need a handy way to list all the email accounts and their passwords?</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sql" data-lang="sql"><span style="color:#719e07">select</span> CONCAT(mail_name,<span style="color:#2aa198">&#34;@&#34;</span>,name) <span style="color:#719e07">as</span> email_address,accounts.password <span style="color:#719e07">from</span> mail <span style="color:#719e07">left</span> <span style="color:#719e07">join</span> domains <span style="color:#719e07">on</span> domains.id<span style="color:#719e07">=</span>mail.dom_id <span style="color:#719e07">left</span> <span style="color:#719e07">join</span> accounts <span style="color:#719e07">on</span> accounts.id<span style="color:#719e07">=</span>mail.account_id;
</code></pre></div>]]></content></item><item><title>Treason Uncloaked</title><link>https://major.io/2007/01/31/treason-uncloaked/</link><pubDate>Wed, 31 Jan 2007 21:58:48 +0000</pubDate><guid>https://major.io/2007/01/31/treason-uncloaked/</guid><description>TCP: Treason uncloaked! Peer 203.12.220.221:59131/80 shrinks window 76154906:76154907. Repaired. TCP: Treason uncloaked! Peer 203.12.220.227:39670/443 shrinks window 280180313:280180314. Repaired. TCP: Treason uncloaked! Peer 203.12.220.227:39670/443 shrinks window 280180313:280180314. Repaired. TCP: Treason uncloaked! Peer 203.12.220.227:39670/443 shrinks window 280180313:280180314. Repaired. TCP: Treason uncloaked! Peer 203.12.220.237:53759/80 shrinks window 283676616:283676617. Repaired. TCP: Treason uncloaked! Peer 203.12.220.237:36407/80 shrinks window 352393585:352393586. Repaired. TCP: Treason uncloaked! Peer 203.12.220.237:38616/443 shrinks window 529411143:529411144. Repaired. TCP: Treason uncloaked! Peer 58.139.248.9:7611/443 shrinks window 2279076446:2279076447.</description><content type="html"><![CDATA[<pre><code>TCP: Treason uncloaked! Peer 203.12.220.221:59131/80 shrinks window
76154906:76154907. Repaired.
TCP: Treason uncloaked! Peer 203.12.220.227:39670/443 shrinks window
280180313:280180314. Repaired.
TCP: Treason uncloaked! Peer 203.12.220.227:39670/443 shrinks window
280180313:280180314. Repaired.
TCP: Treason uncloaked! Peer 203.12.220.227:39670/443 shrinks window
280180313:280180314. Repaired.
TCP: Treason uncloaked! Peer 203.12.220.237:53759/80 shrinks window
283676616:283676617. Repaired.
TCP: Treason uncloaked! Peer 203.12.220.237:36407/80 shrinks window
352393585:352393586. Repaired.
TCP: Treason uncloaked! Peer 203.12.220.237:38616/443 shrinks window
529411143:529411144. Repaired.
TCP: Treason uncloaked! Peer 58.139.248.9:7611/443 shrinks window
2279076446:2279076447. Repaired.
</code></pre><p>If this is caused by sending strange packets that consume kernel memory, perhaps adding some of these attacker IP addresses to an iptables rule to drop the packets would help. The attacker(s) will probably keep moving to another IP address, so you have get a script to read the logs (&ldquo;grep Treason&rdquo;) and add new blocking rules to iptables (maybe your old system uses &lsquo;ipchains&rsquo; instead).</p>
]]></content></item><item><title>Wave the Plesk magic wand</title><link>https://major.io/2007/01/31/wave-the-plesk-magic-wand/</link><pubDate>Wed, 31 Jan 2007 16:01:22 +0000</pubDate><guid>https://major.io/2007/01/31/wave-the-plesk-magic-wand/</guid><description>If Plesk ever appears to be out of sync with the configuration files, or if there&amp;rsquo;s a Plesk issue that&amp;rsquo;s occurring that makes no sense at all, just stand back and wave the Plesk magic wand:
/usr/local/psa/admin/bin/websrvmng -av
Then restart whatever service was acting up, and things should be sorted out.</description><content type="html"><![CDATA[<p>If Plesk ever appears to be out of sync with the configuration files, or if there&rsquo;s a Plesk issue that&rsquo;s occurring that makes no sense at all, just stand back and wave the Plesk magic wand:</p>
<p><code>/usr/local/psa/admin/bin/websrvmng -av</code></p>
<p>Then restart whatever service was acting up, and things should be sorted out.</p>
]]></content></item><item><title>Make Apache logs mimic IIS</title><link>https://major.io/2007/01/29/make-apache-logs-mimic-iis/</link><pubDate>Mon, 29 Jan 2007 17:45:22 +0000</pubDate><guid>https://major.io/2007/01/29/make-apache-logs-mimic-iis/</guid><description>To make Apache write logs similar to IIS, toss this into your Apache configuration:</description><content type="html"><![CDATA[<p>To make Apache write logs similar to IIS, toss this into your Apache configuration:</p>
<pre><code class="language-LogFormat" data-lang="LogFormat"></code></pre>]]></content></item><item><title>Moving mail between some Plesk servers</title><link>https://major.io/2007/01/27/moving-mail-between-some-plesk-servers/</link><pubDate>Sat, 27 Jan 2007 18:29:24 +0000</pubDate><guid>https://major.io/2007/01/27/moving-mail-between-some-plesk-servers/</guid><description>If you&amp;rsquo;re migrating a domain, sometimes their mail will go to the old server for a while after you&amp;rsquo;ve changed the DNS. You can move their mail to the new server by following these steps:
Go to the user&amp;rsquo;s Maildir directory cd /var/qmail/mailnames/&amp;lt;domain&amp;gt;/&amp;lt;user&amp;gt;/Maildir
Tar their mail directories tar cvzf &amp;lt;user&amp;gt;.tar.gz cur new tmp
Move to a web accessible location mv &amp;lt;user&amp;gt;.tar.gz /home/httpd/vhosts/&amp;lt;web-accessible-domain&amp;gt;/httpdocs/
Log onto the second server and go to the user&amp;rsquo;s Maildir directory cd /var/qmail/mailnames/&amp;lt;domain&amp;gt;/&amp;lt;user&amp;gt;/Maildir</description><content type="html"><![CDATA[<p>If you&rsquo;re migrating a domain, sometimes their mail will go to the old server for a while after you&rsquo;ve changed the DNS. You can move their mail to the new server by following these steps:</p>
<ol>
<li>Go to the user&rsquo;s Maildir directory</li>
</ol>
<p><code>cd /var/qmail/mailnames/&lt;domain&gt;/&lt;user&gt;/Maildir</code></p>
<ol start="2">
<li>Tar their mail directories</li>
</ol>
<p><code>tar cvzf &lt;user&gt;.tar.gz cur new tmp</code></p>
<ol start="3">
<li>Move to a web accessible location</li>
</ol>
<p><code>mv &lt;user&gt;.tar.gz /home/httpd/vhosts/&lt;web-accessible-domain&gt;/httpdocs/</code></p>
<ol start="4">
<li>Log onto the second server and go to the user&rsquo;s Maildir directory</li>
</ol>
<p><code>cd /var/qmail/mailnames/&lt;domain&gt;/&lt;user&gt;/Maildir</code></p>
<ol start="5">
<li>Retrieve the user&rsquo;s mail tar file that you created</li>
</ol>
<p><code>wget http://&lt;web-accessible-domain&gt;/&lt;user&gt;.tar.gz</code></p>
<ol start="6">
<li>Un-tar the files to their correct locations</li>
</ol>
<p><code>tar xvzf &lt;user&gt;.tar.gz</code></p>
<ol start="7">
<li>Remove the tar file</li>
</ol>
<p><code>rm &lt;user&gt;.tar.gz</code></p>
<ol start="8">
<li>Go to the original server and remove the tar file</li>
</ol>
<p><code>rm /home/httpd/vhosts/&lt;web-accessible-domain&gt;/httpdocs/&lt;user&gt;.tar.gz</code></p>
]]></content></item><item><title>Enabling CGI in Apache virtual hosts</title><link>https://major.io/2007/01/26/enabling-cgi-in-apache-virtual-hosts/</link><pubDate>Fri, 26 Jan 2007 17:12:34 +0000</pubDate><guid>https://major.io/2007/01/26/enabling-cgi-in-apache-virtual-hosts/</guid><description>Add this to the Apache configuration:
ScriptAlias /cgi-bin/ &amp;#34;/var/www/html/cgi-bin/&amp;#34; &amp;lt;Directory &amp;#34;/var/www/html/cgi-bin&amp;#34;&amp;gt; Options +ExecCGI AddHandler cgi-script .cgi &amp;lt;/Directory&amp;gt; Reload Apache and throw this in as test.cgi into your cgi-bin directory:
#!/usr/bin/perl print &amp;#34;Content-type: text/html\n\n&amp;#34;; print &amp;#34;Hello, World.&amp;#34;; Do not omit the content-type on your perl scripts. If you do, Apache will throw a random 500 Internal Server Error and it won&amp;rsquo;t log anything about it.</description><content type="html"><![CDATA[<p>Add this to the Apache configuration:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-apache" data-lang="apache"><span style="color:#b58900">ScriptAlias</span> <span style="color:#2aa198">/cgi-bin/</span> <span style="color:#2aa198">&#34;/var/www/html/cgi-bin/&#34;</span>
<span style="color:#268bd2">&lt;Directory</span> <span style="color:#2aa198">&#34;/var/www/html/cgi-bin&#34;</span><span style="color:#268bd2">&gt;</span>
        <span style="color:#b58900">Options</span> +ExecCGI
        <span style="color:#b58900">AddHandler</span> cgi-script .cgi
<span style="color:#268bd2">&lt;/Directory&gt;</span>
</code></pre></div><p>Reload Apache and throw this in as test.cgi into your cgi-bin directory:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-perl" data-lang="perl"><span style="color:#586e75">#!/usr/bin/perl</span>
<span style="color:#719e07">print</span> <span style="color:#2aa198">&#34;Content-type: text/html\n\n&#34;</span>;
<span style="color:#719e07">print</span> <span style="color:#2aa198">&#34;Hello, World.&#34;</span>;
</code></pre></div><p><strong>Do not omit</strong> the content-type on your perl scripts. If you do, Apache will throw a random 500 Internal Server Error and it won&rsquo;t log anything about it.</p>
]]></content></item><item><title>Finding usernames and passwords in Plesk DB</title><link>https://major.io/2007/01/26/finding-usernames-and-passwords-in-plesk-db/</link><pubDate>Fri, 26 Jan 2007 15:12:22 +0000</pubDate><guid>https://major.io/2007/01/26/finding-usernames-and-passwords-in-plesk-db/</guid><description>Need a username and password from the Plesk DB? Use this one-liner:
select REPLACE(sys_users.home,&amp;#39;/home/httpd/vhosts/&amp;#39;,&amp;#39;&amp;#39;) AS domain,sys_users.login,accounts.password from sys_users LEFT JOIN accounts on sys_users.account_id=accounts.id;</description><content type="html"><![CDATA[<p>Need a username and password from the Plesk DB? Use this one-liner:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sql" data-lang="sql"><span style="color:#719e07">select</span> <span style="color:#719e07">REPLACE</span>(sys_users.home,<span style="color:#2aa198">&#39;/home/httpd/vhosts/&#39;</span>,<span style="color:#2aa198">&#39;&#39;</span>) <span style="color:#719e07">AS</span> <span style="color:#719e07">domain</span>,sys_users.login,accounts.password <span style="color:#719e07">from</span> sys_users <span style="color:#719e07">LEFT</span> <span style="color:#719e07">JOIN</span> accounts <span style="color:#719e07">on</span> sys_users.account_id<span style="color:#719e07">=</span>accounts.id;
</code></pre></div>]]></content></item><item><title>Increase MySQL connection limit</title><link>https://major.io/2007/01/24/increase-mysql-connection-limit/</link><pubDate>Wed, 24 Jan 2007 17:21:37 +0000</pubDate><guid>https://major.io/2007/01/24/increase-mysql-connection-limit/</guid><description>MySQL&amp;rsquo;s default configuration sets the maximum simultaneous connections to 100. If you need to increase it, you can do it fairly easily:
For MySQL 3.x:
# vi /etc/my.cnf set-variable = max_connections = 250 For MySQL 4.x and 5.x:
# vi /etc/my.cnf max_connections = 250 Restart MySQL once you&amp;rsquo;ve made the changes and verify with:
echo &amp;quot;show variables like 'max_connections';&amp;quot; | mysql WHOA THERE: Before increasing MySQL&amp;rsquo;s connection limit, you really owe it to yourself (and your server), to find out why you&amp;rsquo;re reaching the maximum number of connections.</description><content type="html"><![CDATA[<p>MySQL&rsquo;s default configuration sets the maximum simultaneous connections to 100. If you need to increase it, you can do it fairly easily:</p>
<p>For MySQL 3.x:</p>
<pre><code># vi /etc/my.cnf
set-variable = max_connections = 250
</code></pre><p>For MySQL 4.x and 5.x:</p>
<pre><code># vi /etc/my.cnf
max_connections = 250
</code></pre><p>Restart MySQL once you&rsquo;ve made the changes and verify with:</p>
<pre><code>echo &quot;show variables like 'max_connections';&quot; | mysql
</code></pre><p><strong>WHOA THERE:</strong> Before increasing MySQL&rsquo;s connection limit, you really owe it to yourself (and your server), to <a href="http://rackerhacker.com/2008/06/24/mysql-error-1040-too-many-connections/">find out why you&rsquo;re reaching the maximum number of connections</a>. Over 90% of the MySQL servers that are hitting the maximum connection limit have a performance limiting issue that needs to be corrected instead.</p>
]]></content></item><item><title>Verify that SSLv2 is disabled</title><link>https://major.io/2007/01/24/verify-that-sslv2-is-disabled/</link><pubDate>Wed, 24 Jan 2007 15:57:51 +0000</pubDate><guid>https://major.io/2007/01/24/verify-that-sslv2-is-disabled/</guid><description>If you&amp;rsquo;re looking to get PCI/CISP compliance, or you just like better security, disable SSL version 2. Here&amp;rsquo;s how to check if it&amp;rsquo;s enabled on your server:
Testing a web server:
openssl s_client -connect hostname:443 -ssl2 Testing an SMTP server:
openssl s_client -connect hostname:25 -starttls smtp -ssl2 If you get lines like these, SSLv2 is disabled:
419:error:1407F0E5:SSL routines:SSL2\_WRITE:ssl handshake failure:s2\_pkt.c:428: 420:error:1406D0B8:SSL routines:GET\_SERVER\_HELLO:no cipher list:s2_clnt.c:450: If it shows the actual certificate installed, SSLv2 is enabled!</description><content type="html"><![CDATA[<p>If you&rsquo;re looking to get PCI/CISP compliance, or you just like better security, disable SSL version 2. Here&rsquo;s how to check if it&rsquo;s enabled on your server:</p>
<p>Testing a web server:</p>
<pre><code>openssl s_client -connect hostname:443 -ssl2
</code></pre><p>Testing an SMTP server:</p>
<pre><code>openssl s_client -connect hostname:25 -starttls smtp -ssl2
</code></pre><p>If you get lines like these, SSLv2 is disabled:</p>
<pre><code>419:error:1407F0E5:SSL routines:SSL2\_WRITE:ssl handshake failure:s2\_pkt.c:428:
420:error:1406D0B8:SSL routines:GET\_SERVER\_HELLO:no cipher list:s2_clnt.c:450:
</code></pre><p>If it shows the actual certificate installed, SSLv2 is enabled!</p>
]]></content></item><item><title>Plesk admin user can’t login</title><link>https://major.io/2007/01/24/plesk-admin-user-cant-login/</link><pubDate>Wed, 24 Jan 2007 15:35:37 +0000</pubDate><guid>https://major.io/2007/01/24/plesk-admin-user-cant-login/</guid><description>Okay, so you&amp;rsquo;ve verified that the correct admin password is being used, but you still can&amp;rsquo;t login? Most likely the account has been locked out. You can reset the account by running the following SQL statement:
echo &amp;quot;use psa; truncate lockout;&amp;quot; | mysql -u root -p\`cat /etc/psa/.psa.shadow\`</description><content type="html"><![CDATA[<p>Okay, so you&rsquo;ve verified that the correct admin password is being used, but you still can&rsquo;t login? Most likely the account has been locked out. You can reset the account by running the following SQL statement:</p>
<pre><code>echo &quot;use psa; truncate lockout;&quot; | mysql -u root -p\`cat /etc/psa/.psa.shadow\`
</code></pre>]]></content></item><item><title>Can’t upload large files in PHP</title><link>https://major.io/2007/01/24/cant-upload-large-files-in-php/</link><pubDate>Wed, 24 Jan 2007 15:35:29 +0000</pubDate><guid>https://major.io/2007/01/24/cant-upload-large-files-in-php/</guid><description>First, check max_upload_size in php.ini, but if that doesn&amp;rsquo;t work, look for LimitRequestBody in /etc/httpd/conf.d/php.conf and comment it out. Restart apache and you&amp;rsquo;re all set.</description><content type="html"><![CDATA[<p>First, check <code>max_upload_size</code> in php.ini, but if that doesn&rsquo;t work, look for <code>LimitRequestBody</code> in /etc/httpd/conf.d/php.conf and comment it out. Restart apache and you&rsquo;re all set.</p>
]]></content></item><item><title>Argument list too long</title><link>https://major.io/2007/01/24/argument-list-too-long/</link><pubDate>Wed, 24 Jan 2007 15:35:24 +0000</pubDate><guid>https://major.io/2007/01/24/argument-list-too-long/</guid><description>If you have a ton of files in a directory and you need to remove them, but rm says that the &amp;ldquo;argument list [is] too long&amp;rdquo;, just use find and xargs:
find . -name 'filename*' | xargs rm -vf</description><content type="html"><![CDATA[<p>If you have a ton of files in a directory and you need to remove them, but rm says that the &ldquo;argument list [is] too long&rdquo;, just use find and xargs:</p>
<pre><code>find . -name 'filename*' | xargs rm -vf
</code></pre>]]></content></item><item><title>Strip off www from URLs with mod_rewrite</title><link>https://major.io/2007/01/15/strip-off-www-from-urls-with-mod_rewrite/</link><pubDate>Mon, 15 Jan 2007 19:45:55 +0000</pubDate><guid>https://major.io/2007/01/15/strip-off-www-from-urls-with-mod_rewrite/</guid><description>If you need to remove subdomains from the URL that users enter to visit your website, toss this into your VirtualHost directive:
RewriteEngine On RewriteCond %{HTTP_HOST} ^www.domain.com$ [NC] RewriteRule ^(.*)$ http://domain.com/$1 [R=301,L] Of course, you can tack on a subdomain too, if that&amp;rsquo;s what you need:
RewriteEngine On RewriteCond %{HTTP_HOST} ^domain.com$ [NC] RewriteRule ^(.*)$ http://www.domain.com/$1 [R=301,L]</description><content type="html"><![CDATA[<p>If you need to remove subdomains from the URL that users enter to visit your website, toss this into your VirtualHost directive:</p>
<pre><code>RewriteEngine On
RewriteCond %{HTTP_HOST} ^www.domain.com$ [NC]
RewriteRule ^(.*)$ http://domain.com/$1 [R=301,L]
</code></pre><p>Of course, you can tack on a subdomain too, if that&rsquo;s what you need:</p>
<pre><code>RewriteEngine On
RewriteCond %{HTTP_HOST} ^domain.com$ [NC]
RewriteRule ^(.*)$ http://www.domain.com/$1 [R=301,L]
</code></pre>]]></content></item><item><title>Sum Apache Bandwidth From Logs</title><link>https://major.io/2007/01/15/sum-apache-bandwidth-from-logs/</link><pubDate>Mon, 15 Jan 2007 15:33:09 +0000</pubDate><guid>https://major.io/2007/01/15/sum-apache-bandwidth-from-logs/</guid><description>If you&amp;rsquo;re not a fan of scientific notation, use this to calculate the apache bandwidth used from log files in MB:
cat /var/log/httpd/access_log | awk '{ SUM += $5} END { print SUM/1024/1024 }'</description><content type="html"><![CDATA[<p>If you&rsquo;re not a fan of scientific notation, use this to calculate the apache bandwidth used from log files in MB:</p>
<pre><code>cat /var/log/httpd/access_log | awk '{ SUM += $5} END { print SUM/1024/1024 }'
</code></pre>]]></content></item><item><title>Repairing the qmail queue</title><link>https://major.io/2007/01/11/rebuilding-the-qmail-queue/</link><pubDate>Thu, 11 Jan 2007 22:37:07 +0000</pubDate><guid>https://major.io/2007/01/11/rebuilding-the-qmail-queue/</guid><description>There&amp;rsquo;s three main things to remember when it comes to the qmail queue:
1. Don&amp;rsquo;t mess with the qmail queue while qmail is running.
2. Don&amp;rsquo;t mess with the qmail queue while qmail is stopped.
3. Don&amp;rsquo;t mess with the qmail queue ever.
The qmail application keeps a database (sort of) of the pieces of mail it expects to be in the queue (and on the filesystem).</description><content type="html"><![CDATA[<p>There&rsquo;s three main things to remember when it comes to the qmail queue:</p>
<blockquote>
<p><strong>1.</strong> Don&rsquo;t mess with the qmail queue while qmail is running.</p>
</blockquote>
<blockquote>
<p><strong>2.</strong> Don&rsquo;t mess with the qmail queue while qmail is stopped.</p>
</blockquote>
<blockquote>
<p><strong>3.</strong> Don&rsquo;t mess with the qmail queue <strong>ever</strong>.</p>
</blockquote>
<p>The qmail application keeps a database (sort of) of the pieces of mail it expects to be in the queue (and on the filesystem). Many python scripts (like mailRemove.py) claim they will speed up your qmail queue by removing failure notices and tidying up the queue files. Most of the time, these scripts work just fine, but sometimes they remove something they shouldn&rsquo;t and then qmail can&rsquo;t find the file.</p>
<p>What does qmail do when it can&rsquo;t find the file that corresponds to an item in the queue? It stops delivering mail, eats the CPU, and cranks the load average up. Impressive, isn&rsquo;t it?</p>
<p>Should you find yourself with an impressively hosed qmail queue, do the following (and say goodbye to every e-mail in your queue):</p>
<pre><code>/etc/init.d/qmail stop
cd /var/qmail/queue
rm -rf info intd local mess remote todo
mkdir mess
for i in `seq 0 22`; do
mkdir mess/$i
done
cp -r mess info
cp -r mess intd
cp -r mess local
cp -r mess remote
cp -r mess todo
chmod -R 750 mess todo
chown -R qmailq:qmail mess todo
chmod -R 700 info intd local remote
chown -R qmailq:qmail intd
chown -R qmails:qmail info local remote
/etc/init.d/qmail start
</code></pre><p><strong>Just in case you missed it,</strong> this will delete <strong>all mail messages</strong> that exist in your queue. But, then again, you&rsquo;re not going to get those messages anyways (thanks qmail!), so repairing the queue is your only option.</p>
]]></content></item><item><title>Securing MySQL</title><link>https://major.io/2007/01/04/securing-mysql/</link><pubDate>Fri, 05 Jan 2007 01:46:19 +0000</pubDate><guid>https://major.io/2007/01/04/securing-mysql/</guid><description>If you work on enough servers, you discover that a lot of people put the security of their MySQL server on the back burner. With the importance of databases for dynamic sites, MySQL&amp;rsquo;s security is arguably more important than anything else on the server. If someone were able to shut off the server, or worse, steal sensitive data, the entire server - and possibly the owner - could be in jeopardy.</description><content type="html"><![CDATA[<p>If you work on enough servers, you discover that a lot of people put the security of their MySQL server on the back burner. With the importance of databases for dynamic sites, MySQL&rsquo;s security is arguably more important than anything else on the server. If someone were able to shut off the server, or worse, steal sensitive data, the entire server - and possibly the owner - could be in jeopardy.</p>
<p>Here are some basic tips to secure a MySQL server on any distribution:</p>
<p><strong>Create a strong root password</strong></p>
<p>By default on almost all distributions, MySQL comes with an empty root password. Sometimes the root logins are restricted to the localhost only, which will help somewhat, but anyone with shell access or a knack for writing PHP scripts can do anything to the MySQL server. However you set the root password, set it and make it strong.</p>
<p><strong>Cut off network access</strong></p>
<p>As with any daemon, the more exposure it has to the internet, the higher the chance of it being hacked and brute forced. If your users need network access to MySQL, then restrict it by at least altering the MySQL permissions to their IP only. The better solution would be to restrict it via a firewall and permissions. If you users don&rsquo;t need any network access to MySQL, add the following to your my.cnf:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ini" data-lang="ini">listen <span style="color:#719e07">=</span> <span style="color:#2aa198">127.0.0.1</span>
</code></pre></div><p>Restart MySQL and it shouldn&rsquo;t be listening on any network addresses except the localhost. This won&rsquo;t affect any PHP scripts on your server.</p>
<p><strong>Force the use of named pipes</strong></p>
<p>Removing MySQL&rsquo;s ability to even bind to the network is a great security measure. All access to MySQL will be done through a filesystem socket, which is /var/lib/mysql/mysql.sock on most systems. This will require your PHP scripts to refer to your host as &ldquo;localhost&rdquo; and not &ldquo;127.0.0.1&rdquo;.</p>
<p><strong>Review your user list often</strong></p>
<p>Every once in a while, check the list of users authorized to log into your MySQL server and be sure that when the list changes, the changes are valid. Be careful when allowing GRANT access to certain users.</p>
<p><strong>Backup often</strong></p>
<p>How often should you backup your MySQL databases? Well, ask yourself how important your data is to you. If your MySQL server is generally busy all of the time, you may want to run a slave server and do backups from that server to reduce the amount of table-locking that mysqldump requires. If your MySQL server is not terribly busy, then you can run mysqldumps pretty often on the server.</p>
]]></content></item><item><title>MySQL Row &amp; Data Limits</title><link>https://major.io/2007/01/04/mysql-row-data-limits/</link><pubDate>Fri, 05 Jan 2007 01:24:17 +0000</pubDate><guid>https://major.io/2007/01/04/mysql-row-data-limits/</guid><description>As most folks know, by default, MySQL limits the size of a MyISAM table at 4GB. Where does this limit come from? It&amp;rsquo;s the maximum of a 32-bit address:
232= 4,294,967,296 bytes = 4GB
How is this 4GB allocated? Well here&amp;rsquo;s the math:
row count X row length = 4GB max
Basically, if your rows don&amp;rsquo;t contain much information, you can cram a lot of rows into a table.</description><content type="html"><![CDATA[<p>As most folks know, by default, MySQL limits the size of a MyISAM table at 4GB. Where does this limit come from? It&rsquo;s the maximum of a 32-bit address:</p>
<blockquote>
<p>2<!-- raw HTML omitted -->32<!-- raw HTML omitted --> = 4,294,967,296 bytes = 4GB</p>
</blockquote>
<p>How is this 4GB allocated? Well here&rsquo;s the math:</p>
<blockquote>
<p>row count X row length = 4GB max</p>
</blockquote>
<p>Basically, if your rows don&rsquo;t contain much information, you can cram a lot of rows into a table. On the flip side, if you don&rsquo;t plan on having too many rows, you can cram a lot of information in each row.</p>
<p>Here&rsquo;s where things get ugly. If you have a MyISAM table and you exceed the maximum data length for the table, it may or may not tell you that you&rsquo;ve exceeded the limit (depending on the version). If it doesn&rsquo;t tell you, your data will actually become corrupt.</p>
<p>So, how can you find out what a table&rsquo;s limit is? Run <code>show table status like 'tablename'</code> and check the value for <code>Max_data_length</code>. The default, of course, is <code>4294967295</code>.</p>
<p>How can the <code>Max_data_length</code> be increased? Just run something like <code>alter table tablename max_rows = 200000000000 avg_row_length = 50</code>. This example would increase your <code>Max_data_length</code> to 1,099,511,627,775.</p>
]]></content></item><item><title>About Sticky Bits</title><link>https://major.io/2006/12/30/about-sticky-bits/</link><pubDate>Sun, 31 Dec 2006 03:35:26 +0000</pubDate><guid>https://major.io/2006/12/30/about-sticky-bits/</guid><description>Sticky bits help you take file permissions to the next level. Here&amp;rsquo;s an example of a situation where sticky bits help:
Let&amp;rsquo;s say you have a directory on a server called &amp;ldquo;share&amp;rdquo;. For this directory, you have 3 users: adam, bill, and carl. You are the administrator, so you want to create a directory where all three users can manage files in the share directory. That&amp;rsquo;s easily done: put all three users in the same group, set the permissions as 664, set the owner of the directory as the group that all three users are in, and you&amp;rsquo;re done.</description><content type="html"><![CDATA[<p>Sticky bits help you take file permissions to the next level. Here&rsquo;s an example of a situation where sticky bits help:</p>
<p>Let&rsquo;s say you have a directory on a server called &ldquo;share&rdquo;. For this directory, you have 3 users: adam, bill, and carl. You are the administrator, so you want to create a directory where all three users can manage files in the share directory. That&rsquo;s easily done: put all three users in the same group, set the permissions as 664, set the owner of the directory as the group that all three users are in, and you&rsquo;re done.</p>
<p>Hold on - adam is going to be upset if bill or carl changes or removes adam&rsquo;s files. How can you let all three users manage files in the same directory but not let them alter each other&rsquo;s files? Sticky bits!</p>
<p>After a <code>chmod 664</code>, and a <code>chown user:group</code> to fix the group, the directory looks like this:</p>
<pre><code>-rw-rw-r--   1 admin sharegroup    18367 Dec 30 22:05 shared
</code></pre><p>Now, run a <code>chmod 1664</code> on the directory:</p>
<pre><code>-rw-rw-r-t   1 admin sharegroup    18367 Dec 30 22:05 shared
</code></pre><p>What&rsquo;s the <code>t</code> all about? That&rsquo;s your sticky bit! Whenever adam creates a file, bill and carl can&rsquo;t delete it, modify it, or rename it. They can read it all they want, but adam is the only one who can make the modifications because write priviliges are &ldquo;stuck&rdquo; to his user (even though the folder is writable to the group).</p>
<p>Okay, so why do you need sticky bits? This all sounds like fun and games for shared folders, but how can you use this in the real world? Well, think about your <code>/tmp</code> directory. Users write to the directory all the time whether they know it or not, but what if one user trashed another users temporary files? Or what if a user hosed out the whole directory? That&rsquo;s where sticky bits can save the day. Always <code>chmod 1777</code> your <code>/tmp</code> directory for good security on a shared temporary directory.</p>
]]></content></item><item><title>Can’t Kill Sendmail Processes</title><link>https://major.io/2006/12/28/cant-kill-sendmail-processes/</link><pubDate>Fri, 29 Dec 2006 00:35:18 +0000</pubDate><guid>https://major.io/2006/12/28/cant-kill-sendmail-processes/</guid><description>If you find yourself in the sticky situation where kill -9 still won&amp;rsquo;t kill a sendmail process, check the process list. If ps fax returns a &amp;ldquo;D&amp;rdquo; status code, you won&amp;rsquo;t be able to stop the process. It&amp;rsquo;s in an &amp;ldquo;uninterruptable sleep&amp;rdquo; state which cannot be killed.
What can you do to fix this? Check for file locking. Are files in the mail queue directory locked? Are the files in the mail queue mounted over NFS (by an idiotic administrator)?</description><content type="html"><![CDATA[<p>If you find yourself in the sticky situation where <code>kill -9</code> still won&rsquo;t kill a sendmail process, check the process list. If <code>ps fax</code> returns a &ldquo;D&rdquo; status code, you won&rsquo;t be able to stop the process. It&rsquo;s in an &ldquo;uninterruptable sleep&rdquo; state which cannot be killed.</p>
<p>What can you do to fix this? Check for file locking. Are files in the mail queue directory locked? Are the files in the mail queue mounted over NFS (by an idiotic administrator)? If so, the only fix is to set sendmail to not start on reboot, then reboot the box.</p>
]]></content></item><item><title>Fedora/RHEL/Centos Won’t Init</title><link>https://major.io/2006/12/28/fedorarhelcentos-wont-init/</link><pubDate>Fri, 29 Dec 2006 00:35:13 +0000</pubDate><guid>https://major.io/2006/12/28/fedorarhelcentos-wont-init/</guid><description>In the event that a Fedora/RHEL/CentOS box won&amp;rsquo;t perform the init (which comes right after the initial kernel load), don&amp;rsquo;t fret - it can be fixed. Make a note of the missing libraries or executables. Simply boot onto a livecd or rescuecd and chroot into your main installation. Once you&amp;rsquo;re chrooted, just forcefully install any RPM&amp;rsquo;s which might contain files that are missing when the init is loaded.
Forcing the installation of an RPM:</description><content type="html"><![CDATA[<p>In the event that a Fedora/RHEL/CentOS box won&rsquo;t perform the init (which comes right after the initial kernel load), don&rsquo;t fret - it can be fixed. Make a note of the missing libraries or executables. Simply boot onto a livecd or rescuecd and chroot into your main installation. Once you&rsquo;re chrooted, just forcefully install any RPM&rsquo;s which might contain files that are missing when the init is loaded.</p>
<p>Forcing the installation of an RPM:</p>
<blockquote>
<p>rpm -ivh -force filename.rpm<!-- raw HTML omitted --></p>
<p>Listing the files that an installed RPM contains:</p>
<blockquote>
<p>rpm -ql rpmname</p>
</blockquote>
<p>Listing the files that an RPM file contains:</p>
<blockquote>
<p>rpm -qpl filename.rpm</p>
</blockquote>
<p>Finding the RPM that contains a certain file/executable:</p>
<blockquote>
<p>rpm -qf /usr/bin/filename</p>
</blockquote>
<p>Figuring out what might be wrong with files already installed from an RPM:</p>
<blockquote>
<p>rpm -V rpmname</p>
</blockquote>
</blockquote>
]]></content></item><item><title>PHPLive Has No session.save_path</title><link>https://major.io/2006/12/27/phplive-has-no-sessionsave_path/</link><pubDate>Wed, 27 Dec 2006 17:29:52 +0000</pubDate><guid>https://major.io/2006/12/27/phplive-has-no-sessionsave_path/</guid><description>Add this to the virtual host configuration if PHPLive says it has no session.save_path:
php_admin_flag safe_mode off php_admin_flag register_globals off PHPLive cannot operate with safe_mode enabled.</description><content type="html"><![CDATA[<p>Add this to the virtual host configuration if PHPLive says it has no session.save_path:</p>
<pre><code>php_admin_flag safe_mode off
php_admin_flag register_globals off
</code></pre><p>PHPLive <strong>cannot</strong> operate with safe_mode enabled.</p>
]]></content></item><item><title>Raising MaxClients? Change ServerLimit.</title><link>https://major.io/2006/12/27/raising-maxclients-change-serverlimit/</link><pubDate>Wed, 27 Dec 2006 14:36:40 +0000</pubDate><guid>https://major.io/2006/12/27/raising-maxclients-change-serverlimit/</guid><description>Remember, if you raise MaxClients for an MPM in Apache, you must raise the ServerLimit directive, which is normally set to 256 on most servers.Â The ServerLimit maximum is always obeyed, no matter what MaxClients says. For example, if MaxClients is set to 500 and ServerLimit is 256 (or it is unspecified), then Apache can only serve 256 clients at a time.
Important items to remember:
Only add ServerLimit in the actual MPM configuration section itself.</description><content type="html"><![CDATA[<p>Remember, if you raise MaxClients for an MPM in Apache, you must raise the ServerLimit directive, which is normally set to 256 on most servers.Â  The ServerLimit maximum is always obeyed, no matter what MaxClients says. For example, if MaxClients is set to 500 and ServerLimit is 256 (or it is unspecified), then Apache can only serve 256 clients at a time.</p>
<p>Important items to remember:</p>
<ul>
<li>Only add ServerLimit in the actual MPM configuration section itself.</li>
<li>Increase the MaxClients/ServerLimit in a sane manner - make small increments and test.</li>
<li>Keep in mind that 500 concurrent requests can use 75% or more of modern CPU&rsquo;s and upwards of 1.5GB of RAM, depending on the content.</li>
</ul>
]]></content></item><item><title>Rootkit Checks on RHEL</title><link>https://major.io/2006/12/26/rootkit-checks-on-rhel/</link><pubDate>Wed, 27 Dec 2006 04:01:45 +0000</pubDate><guid>https://major.io/2006/12/26/rootkit-checks-on-rhel/</guid><description>If you think you have a rooted RHEL box, you&amp;rsquo;ll want to run the usual rkhunter, chkrootkit, and you will want to inspect for rogue processes. However, if the rootkit hasn&amp;rsquo;t exposed its malfeasance yet, how do you know it&amp;rsquo;s there?
rpm -Va RPM&amp;rsquo;s verify functionality can tell you what&amp;rsquo;s happened to files installed by an RPM since they were installed. Changes in permissions, file sizes, locations, and ownership can all be detected.</description><content type="html"><![CDATA[<p>If you think you have a rooted RHEL box, you&rsquo;ll want to run the usual rkhunter, chkrootkit, and you will want to inspect for rogue processes. However, if the rootkit hasn&rsquo;t exposed its malfeasance yet, how do you know it&rsquo;s there?</p>
<pre><code>rpm -Va
</code></pre><p>RPM&rsquo;s verify functionality can tell you what&rsquo;s happened to files installed by an RPM since they were installed. Changes in permissions, file sizes, locations, and ownership can all be detected. Here&rsquo;s some example output:</p>
<pre><code>.M.......   /etc/cups
S.5....TC c /etc/cups/cupsd.conf
.......TC c /etc/cups/printers.conf
.M.......   /var/spool/cups/tmp
S.5....T. c /etc/sysconfig/system-config-securitylevel
S.5....T. c /etc/xml/catalog
S.5....T. c /usr/share/sgml/docbook/xmlcatalog
........C   /var/lib/scrollkeeper
S.?......   /usr/lib/libcurl.so.3.0.0
</code></pre><p>So what do the letters mean?</p>
<pre><code>S   file Size differs
M   Mode differs (includes permissions and file type)
5   MD5 sum differs
D   Device major/minor number mismatch
L   readLink(2) path mismatch
U   User ownership differs
G   Group ownership differs
T   mTime differs
c   %config configuration file.
d   %doc documentation file.
g   %ghost file (i.e. the file contents are not included in the package payload).
l   %license license file.
r   %readme readme file.
</code></pre><p>Lots of MD5&rsquo;s and ownerships will change from time to time, but watch out for any action in important executables, such as /bin/ls or /bin/passwd - if these have changed, you may be rooted.</p>
]]></content></item><item><title>Group Editing With FTP</title><link>https://major.io/2006/12/26/group-editing-with-vsftpd/</link><pubDate>Wed, 27 Dec 2006 03:44:55 +0000</pubDate><guid>https://major.io/2006/12/26/group-editing-with-vsftpd/</guid><description>So you have multiple users that need to read and write to certain files on the filesystem? This can be done with vsftpd or proftpd quite easily. Let&amp;rsquo;s say you have users called ann, bill and carl and they need to manage files in /var/www/html. Here&amp;rsquo;s the steps:
For vsftpd, change the umask for files created by FTP users. Open the vsftpd.conf file and edit the following:
local_umask = 077 &amp;lt;-- old local_umask = 022 &amp;lt;-- new For proftpd, change the umask for files created by FTP users.</description><content type="html"><![CDATA[<p>So you have multiple users that need to read and write to certain files on the filesystem? This can be done with vsftpd or proftpd quite easily. Let&rsquo;s say you have users called ann, bill and carl and they need to manage files in <strong>/var/www/html</strong>. Here&rsquo;s the steps:</p>
<p><strong>For vsftpd,</strong> change the umask for files created by FTP users. Open the vsftpd.conf file and edit the following:</p>
<pre><code>local_umask = 077     &lt;-- old
local_umask = 022     &lt;-- new
</code></pre><p><strong>For proftpd,</strong> change the umask for files created by FTP users. Open the proftpd.conf file and edit the following:</p>
<pre><code>Umask 022
</code></pre><p>This makes sure that new files are chmodded as 775 (full read/write for users/group, but only read for everyone else).</p>
<p>Next, create a new group. We will call ours &ldquo;sharedweb&rdquo;:</p>
<pre><code>groupadd sharedweb
</code></pre><p>Now, put the users into that group by adding them in /etc/group:</p>
<pre><code>sharedweb:*:##:ann,bill,carl
</code></pre><p>Modify the users so that their primary group is sharedweb. If you forget this step, when they make new FTP files, they will be owned by each user&rsquo;s primary group (sometimes named the same as the user on some systems) and the permissions will be completeld hosed.</p>
<pre><code>usermod -g ann sharedweb
usermod -g bill sharedweb
usermod -g carl sharedweb
</code></pre><p>Restart vsftpd to pick up the new configuration and your users should be able upload, delete, and edit each other&rsquo;s files.</p>
]]></content></item><item><title>Fixing Invalid HELO’s</title><link>https://major.io/2006/12/26/fixing-invalid-helos/</link><pubDate>Wed, 27 Dec 2006 03:02:40 +0000</pubDate><guid>https://major.io/2006/12/26/fixing-invalid-helos/</guid><description>If your server is spewing an invalid HELO, you could be blacklisted pretty quickly. The Spamhaus SBL-XBL list and CBL list work together to find servers announcing themselves improperly.
The common reasons why mail servers are blocked for bad HELO&amp;rsquo;s are:
Server is announcing itself as “localhost”. Server is announcing itself as an IP address. Server is announcing itself as a hostname that does not exist. Are you unsure what your server&amp;rsquo;s announcing itself as?</description><content type="html"><![CDATA[<p>If your server is spewing an invalid HELO, you could be blacklisted pretty quickly. The <a href="http://www.spamhaus.org/">Spamhaus SBL-XBL</a> list and <a href="http://cbl.abuseat.org/">CBL</a> list work together to find servers announcing themselves improperly.</p>
<p>The common reasons why mail servers are blocked for bad HELO&rsquo;s are:</p>
<ul>
<li>Server is announcing itself as “localhost”.</li>
<li>Server is announcing itself as an IP address.</li>
<li>Server is announcing itself as a hostname that does not exist.</li>
</ul>
<p>Are you unsure what your server&rsquo;s announcing itself as? Try these:</p>
<ul>
<li>Send an e-mail to <a href="mailto:helocheck@cbl.abuseat.org">helocheck@cbl.abuseat.org</a>. You will get an immediate response with exactly what your HELO contains.</li>
<li>Telnet to port 25 on your mailserver. Run <code>telnet mail.yourdomain.com 25</code> and wait a few seconds. Your server&rsquo;s HELO message should appear.</li>
</ul>
<p>So your server is announcing itself as the wrong thing? Well, fix it!</p>
<p><strong>Managing HELO with QMail</strong></p>
<p>If <strong>/var/qmail/control/me</strong> exists, edit it so that it matches your reverse DNS record for your server&rsquo;s primary IP address. If the file doesn&rsquo;t exist, you can create the file and add the correct hostname to it, or adjust your hostname on your operating system. Try running <code>hostname mail.yourdomain.com</code> to fix things immediately, and edit the proper configuration files to correct your hostname at boot time.</p>
<p><strong>Managing HELO with Postfix</strong></p>
<p>The default value for Postfix&rsquo;s HELO is the value of <code>$myhostname</code>. If that variable is defined in the main.cf, adjust it so that it matches the reverse DNS record of your server. If it isn&rsquo;t defined in main.cf, then adjust the hostname on your operating system. Try running <code>hostname mail.yourdomain.com</code> to fix things immediately, and edit the proper configuration files to correct your hostname at boot time. Should neither of those methods suffice on your server, simply adjust the <code>smtp_helo_name</code> variable to match the reverse DNS record of your server. For example:</p>
<!-- raw HTML omitted -->
<p><strong>Managing HELO with Sendmail</strong></p>
<p>Adjust the hostname on your operating system. Try running <code>hostname mail.yourdomain.com</code> to fix things immediately, and edit the proper configuration files to correct your hostname at boot time.</p>
]]></content></item><item><title>Postfix – Forwarding Virtual Mailboxes</title><link>https://major.io/2006/12/26/postfix-virtual-mailboxes-forwarding-externally/</link><pubDate>Wed, 27 Dec 2006 02:47:46 +0000</pubDate><guid>https://major.io/2006/12/26/postfix-virtual-mailboxes-forwarding-externally/</guid><description>Setting up Postfix to handle mail for a virtual domain and forward it to external mailboxes is pretty easy. Here&amp;rsquo;s an example for a few domains:
/etc/postfix/main.cf
virtual_alias_domains = hash:/etc/postfix/mydomains virtual_alias_maps = hash:/etc/postfix/virtual /etc/postfix/mydomains
foo.com OK foo1.com OK foo2.com OK /etc/postfix/virtual
frank@foo.com frank@gmail.com jane@foo.com jane@earthlink.net jim@foo1.com jimmy@yahoo.com peter@foo2.com pete@hotmail.com Remember, each time you edit /etc/postfix/virtual, do the following:
postmap /etc/postfix/virtual /etc/postfix/mydomains postfix reload</description><content type="html"><![CDATA[<p>Setting up Postfix to handle mail for a virtual domain and forward it to external mailboxes is pretty easy. Here&rsquo;s an example for a few domains:</p>
<p><strong>/etc/postfix/main.cf</strong></p>
<pre><code>virtual_alias_domains = hash:/etc/postfix/mydomains
virtual_alias_maps = hash:/etc/postfix/virtual
</code></pre><p><strong>/etc/postfix/mydomains</strong></p>
<pre><code>foo.com          OK
foo1.com         OK
foo2.com         OK
</code></pre><p><strong>/etc/postfix/virtual</strong></p>
<pre><code>frank@foo.com           frank@gmail.com
jane@foo.com            jane@earthlink.net
jim@foo1.com            jimmy@yahoo.com
peter@foo2.com          pete@hotmail.com
</code></pre><p>Remember, each time you edit <strong>/etc/postfix/virtual</strong>, do the following:</p>
<pre><code>postmap /etc/postfix/virtual /etc/postfix/mydomains
postfix reload
</code></pre>]]></content></item></channel></rss>